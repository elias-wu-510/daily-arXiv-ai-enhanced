<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 73]
- [eess.IV](#eess.IV) [Total: 1]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [HyperCLOVA X 32B Think](https://arxiv.org/abs/2601.03286)
*NAVER Cloud HyperCLOVA X Team*

Main category: cs.CV

TL;DR: HyperCLOVA X 32B Think is a Korean-focused vision-language model with enhanced reasoning and agentic capabilities, achieving strong performance on Korean benchmarks and agent tasks.


<details>
  <summary>Details</summary>
Motivation: To develop a vision-language model specifically designed for the Korean linguistic and cultural context with emphasis on reasoning capabilities and agentic abilities, addressing the need for culturally-aware AI models in Korean applications.

Method: Two-stage approach: 1) Pre-training with strong focus on reasoning capabilities, 2) Post-training to support multimodal understanding, enhanced reasoning, agentic behaviors, and human preference alignment.

Result: The model achieves strong performance on Korean text-to-text and vision-to-text benchmarks, as well as on agent-oriented evaluation tasks compared to similarly sized models.

Conclusion: HyperCLOVA X 32B Think demonstrates effective Korean-focused multimodal reasoning and agentic capabilities, and its open-source release aims to support broader adoption and further research in academic and industrial communities.

Abstract: In this report, we present HyperCLOVA X 32B Think, a vision-language model designed with particular emphasis on reasoning within the Korean linguistic and cultural context, as well as agentic ability. HyperCLOVA X 32B Think is pre-trained with a strong focus on reasoning capabilities and subsequently post-trained to support multimodal understanding, enhanced reasoning, agentic behaviors, and alignment with human preferences. Experimental evaluations against comparably sized models demonstrate that our model achieves strong performance on Korean text-to-text and vision-to-text benchmarks, as well as on agent-oriented evaluation tasks. By open-sourcing HyperCLOVA X 32B Think, we aim to support broader adoption and facilitate further research and innovation across both academic and industrial communities.

</details>


### [2] [CageDroneRF: A Large-Scale RF Benchmark and Toolkit for Drone Perception](https://arxiv.org/abs/2601.03302)
*Mohammad Rostami,Atik Faysal,Hongtao Xia,Hadi Kasasbeh,Ziang Gao,Huaxia Wang*

Main category: cs.CV

TL;DR: CDRF is a large-scale RF drone detection benchmark combining real captures with synthetic augmentation to address dataset scarcity and diversity limitations.


<details>
  <summary>Details</summary>
Motivation: Existing RF datasets for drone detection suffer from scarcity and limited diversity, hindering development of robust RF perception models.

Method: Combines extensive real-world recordings with systematic synthetic augmentation pipeline that controls SNR, injects interference, applies frequency shifts with label-consistent bounding-box transformations.

Result: Created comprehensive benchmark spanning wide range of contemporary drone models and acquisition conditions, with interoperable open-source tools for data generation, preprocessing, augmentation, and evaluation.

Conclusion: CDRF enables standardized benchmarking for classification, open-set recognition, and object detection, accelerating progress toward robust, generalizable RF perception models.

Abstract: We present CageDroneRF (CDRF), a large-scale benchmark for Radio-Frequency (RF) drone detection and identification built from real-world captures and systematically generated synthetic variants. CDRF addresses the scarcity and limited diversity of existing RF datasets by coupling extensive raw recordings with a principled augmentation pipeline that (i) precisely controls Signal-to-Noise Ratio (SNR), (ii) injects interfering emitters, and (iii) applies frequency shifts with label-consistent bounding-box transformations for detection. This dataset spans a wide range of contemporary drone models, many unavailable in current public datasets, and acquisition conditions, derived from data collected at the Rowan University campus and within a controlled RF-cage facility. CDRF is released with interoperable open-source tools for data generation, preprocessing, augmentation, and evaluation that also operate on existing public benchmarks. CDRF enables standardized benchmarking for classification, open-set recognition, and object detection, supporting rigorous comparisons and reproducible pipelines. By releasing this comprehensive benchmark and tooling, CDRF aims to accelerate progress toward robust, generalizable RF perception models.

</details>


### [3] [Mass Concept Erasure in Diffusion Models with Concept Hierarchy](https://arxiv.org/abs/2601.03305)
*Jiahang Tu,Ye Li,Yiming Wu,Hanbin Zhao,Chao Zhang,Hui Qian*

Main category: cs.CV

TL;DR: Proposes SuPLoRA, a hierarchical concept erasure method for diffusion models that groups related concepts under supertypes and uses shared parameters for efficient multi-concept suppression while preserving generation quality.


<details>
  <summary>Details</summary>
Motivation: Existing concept erasure methods become inefficient and ineffective as the number of erased concepts grows, requiring separate fine-tuned parameters for each concept and degrading overall generation quality.

Method: Introduces supertype-subtype concept hierarchy where related concepts are grouped under parent nodes. Uses group-wise suppression with shared parameters and SuPLoRA (Supertype-Preserving Low-Rank Adaptation) that encodes supertype information in frozen down-projection matrix while updating only up-projection matrix during erasure.

Result: Theoretical analysis demonstrates effectiveness of SuPLoRA in mitigating generation degradation. Constructs challenging benchmark for simultaneous erasure across diverse domains (celebrities, objects, pornographic content).

Conclusion: Proposed hierarchical approach enables efficient and effective multi-concept erasure in diffusion models while preserving general generative capabilities and mitigating quality degradation through parameter sharing and supertype preservation.

Abstract: The success of diffusion models has raised concerns about the generation of unsafe or harmful content, prompting concept erasure approaches that fine-tune modules to suppress specific concepts while preserving general generative capabilities. However, as the number of erased concepts grows, these methods often become inefficient and ineffective, since each concept requires a separate set of fine-tuned parameters and may degrade the overall generation quality. In this work, we propose a supertype-subtype concept hierarchy that organizes erased concepts into a parent-child structure. Each erased concept is treated as a child node, and semantically related concepts (e.g., macaw, and bald eagle) are grouped under a shared parent node, referred to as a supertype concept (e.g., bird). Rather than erasing concepts individually, we introduce an effective and efficient group-wise suppression method, where semantically similar concepts are grouped and erased jointly by sharing a single set of learnable parameters. During the erasure phase, standard diffusion regularization is applied to preserve denoising process in unmasked regions. To mitigate the degradation of supertype generation caused by excessive erasure of semantically related subtypes, we propose a novel method called Supertype-Preserving Low-Rank Adaptation (SuPLoRA), which encodes the supertype concept information in the frozen down-projection matrix and updates only the up-projection matrix during erasure. Theoretical analysis demonstrates the effectiveness of SuPLoRA in mitigating generation performance degradation. We construct a more challenging benchmark that requires simultaneous erasure of concepts across diverse domains, including celebrities, objects, and pornographic content.

</details>


### [4] [VLM4VLA: Revisiting Vision-Language-Models in Vision-Language-Action Models](https://arxiv.org/abs/2601.03309)
*Jianke Zhang,Xiaoyu Chen,Qiuyue Wang,Mingsheng Li,Yanjiang Guo,Yucheng Hu,Jiajun Zhang,Shuai Bai,Junyang Lin,Jianyu Chen*

Main category: cs.CV

TL;DR: VLM capabilities don't directly predict VLA policy performance; visual module is the bottleneck, and control-relevant supervision in vision encoder helps even when frozen.


<details>
  <summary>Details</summary>
Motivation: To systematically study how VLM choice and competence translate to downstream VLA policy performance, challenging common assumptions about VLM capabilities for embodied control.

Method: Introduces VLM4VLA, a minimal adaptation pipeline that converts general-purpose VLMs into VLA policies using only a small set of new learnable parameters for fair comparison. Conducts extensive empirical studies across three benchmarks, investigates embodied capabilities via fine-tuning on seven auxiliary tasks, and performs modality-level ablations.

Result: VLM initialization helps but general VLM capabilities are poor predictors of downstream task performance. Improving specific embodied skills doesn't guarantee better control. Visual module in VLM is the primary bottleneck, and injecting control-relevant supervision into vision encoder yields consistent gains even when frozen.

Conclusion: Standard VLM competence is necessary but insufficient for effective embodied control, revealing a persistent domain gap between current VLM pretraining objectives and embodied action-planning requirements.

Abstract: Vision-Language-Action (VLA) models, which integrate pretrained large Vision-Language Models (VLM) into their policy backbone, are gaining significant attention for their promising generalization capabilities. This paper revisits a fundamental yet seldom systematically studied question: how VLM choice and competence translate to downstream VLA policies performance? We introduce VLM4VLA, a minimal adaptation pipeline that converts general-purpose VLMs into VLA policies using only a small set of new learnable parameters for fair and efficient comparison. Despite its simplicity, VLM4VLA proves surprisingly competitive with more sophisticated network designs. Through extensive empirical studies on various downstream tasks across three benchmarks, we find that while VLM initialization offers a consistent benefit over training from scratch, a VLM's general capabilities are poor predictors of its downstream task performance. This challenges common assumptions, indicating that standard VLM competence is necessary but insufficient for effective embodied control. We further investigate the impact of specific embodied capabilities by fine-tuning VLMs on seven auxiliary embodied tasks (e.g., embodied QA, visual pointing, depth estimation). Contrary to intuition, improving a VLM's performance on specific embodied skills does not guarantee better downstream control performance. Finally, modality-level ablations identify the visual module in VLM, rather than the language component, as the primary performance bottleneck. We demonstrate that injecting control-relevant supervision into the vision encoder of the VLM yields consistent gains, even when the encoder remains frozen during downstream fine-tuning. This isolates a persistent domain gap between current VLM pretraining objectives and the requirements of embodied action-planning.

</details>


### [5] [Deep Learning-Based Image Recognition for Soft-Shell Shrimp Classification](https://arxiv.org/abs/2601.03317)
*Yun-Hao Zhang,I-Hsien Ting,Dario Liberona,Yun-Hsiu Liu,Kazunori Minetaki*

Main category: cs.CV

TL;DR: Deep learning-based image recognition system for automated classification of white shrimp post-harvest to improve freshness and prevent head-body separation issues.


<details>
  <summary>Details</summary>
Motivation: Consumer demand for high-quality aquatic products is increasing, with freshness and appearance integrity being key concerns. Shrimp freshness declines rapidly after harvest, and soft-shell shrimp often suffer from head-body separation during cooking/freezing, affecting product appearance and consumer perception.

Method: Leverages deep learning-based image recognition using a convolutional neural network (CNN) model to replace manual sorting of white shrimp immediately after harvest.

Result: The automated system enhances classification accuracy, efficiency, and consistency compared to manual sorting, reducing processing time to help maintain freshness.

Conclusion: This technology helps shrimp transportation businesses meet customer demands more effectively by ensuring better freshness and appearance integrity through automated post-harvest classification.

Abstract: With the integration of information technology into aquaculture, production has become more stable and continues to grow annually. As consumer demand for high-quality aquatic products rises, freshness and appearance integrity are key concerns. In shrimp-based processed foods, freshness declines rapidly post-harvest, and soft-shell shrimp often suffer from head-body separation after cooking or freezing, affecting product appearance and consumer perception. To address these issues, this study leverages deep learning-based image recognition for automated classification of white shrimp immediately after harvest. A convolutional neural network (CNN) model replaces manual sorting, enhancing classification accuracy, efficiency, and consistency. By reducing processing time, this technology helps maintain freshness and ensures that shrimp transportation businesses meet customer demands more effectively.

</details>


### [6] [Higher order PCA-like rotation-invariant features for detailed shape descriptors modulo rotation](https://arxiv.org/abs/2601.03326)
*Jarek Duda*

Main category: cs.CV

TL;DR: Extension of PCA to higher-order tensors for rotation-invariant shape descriptors with arbitrarily high accuracy


<details>
  <summary>Details</summary>
Motivation: PCA's covariance matrix only approximates shapes as ellipsoids, which is insufficient for complex real shapes. There's a need for more accurate rotation-invariant shape descriptors that can handle intricate shapes beyond simple ellipsoidal approximations.

Method: Extends PCA approach to higher-order tensors (order-3 and above) describing central moments, or uses polynomial times Gaussian for decodable shape descriptors. These higher-order descriptors provide more detailed shape information while maintaining rotation invariance through analogous invariants like traces of powers.

Result: Proposes a framework for creating rotation-invariant shape descriptors with arbitrarily high accuracy, enabling shape description modulo rotation without costly optimization over rotations.

Conclusion: Higher-order tensor extensions of PCA provide powerful rotation-invariant shape descriptors suitable for complex real-world applications including molecular shape analysis, object recognition in 2D/3D, and efficient shape similarity comparison.

Abstract: PCA can be used for rotation invariant features, describing a shape with its $p_{ab}=E[(x_i-E[x_a])(x_b-E[x_b])]$ covariance matrix approximating shape by ellipsoid, allowing for rotation invariants like its traces of powers. However, real shapes are usually much more complicated, hence there is proposed its extension to e.g. $p_{abc}=E[(x_a-E[x_a])(x_b-E[x_b])(x_c-E[x_c])]$ order-3 or higher tensors describing central moments, or polynomial times Gaussian allowing decodable shape descriptors of arbitrarily high accuracy, and their analogous rotation invariants. Its practical applications could be rotation-invariant features to include shape modulo rotation e.g. for molecular shape descriptors, or for up to rotation object recognition in 2D images/3D scans, or shape similarity metric allowing their inexpensive comparison (modulo rotation) without costly optimization over rotations.

</details>


### [7] [MMErroR: A Benchmark for Erroneous Reasoning in Vision-Language Models](https://arxiv.org/abs/2601.03331)
*Yang Shi,Yifeng Xie,Minzhe Guo,Liangsi Lu,Mingxuan Huang,Jingchao Wang,Zhihong Zhu,Boyan Xu,Zhiqi Huang*

Main category: cs.CV

TL;DR: MMErroR is a multi-modal benchmark with 2,013 samples containing single reasoning errors across 24 subdomains, designed to evaluate VLMs' ability to detect incorrect reasoning and classify error types, not just answer correctness.


<details>
  <summary>Details</summary>
Motivation: To determine whether Vision-Language Models truly understand content by testing their ability to detect when reasoning processes are wrong and identify specific error types, moving beyond simple answer correctness evaluation.

Method: Created MMErroR benchmark with 2,013 samples, each embedding a single coherent reasoning error, spanning 24 subdomains across six top-level domains. Evaluated 20 advanced VLMs on their ability to detect incorrect reasoning and classify error types in both visual and linguistic contexts.

Result: Even the best model (Gemini-3.0-Pro) only classified errors correctly in 66.47% of cases, showing significant difficulty in identifying erroneous reasoning. The benchmark reveals VLMs' limitations in process-level error detection.

Conclusion: MMErroR demonstrates that current VLMs struggle with error detection and classification, highlighting the need for improved reasoning capabilities and providing valuable insights into multi-modal reasoning model limitations.

Abstract: Recent advances in Vision-Language Models (VLMs) have improved performance in multi-modal learning, raising the question of whether these models truly understand the content they process. Crucially, can VLMs detect when a reasoning process is wrong and identify its error type? To answer this, we present MMErroR, a multi-modal benchmark of 2,013 samples, each embedding a single coherent reasoning error. These samples span 24 subdomains across six top-level domains, ensuring broad coverage and taxonomic richness. Unlike existing benchmarks that focus on answer correctness, MMErroR targets a process-level, error-centric evaluation that requires models to detect incorrect reasoning and classify the error type within both visual and linguistic contexts. We evaluate 20 advanced VLMs, even the best model (Gemini-3.0-Pro) classifies the error in only 66.47\% of cases, underscoring the challenge of identifying erroneous reasoning. Furthermore, the ability to accurately identify errors offers valuable insights into the capabilities of multi-modal reasoning models. Project Page: https://mmerror-benchmark.github.io

</details>


### [8] [RelightAnyone: A Generalized Relightable 3D Gaussian Head Model](https://arxiv.org/abs/2601.03357)
*Yingyan Xu,Pramod Rao,Sebastian Weiss,Gaspard Zoss,Markus Gross,Christian Theobalt,Marc Habermann,Derek Bradley*

Main category: cs.CV

TL;DR: A two-stage 3D Gaussian Splatting model that enables relightable head avatars from single/multi-view images without requiring complex OLAT capture, using a mapping from flat-lit avatars to physically-based reflectance parameters.


<details>
  <summary>Details</summary>
Motivation: Existing high-quality relighting methods require subjects to be captured under complex time-multiplexed illumination (OLAT), which is impractical. There's a need for relightable 3D head avatars that can work with standard single- or multi-view images without requiring specialized OLAT capture.

Method: Two-stage approach: 1) First stage learns flat-lit 3DGS avatars from diverse multi-view datasets without OLAT, using dataset-specific lighting codes for self-supervised alignment. 2) Second stage learns mapping from flat-lit avatars to physically-based reflectance parameters, trained on a smaller OLAT dataset to enable high-quality relighting.

Result: The method generalizes well to relight any subject from the first stage as if captured under OLAT lighting. It can fit to unseen subjects from as little as a single image, enabling novel view synthesis and relighting applications for digital avatars.

Conclusion: Proposed approach enables high-quality relightable 3D head avatars without requiring complex OLAT capture, making relightable avatar creation more accessible and practical from standard image inputs.

Abstract: 3D Gaussian Splatting (3DGS) has become a standard approach to reconstruct and render photorealistic 3D head avatars. A major challenge is to relight the avatars to match any scene illumination. For high quality relighting, existing methods require subjects to be captured under complex time-multiplexed illumination, such as one-light-at-a-time (OLAT). We propose a new generalized relightable 3D Gaussian head model that can relight any subject observed in a single- or multi-view images without requiring OLAT data for that subject. Our core idea is to learn a mapping from flat-lit 3DGS avatars to corresponding relightable Gaussian parameters for that avatar. Our model consists of two stages: a first stage that models flat-lit 3DGS avatars without OLAT lighting, and a second stage that learns the mapping to physically-based reflectance parameters for high-quality relighting. This two-stage design allows us to train the first stage across diverse existing multi-view datasets without OLAT lighting ensuring cross-subject generalization, where we learn a dataset-specific lighting code for self-supervised lighting alignment. Subsequently, the second stage can be trained on a significantly smaller dataset of subjects captured under OLAT illumination. Together, this allows our method to generalize well and relight any subject from the first stage as if we had captured them under OLAT lighting. Furthermore, we can fit our model to unseen subjects from as little as a single image, allowing several applications in novel view synthesis and relighting for digital avatars.

</details>


### [9] [Guardians of the Hair: Rescuing Soft Boundaries in Depth, Stereo, and Novel Views](https://arxiv.org/abs/2601.03362)
*Xiang Zhang,Yang Zhang,Lukas Mehl,Markus Gross,Christopher Schroers*

Main category: cs.CV

TL;DR: HairGuard is a framework that recovers fine-grained soft boundary details in 3D vision tasks by addressing ambiguous foreground-background mixing in thin hair-like structures.


<details>
  <summary>Details</summary>
Motivation: Soft boundaries like thin hairs are common in natural and computer-generated imagery but challenging for 3D vision due to ambiguous mixing of foreground and background cues, leading to poor depth estimation and view synthesis quality.

Method: 1) Data curation pipeline using image matting datasets; 2) Depth fixer network with gated residual module to refine depth around soft boundaries; 3) Depth-based forward warping for view synthesis; 4) Generative scene painter for disoccluded regions; 5) Color fuser to combine warped and inpainted results.

Result: Achieves state-of-the-art performance in monocular depth estimation, stereo image/video conversion, and novel view synthesis with significant improvements in soft boundary regions.

Conclusion: HairGuard effectively addresses soft boundary challenges in 3D vision through specialized depth refinement and view synthesis components, enabling plug-and-play integration with existing depth models while preserving fine details.

Abstract: Soft boundaries, like thin hairs, are commonly observed in natural and computer-generated imagery, but they remain challenging for 3D vision due to the ambiguous mixing of foreground and background cues. This paper introduces Guardians of the Hair (HairGuard), a framework designed to recover fine-grained soft boundary details in 3D vision tasks. Specifically, we first propose a novel data curation pipeline that leverages image matting datasets for training and design a depth fixer network to automatically identify soft boundary regions. With a gated residual module, the depth fixer refines depth precisely around soft boundaries while maintaining global depth quality, allowing plug-and-play integration with state-of-the-art depth models. For view synthesis, we perform depth-based forward warping to retain high-fidelity textures, followed by a generative scene painter that fills disoccluded regions and eliminates redundant background artifacts within soft boundaries. Finally, a color fuser adaptively combines warped and inpainted results to produce novel views with consistent geometry and fine-grained details. Extensive experiments demonstrate that HairGuard achieves state-of-the-art performance across monocular depth estimation, stereo image/video conversion, and novel view synthesis, with significant improvements in soft boundary regions.

</details>


### [10] [RiskCueBench: Benchmarking Anticipatory Reasoning from Early Risk Cues in Video-Language Models](https://arxiv.org/abs/2601.03369)
*Sha Luo,Yogesh Prabhu,Tim Ossowski,Kaiping Chen,Junjie Hu*

Main category: cs.CV

TL;DR: A new video risk assessment benchmark called RiskCueBench is introduced, focusing on identifying early risk signals before accidents occur, revealing current models' limitations in anticipating risky events from early visual cues.


<details>
  <summary>Details</summary>
Motivation: Current video risk assessment models often have access to full video sequences including accidents, which reduces task difficulty and doesn't reflect real-world conditions where early anticipation is crucial for preventing accidents and ensuring public safety.

Method: Introduces RiskCueBench, a new video understanding benchmark where videos are carefully annotated to identify a "risk signal clip" - the earliest moment that indicates a potential safety concern, requiring models to anticipate risky events from early visual signals.

Result: Experimental results show a significant gap in current systems' ability to interpret evolving situations and anticipate future risky events from early visual signals, highlighting challenges for practical deployment of video risk prediction models.

Conclusion: The benchmark reveals important limitations in current video risk assessment systems and establishes a more realistic evaluation framework that better reflects real-world conditions where early risk anticipation is critical for safety applications.

Abstract: With the rapid growth of video centered social media, the ability to anticipate risky events from visual data is a promising direction for ensuring public safety and preventing real world accidents. Prior work has extensively studied supervised video risk assessment across domains such as driving, protests, and natural disasters. However, many existing datasets provide models with access to the full video sequence, including the accident itself, which substantially reduces the difficulty of the task. To better reflect real world conditions, we introduce a new video understanding benchmark RiskCueBench in which videos are carefully annotated to identify a risk signal clip, defined as the earliest moment that indicates a potential safety concern. Experimental results reveal a significant gap in current systems ability to interpret evolving situations and anticipate future risky events from early visual signals, highlighting important challenges for deploying video risk prediction models in practice.

</details>


### [11] [A Novel Unified Approach to Deepfake Detection](https://arxiv.org/abs/2601.03382)
*Lord Sen,Shyamapada Mukherjee*

Main category: cs.CV

TL;DR: Novel deepfake detection architecture using cross-attention between spatial/frequency domain features with blood detection module achieves SOTA results (up to 99.88% AUC) and strong cross-dataset generalization.


<details>
  <summary>Details</summary>
Motivation: Deepfake synthesis poses significant threats to digital trust, necessitating robust detection and tagging systems to combat AI-generated misinformation and media manipulation.

Method: Proposes unified architecture with cross-attention mechanism between spatial and frequency domain features, combined with a blood detection module. Uses Swin Transformer/BERT or EfficientNet-B4/BERT combinations for classification.

Result: Achieves 99.80% AUC on FF++ and 99.88% on Celeb-DF with Swin Transformer/BERT, and 99.55%/99.38% with EfficientNet-B4/BERT. Shows excellent cross-dataset generalization performance.

Conclusion: The proposed architecture outperforms state-of-the-art methods, provides a unified framework for deepfake detection, and demonstrates strong generalization across different datasets.

Abstract: The advancements in the field of AI is increasingly giving rise to various threats. One of the most prominent of them is the synthesis and misuse of Deepfakes. To sustain trust in this digital age, detection and tagging of deepfakes is very necessary. In this paper, a novel architecture for Deepfake detection in images and videos is presented. The architecture uses cross attention between spatial and frequency domain features along with a blood detection module to classify an image as real or fake. This paper aims to develop a unified architecture and provide insights into each step. Though this approach we achieve results better than SOTA, specifically 99.80%, 99.88% AUC on FF++ and Celeb-DF upon using Swin Transformer and BERT and 99.55, 99.38 while using EfficientNet-B4 and BERT. The approach also generalizes very well achieving great cross dataset results as well.

</details>


### [12] [Better, But Not Sufficient: Testing Video ANNs Against Macaque IT Dynamics](https://arxiv.org/abs/2601.03392)
*Matteo Dunnhofer,Christian Micheloni,Kohitij Kar*

Main category: cs.CV

TL;DR: Current video-based ANN models show modest improvements over static models in predicting macaque IT cortex responses to naturalistic videos, but fail to capture appearance-invariant motion processing that IT exhibits.


<details>
  <summary>Details</summary>
Motivation: While feedforward ANNs dominate ventral stream modeling, they're limited to static computations. Primate vision is dynamic, and IT cortex encodes both object recognition and motion velocity. The paper investigates whether IT's temporal responses reflect simple frame-by-frame processing or richer dynamic computations.

Method: Compared macaque IT responses during naturalistic videos against static, recurrent, and video-based ANN models. Applied a stress test using "appearance-free" video variants that preserve motion but remove shape/texture, then tested decoder generalization across this manipulation.

Result: Video models provided modest improvements in neural predictivity, especially at later response stages. IT population activity generalized across appearance-free manipulation, but all ANN classes failed to generalize, showing they capture appearance-bound dynamics rather than appearance-invariant temporal computations.

Conclusion: Current video models better capture appearance-bound dynamics rather than the appearance-invariant temporal computations expressed in IT. This underscores the need for new objectives that encode biological temporal statistics and invariances.

Abstract: Feedforward artificial neural networks (ANNs) trained on static images remain the dominant models of the the primate ventral visual stream, yet they are intrinsically limited to static computations. The primate world is dynamic, and the macaque ventral visual pathways, specifically the inferior temporal (IT) cortex not only supports object recognition but also encodes object motion velocity during naturalistic video viewing. Does IT's temporal responses reflect nothing more than time-unfolded feedforward transformations, framewise features with shallow temporal pooling, or do they embody richer dynamic computations? We tested this by comparing macaque IT responses during naturalistic videos against static, recurrent, and video-based ANN models. Video models provided modest improvements in neural predictivity, particularly at later response stages, raising the question of what kind of dynamics they capture. To probe this, we applied a stress test: decoders trained on naturalistic videos were evaluated on "appearance-free" variants that preserve motion but remove shape and texture. IT population activity generalized across this manipulation, but all ANN classes failed. Thus, current video models better capture appearance-bound dynamics rather than the appearance-invariant temporal computations expressed in IT, underscoring the need for new objectives that encode biological temporal statistics and invariances.

</details>


### [13] [Eye-Q: A Multilingual Benchmark for Visual Word Puzzle Solving and Image-to-Phrase Reasoning](https://arxiv.org/abs/2601.03400)
*Ali Najar,Alireza Mirrokni,Arshia Izadyari,Sadegh Mohammadian,Amir Homayoon Sharifizade,Asal Meskin,Mobin Bagherian,Ehsaneddin Asgari*

Main category: cs.CV

TL;DR: Eye-Q is a multilingual visual word puzzle benchmark that tests complex visual reasoning beyond surface-level recognition, revealing significant gaps in current VLMs' ability to handle abstract, cross-lingual puzzles.


<details>
  <summary>Details</summary>
Motivation: Current VLMs perform well on standard benchmarks but often rely on surface-level recognition rather than deeper reasoning. There's a need for benchmarks that test complex visual understanding involving implicit cues, hypothesis generation/revision, and non-literal concept mapping.

Method: Created Eye-Q benchmark with 1,343 multilingual puzzles (English, Persian, Arabic, cross-lingual) featuring conceptually dense scenes with brief descriptions. Puzzles are intentionally unstructured and cue-implicit with distractors. Evaluated state-of-the-art VLMs using open-ended, human-aligned protocol that probes hypothesis formation and revision under lightweight assistance.

Result: Substantial performance gaps revealed, especially on abstract and cross-lingual puzzles. Maximum accuracy reached only 60.27%, highlighting limitations in current models' ability to construct and search over appropriate conceptual representations for flexible image-to-phrase inference.

Conclusion: Visual word puzzles like Eye-Q expose critical limitations in current VLMs' reasoning capabilities, showing they struggle with tasks requiring selective attention, abstraction, associative inference, and cross-lingual understanding beyond literal grounding.

Abstract: Vision-Language Models (VLMs) have achieved strong performance on standard vision-language benchmarks, yet often rely on surface-level recognition rather than deeper reasoning. We propose visual word puzzles as a challenging alternative, as they require discovering implicit visual cues, generating and revising hypotheses, and mapping perceptual evidence to non-literal concepts in ways that are difficult to solve via literal grounding, OCR-heavy shortcuts, or simple retrieval-style matching. We introduce Eye-Q, a multilingual benchmark designed to assess this form of complex visual understanding. Eye-Q contains 1,343 puzzles in which a model observes a conceptually dense scene with a brief description and must infer a specific target word or phrase. The puzzles are intentionally unstructured and cue-implicit, with distractors and contextual relationships that demand selective attention, abstraction, and associative inference. The benchmark spans English, Persian, Arabic, and cross-lingual puzzles. We evaluate state-of-the-art VLMs using an open-ended, human-aligned protocol that probes hypothesis formation and revision under lightweight assistance. Results reveal substantial performance gaps, especially on abstract and cross-lingual puzzles, highlighting limitations in current models' ability to construct and search over appropriate conceptual representations for flexible image-to-phrase inference; maximum accuracy reaches only 60.27%.

</details>


### [14] [GAMBIT: A Gamified Jailbreak Framework for Multimodal Large Language Models](https://arxiv.org/abs/2601.03416)
*Xiangdong Hu,Yangyang Jiang,Qin Hu,Xiaojun Jia*

Main category: cs.CV

TL;DR: GAMBIT is a novel multimodal jailbreak framework that uses gamified scenes to manipulate MLLMs' reasoning incentives, achieving high attack success rates by making models proactively complete harmful tasks as part of winning a game.


<details>
  <summary>Details</summary>
Motivation: Current multimodal jailbreak attacks underperform on reasoning models because they don't leverage the model's own reasoning incentives. The authors hypothesize that if models think like humans, their cognitive-stage decisions can be influenced to proactively complete jailbreaks.

Method: GAMBIT decomposes and reassembles harmful visual semantics, then constructs a gamified scene that drives the model to explore, reconstruct intent, and answer as part of winning the game. This structured reasoning chain increases task complexity in both vision and text, positioning the model as a participant whose goal pursuit reduces safety attention.

Result: Extensive experiments show GAMBIT achieves high Attack Success Rates: 92.13% on Gemini 2.5 Flash, 91.20% on QvQ-MAX, and 85.87% on GPT-4o, significantly outperforming baselines on both reasoning and non-reasoning MLLMs.

Conclusion: GAMBIT demonstrates that leveraging models' reasoning incentives through gamified adversarial scenes is an effective approach for multimodal jailbreaks, achieving high success rates by making safety mechanisms vulnerable when models are engaged in goal-oriented tasks.

Abstract: Multimodal Large Language Models (MLLMs) have become widely deployed, yet their safety alignment remains fragile under adversarial inputs. Previous work has shown that increasing inference steps can disrupt safety mechanisms and lead MLLMs to generate attacker-desired harmful content. However, most existing attacks focus on increasing the complexity of the modified visual task itself and do not explicitly leverage the model's own reasoning incentives. This leads to them underperforming on reasoning models (Models with Chain-of-Thoughts) compared to non-reasoning ones (Models without Chain-of-Thoughts). If a model can think like a human, can we influence its cognitive-stage decisions so that it proactively completes a jailbreak? To validate this idea, we propose GAMBI} (Gamified Adversarial Multimodal Breakout via Instructional Traps), a novel multimodal jailbreak framework that decomposes and reassembles harmful visual semantics, then constructs a gamified scene that drives the model to explore, reconstruct intent, and answer as part of winning the game. The resulting structured reasoning chain increases task complexity in both vision and text, positioning the model as a participant whose goal pursuit reduces safety attention and induces it to answer the reconstructed malicious query. Extensive experiments on popular reasoning and non-reasoning MLLMs demonstrate that GAMBIT achieves high Attack Success Rates (ASR), reaching 92.13% on Gemini 2.5 Flash, 91.20% on QvQ-MAX, and 85.87% on GPT-4o, significantly outperforming baselines.

</details>


### [15] [WeedRepFormer: Reparameterizable Vision Transformers for Real-Time Waterhemp Segmentation and Gender Classification](https://arxiv.org/abs/2601.03431)
*Toqi Tahamid Sarker,Taminul Islam,Khaled R. Ahmed,Cristiana Bernardi Rankrape,Kaitlin E. Creager,Karla Gage*

Main category: cs.CV

TL;DR: WeedRepFormer is a lightweight multi-task Vision Transformer for simultaneous waterhemp weed segmentation and gender classification, achieving high accuracy with minimal parameters and real-time performance.


<details>
  <summary>Details</summary>
Motivation: Existing agricultural models struggle to balance fine-grained feature extraction for biological attribute classification with the efficiency needed for real-time deployment in farming applications.

Method: Integrates structural reparameterization across entire architecture (Vision Transformer backbone, Lite R-ASPP decoder, reparameterizable classification head) to decouple training-time capacity from inference-time latency.

Result: Achieves 92.18% mIoU for segmentation and 81.91% accuracy for gender classification with only 3.59M parameters and 3.80 GFLOPs, running at 108.95 FPS. Outperforms state-of-the-art iFormer-T by 4.40% in classification accuracy while reducing parameters by 1.9x.

Conclusion: WeedRepFormer successfully addresses the efficiency-accuracy trade-off for agricultural vision tasks, enabling simultaneous weed segmentation and gender classification with real-time performance suitable for practical farming applications.

Abstract: We present WeedRepFormer, a lightweight multi-task Vision Transformer designed for simultaneous waterhemp segmentation and gender classification. Existing agricultural models often struggle to balance the fine-grained feature extraction required for biological attribute classification with the efficiency needed for real-time deployment. To address this, WeedRepFormer systematically integrates structural reparameterization across the entire architecture - comprising a Vision Transformer backbone, a Lite R-ASPP decoder, and a novel reparameterizable classification head - to decouple training-time capacity from inference-time latency. We also introduce a comprehensive waterhemp dataset containing 10,264 annotated frames from 23 plants. On this benchmark, WeedRepFormer achieves 92.18% mIoU for segmentation and 81.91% accuracy for gender classification using only 3.59M parameters and 3.80 GFLOPs. At 108.95 FPS, our model outperforms the state-of-the-art iFormer-T by 4.40% in classification accuracy while maintaining competitive segmentation performance and significantly reducing parameter count by 1.9x.

</details>


### [16] [FROST-Drive: Scalable and Efficient End-to-End Driving with a Frozen Vision Encoder](https://arxiv.org/abs/2601.03460)
*Zeyu Dong,Yimin Zhu,Yu Wu,Yu Sun*

Main category: cs.CV

TL;DR: FROST-Drive proposes a frozen vision encoder approach for autonomous driving E2E models, preserving VLM generalization capabilities instead of full fine-tuning, achieving better performance on challenging scenarios.


<details>
  <summary>Details</summary>
Motivation: Current E2E autonomous driving models suffer from poor generalization to novel/complex scenarios due to over-specialization from fully fine-tuning vision encoders on driving data, losing the broad world knowledge from pretrained VLMs.

Method: FROST-Drive keeps VLM vision encoder weights frozen, combines it with a transformer-based adapter for multimodal fusion and GRU-based decoder for waypoint generation, plus a custom loss function optimized for Rater Feedback Score.

Result: Extensive experiments on Waymo Open E2E Dataset show frozen-encoder approach significantly outperforms full fine-tuning models, especially on long-tail scenarios, demonstrating better generalization.

Conclusion: Preserving VLM's broad knowledge via frozen encoders is more effective for robust, generalizable driving than intensive domain-specific adaptation, offering a new pathway for vision-based models in real-world applications.

Abstract: End-to-end (E2E) models in autonomous driving aim to directly map sensor inputs to control commands, but their ability to generalize to novel and complex scenarios remains a key challenge. The common practice of fully fine-tuning the vision encoder on driving datasets potentially limits its generalization by causing the model to specialize too heavily in the training data. This work challenges the necessity of this training paradigm. We propose FROST-Drive, a novel E2E architecture designed to preserve and leverage the powerful generalization capabilities of a pretrained vision encoder from a Vision-Language Model (VLM). By keeping the encoder's weights frozen, our approach directly transfers the rich, generalized world knowledge from the VLM to the driving task. Our model architecture combines this frozen encoder with a transformer-based adapter for multimodal fusion and a GRU-based decoder for smooth waypoint generation. Furthermore, we introduce a custom loss function designed to directly optimize for Rater Feedback Score (RFS), a metric that prioritizes robust trajectory planning. We conduct extensive experiments on Waymo Open E2E Dataset, a large-scale datasets deliberately curated to capture the long-tail scenarios, demonstrating that our frozen-encoder approach significantly outperforms models that employ full fine-tuning. Our results provide substantial evidence that preserving the broad knowledge of a capable VLM is a more effective strategy for achieving robust, generalizable driving performance than intensive domain-specific adaptation. This offers a new pathway for developing vision-based models that can better handle the complexities of real-world application domains.

</details>


### [17] [Experimental Comparison of Light-Weight and Deep CNN Models Across Diverse Datasets](https://arxiv.org/abs/2601.03463)
*Md. Hefzul Hossain Papon,Shadman Rabby*

Main category: cs.CV

TL;DR: Lightweight shallow CNN architecture serves as competitive baseline for Bangladeshi vision tasks without requiring large GPUs or specialized pre-trained models.


<details>
  <summary>Details</summary>
Motivation: To provide practical, deployable vision solutions for low-resource settings in Bangladesh across diverse domains like smart-city surveillance and agricultural classification.

Method: Well-regularized shallow CNN architecture that doesn't require large computational resources or specialized pre-trained models.

Result: The lightweight CNN serves as highly competitive baseline across heterogeneous domains and establishes unified, reproducible benchmark for multiple Bangladeshi vision datasets.

Conclusion: Lightweight CNNs have practical value for real-world deployment in low-resource settings, offering accessible alternatives to resource-intensive deep learning approaches.

Abstract: Our results reveal that a well-regularized shallow architecture can serve as a highly competitive baseline across heterogeneous domains - from smart-city surveillance to agricultural variety classification - without requiring large GPUs or specialized pre-trained models. This work establishes a unified, reproducible benchmark for multiple Bangladeshi vision datasets and highlights the practical value of lightweight CNNs for real-world deployment in low-resource settings.

</details>


### [18] [Latent Geometry of Taste: Scalable Low-Rank Matrix Factorization](https://arxiv.org/abs/2601.03466)
*Joshua Salako*

Main category: cs.CV

TL;DR: Parallelized ALS framework on MovieLens 32M shows constrained low-rank models outperform higher-dimensional ones, revealing semantic genre clusters and effective cold-start handling.


<details>
  <summary>Details</summary>
Motivation: Address scalability and data sparsity bottlenecks in collaborative filtering on massive interaction datasets like MovieLens 32M.

Method: High-performance parallelized Alternating Least Squares (ALS) framework with extensive hyperparameter optimization on MovieLens 32M dataset.

Result: Constrained low-rank models significantly outperform higher dimensional counterparts in generalization (RMSE and ranking precision), reveal semantic genre clusters in embedding space, and effectively handle cold-start scenarios with tunable scoring parameter.

Conclusion: The ALS framework successfully balances model complexity and performance, captures deep structural relationships from interaction data, and provides practical utility for real-world recommendation systems.

Abstract: Scalability and data sparsity remain critical bottlenecks for collaborative filtering on massive interaction datasets. This work investigates the latent geometry of user preferences using the MovieLens 32M dataset, implementing a high-performance, parallelized Alternating Least Squares (ALS) framework. Through extensive hyperparameter optimization, we demonstrate that constrained low-rank models significantly outperform higher dimensional counterparts in generalization, achieving an optimal balance between Root Mean Square Error (RMSE) and ranking precision. We visualize the learned embedding space to reveal the unsupervised emergence of semantic genre clusters, confirming that the model captures deep structural relationships solely from interaction data. Finally, we validate the system's practical utility in a cold-start scenario, introducing a tunable scoring parameter to manage the trade-off between popularity bias and personalized affinity effectively. The codebase for this research can be found here: https://github.com/joshsalako/recommender.git

</details>


### [19] [ThinkRL-Edit: Thinking in Reinforcement Learning for Reasoning-Centric Image Editing](https://arxiv.org/abs/2601.03467)
*Hengjia Li,Liming Jiang,Qing Yan,Yizhi Song,Hao Kang,Zichuan Liu,Xin Lu,Boxi Wu,Deng Cai*

Main category: cs.CV

TL;DR: ThinkRL-Edit is a reasoning-centric RL framework that improves image editing by decoupling visual reasoning from synthesis, using CoT-based reasoning sampling, unbiased reward grouping, and binary checklist rewards.


<details>
  <summary>Details</summary>
Motivation: Current instruction-driven image editing models have limited visual reasoning capabilities, leading to poor performance on reasoning-centric edits. Existing RL approaches face challenges with limited reasoning exploration, biased reward fusion, and unstable VLM-based instruction rewards.

Method: Proposes ThinkRL-Edit framework that: 1) decouples visual reasoning from image synthesis, 2) uses Chain-of-Thought-based reasoning sampling with planning and reflection stages before generation, 3) implements unbiased chain preference grouping across reward dimensions, and 4) replaces interval-based VLM scores with binary checklist rewards.

Result: The method significantly outperforms prior work on reasoning-centric image editing, producing instruction-faithful, visually coherent, and semantically grounded edits.

Conclusion: ThinkRL-Edit effectively addresses key challenges in RL-based image editing by expanding reasoning exploration, providing unbiased reward aggregation, and using more precise reward signals, leading to superior reasoning-centric image editing performance.

Abstract: Instruction-driven image editing with unified multimodal generative models has advanced rapidly, yet their underlying visual reasoning remains limited, leading to suboptimal performance on reasoning-centric edits. Reinforcement learning (RL) has been investigated for improving the quality of image editing, but it faces three key challenges: (1) limited reasoning exploration confined to denoising stochasticity, (2) biased reward fusion, and (3) unstable VLM-based instruction rewards. In this work, we propose ThinkRL-Edit, a reasoning-centric RL framework that decouples visual reasoning from image synthesis and expands reasoning exploration beyond denoising. To the end, we introduce Chain-of-Thought (CoT)-based reasoning sampling with planning and reflection stages prior to generation in online sampling, compelling the model to explore multiple semantic hypotheses and validate their plausibility before committing to a visual outcome. To avoid the failures of weighted aggregation, we propose an unbiased chain preference grouping strategy across multiple reward dimensions. Moreover, we replace interval-based VLM scores with a binary checklist, yielding more precise, lower-variance, and interpretable rewards for complex reasoning. Experiments show our method significantly outperforms prior work on reasoning-centric image editing, producing instruction-faithful, visually coherent, and semantically grounded edits.

</details>


### [20] [Understanding Reward Hacking in Text-to-Image Reinforcement Learning](https://arxiv.org/abs/2601.03468)
*Yunqi Hong,Kuei-Chun Kao,Hengguang Zhou,Cho-Jui Hsieh*

Main category: cs.CV

TL;DR: The paper analyzes reward hacking in text-to-image RL post-training, identifies artifact generation as a common failure mode, and proposes a lightweight artifact reward model as a regularizer to improve visual realism and reduce reward hacking.


<details>
  <summary>Details</summary>
Motivation: Existing reward functions for RL post-training of image generation models are imperfect proxies for human judgment, leading to reward hacking where models produce unrealistic or low-quality images that still achieve high reward scores.

Method: Systematically analyze reward hacking behaviors in T2I RL post-training, investigate individual contributions of aesthetic/human preference rewards and prompt-image consistency rewards, and propose a lightweight adaptive artifact reward model trained on a small curated dataset of artifact-free and artifact-containing samples.

Result: Experiments show that incorporating the artifact reward significantly improves visual realism and reduces reward hacking across multiple T2I RL setups, demonstrating effectiveness as a safeguard against reward hacking.

Conclusion: Lightweight reward augmentation with an artifact reward model serves as an effective regularizer for existing RL pipelines, addressing the common failure mode of artifact-prone image generation in reward hacking scenarios.

Abstract: Reinforcement learning (RL) has become a standard approach for post-training large language models and, more recently, for improving image generation models, which uses reward functions to enhance generation quality and human preference alignment. However, existing reward designs are often imperfect proxies for true human judgment, making models prone to reward hacking--producing unrealistic or low-quality images that nevertheless achieve high reward scores. In this work, we systematically analyze reward hacking behaviors in text-to-image (T2I) RL post-training. We investigate how both aesthetic/human preference rewards and prompt-image consistency rewards individually contribute to reward hacking and further show that ensembling multiple rewards can only partially mitigate this issue. Across diverse reward models, we identify a common failure mode: the generation of artifact-prone images. To address this, we propose a lightweight and adaptive artifact reward model, trained on a small curated dataset of artifact-free and artifact-containing samples. This model can be integrated into existing RL pipelines as an effective regularizer for commonly used reward models. Experiments demonstrate that incorporating our artifact reward significantly improves visual realism and reduces reward hacking across multiple T2I RL setups, demonstrating the effectiveness of lightweight reward augment serving as a safeguard against reward hacking.

</details>


### [21] [CroBIM-U: Uncertainty-Driven Referring Remote Sensing Image Segmentation](https://arxiv.org/abs/2601.03490)
*Yuzhe Sun,Zhe Dong,Haochen Jiang,Tianzhu Liu,Yanfeng Gu*

Main category: cs.CV

TL;DR: Proposes an uncertainty-guided framework for referring remote sensing image segmentation that uses a referring uncertainty map to adaptively modulate language fusion and refinement based on spatial ambiguity.


<details>
  <summary>Details</summary>
Motivation: Existing methods use uniform fusion and refinement across entire images, which introduces unnecessary linguistic noise in clear regions while failing to provide sufficient disambiguation in confused areas with extreme scale variations, dense distractors, and complex boundaries.

Method: Introduces a Referring Uncertainty Scorer (RUS) trained via online error-consistency supervision to predict spatial referential ambiguity. Uses this uncertainty map to guide two plug-and-play modules: Uncertainty-Gated Fusion (UGF) for dynamic language injection modulation, and Uncertainty-Driven Local Refinement (UDLR) for focusing refinement on error-prone boundaries.

Result: Extensive experiments show the method significantly improves robustness and geometric fidelity in complex remote sensing scenes without altering backbone architecture, serving as a unified plug-and-play solution.

Conclusion: The uncertainty-guided framework effectively addresses spatial non-uniformity in cross-modal alignment for referring remote sensing segmentation by adaptively orchestrating inference based on predicted referential ambiguity.

Abstract: Referring remote sensing image segmentation aims to localize specific targets described by natural language within complex overhead imagery. However, due to extreme scale variations, dense similar distractors, and intricate boundary structures, the reliability of cross-modal alignment exhibits significant \textbf{spatial non-uniformity}. Existing methods typically employ uniform fusion and refinement strategies across the entire image, which often introduces unnecessary linguistic perturbations in visually clear regions while failing to provide sufficient disambiguation in confused areas. To address this, we propose an \textbf{uncertainty-guided framework} that explicitly leverages a pixel-wise \textbf{referring uncertainty map} as a spatial prior to orchestrate adaptive inference. Specifically, we introduce a plug-and-play \textbf{Referring Uncertainty Scorer (RUS)}, which is trained via an online error-consistency supervision strategy to interpretably predict the spatial distribution of referential ambiguity. Building on this prior, we design two plug-and-play modules: 1) \textbf{Uncertainty-Gated Fusion (UGF)}, which dynamically modulates language injection strength to enhance constraints in high-uncertainty regions while suppressing noise in low-uncertainty ones; and 2) \textbf{Uncertainty-Driven Local Refinement (UDLR)}, which utilizes uncertainty-derived soft masks to focus refinement on error-prone boundaries and fine details. Extensive experiments demonstrate that our method functions as a unified, plug-and-play solution that significantly improves robustness and geometric fidelity in complex remote sensing scenes without altering the backbone architecture.

</details>


### [22] [SDCD: Structure-Disrupted Contrastive Decoding for Mitigating Hallucinations in Large Vision-Language Models](https://arxiv.org/abs/2601.03500)
*Yuxuan Xia,Siheng Wang,Peng Li*

Main category: cs.CV

TL;DR: SDCD is a training-free method that reduces object hallucinations in LVLMs by disrupting visual structure to suppress texture-driven biases.


<details>
  <summary>Details</summary>
Motivation: Object hallucination remains a critical challenge in LVLMs. Existing approaches focus on language priors but overlook visual encoding complexities. The paper identifies visual statistical bias from Vision Encoders' Bag-of-Patches behavior as a key factor causing hallucinations.

Method: Structure-Disrupted Contrastive Decoding (SDCD): A training-free algorithm that introduces a shuffled structure-disrupted view of the input. It performs contrastive calibration of output distributions by penalizing tokens that maintain high confidence under this structure-less view, suppressing texture-driven bias.

Result: SDCD significantly mitigates hallucinations across multiple benchmarks and enhances overall multimodal capabilities of LVLMs.

Conclusion: Addressing visual statistical bias through structure disruption effectively reduces object hallucinations in LVLMs, offering a training-free solution that improves multimodal understanding.

Abstract: Large Vision-Language Models (LVLMs) demonstrate significant progress in multimodal understanding and reasoning, yet object hallucination remains a critical challenge. While existing research focuses on mitigating language priors or high-level statistical biases, they often overlook the internal complexities of the visual encoding process. We identify that visual statistical bias, arising from the inherent Bag-of-Patches behavior of Vision Encoders under weak structural supervision, acts as a contributing factor of object hallucinations. Under this bias, models prioritize local texture features within individual patches over holistic geometric structures. This tendency may induce spurious visual confidence and result in hallucinations. To address this, we introduce a training-free algorithm called Structure-Disrupted Contrastive Decoding (SDCD), which performs contrastive calibration of the output distribution by introducing a shuffled structure-disrupted view. By penalizing tokens that maintain high confidence under this structure-less view, SDCD effectively suppresses the texture-driven bias. Experimental results demonstrate that SDCD significantly mitigates hallucinations across multiple benchmarks and enhances the overall multimodal capabilities of LVLMs.

</details>


### [23] [REFA: Real-time Egocentric Facial Animations for Virtual Reality](https://arxiv.org/abs/2601.03507)
*Qiang Zhang,Tong Xiao,Haroun Habeeb,Larissa Laich,Sofien Bouaziz,Patrick Snape,Wenjing Zhang,Matthew Cioffi,Peizhao Zhang,Pavel Pidlypenskyi,Winnie Lin,Luming Ma,Mengjiao Wang,Kunpeng Li,Chengjiang Long,Steven Song,Martin Prazak,Alexander Sjoholm,Ajinkya Deogade,Jaebong Lee,Julio Delgado Mangas,Amaury Aubel*

Main category: cs.CV

TL;DR: Real-time facial expression tracking system using VR headset infrared cameras to drive virtual characters without calibration, trained on heterogeneous synthetic/real data from 18k subjects.


<details>
  <summary>Details</summary>
Motivation: Enable non-intrusive, real-time facial expression tracking for VR applications without lengthy calibration, facilitating natural communication and expression in virtual environments.

Method: Distillation-based ML approach trained on heterogeneous data from multiple sources (synthetic/real images), using differentiable rendering pipeline to automatically extract facial expression labels from 18k-subject dataset captured with mobile phone + custom VR headset.

Result: Developed system that accurately tracks facial expressions in real-time using egocentric infrared camera views, enabling users to drive virtual character expressions without calibration.

Conclusion: System opens new avenues for communication in virtual environments with applications in video conferencing, gaming, entertainment, and remote collaboration.

Abstract: We present a novel system for real-time tracking of facial expressions using egocentric views captured from a set of infrared cameras embedded in a virtual reality (VR) headset. Our technology facilitates any user to accurately drive the facial expressions of virtual characters in a non-intrusive manner and without the need of a lengthy calibration step. At the core of our system is a distillation based approach to train a machine learning model on heterogeneous data and labels coming form multiple sources, \eg synthetic and real images. As part of our dataset, we collected 18k diverse subjects using a lightweight capture setup consisting of a mobile phone and a custom VR headset with extra cameras. To process this data, we developed a robust differentiable rendering pipeline enabling us to automatically extract facial expression labels. Our system opens up new avenues for communication and expression in virtual environments, with applications in video conferencing, gaming, entertainment, and remote collaboration.

</details>


### [24] [G2P: Gaussian-to-Point Attribute Alignment for Boundary-Aware 3D Semantic Segmentation](https://arxiv.org/abs/2601.03510)
*Hojun Song,Chae-yeong Song,Jeong-hun Hong,Chaewon Moon,Dong-hwi Kim,Gahyeon Kim,Soo Ye Kim,Yiyi Liao,Jaehyup Lee,Sang-hyo Park*

Main category: cs.CV

TL;DR: G2P transfers appearance-aware attributes from 3D Gaussian Splatting to point clouds for better semantic segmentation by addressing geometric ambiguity and improving boundary localization.


<details>
  <summary>Details</summary>
Motivation: Point clouds have sparse and irregular distributions with limited appearance evidence. Geometry-only features are insufficient to distinguish objects with similar shapes but different appearances (color, texture, material).

Method: Gaussian-to-Point (G2P) transfers appearance-aware attributes from 3D Gaussian Splatting to point clouds. It establishes point-wise correspondences to address misalignment between optimized Gaussians and original point geometry, uses Gaussian opacity attributes to resolve geometric ambiguity, and leverages Gaussian scale attributes for precise boundary localization.

Result: Achieves superior performance on standard benchmarks with significant improvements on geometrically challenging classes, all without any 2D or language supervision.

Conclusion: G2P enables more discriminative and appearance-consistent segmentation for point clouds by effectively transferring appearance-aware attributes from 3D Gaussian Splatting, overcoming limitations of geometry-only approaches.

Abstract: Semantic segmentation on point clouds is critical for 3D scene understanding. However, sparse and irregular point distributions provide limited appearance evidence, making geometry-only features insufficient to distinguish objects with similar shapes but distinct appearances (e.g., color, texture, material). We propose Gaussian-to-Point (G2P), which transfers appearance-aware attributes from 3D Gaussian Splatting to point clouds for more discriminative and appearance-consistent segmentation. Our G2P address the misalignment between optimized Gaussians and original point geometry by establishing point-wise correspondences. By leveraging Gaussian opacity attributes, we resolve the geometric ambiguity that limits existing models. Additionally, Gaussian scale attributes enable precise boundary localization in complex 3D scenes. Extensive experiments demonstrate that our approach achieves superior performance on standard benchmarks and shows significant improvements on geometrically challenging classes, all without any 2D or language supervision.

</details>


### [25] [Semantic Belief-State World Model for 3D Human Motion Prediction](https://arxiv.org/abs/2601.03517)
*Sarim Chaudhry*

Main category: cs.CV

TL;DR: SBWM reframes human motion prediction as latent dynamical simulation on the human body manifold using belief-state world models, enabling stable long-horizon rollouts with better uncertainty calibration.


<details>
  <summary>Details</summary>
Motivation: Traditional sequence regression approaches for human motion prediction suffer from compounding drift, mean-pose collapse, and poorly calibrated uncertainty when rolled forward beyond training, as they don't separate observation reconstruction from dynamics modeling or represent latent causes of motion.

Method: Proposes Semantic Belief-State World Model (SBWM) that maintains a recurrent probabilistic belief state whose evolution is learned independently of pose reconstruction and explicitly aligned with SMPL-X anatomical parameterization. Uses stochastic latent transitions and rollout-centric training adapted from model-based reinforcement learning.

Result: Demonstrates coherent long-horizon rollouts and competitive accuracy at substantially lower computational cost compared to RSSM-based, transformer, and diffusion approaches optimized for reconstruction fidelity.

Conclusion: Treating the human body as part of the world model's state space rather than its output fundamentally changes how motion is simulated and predicted, enabling stable forward simulation with better dynamics representation.

Abstract: Human motion prediction has traditionally been framed as a sequence regression problem where models extrapolate future joint coordinates from observed pose histories. While effective over short horizons this approach does not separate observation reconstruction with dynamics modeling and offers no explicit representation of the latent causes governing motion. As a result, existing methods exhibit compounding drift, mean-pose collapse, and poorly calibrated uncertainty when rolled forward beyond the training regime. Here we propose a Semantic Belief-State World Model (SBWM) that reframes human motion prediction as latent dynamical simulation on the human body manifold. Rather than predicting poses directly, SBWM maintains a recurrent probabilistic belief state whose evolution is learned independently of pose reconstruction and explicitly aligned with the SMPL-X anatomical parameterization. This alignment imposes a structural information bottleneck that prevents the latent state from encoding static geometry or sensor noise, forcing it to capture motion dynamics, intent, and control-relevant structure. Inspired by belief-state world models developed for model-based reinforcement learning, SBWM adapts stochastic latent transitions and rollout-centric training to the domain of human motion. In contrast to RSSM-based, transformer, and diffusion approaches optimized for reconstruction fidelity, SBWM prioritizes stable forward simulation. We demonstrate coherent long-horizon rollouts, and competitive accuracy at substantially lower computational cost. These results suggest that treating the human body as part of the world models state space rather than its output fundamentally changes how motion is simulated, and predicted.

</details>


### [26] [Physics-Constrained Cross-Resolution Enhancement Network for Optics-Guided Thermal UAV Image Super-Resolution](https://arxiv.org/abs/2601.03526)
*Zhicheng Zhao,Fengjiao Peng,Jinquan Yan,Wei Lu,Chenglong Li,Jin Tang*

Main category: cs.CV

TL;DR: PCNet: A physics-constrained network for thermal UAV image super-resolution using optical guidance with cross-resolution mutual enhancement and thermal conduction modeling.


<details>
  <summary>Details</summary>
Motivation: Existing thermal UAV image super-resolution methods compress optical features, losing high-frequency information and causing physically inconsistent artifacts due to overlooking differences in imaging physics between optical and thermal modalities.

Method: Proposes PCNet with three key components: 1) Cross-Resolution Mutual Enhancement Module (CRME) for joint optimization of thermal super-resolution and optical-to-thermal conversion, 2) Physics-Driven Thermal Conduction Module (PDTM) incorporating 2D heat conduction into optical guidance, and 3) temperature consistency loss for regional distribution and boundary gradient constraints.

Result: Extensive experiments on VGTSR2.0 and DroneVehicle datasets show PCNet significantly outperforms state-of-the-art methods in both reconstruction quality and downstream tasks (semantic segmentation and object detection).

Conclusion: PCNet effectively addresses limitations of existing methods by enabling cross-resolution mutual enhancement while physically constraining optical guidance through thermal conduction, resulting in robust thermal UAV image super-resolution that preserves high-frequency information and avoids physically inconsistent artifacts.

Abstract: Optics-guided thermal UAV image super-resolution has attracted significant research interest due to its potential in all-weather monitoring applications. However, existing methods typically compress optical features to match thermal feature dimensions for cross-modal alignment and fusion, which not only causes the loss of high-frequency information that is beneficial for thermal super-resolution, but also introduces physically inconsistent artifacts such as texture distortions and edge blurring by overlooking differences in the imaging physics between modalities. To address these challenges, we propose PCNet to achieve cross-resolution mutual enhancement between optical and thermal modalities, while physically constraining the optical guidance process via thermal conduction to enable robust thermal UAV image super-resolution. In particular, we design a Cross-Resolution Mutual Enhancement Module (CRME) to jointly optimize thermal image super-resolution and optical-to-thermal modality conversion, facilitating effective bidirectional feature interaction across resolutions while preserving high-frequency optical priors. Moreover, we propose a Physics-Driven Thermal Conduction Module (PDTM) that incorporates two-dimensional heat conduction into optical guidance, modeling spatially-varying heat conduction properties to prevent inconsistent artifacts. In addition, we introduce a temperature consistency loss that enforces regional distribution consistency and boundary gradient smoothness to ensure generated thermal images align with real-world thermal radiation principles. Extensive experiments on VGTSR2.0 and DroneVehicle datasets demonstrate that PCNet significantly outperforms state-of-the-art methods on both reconstruction quality and downstream tasks including semantic segmentation and object detection.

</details>


### [27] [CloudMatch: Weak-to-Strong Consistency Learning for Semi-Supervised Cloud Detection](https://arxiv.org/abs/2601.03528)
*Jiayi Zhao,Changlu Chen,Jingsheng Li,Tianxiang Xue,Kun Zhan*

Main category: cs.CV

TL;DR: CloudMatch: A semi-supervised framework for cloud detection using view-consistency learning with scene-mixing augmentations to leverage unlabeled remote sensing imagery.


<details>
  <summary>Details</summary>
Motivation: Pixel-level annotation for cloud detection is expensive, creating a need for semi-supervised approaches that can effectively utilize abundant unlabeled remote sensing data.

Method: Uses view-consistency learning with three augmented views per unlabeled image: one weakly augmented view plus two strongly augmented views with inter-scene mixing (for contextual variety) and intra-scene mixing (for semantic coherence). This guides pseudolabel generation and captures cloud pattern diversity.

Result: Extensive experiments show CloudMatch achieves good performance, demonstrating efficient utilization of unlabeled data and advancing semi-supervised cloud detection.

Conclusion: CloudMatch effectively leverages unlabeled remote sensing imagery through view-consistency learning with scene-mixing augmentations, enabling better capture of cloud pattern diversity and advancing semi-supervised cloud detection.

Abstract: Due to the high cost of annotating accurate pixel-level labels, semi-supervised learning has emerged as a promising approach for cloud detection. In this paper, we propose CloudMatch, a semi-supervised framework that effectively leverages unlabeled remote sensing imagery through view-consistency learning combined with scene-mixing augmentations. An observation behind CloudMatch is that cloud patterns exhibit structural diversity and contextual variability across different scenes and within the same scene category. Our key insight is that enforcing prediction consistency across diversely augmented views, incorporating both inter-scene and intra-scene mixing, enables the model to capture the structural diversity and contextual richness of cloud patterns. Specifically, CloudMatch generates one weakly augmented view along with two complementary strongly augmented views for each unlabeled image: one integrates inter-scene patches to simulate contextual variety, while the other employs intra-scene mixing to preserve semantic coherence. This approach guides pseudolabel generation and enhances generalization. Extensive experiments show that CloudMatch achieves good performance, demonstrating its capability to utilize unlabeled data efficiently and advance semi-supervised cloud detection.

</details>


### [28] [EASLT: Emotion-Aware Sign Language Translation](https://arxiv.org/abs/2601.03549)
*Guobin Tu,Di Weng*

Main category: cs.CV

TL;DR: EASLT is an emotion-aware sign language translation framework that treats facial expressions as semantic anchors rather than auxiliary information, using emotional encoding and fusion to resolve ambiguities in gloss-free SLT.


<details>
  <summary>Details</summary>
Motivation: Current gloss-free SLT methods focus on manual gestures but overlook the semantic importance of facial expressions, leading to ambiguity when different concepts share identical manual articulations.

Method: EASLT incorporates a dedicated emotional encoder to capture continuous affective dynamics and uses an Emotion-Aware Fusion module to adaptively recalibrate spatio-temporal sign features based on affective context.

Result: Achieves state-of-the-art performance on PHOENIX14T (BLEU-4: 26.15, BLEURT: 61.0) and CSL-Daily (BLEU-4: 22.80, BLEURT: 57.8) benchmarks among gloss-free methods.

Conclusion: Explicitly modeling emotion effectively decouples affective semantics from manual dynamics, significantly enhancing translation fidelity in sign language translation.

Abstract: Sign Language Translation (SLT) is a complex cross-modal task requiring the integration of Manual Signals (MS) and Non-Manual Signals (NMS). While recent gloss-free SLT methods have made strides in translating manual gestures, they frequently overlook the semantic criticality of facial expressions, resulting in ambiguity when distinct concepts share identical manual articulations. To address this, we present **EASLT** (**E**motion-**A**ware **S**ign **L**anguage **T**ranslation), a framework that treats facial affect not as auxiliary information, but as a robust semantic anchor. Unlike methods that relegate facial expressions to a secondary role, EASLT incorporates a dedicated emotional encoder to capture continuous affective dynamics. These representations are integrated via a novel *Emotion-Aware Fusion* (EAF) module, which adaptively recalibrates spatio-temporal sign features based on affective context to resolve semantic ambiguities. Extensive evaluations on the PHOENIX14T and CSL-Daily benchmarks demonstrate that EASLT establishes advanced performance among gloss-free methods, achieving BLEU-4 scores of 26.15 and 22.80, and BLEURT scores of 61.0 and 57.8, respectively. Ablation studies confirm that explicitly modeling emotion effectively decouples affective semantics from manual dynamics, significantly enhancing translation fidelity. Code is available at https://github.com/TuGuobin/EASLT.

</details>


### [29] [SpatiaLoc: Leveraging Multi-Level Spatial Enhanced Descriptors for Cross-Modal Localization](https://arxiv.org/abs/2601.03579)
*Tianyi Shang,Pengjie Xu,Zhaojun Deng,Zhenyu Li,Zhicong Chen,Lijun Wu*

Main category: cs.CV

TL;DR: SpatiaLoc is a cross-modal localization framework that uses text descriptions and point clouds to localize robots, employing a coarse-to-fine strategy with Bezier curves for instance-level spatial modeling and frequency domain encoding for global relationships.


<details>
  <summary>Details</summary>
Motivation: Cross-modal localization using text and point clouds enables robots to localize themselves via natural language descriptions, which is crucial for autonomous navigation and human-robot interaction. Since objects often recur across modalities, spatial relationships become the most discriminative cues for accurate localization.

Method: SpatiaLoc uses a coarse-to-fine strategy: 1) Coarse stage: BEOSE (Bezier Enhanced Object Spatial Encoder) models instance-level spatial relationships using quadratic Bezier curves, and FAE (Frequency Aware Encoder) generates global-level spatial representations in frequency domain. 2) Fine stage: UGFL (Uncertainty Aware Gaussian Fine Localizer) regresses 2D positions by modeling predictions as Gaussian distributions with uncertainty-aware loss function.

Result: Extensive experiments on KITTI360Pose dataset demonstrate that SpatiaLoc significantly outperforms existing state-of-the-art (SOTA) methods in cross-modal localization performance.

Conclusion: SpatiaLoc effectively addresses cross-modal localization by emphasizing spatial relationships at both instance and global levels through a novel coarse-to-fine framework, achieving superior performance compared to existing methods.

Abstract: Cross-modal localization using text and point clouds enables robots to localize themselves via natural language descriptions, with applications in autonomous navigation and interaction between humans and robots. In this task, objects often recur across text and point clouds, making spatial relationships the most discriminative cues for localization. Given this characteristic, we present SpatiaLoc, a framework utilizing a coarse-to-fine strategy that emphasizes spatial relationships at both the instance and global levels. In the coarse stage, we introduce a Bezier Enhanced Object Spatial Encoder (BEOSE) that models spatial relationships at the instance level using quadratic Bezier curves. Additionally, a Frequency Aware Encoder (FAE) generates spatial representations in the frequency domain at the global level. In the fine stage, an Uncertainty Aware Gaussian Fine Localizer (UGFL) regresses 2D positions by modeling predictions as Gaussian distributions with a loss function aware of uncertainty. Extensive experiments on KITTI360Pose demonstrate that SpatiaLoc significantly outperforms existing state-of-the-art (SOTA) methods.

</details>


### [30] [Detecting AI-Generated Images via Distributional Deviations from Real Images](https://arxiv.org/abs/2601.03586)
*Yakun Niu,Yingjian Chen,Lei Zhang*

Main category: cs.CV

TL;DR: A new method for detecting AI-generated images using a Masking-based Pre-trained model Fine-Tuning strategy with Texture-Aware Masking that improves generalization to unseen generative models.


<details>
  <summary>Details</summary>
Motivation: The rapid advancement of generative models has raised concerns about misinformation and erosion of public trust, making AI-generated image detection critical. Existing methods using frozen CLIP models treat the image encoder as a basic feature extractor and fail to fully exploit its potential for distinguishing real from AI-generated images.

Method: Proposes Masking-based Pre-trained model Fine-Tuning (MPFT) with Texture-Aware Masking (TAM) mechanism. During fine-tuning, TAM masks textured areas containing generative model-specific patterns, forcing CLIP-ViT to focus on "distributional deviations" from authentic images for detection.

Result: Extensive experiments on GenImage and UniversalFakeDetect datasets show the method achieves up to 98.2% and 94.6% average accuracy respectively, significantly outperforming existing approaches while requiring only minimal training images.

Conclusion: The proposed MPFT strategy with Texture-Aware Masking effectively enhances CLIP-ViT's ability to detect AI-generated images by focusing on distributional deviations, achieving superior generalization performance to unseen generative models with minimal training data.

Abstract: The rapid advancement of generative models has significantly enhanced the quality of AI-generated images, raising concerns about misinformation and the erosion of public trust. Detecting AI-generated images has thus become a critical challenge, particularly in terms of generalizing to unseen generative models. Existing methods using frozen pre-trained CLIP models show promise in generalization but treat the image encoder as a basic feature extractor, failing to fully exploit its potential. In this paper, we perform an in-depth analysis of the frozen CLIP image encoder (CLIP-ViT), revealing that it effectively clusters real images in a high-level, abstract feature space. However, it does not truly possess the ability to distinguish between real and AI-generated images. Based on this analysis, we propose a Masking-based Pre-trained model Fine-Tuning (MPFT) strategy, which introduces a Texture-Aware Masking (TAM) mechanism to mask textured areas containing generative model-specific patterns during fine-tuning. This approach compels CLIP-ViT to attend to the "distributional deviations"from authentic images for AI-generated image detection, thereby achieving enhanced generalization performance. Extensive experiments on the GenImage and UniversalFakeDetect datasets demonstrate that our method, fine-tuned with only a minimal number of images, significantly outperforms existing approaches, achieving up to 98.2% and 94.6% average accuracy on the two datasets, respectively.

</details>


### [31] [Can LLMs See Without Pixels? Benchmarking Spatial Intelligence from Textual Descriptions](https://arxiv.org/abs/2601.03590)
*Zhongbin Guo,Zhen Yang,Yushan Li,Xinyue Zhang,Wenyu Gao,Jiacheng Wang,Chengzhi Li,Xiangrui Liu,Ping Jian*

Main category: cs.CV

TL;DR: SiT-Bench is a novel benchmark for evaluating spatial intelligence in LLMs without visual inputs, using 3,800+ textual descriptions of spatial scenes to test symbolic reasoning across navigation, perspective transformation, and robotic manipulation tasks.


<details>
  <summary>Details</summary>
Motivation: To determine whether spatial understanding in Vision-Language Models originates from visual encoders or the fundamental reasoning backbone, and to evaluate LLMs' spatial intelligence without pixel-level visual inputs.

Method: Created SiT-Bench with 3,800+ expert-annotated items across 5 categories and 17 subtasks, converting single/multi-view scenes into high-fidelity, coordinate-aware textual descriptions to test LLMs' symbolic textual reasoning.

Result: SOTA LLMs show proficiency in localized semantic tasks but reveal a significant "spatial gap" in global consistency. Explicit spatial reasoning significantly boosts performance, indicating LLMs have latent world-modeling potential.

Conclusion: SiT-Bench serves as a foundational resource to develop spatially-grounded LLM backbones for future VLMs and embodied agents, showing that spatial reasoning capabilities exist in LLMs independent of visual encoders.

Abstract: Recent advancements in Spatial Intelligence (SI) have predominantly relied on Vision-Language Models (VLMs), yet a critical question remains: does spatial understanding originate from visual encoders or the fundamental reasoning backbone? Inspired by this question, we introduce SiT-Bench, a novel benchmark designed to evaluate the SI performance of Large Language Models (LLMs) without pixel-level input, comprises over 3,800 expert-annotated items across five primary categories and 17 subtasks, ranging from egocentric navigation and perspective transformation to fine-grained robotic manipulation. By converting single/multi-view scenes into high-fidelity, coordinate-aware textual descriptions, we challenge LLMs to perform symbolic textual reasoning rather than visual pattern matching. Evaluation results of state-of-the-art (SOTA) LLMs reveals that while models achieve proficiency in localized semantic tasks, a significant "spatial gap" remains in global consistency. Notably, we find that explicit spatial reasoning significantly boosts performance, suggesting that LLMs possess latent world-modeling potential. Our proposed dataset SiT-Bench serves as a foundational resource to foster the development of spatially-grounded LLM backbones for future VLMs and embodied agents. Our code and benchmark will be released at https://github.com/binisalegend/SiT-Bench .

</details>


### [32] [Adaptive Attention Distillation for Robust Few-Shot Segmentation under Environmental Perturbations](https://arxiv.org/abs/2601.03596)
*Qianyu Guo,Jingrong Wu,Jieji Ren,Weifeng Ge,Wenqiang Zhang*

Main category: cs.CV

TL;DR: Proposes environment-robust few-shot segmentation with Adaptive Attention Distillation to handle real-world challenges like motion blur, small objects, and camouflaged targets.


<details>
  <summary>Details</summary>
Motivation: Existing few-shot segmentation models fail in real-world scenarios due to complex environmental factors (illumination, background, viewpoint, etc.) that increase test difficulty. Models trained in lab conditions don't meet practical deployment needs.

Method: Introduces environment-robust FSS setting with challenging test cases. Proposes Adaptive Attention Distillation (AAD) that repeatedly contrasts and distills key shared semantics between support and query images to derive class-specific attention for novel categories.

Result: Establishes ER-FSS benchmark covering 8 datasets across multiple real-world scenarios. AAD improves mIoU by 3.3%-8.5% across all datasets and settings, demonstrating superior performance and strong generalization.

Conclusion: The proposed environment-robust FSS setting and Adaptive Attention Distillation method effectively enhance model robustness in complex real-world environments, bridging the gap between laboratory training and practical deployment.

Abstract: Few-shot segmentation (FSS) aims to rapidly learn novel class concepts from limited examples to segment specific targets in unseen images, and has been widely applied in areas such as medical diagnosis and industrial inspection. However, existing studies largely overlook the complex environmental factors encountered in real world scenarios-such as illumination, background, and camera viewpoint-which can substantially increase the difficulty of test images. As a result, models trained under laboratory conditions often fall short of practical deployment requirements. To bridge this gap, in this paper, an environment-robust FSS setting is introduced that explicitly incorporates challenging test cases arising from complex environments-such as motion blur, small objects, and camouflaged targets-to enhance model's robustness under realistic, dynamic conditions. An environment robust FSS benchmark (ER-FSS) is established, covering eight datasets across multiple real world scenarios. In addition, an Adaptive Attention Distillation (AAD) method is proposed, which repeatedly contrasts and distills key shared semantics between known (support) and unknown (query) images to derive class-specific attention for novel categories. This strengthens the model's ability to focus on the correct targets in complex environments, thereby improving environmental robustness. Comparative experiments show that AAD improves mIoU by 3.3% - 8.5% across all datasets and settings, demonstrating superior performance and strong generalization. The source code and dataset are available at: https://github.com/guoqianyu-alberta/Adaptive-Attention-Distillation-for-FSS.

</details>


### [33] [Unveiling Text in Challenging Stone Inscriptions: A Character-Context-Aware Patching Strategy for Binarization](https://arxiv.org/abs/2601.03609)
*Pratyush Jena,Amal Joseph,Arnav Sharma,Ravi Kiran Sarvadevabhatla*

Main category: cs.CV

TL;DR: A novel adaptive patching strategy with Attention U-Net for binarizing challenging stone inscription images, showing strong performance and zero-shot generalization across scripts.


<details>
  <summary>Details</summary>
Motivation: Stone inscription images present severe challenges for binarization due to poor contrast, non-uniform degradation, distracting artifacts, and variable text density/layouts, causing existing techniques to fail in isolating coherent character regions.

Method: Proposes a robust adaptive patching strategy for binarizing Indic inscriptions, using patches to train an Attention U-Net. The attention mechanism focuses on subtle structural cues, while dynamic sampling and patch selection help overcome surface noise and layout irregularities. Also introduces a carefully annotated dataset of Indic stone inscriptions.

Result: The novel patching mechanism significantly boosts binarization performance across classical and deep learning baselines. Despite training only on single script Indic dataset, the model exhibits strong zero-shot generalization to other Indic and non-Indic scripts, demonstrating robustness and script-agnostic generalization capabilities.

Conclusion: The method produces clean, structured representations of inscription content, laying the foundation for downstream tasks like script identification, OCR, and historical text analysis. The approach shows promise for handling challenging historical artifacts with poor preservation.

Abstract: Binarization is a popular first step towards text extraction in historical artifacts. Stone inscription images pose severe challenges for binarization due to poor contrast between etched characters and the stone background, non-uniform surface degradation, distracting artifacts, and highly variable text density and layouts. These conditions frequently cause existing binarization techniques to fail and struggle to isolate coherent character regions. Many approaches sub-divide the image into patches to improve text fragment resolution and improve binarization performance. With this in mind, we present a robust and adaptive patching strategy to binarize challenging Indic inscriptions. The patches from our approach are used to train an Attention U-Net for binarization. The attention mechanism allows the model to focus on subtle structural cues, while our dynamic sampling and patch selection method ensures that the model learns to overcome surface noise and layout irregularities. We also introduce a carefully annotated, pixel-precise dataset of Indic stone inscriptions at the character-fragment level. We demonstrate that our novel patching mechanism significantly boosts binarization performance across classical and deep learning baselines. Despite training only on single script Indic dataset, our model exhibits strong zero-shot generalization to other Indic and non-indic scripts, highlighting its robustness and script-agnostic generalization capabilities. By producing clean, structured representations of inscription content, our method lays the foundation for downstream tasks such as script identification, OCR, and historical text analysis. Project page: https://ihdia.iiit.ac.in/shilalekhya-binarization/

</details>


### [34] [Systematic Evaluation of Depth Backbones and Semantic Cues for Monocular Pseudo-LiDAR 3D Detection](https://arxiv.org/abs/2601.03617)
*Samson Oseiwe Ajadalu*

Main category: cs.CV

TL;DR: Monocular 3D detection study shows depth backbone choice (NeWCRFs vs Depth Anything V2) and geometric fidelity matter most, with semantic features providing only marginal gains.


<details>
  <summary>Details</summary>
Motivation: Monocular 3D object detection offers a low-cost alternative to LiDAR but suffers from inaccurate metric depth estimation from single images. The paper aims to systematically evaluate how different depth estimation backbones and feature engineering affect monocular pseudo-LiDAR pipelines.

Method: Systematically compares NeWCRFs (supervised metric depth) against Depth Anything V2 Metric-Outdoor (Base) under identical pseudo-LiDAR generation and PointRCNN detection protocol on KITTI validation split. Tests point-cloud augmentations using appearance cues (grayscale intensity) and semantic cues (instance segmentation confidence). Also reports depth-accuracy-versus-distance diagnostic using ground-truth 2D boxes.

Result: NeWCRFs yields stronger downstream 3D detection (10.50% APD at IoU=0.7 on Moderate split). Semantic features provide only marginal gains, and mask-based sampling can degrade performance by removing contextual geometry. Depth backbone choice and geometric fidelity dominate performance, outweighing secondary feature injection.

Conclusion: Under an off-the-shelf LiDAR detector, depth-backbone choice and geometric fidelity are the most critical factors for monocular 3D detection performance, while semantic feature injection provides limited benefits. Coarse depth correctness doesn't fully predict strict 3D IoU performance.

Abstract: Monocular 3D object detection offers a low-cost alternative to LiDAR, yet remains less accurate due to the difficulty of estimating metric depth from a single image. We systematically evaluate how depth backbones and feature engineering affect a monocular Pseudo-LiDAR pipeline on the KITTI validation split. Specifically, we compare NeWCRFs (supervised metric depth) against Depth Anything V2 Metric-Outdoor (Base) under an identical pseudo-LiDAR generation and PointRCNN detection protocol. NeWCRFs yields stronger downstream 3D detection, achieving 10.50\% AP$_{3D}$ at IoU$=0.7$ on the Moderate split using grayscale intensity (Exp~2). We further test point-cloud augmentations using appearance cues (grayscale intensity) and semantic cues (instance segmentation confidence). Contrary to the expectation that semantics would substantially close the gap, these features provide only marginal gains, and mask-based sampling can degrade performance by removing contextual geometry. Finally, we report a depth-accuracy-versus-distance diagnostic using ground-truth 2D boxes (including Ped/Cyc), highlighting that coarse depth correctness does not fully predict strict 3D IoU. Overall, under an off-the-shelf LiDAR detector, depth-backbone choice and geometric fidelity dominate performance, outweighing secondary feature injection.

</details>


### [35] [Shape Classification using Approximately Convex Segment Features](https://arxiv.org/abs/2601.03625)
*Bimal Kumar Ray*

Main category: cs.CV

TL;DR: Proposes a novel object classification method that eliminates the need for object alignment by sorting features from normalized boundary segments.


<details>
  <summary>Details</summary>
Motivation: Existing object classification techniques require object alignment to compute similarity, which can be computationally expensive and sensitive to orientation. The paper aims to develop an alignment-free approach.

Method: Normalizes object boundaries, segments them into approximately convex segments, sorts segments by length in descending order, and extracts a bag of features (segment length, extreme points, area, base, width) for similarity measurement.

Result: The method was tested on datasets and produced acceptable classification results without requiring object alignment.

Conclusion: The proposed feature-sorting approach successfully eliminates the alignment requirement in object classification while maintaining acceptable performance, offering a more robust and efficient alternative to traditional alignment-based methods.

Abstract: The existing object classification techniques based on descriptive features rely on object alignment to compute the similarity of objects for classification. This paper replaces the necessity of object alignment through sorting of feature. The object boundary is normalized and segmented into approximately convex segments and the segments are then sorted in descending order of their length. The segment length, number of extreme points in segments, area of segments, the base and the width of the segments - a bag of features - is used to measure the similarity between image boundaries. The proposed method is tested on datasets and acceptable results are observed.

</details>


### [36] [MFC-RFNet: A Multi-scale Guided Rectified Flow Network for Radar Sequence Prediction](https://arxiv.org/abs/2601.03633)
*Wenjie Luo,Chuanhu Deng,Chaorong Li,Rongyao Deng,Qiang Yang*

Main category: cs.CV

TL;DR: MFC-RFNet: A generative framework for radar precipitation nowcasting that integrates multi-scale feature communication with rectified flow training, wavelet-guided skip connections, and spatial alignment to improve accuracy and resolution.


<details>
  <summary>Details</summary>
Motivation: Accurate high-resolution precipitation nowcasting from radar echo sequences is crucial for disaster mitigation and economic planning, but faces challenges in modeling complex multi-scale evolution, correcting inter-frame feature misalignment from displacement, and efficiently capturing long-range spatiotemporal context without sacrificing spatial fidelity.

Method: Proposes MFC-RFNet with: 1) Wavelet-Guided Skip Connection (WGSC) to preserve high-frequency components, 2) Feature Communication Module (FCM) for bidirectional cross-scale interaction, 3) Condition-Guided Spatial Transform Fusion (CGSTF) to align shallow features using spatial transforms from conditioning echoes, 4) Rectified flow training for few-step sampling, and 5) Lightweight Vision-RWKV blocks at key positions to capture long-range dependencies efficiently.

Result: Evaluations on four public datasets (SEVIR, MeteoNet, Shanghai, and CIKM) show consistent improvements over strong baselines, yielding clearer echo morphology at higher rain-rate thresholds and sustained skill at longer lead times.

Conclusion: The synergy of rectified flow training with scale-aware communication, spatial alignment, and frequency-aware fusion presents an effective and robust approach for radar-based nowcasting.

Abstract: Accurate and high-resolution precipitation nowcasting from radar echo sequences is crucial for disaster mitigation and economic planning, yet it remains a significant challenge. Key difficulties include modeling complex multi-scale evolution, correcting inter-frame feature misalignment caused by displacement, and efficiently capturing long-range spatiotemporal context without sacrificing spatial fidelity. To address these issues, we present the Multi-scale Feature Communication Rectified Flow (RF) Network (MFC-RFNet), a generative framework that integrates multi-scale communication with guided feature fusion. To enhance multi-scale fusion while retaining fine detail, a Wavelet-Guided Skip Connection (WGSC) preserves high-frequency components, and a Feature Communication Module (FCM) promotes bidirectional cross-scale interaction. To correct inter-frame displacement, a Condition-Guided Spatial Transform Fusion (CGSTF) learns spatial transforms from conditioning echoes to align shallow features. The backbone adopts rectified flow training to learn near-linear probability-flow trajectories, enabling few-step sampling with stable fidelity. Additionally, lightweight Vision-RWKV (RWKV) blocks are placed at the encoder tail, the bottleneck, and the first decoder layer to capture long-range spatiotemporal dependencies at low spatial resolutions with moderate compute. Evaluations on four public datasets (SEVIR, MeteoNet, Shanghai, and CIKM) demonstrate consistent improvements over strong baselines, yielding clearer echo morphology at higher rain-rate thresholds and sustained skill at longer lead times. These results suggest that the proposed synergy of RF training with scale-aware communication, spatial alignment, and frequency-aware fusion presents an effective and robust approach for radar-based nowcasting.

</details>


### [37] [CrackSegFlow: Controllable Flow-Matching Synthesis for Generalizable Crack Segmentation with the CSF-50K Benchmark](https://arxiv.org/abs/2601.03637)
*Babak Asadi,Peiyang Wu,Mani Golparvar-Fard,Ramez Hajj*

Main category: cs.CV

TL;DR: CrackSegFlow is a controllable flow-matching framework that generates photorealistic crack images with pixel-accurate masks to address data scarcity and domain shift in automated crack segmentation.


<details>
  <summary>Details</summary>
Motivation: Automated crack segmentation faces practical deployment limitations due to scarce pixel-level labels and severe domain shift across different sensors, illumination conditions, textures, and annotation conventions.

Method: A controllable flow-matching synthesis framework with two components: (1) generator combining topology-preserving mask injection with boundary-gated modulation to maintain thin-structure continuity, (2) class-conditional flow-matching model to synthesize crack masks with explicit control over crack coverage. Also injects crack masks into crack-free backgrounds to diversify illumination and reduce false positives.

Result: Consistent improvements across five benchmarks: in-domain performance improved by average 5.37 mIoU and 5.13 F1; cross-domain synthesis yielded average gains of 13.12 mIoU and 14.82 F1. Provides faster deterministic sampling than diffusion-based methods with better fidelity and mask-image alignment. Releases CSF-50K dataset of 50,000 paired crack images and masks.

Conclusion: CrackSegFlow effectively addresses data scarcity and domain shift in crack segmentation through controllable synthesis, enabling practical deployment with improved performance and releasing a large-scale benchmark dataset for future research.

Abstract: Automated crack segmentation is essential for scalable condition assessment of pavements and civil infrastructure, yet practical deployment is limited by scarce pixel-level labels and severe domain shift across sensors, illumination, textures, and annotation conventions. This paper presents CrackSegFlow, a controllable flow-matching synthesis framework that generates photorealistic crack images conditioned on binary masks while preserving strict mask-image alignment. The generator combines topology-preserving mask injection with boundary-gated modulation to maintain thin-structure continuity and suppress texture-driven false positives. A second class-conditional flow-matching model synthesizes crack masks with explicit control over crack coverage, enabling balanced, topology-diverse paired data without additional manual annotation. We further inject crack masks into crack-free backgrounds to diversify illumination and surface artifacts and reduce false positives caused by shadows, joints, and pavement markings. Experiments on five benchmarks spanning four asphalt datasets and the crack class of a concrete-domain dataset demonstrate consistent improvements under an established hybrid CNN--Transformer segmentation backbone and a fixed training protocol. With real plus synthesized pairs, in-domain performance improves on average by 5.37 mIoU and 5.13 F1, and target-guided cross-domain synthesis yields average gains of 13.12 mIoU and 14.82 F1 using only limited target mask statistics. Compared with diffusion-based semantic synthesis, CrackSegFlow provides substantially faster deterministic sampling and improves fidelity and mask-image alignment for thin-structure crack geometry. Finally, we release CSF-50K, a public dataset of 50,000 paired crack images and pixel-accurate masks for large-scale benchmarking of generalizable crack segmentation.

</details>


### [38] [VideoMemory: Toward Consistent Video Generation via Memory Integration](https://arxiv.org/abs/2601.03655)
*Jinsong Zhou,Yihua Du,Xinli Xu,Luozhou Wang,Zijie Zhuang,Yehang Zhang,Shuaibo Li,Xiaojun Hu,Bolan Su,Ying-cong Chen*

Main category: cs.CV

TL;DR: VideoMemory is an entity-centric framework for consistent narrative video generation using a Dynamic Memory Bank to preserve character, prop, and environment identity across multiple shots and temporal gaps.


<details>
  <summary>Details</summary>
Motivation: Existing video generation models produce high-quality short clips but fail to maintain consistent entity identity and appearance when scenes change or when entities reappear after long temporal gaps, which is crucial for narrative storytelling.

Method: Uses a multi-agent system to decompose narratives into shots, retrieves entity representations from a Dynamic Memory Bank (storing visual and semantic descriptors), synthesizes keyframes/videos conditioned on retrieved states, and updates memory after each shot to reflect story-driven changes while preserving identity.

Result: VideoMemory achieves strong entity-level coherence and high perceptual quality across diverse narrative sequences, as demonstrated on a 54-case multi-shot consistency benchmark covering character-, prop-, and background-persistent scenarios.

Conclusion: The Dynamic Memory Bank with retrieval-update mechanism enables consistent portrayal of entities across distant shots and supports coherent long-form video generation, addressing a central challenge in narrative storytelling.

Abstract: Maintaining consistent characters, props, and environments across multiple shots is a central challenge in narrative video generation. Existing models can produce high-quality short clips but often fail to preserve entity identity and appearance when scenes change or when entities reappear after long temporal gaps. We present VideoMemory, an entity-centric framework that integrates narrative planning with visual generation through a Dynamic Memory Bank. Given a structured script, a multi-agent system decomposes the narrative into shots, retrieves entity representations from memory, and synthesizes keyframes and videos conditioned on these retrieved states. The Dynamic Memory Bank stores explicit visual and semantic descriptors for characters, props, and backgrounds, and is updated after each shot to reflect story-driven changes while preserving identity. This retrieval-update mechanism enables consistent portrayal of entities across distant shots and supports coherent long-form generation. To evaluate this setting, we construct a 54-case multi-shot consistency benchmark covering character-, prop-, and background-persistent scenarios. Extensive experiments show that VideoMemory achieves strong entity-level coherence and high perceptual quality across diverse narrative sequences.

</details>


### [39] [MGPC: Multimodal Network for Generalizable Point Cloud Completion With Modality Dropout and Progressive Decoding](https://arxiv.org/abs/2601.03660)
*Jiangyuan Liu,Hongxuan Ma,Yuhao Zhao,Zhe Liu,Jian Wang,Wei Zou*

Main category: cs.CV

TL;DR: MGPC is a multimodal point cloud completion framework that integrates point clouds, RGB images, and text to improve generalization to novel objects and real-world scenarios.


<details>
  <summary>Details</summary>
Motivation: Existing point cloud completion methods (3D CNN-based, point-based, Transformer-based) have limitations in modality, scalability, and generative capacity, making them struggle with generalization to novel objects and real-world scenarios.

Method: MGPC integrates point clouds, RGB images, and text in a unified architecture with modality dropout strategy, Transformer-based fusion module, and progressive generator. Also created MGPC-1M benchmark with 1,000+ categories and 1M training pairs.

Result: Extensive experiments on MGPC-1M and in-the-wild data show MGPC consistently outperforms prior baselines and exhibits strong generalization under real-world conditions.

Conclusion: MGPC provides a robust, scalable multimodal framework for point cloud completion that addresses generalization challenges in real-world applications.

Abstract: Point cloud completion aims to recover complete 3D geometry from partial observations caused by limited viewpoints and occlusions. Existing learning-based works, including 3D Convolutional Neural Network (CNN)-based, point-based, and Transformer-based methods, have achieved strong performance on synthetic benchmarks. However, due to the limitations of modality, scalability, and generative capacity, their generalization to novel objects and real-world scenarios remains challenging. In this paper, we propose MGPC, a generalizable multimodal point cloud completion framework that integrates point clouds, RGB images, and text within a unified architecture. MGPC introduces an innovative modality dropout strategy, a Transformer-based fusion module, and a novel progressive generator to improve robustness, scalability, and geometric modeling capability. We further develop an automatic data generation pipeline and construct MGPC-1M, a large-scale benchmark with over 1,000 categories and one million training pairs. Extensive experiments on MGPC-1M and in-the-wild data demonstrate that the proposed method consistently outperforms prior baselines and exhibits strong generalization under real-world conditions.

</details>


### [40] [PhysVideoGenerator: Towards Physically Aware Video Generation via Latent Physics Guidance](https://arxiv.org/abs/2601.03665)
*Siddarth Nilol Kundur Satish,Devesh Jaiswal,Hongyu Chen,Abhishek Bakshi*

Main category: cs.CV

TL;DR: PhysVideoGenerator embeds learnable physics priors into video generation to address unnatural physics artifacts in current models.


<details>
  <summary>Details</summary>
Motivation: Current video generation models produce high-quality aesthetic videos but struggle with real-world physics dynamics, resulting in artifacts like unnatural object collisions, inconsistent gravity, and temporal flickering.

Method: Proposes PhysVideoGenerator framework with lightweight predictor network (PredictorP) that regresses physical features from V-JEPA 2 from diffusion latents, injecting predicted physics tokens into temporal attention layers of DiT-based generator via cross-attention.

Result: Demonstrates technical feasibility: diffusion latents contain sufficient information to recover V-JEPA 2 physical representations, and multi-task optimization remains stable over training.

Conclusion: Establishes foundation for future large-scale evaluation of physics-aware generative models through documented architectural design, technical challenges, and validation of training stability.

Abstract: Current video generation models produce high-quality aesthetic videos but often struggle to learn representations of real-world physics dynamics, resulting in artifacts such as unnatural object collisions, inconsistent gravity, and temporal flickering. In this work, we propose PhysVideoGenerator, a proof-of-concept framework that explicitly embeds a learnable physics prior into the video generation process. We introduce a lightweight predictor network, PredictorP, which regresses high-level physical features extracted from a pre-trained Video Joint Embedding Predictive Architecture (V-JEPA 2) directly from noisy diffusion latents. These predicted physics tokens are injected into the temporal attention layers of a DiT-based generator (Latte) via a dedicated cross-attention mechanism. Our primary contribution is demonstrating the technical feasibility of this joint training paradigm: we show that diffusion latents contain sufficient information to recover V-JEPA 2 physical representations, and that multi-task optimization remains stable over training. This report documents the architectural design, technical challenges, and validation of training stability, establishing a foundation for future large-scale evaluation of physics-aware generative models.

</details>


### [41] [TRec: Egocentric Action Recognition using 2D Point Tracks](https://arxiv.org/abs/2601.03667)
*Dennis Holzmann,Sven Wachsmuth*

Main category: cs.CV

TL;DR: Using 2D point tracks from random image points as motion cues improves egocentric action recognition without needing hand/object detection.


<details>
  <summary>Details</summary>
Motivation: Most existing egocentric action recognition methods rely on RGB appearance, human pose estimation, or their combination, but there's potential in using simpler motion cues from point tracking.

Method: Track randomly sampled image points across video frames using CoTracker, then use resulting trajectories with image frames as input to a Transformer-based recognition model. No hand/object detection needed.

Result: Method achieves notable performance gains even with only initial frame and point tracks (no full video). Integrating 2D point tracks consistently enhances performance compared to same model without motion information.

Conclusion: 2D point tracks serve as a lightweight yet effective representation for egocentric action understanding, offering substantial accuracy improvements without complex hand/object detection.

Abstract: We present a novel approach for egocentric action recognition that leverages 2D point tracks as an additional motion cue. While most existing methods rely on RGB appearance, human pose estimation, or their combination, our work demonstrates that tracking randomly sampled image points across video frames can substantially improve recognition accuracy. Unlike prior approaches, we do not detect hands, objects, or interaction regions. Instead, we employ CoTracker to follow a set of randomly initialized points through each video and use the resulting trajectories, together with the corresponding image frames, as input to a Transformer-based recognition model. Surprisingly, our method achieves notable gains even when only the initial frame and its associated point tracks are provided, without incorporating the full video sequence. Experimental results confirm that integrating 2D point tracks consistently enhances performance compared to the same model trained without motion information, highlighting their potential as a lightweight yet effective representation for egocentric action understanding.

</details>


### [42] [BREATH-VL: Vision-Language-Guided 6-DoF Bronchoscopy Localization via Semantic-Geometric Fusion](https://arxiv.org/abs/2601.03713)
*Qingyao Tian,Bingyu Yang,Huai Liao,Xinyan Huang,Junyong Li,Dong Yi,Hongbin Liu*

Main category: cs.CV

TL;DR: BREATH-VL: A hybrid vision-language framework for 6-DoF endoscopic camera localization that combines semantic understanding from VLMs with geometric information from vision-based registration, achieving state-of-the-art accuracy with reduced computational latency.


<details>
  <summary>Details</summary>
Motivation: Address three key challenges in applying vision-language models to endoscopic camera localization: 1) lack of large-scale medical vision-language datasets, 2) limited fine-grained pose regression capability, and 3) high computational latency for temporal feature extraction.

Method: 1) Construct BREATH dataset - largest in-vivo endoscopic localization dataset; 2) Develop BREATH-VL hybrid framework integrating semantic cues from VLMs with geometric information from vision-based registration; 3) Introduce lightweight context-learning mechanism that encodes motion history as linguistic prompts for efficient temporal reasoning.

Result: BREATH-VL outperforms state-of-the-art vision-only localization methods in accuracy and generalization, reducing translational error by 25.5% compared to best-performing baseline while achieving competitive computational latency.

Conclusion: The hybrid vision-language approach successfully addresses key challenges in endoscopic camera localization by leveraging complementary strengths of semantic understanding and geometric alignment, demonstrating superior performance in complex surgical environments.

Abstract: Vision-language models (VLMs) have recently shown remarkable performance in navigation and localization tasks by leveraging large-scale pretraining for semantic understanding. However, applying VLMs to 6-DoF endoscopic camera localization presents several challenges: 1) the lack of large-scale, high-quality, densely annotated, and localization-oriented vision-language datasets in real-world medical settings; 2) limited capability for fine-grained pose regression; and 3) high computational latency when extracting temporal features from past frames. To address these issues, we first construct BREATH dataset, the largest in-vivo endoscopic localization dataset to date, collected in the complex human airway. Building on this dataset, we propose BREATH-VL, a hybrid framework that integrates semantic cues from VLMs with geometric information from vision-based registration methods for accurate 6-DoF pose estimation. Our motivation lies in the complementary strengths of both approaches: VLMs offer generalizable semantic understanding, while registration methods provide precise geometric alignment. To further enhance the VLM's ability to capture temporal context, we introduce a lightweight context-learning mechanism that encodes motion history as linguistic prompts, enabling efficient temporal reasoning without expensive video-level computation. Extensive experiments demonstrate that the vision-language module delivers robust semantic localization in challenging surgical scenes. Building on this, our BREATH-VL outperforms state-of-the-art vision-only localization methods in both accuracy and generalization, reducing translational error by 25.5% compared with the best-performing baseline, while achieving competitive computational latency.

</details>


### [43] [Towards Real-world Lens Active Alignment with Unlabeled Data via Domain Adaptation](https://arxiv.org/abs/2601.03718)
*Wenyong Lia,Qi Jiang,Weijian Hu,Kailun Yang,Zhanjun Zhang,Wenjun Tian,Kaiwei Wang,Jian Bai*

Main category: cs.CV

TL;DR: DA3 uses domain adaptation with minimal unlabeled real data to bridge simulation-reality gap for active alignment in optical systems, achieving 46% accuracy improvement over simulation-only methods while reducing data collection time by 98.7%.


<details>
  <summary>Details</summary>
Motivation: Active alignment is crucial for automated assembly of high-precision optical systems, but simulation-trained models suffer from domain gaps when applied to real-world images due to complex imaging conditions. Traditional per-model calibration is labor-intensive, creating a need for efficient domain adaptation methods.

Method: Proposes Domain Adaptive Active Alignment (DA3) with two key components: 1) an autoregressive domain transformation generator, and 2) an adversarial-based feature alignment strategy for self-supervised learning. This extracts domain-invariant image degradation features to enable robust misalignment prediction using minimal unlabeled real-world images.

Result: DA3 improves accuracy by 46% over purely simulation-based pipelines. It approaches performance of precisely labeled real-world data from 3 lens samples while reducing on-device data collection time by 98.7%. Validated on two lens types.

Conclusion: Domain adaptation effectively enables simulation-trained models to perform robustly in real-world settings, validating digital-twin pipelines as practical solutions for enhancing efficiency in large-scale optical assembly by bridging the simulation-reality gap.

Abstract: Active Alignment (AA) is a key technology for the large-scale automated assembly of high-precision optical systems. Compared with labor-intensive per-model on-device calibration, a digital-twin pipeline built on optical simulation offers a substantial advantage in generating large-scale labeled data. However, complex imaging conditions induce a domain gap between simulation and real-world images, limiting the generalization of simulation-trained models. To address this, we propose augmenting a simulation baseline with minimal unlabeled real-world images captured at random misalignment positions, mitigating the gap from a domain adaptation perspective. We introduce Domain Adaptive Active Alignment (DA3), which utilizes an autoregressive domain transformation generator and an adversarial-based feature alignment strategy to distill real-world domain information via self-supervised learning. This enables the extraction of domain-invariant image degradation features to facilitate robust misalignment prediction. Experiments on two lens types reveal that DA3 improves accuracy by 46% over a purely simulation pipeline. Notably, it approaches the performance achieved with precisely labeled real-world data collected on 3 lens samples, while reducing on-device data collection time by 98.7%. The results demonstrate that domain adaptation effectively endows simulation-trained models with robust real-world performance, validating the digital-twin pipeline as a practical solution to significantly enhance the efficiency of large-scale optical assembly.

</details>


### [44] [CSMCIR: CoT-Enhanced Symmetric Alignment with Memory Bank for Composed Image Retrieval](https://arxiv.org/abs/2601.03728)
*Zhipeng Qian,Zihan Liang,Yufei Ma,Ben Chen,Huangyu Dai,Yiwei Ma,Jiayi Ji,Chenyi Lei,Han Li,Xiaoshuai Sun*

Main category: cs.CV

TL;DR: CSMCIR proposes a unified representation framework for Composed Image Retrieval that addresses representation space fragmentation through symmetric architecture, multi-level prompting, and dynamic memory bank.


<details>
  <summary>Details</summary>
Motivation: Existing CIR methods suffer from representation space fragmentation where queries and targets use heterogeneous modalities processed by different encoders, creating misaligned representation spaces that limit retrieval performance.

Method: Three synergistic components: 1) Multi-level Chain-of-Thought prompting for generating discriminative captions, 2) Symmetric dual-tower architecture with shared-parameter Q-Former for consistent encoding, 3) Entropy-based temporally dynamic Memory Bank for high-quality negative samples.

Result: Achieves state-of-the-art performance on four benchmark datasets with superior training efficiency. Comprehensive ablation studies validate each component's effectiveness.

Conclusion: CSMCIR successfully addresses representation space fragmentation in CIR through architectural symmetry and unified representation learning, demonstrating improved retrieval performance and efficiency.

Abstract: Composed Image Retrieval (CIR) enables users to search for target images using both a reference image and manipulation text, offering substantial advantages over single-modality retrieval systems. However, existing CIR methods suffer from representation space fragmentation: queries and targets comprise heterogeneous modalities and are processed by distinct encoders, forcing models to bridge misaligned representation spaces only through post-hoc alignment, which fundamentally limits retrieval performance. This architectural asymmetry manifests as three distinct, well-separated clusters in the feature space, directly demonstrating how heterogeneous modalities create fundamentally misaligned representation spaces from initialization. In this work, we propose CSMCIR, a unified representation framework that achieves efficient query-target alignment through three synergistic components. First, we introduce a Multi-level Chain-of-Thought (MCoT) prompting strategy that guides Multimodal Large Language Models to generate discriminative, semantically compatible captions for target images, establishing modal symmetry. Building upon this, we design a symmetric dual-tower architecture where both query and target sides utilize the identical shared-parameter Q-Former for cross-modal encoding, ensuring consistent feature representations and further reducing the alignment gap. Finally, this architectural symmetry enables an entropy-based, temporally dynamic Memory Bank strategy that provides high-quality negative samples while maintaining consistency with the evolving model state. Extensive experiments on four benchmark datasets demonstrate that our CSMCIR achieves state-of-the-art performance with superior training efficiency. Comprehensive ablation studies further validate the effectiveness of each proposed component.

</details>


### [45] [MATANet: A Multi-context Attention and Taxonomy-Aware Network for Fine-Grained Underwater Recognition of Marine Species](https://arxiv.org/abs/2601.03729)
*Donghwan Lee,Byeongjin Kim,Geunhee Kim,Hyukjin Kwon,Nahyeon Maeng,Wooju Kim*

Main category: cs.CV

TL;DR: MATANet is a novel model for fine-grained marine species classification that uses multi-context environmental attention and taxonomic hierarchy encoding to improve classification accuracy.


<details>
  <summary>Details</summary>
Motivation: Existing methods for marine animal classification often overlook contextual interactions from the surrounding environment and insufficiently incorporate the hierarchical structure of marine biological taxonomy, limiting their effectiveness for fine-grained classification tasks.

Method: MATANet consists of two key components: 1) Multi-Context Environmental Attention Module (MCEAM) that learns relationships between regions of interest (ROIs) and their surrounding environments, and 2) Hierarchical Separation-Induced Learning Module (HSLM) that encodes taxonomic hierarchy into the feature space. The model combines instance features, environmental context, and taxonomic structure.

Result: Experiments on FathomNet2025, FAIR1M, and LifeCLEF2015-Fish datasets demonstrate state-of-the-art performance in fine-grained marine species classification.

Conclusion: MATANet effectively addresses the limitations of existing methods by incorporating environmental context and taxonomic hierarchy, mimicking expert strategies for interpreting ambiguous features of underwater animals, and achieving superior classification performance for marine species.

Abstract: Fine-grained classification of marine animals supports ecology, biodiversity and habitat conservation, and evidence-based policy-making. However, existing methods often overlook contextual interactions from the surrounding environment and insufficiently incorporate the hierarchical structure of marine biological taxonomy. To address these challenges, we propose MATANet (Multi-context Attention and Taxonomy-Aware Network), a novel model designed for fine-grained marine species classification. MATANet mimics expert strategies by using taxonomy and environmental context to interpret ambiguous features of underwater animals. It consists of two key components: a Multi-Context Environmental Attention Module (MCEAM), which learns relationships between regions of interest (ROIs) and their surrounding environments, and a Hierarchical Separation-Induced Learning Module (HSLM), which encodes taxonomic hierarchy into the feature space. MATANet combines instance and environmental features with taxonomic structure to enhance fine-grained classification. Experiments on the FathomNet2025, FAIR1M, and LifeCLEF2015-Fish datasets demonstrate state-of-the-art performance. The source code is available at: https://github.com/dhlee-work/fathomnet-cvpr2025-ssl

</details>


### [46] [RadDiff: Describing Differences in Radiology Image Sets with Natural Language](https://arxiv.org/abs/2601.03733)
*Xiaoxian Shen,Yuhui Zhang,Sahithi Ankireddy,Xiaohan Wang,Maya Varma,Henry Guo,Curtis Langlotz,Serena Yeung-Levy*

Main category: cs.CV

TL;DR: RadDiff is a multimodal AI system that performs radiologist-style comparative reasoning to identify clinically meaningful differences between paired radiology studies, outperforming general-domain baselines by 47-50% on a new benchmark.


<details>
  <summary>Details</summary>
Motivation: Understanding differences between radiology image sets is critical for clinical insights and interpreting medical AI systems. Current methods lack the nuanced comparative reasoning that radiologists perform when analyzing study pairs.

Method: RadDiff uses a proposer-ranker framework with four innovations: (1) medical knowledge injection via domain-adapted vision-language models, (2) multimodal reasoning integrating images with clinical reports, (3) iterative hypothesis refinement across multiple reasoning rounds, and (4) targeted visual search that localizes and zooms on salient regions.

Result: On RadDiffBench (57 expert-validated study pairs), RadDiff achieves 47% accuracy (50% with ground-truth reports), significantly outperforming the general-domain VisDiff baseline. The system demonstrates versatility across clinical tasks including COVID-19 phenotype comparison, racial subgroup analysis, and survival-related feature discovery.

Conclusion: RadDiff and RadDiffBench provide the first method-and-benchmark foundation for systematically uncovering meaningful differences in radiological data, advancing comparative reasoning in medical AI systems.

Abstract: Understanding how two radiology image sets differ is critical for generating clinical insights and for interpreting medical AI systems. We introduce RadDiff, a multimodal agentic system that performs radiologist-style comparative reasoning to describe clinically meaningful differences between paired radiology studies. RadDiff builds on a proposer-ranker framework from VisDiff, and incorporates four innovations inspired by real diagnostic workflows: (1) medical knowledge injection through domain-adapted vision-language models; (2) multimodal reasoning that integrates images with their clinical reports; (3) iterative hypothesis refinement across multiple reasoning rounds; and (4) targeted visual search that localizes and zooms in on salient regions to capture subtle findings. To evaluate RadDiff, we construct RadDiffBench, a challenging benchmark comprising 57 expert-validated radiology study pairs with ground-truth difference descriptions. On RadDiffBench, RadDiff achieves 47% accuracy, and 50% accuracy when guided by ground-truth reports, significantly outperforming the general-domain VisDiff baseline. We further demonstrate RadDiff's versatility across diverse clinical tasks, including COVID-19 phenotype comparison, racial subgroup analysis, and discovery of survival-related imaging features. Together, RadDiff and RadDiffBench provide the first method-and-benchmark foundation for systematically uncovering meaningful differences in radiological data.

</details>


### [47] [HyperCOD: The First Challenging Benchmark and Baseline for Hyperspectral Camouflaged Object Detection](https://arxiv.org/abs/2601.03736)
*Shuyan Bai,Tingfa Xu,Peifu Liu,Yuhao Qiu,Huiyan Bai,Huan Chen,Yanyan Peng,Jianan Li*

Main category: cs.CV

TL;DR: The paper introduces HyperCOD, the first large-scale benchmark for hyperspectral camouflaged object detection (HCOD), and proposes HSC-SAM, a method that adapts the Segment Anything Model for HCOD by decoupling hyperspectral images into spatial and spectral components.


<details>
  <summary>Details</summary>
Motivation: RGB-based camouflaged object detection struggles in real-world scenarios with ambiguous color/texture cues. Hyperspectral imaging offers better spectral signatures, but progress in HCOD has been hampered by the lack of a dedicated, large-scale benchmark.

Method: Proposes HSC-SAM (HyperSpectral Camouflage-aware SAM) that reformulates hyperspectral images by decoupling them into: 1) a spatial map fed to SAM's image encoder, and 2) a spectral saliency map that serves as an adaptive prompt, effectively bridging the modality gap.

Result: HSC-SAM sets a new state-of-the-art on the HyperCOD benchmark and generalizes robustly to other public HSI datasets. The HyperCOD dataset contains 350 high-resolution hyperspectral images with complex real-world scenarios.

Conclusion: The HyperCOD dataset and HSC-SAM baseline provide a robust foundation to foster future research in hyperspectral camouflaged object detection, addressing the limitations of RGB-based approaches and leveraging foundation models like SAM.

Abstract: RGB-based camouflaged object detection struggles in real-world scenarios where color and texture cues are ambiguous. While hyperspectral image offers a powerful alternative by capturing fine-grained spectral signatures, progress in hyperspectral camouflaged object detection (HCOD) has been critically hampered by the absence of a dedicated, large-scale benchmark. To spur innovation, we introduce HyperCOD, the first challenging benchmark for HCOD. Comprising 350 high-resolution hyperspectral images, It features complex real-world scenarios with minimal objects, intricate shapes, severe occlusions, and dynamic lighting to challenge current models. The advent of foundation models like the Segment Anything Model (SAM) presents a compelling opportunity. To adapt the Segment Anything Model (SAM) for HCOD, we propose HyperSpectral Camouflage-aware SAM (HSC-SAM). HSC-SAM ingeniously reformulates the hyperspectral image by decoupling it into a spatial map fed to SAM's image encoder and a spectral saliency map that serves as an adaptive prompt. This translation effectively bridges the modality gap. Extensive experiments show that HSC-SAM sets a new state-of-the-art on HyperCOD and generalizes robustly to other public HSI datasets. The HyperCOD dataset and our HSC-SAM baseline provide a robust foundation to foster future research in this emerging area.

</details>


### [48] [I2E: From Image Pixels to Actionable Interactive Environments for Text-Guided Image Editing](https://arxiv.org/abs/2601.03741)
*Jinghan Yu,Junhao Xiao,Chenyu Zhu,Jiaming Li,Jia Li,HanMing Deng,Xirui Wang,Guoli Jia,Jianjun Li,Zhiyuan Ma,Xiang Bai,Bowen Zhou*

Main category: cs.CV

TL;DR: I2E introduces a "Decompose-then-Action" paradigm for compositional image editing, using object decomposition and physics-aware agents to handle complex multi-object spatial reasoning.


<details>
  <summary>Details</summary>
Motivation: Existing pixel-level inpainting methods struggle with compositional editing requiring precise local control and multi-object spatial reasoning. Limitations include implicit coupling of planning/execution, lack of object-level control granularity, and reliance on unstructured pixel-centric modeling.

Method: I2E uses a Decomposer to transform images into discrete, manipulable object layers, then employs a physics-aware Vision-Language-Action Agent that parses complex instructions into atomic actions via Chain-of-Thought reasoning. Also introduces I2E-Bench benchmark for multi-instance spatial reasoning.

Result: I2E significantly outperforms state-of-the-art methods on I2E-Bench and multiple public benchmarks in handling complex compositional instructions, maintaining physical plausibility, and ensuring multi-turn editing stability.

Conclusion: The "Decompose-then-Action" paradigm effectively addresses limitations of existing pixel-level editing methods by providing structured, object-level control and explicit planning for complex compositional image editing tasks.

Abstract: Existing text-guided image editing methods primarily rely on end-to-end pixel-level inpainting paradigm. Despite its success in simple scenarios, this paradigm still significantly struggles with compositional editing tasks that require precise local control and complex multi-object spatial reasoning. This paradigm is severely limited by 1) the implicit coupling of planning and execution, 2) the lack of object-level control granularity, and 3) the reliance on unstructured, pixel-centric modeling. To address these limitations, we propose I2E, a novel "Decompose-then-Action" paradigm that revisits image editing as an actionable interaction process within a structured environment. I2E utilizes a Decomposer to transform unstructured images into discrete, manipulable object layers and then introduces a physics-aware Vision-Language-Action Agent to parse complex instructions into a series of atomic actions via Chain-of-Thought reasoning. Further, we also construct I2E-Bench, a benchmark designed for multi-instance spatial reasoning and high-precision editing. Experimental results on I2E-Bench and multiple public benchmarks demonstrate that I2E significantly outperforms state-of-the-art methods in handling complex compositional instructions, maintaining physical plausibility, and ensuring multi-turn editing stability.

</details>


### [49] [MVP: Enhancing Video Large Language Models via Self-supervised Masked Video Prediction](https://arxiv.org/abs/2601.03781)
*Xiaokun Sun,Zezhong Wu,Zewen Ding,Linli Xu*

Main category: cs.CV

TL;DR: The paper proposes Masked Video Prediction (MVP), a novel post-training objective for VideoLLMs that focuses on temporal coherence and inter-frame correlations, addressing limitations of current methods that only optimize for holistic content understanding.


<details>
  <summary>Details</summary>
Motivation: Current reinforcement learning based post-training for VideoLLMs primarily targets holistic content understanding (captioning, VideoQA) but lacks explicit supervision for temporal coherence and inter-frame correlations, limiting models' ability to capture intricate dynamics and fine-grained visual causality.

Method: Proposes Masked Video Prediction (MVP) objective requiring models to reconstruct masked continuous segments from distractors; introduces scalable data synthesis pipeline to transform arbitrary video corpora into MVP samples; uses Group Relative Policy Optimization (GRPO) with fine-grained reward function.

Result: Comprehensive evaluations demonstrate that MVP enhances video reasoning capabilities by directly reinforcing temporal reasoning and causal understanding.

Conclusion: MVP effectively bridges the gap in temporal reasoning for VideoLLMs by explicitly supervising models to attend to sequential logic and temporal context, improving their ability to understand video dynamics and causality.

Abstract: Reinforcement learning based post-training paradigms for Video Large Language Models (VideoLLMs) have achieved significant success by optimizing for visual-semantic tasks such as captioning or VideoQA. However, while these approaches effectively enhance perception abilities, they primarily target holistic content understanding, often lacking explicit supervision for intrinsic temporal coherence and inter-frame correlations. This tendency limits the models' ability to capture intricate dynamics and fine-grained visual causality. To explicitly bridge this gap, we propose a novel post-training objective: Masked Video Prediction (MVP). By requiring the model to reconstruct a masked continuous segment from a set of challenging distractors, MVP forces the model to attend to the sequential logic and temporal context of events. To support scalable training, we introduce a scalable data synthesis pipeline capable of transforming arbitrary video corpora into MVP training samples, and further employ Group Relative Policy Optimization (GRPO) with a fine-grained reward function to enhance the model's understanding of video context and temporal properties. Comprehensive evaluations demonstrate that MVP enhances video reasoning capabilities by directly reinforcing temporal reasoning and causal understanding.

</details>


### [50] [A Comparative Study of 3D Model Acquisition Methods for Synthetic Data Generation of Agricultural Products](https://arxiv.org/abs/2601.03784)
*Steven Moonen,Rob Salaets,Kenneth Batstone,Abdellatif Bey-Temsamani,Nick Michiels*

Main category: cs.CV

TL;DR: Using synthetic data from 3D models (scanned or image-to-3D) instead of CAD files to train AI object detection for stone/potato separation in agriculture, with finetuning on small real datasets.


<details>
  <summary>Details</summary>
Motivation: AI vision systems need large training data, but real data is costly in agriculture where CAD models aren't available like in manufacturing. Need alternative synthetic data approaches.

Method: Develop techniques to substitute CAD files: use highly representative 3D models from scanning or image-to-3D approaches to generate synthetic datasets. Train object detection models and finetune on small real datasets.

Result: Scanned or image-to-3D models can generate effective synthetic training data. Finetuning with small real datasets significantly improves performance, achieving similar results even with less representative models.

Conclusion: Synthetic data from alternative 3D modeling approaches (not CAD) is viable for agricultural AI object detection, especially when combined with minimal real data finetuning.

Abstract: In the manufacturing industry, computer vision systems based on artificial intelligence (AI) are widely used to reduce costs and increase production. Training these AI models requires a large amount of training data that is costly to acquire and annotate, especially in high-variance, low-volume manufacturing environments. A popular approach to reduce the need for real data is the use of synthetic data that is generated by leveraging computer-aided design (CAD) models available in the industry. However, in the agricultural industry these models are not readily available, increasing the difficulty in leveraging synthetic data. In this paper, we present different techniques for substituting CAD files to create synthetic datasets. We measure their relative performance when used to train an AI object detection model to separate stones and potatoes in a bin picking environment. We demonstrate that using highly representative 3D models acquired by scanning or using image-to-3D approaches can be used to generate synthetic data for training object detection models. Finetuning on a small real dataset can significantly improve the performance of the models and even get similar performance when less representative models are used.

</details>


### [51] [From Brute Force to Semantic Insight: Performance-Guided Data Transformation Design with LLMs](https://arxiv.org/abs/2601.03808)
*Usha Shrestha,Dmitry Ignatov,Radu Timofte*

Main category: cs.CV

TL;DR: LLMs can autonomously engineer optimal code transformations by learning from empirical performance feedback without reinforcement learning or symbolic objectives, achieving 600x fewer evaluations than brute-force search.


<details>
  <summary>Details</summary>
Motivation: Current data-aware augmentation for code synthesis relies on heuristic design or brute-force approaches, which are inefficient and don't leverage LLMs' potential for autonomous optimization based on empirical performance.

Method: Fine-tune LLMs with Low-Rank Adaptation on a repository of 6,000+ PyTorch augmentation functions annotated by downstream accuracy. Use pairwise performance ordering (better-worse transformations) to align models with empirical feedback without reinforcement learning or symbolic objectives.

Result: Achieves up to 600x fewer evaluated candidates than brute-force discovery while maintaining competitive peak accuracy. The model internalizes semantic performance cues rather than memorizing syntax, and direct prompting outperforms Chain-of-Thought prompting which introduces syntactic noise.

Conclusion: LLMs can exhibit task-level reasoning through non-textual feedback loops, bypassing explicit symbolic rewards, enabling autonomous engineering of optimal code transformations based on empirical performance cues.

Abstract: Large language models (LLMs) have achieved notable performance in code synthesis; however, data-aware augmentation remains a limiting factor, handled via heuristic design or brute-force approaches. We introduce a performance-aware, closed-loop solution in the NNGPT ecosystem of projects that enables LLMs to autonomously engineer optimal transformations by internalizing empirical performance cues. We fine-tune LLMs with Low-Rank Adaptation on a novel repository of more than 6,000 empirically evaluated PyTorch augmentation functions, each annotated solely by downstream model accuracy. Training uses pairwise performance ordering (better-worse transformations), enabling alignment through empirical feedback without reinforcement learning, reward models, or symbolic objectives. This reduces the need for exhaustive search, achieving up to 600x times fewer evaluated candidates than brute-force discovery while maintaining competitive peak accuracy and shifting generation from random synthesis to task-aligned design. Ablation studies show that structured Chain-of-Thought prompting introduces syntactic noise and degrades performance, whereas direct prompting ensures stable optimization in performance-critical code tasks. Qualitative and quantitative analyses demonstrate that the model internalizes semantic performance cues rather than memorizing syntax. These results show that LLMs can exhibit task-level reasoning through non-textual feedback loops, bypassing explicit symbolic rewards.

</details>


### [52] [EvalBlocks: A Modular Pipeline for Rapidly Evaluating Foundation Models in Medical Imaging](https://arxiv.org/abs/2601.03811)
*Jan Tagscherer,Sarah de Boer,Lena Philipp,Fennie van der Graaf,Dr Peeters,Joeran Bosma,Lars Leijten,Bogdan Obreja,Ewoud Smit,Alessa Hering*

Main category: cs.CV

TL;DR: EvalBlocks is a modular framework built on Snakemake for efficient, reproducible evaluation of medical imaging foundation models, automating experiment tracking and enabling scalable parallel execution.


<details>
  <summary>Details</summary>
Motivation: Researchers developing medical imaging foundation models face burdensome manual workflows for tracking experiments and performance, which are slow, error-prone, and hinder innovation.

Method: Built on Snakemake workflow management system, EvalBlocks provides a modular plug-and-play framework that supports seamless integration of new datasets, foundation models, aggregation methods, and evaluation strategies with centralized tracking and efficient caching.

Result: Demonstrated on five state-of-the-art foundation models and three medical imaging classification tasks, EvalBlocks successfully streamlines model evaluation, enabling faster iteration and reproducible results with single-command execution.

Conclusion: EvalBlocks addresses the evaluation bottleneck in medical imaging foundation model development by providing an open-source framework that automates evaluation logistics, allowing researchers to focus on model innovation rather than manual tracking.

Abstract: Developing foundation models in medical imaging requires continuous monitoring of downstream performance. Researchers are burdened with tracking numerous experiments, design choices, and their effects on performance, often relying on ad-hoc, manual workflows that are inherently slow and error-prone. We introduce EvalBlocks, a modular, plug-and-play framework for efficient evaluation of foundation models during development. Built on Snakemake, EvalBlocks supports seamless integration of new datasets, foundation models, aggregation methods, and evaluation strategies. All experiments and results are tracked centrally and are reproducible with a single command, while efficient caching and parallel execution enable scalable use on shared compute infrastructure. Demonstrated on five state-of-the-art foundation models and three medical imaging classification tasks, EvalBlocks streamlines model evaluation, enabling researchers to iterate faster and focus on model innovation rather than evaluation logistics. The framework is released as open source software at https://github.com/DIAGNijmegen/eval-blocks.

</details>


### [53] [IDESplat: Iterative Depth Probability Estimation for Generalizable 3D Gaussian Splatting](https://arxiv.org/abs/2601.03824)
*Wei Long,Haifeng Wu,Shiyin Jiang,Jinhua Zhang,Xinchun Ji,Shuhang Gu*

Main category: cs.CV

TL;DR: IDESplat improves 3D Gaussian Splatting by iteratively boosting depth probability estimation through cascaded warp operations, enabling more accurate Gaussian mean prediction and better scene reconstruction.


<details>
  <summary>Details</summary>
Motivation: Existing methods for generalizable 3D Gaussian Splatting rely on single warp operations for depth estimation, which fails to fully leverage cross-view geometric cues and produces unstable, coarse depth maps, making Gaussian mean prediction difficult.

Method: Proposes IDESplat with Depth Probability Boosting Unit (DPBU) that integrates epipolar attention maps from cascaded warp operations multiplicatively, and stacks multiple DPBUs in an iterative process to progressively refine depth probability estimates and update depth candidates.

Result: Achieves state-of-the-art performance on RealEstate10K, ACID, and DL3DV datasets with real-time efficiency. Outperforms DepthSplat by 0.33 dB PSNR on RE10K using only 10.7% parameters and 70% memory, and improves by 2.95 dB PSNR on DTU in cross-dataset experiments.

Conclusion: IDESplat's iterative depth probability boosting approach significantly improves Gaussian mean prediction accuracy, leading to better reconstruction quality and strong generalization ability while maintaining computational efficiency.

Abstract: Generalizable 3D Gaussian Splatting aims to directly predict Gaussian parameters using a feed-forward network for scene reconstruction. Among these parameters, Gaussian means are particularly difficult to predict, so depth is usually estimated first and then unprojected to obtain the Gaussian sphere centers. Existing methods typically rely solely on a single warp to estimate depth probability, which hinders their ability to fully leverage cross-view geometric cues, resulting in unstable and coarse depth maps. To address this limitation, we propose IDESplat, which iteratively applies warp operations to boost depth probability estimation for accurate Gaussian mean prediction. First, to eliminate the inherent instability of a single warp, we introduce a Depth Probability Boosting Unit (DPBU) that integrates epipolar attention maps produced by cascading warp operations in a multiplicative manner. Next, we construct an iterative depth estimation process by stacking multiple DPBUs, progressively identifying potential depth candidates with high likelihood. As IDESplat iteratively boosts depth probability estimates and updates the depth candidates, the depth map is gradually refined, resulting in accurate Gaussian means. We conduct experiments on RealEstate10K, ACID, and DL3DV. IDESplat achieves outstanding reconstruction quality and state-of-the-art performance with real-time efficiency. On RE10K, it outperforms DepthSplat by 0.33 dB in PSNR, using only 10.7% of the parameters and 70% of the memory. Additionally, our IDESplat improves PSNR by 2.95 dB over DepthSplat on the DTU dataset in cross-dataset experiments, demonstrating its strong generalization ability.

</details>


### [54] [Bayesian Monocular Depth Refinement via Neural Radiance Fields](https://arxiv.org/abs/2601.03869)
*Arun Muthukkumar*

Main category: cs.CV

TL;DR: MDENeRF: Iterative framework that refines monocular depth estimates using Neural Radiance Fields with uncertainty modeling and Bayesian fusion to add fine geometric details while maintaining global structure.


<details>
  <summary>Details</summary>
Motivation: Current monocular depth estimation methods produce smooth depth maps lacking fine geometric details needed for accurate scene understanding, which is crucial for applications like autonomous navigation and extended reality.

Method: Three-component iterative framework: (1) initial monocular estimate for global structure, (2) NeRF trained on perturbed viewpoints with per-pixel uncertainty derived from volume rendering, (3) Bayesian fusion of noisy monocular and NeRF depths to iteratively inject high-frequency details.

Result: Demonstrates superior performance on key metrics and experiments using indoor scenes from the SUN RGB-D dataset.

Conclusion: MDENeRF effectively combines monocular depth estimation with NeRF-based refinement to produce detailed depth maps with both global structure and fine geometric details, addressing limitations of current smooth depth estimation methods.

Abstract: Monocular depth estimation has applications in many fields, such as autonomous navigation and extended reality, making it an essential computer vision task. However, current methods often produce smooth depth maps that lack the fine geometric detail needed for accurate scene understanding. We propose MDENeRF, an iterative framework that refines monocular depth estimates using depth information from Neural Radiance Fields (NeRFs). MDENeRF consists of three components: (1) an initial monocular estimate for global structure, (2) a NeRF trained on perturbed viewpoints, with per-pixel uncertainty, and (3) Bayesian fusion of the noisy monocular and NeRF depths. We derive NeRF uncertainty from the volume rendering process to iteratively inject high-frequency fine details. Meanwhile, our monocular prior maintains global structure. We demonstrate superior performance on key metrics and experiments using indoor scenes from the SUN RGB-D dataset.

</details>


### [55] [FLNet: Flood-Induced Agriculture Damage Assessment using Super Resolution of Satellite Images](https://arxiv.org/abs/2601.03884)
*Sanidhya Ghosal,Anurag Sharma,Sushil Ghildiyal,Mukesh Saini*

Main category: cs.CV

TL;DR: FLNet uses deep learning with super-resolution to enhance Sentinel-2 satellite images from 10m to 3m resolution for improved crop damage assessment after floods, achieving near-commercial high-resolution performance at lower cost.


<details>
  <summary>Details</summary>
Motivation: Traditional manual flood damage surveys are slow and biased, while current satellite methods face limitations like cloud cover and low spatial resolution. There's a need for rapid, accurate, and cost-effective crop damage assessment for post-disaster agricultural management in flood-prone regions like India.

Method: FLNet is a novel deep learning architecture that first applies super-resolution to enhance Sentinel-2 satellite images from 10m to 3m spatial resolution, then classifies crop damage. The model was tested on the Bihar Flood Impacted Croplands Dataset (BFCD-22).

Result: The model improved the critical "Full Damage" F1-score from 0.83 to 0.89, nearly matching the 0.89 score achieved using commercial high-resolution imagery, while being more cost-effective.

Conclusion: FLNet provides a cost-effective, scalable solution for automated high-fidelity crop damage assessment, enabling a nationwide shift from manual to automated methods for post-flood agricultural management.

Abstract: Distributing government relief efforts after a flood is challenging. In India, the crops are widely affected by floods; therefore, making rapid and accurate crop damage assessment is crucial for effective post-disaster agricultural management. Traditional manual surveys are slow and biased, while current satellite-based methods face challenges like cloud cover and low spatial resolution. Therefore, to bridge this gap, this paper introduced FLNet, a novel deep learning based architecture that used super-resolution to enhance the 10 m spatial resolution of Sentinel-2 satellite images into 3 m resolution before classifying damage. We tested our model on the Bihar Flood Impacted Croplands Dataset (BFCD-22), and the results showed an improved critical "Full Damage" F1-score from 0.83 to 0.89, nearly matching the 0.89 score of commercial high-resolution imagery. This work presented a cost-effective and scalable solution, paving the way for a nationwide shift from manual to automated, high-fidelity damage assessment.

</details>


### [56] [HemBLIP: A Vision-Language Model for Interpretable Leukemia Cell Morphology Analysis](https://arxiv.org/abs/2601.03915)
*Julie van Logtestijn,Petru Manescu*

Main category: cs.CV

TL;DR: HemBLIP is a vision-language model that generates interpretable descriptions of blood cell morphology for leukemia diagnosis, outperforming existing models while being more transparent and computationally efficient.


<details>
  <summary>Details</summary>
Motivation: Current deep learning models for leukemia diagnosis act as black boxes, limiting clinical trust and adoption. There's a need for interpretable models that can provide transparent morphological descriptions of blood cells to improve diagnostic confidence.

Method: Developed HemBLIP, a vision language model trained on a new dataset of 14k healthy and leukemic cells paired with expert-derived attribute captions. Used both full fine-tuning and LoRA-based parameter-efficient training, and benchmarked against biomedical foundation model MedGEMMA.

Result: HemBLIP achieves higher caption quality and morphological accuracy compared to MedGEMMA. LoRA adaptation provides further performance gains with significantly reduced computational cost.

Conclusion: Vision language models like HemBLIP show promise for transparent and scalable hematological diagnostics by generating interpretable, morphology-aware descriptions that can build clinical trust.

Abstract: Microscopic evaluation of white blood cell morphology is central to leukemia diagnosis, yet current deep learning models often act as black boxes, limiting clinical trust and adoption. We introduce HemBLIP, a vision language model designed to generate interpretable, morphology aware descriptions of peripheral blood cells. Using a newly constructed dataset of 14k healthy and leukemic cells paired with expert-derived attribute captions, we adapt a general-purpose VLM via both full fine-tuning and LoRA based parameter efficient training, and benchmark against the biomedical foundation model MedGEMMA. HemBLIP achieves higher caption quality and morphological accuracy, while LoRA adaptation provides further gains with significantly reduced computational cost. These results highlight the promise of vision language models for transparent and scalable hematological diagnostics.

</details>


### [57] [FocusUI: Efficient UI Grounding via Position-Preserving Visual Token Selection](https://arxiv.org/abs/2601.03928)
*Mingyu Ouyang,Kevin Qinghong Lin,Mike Zheng Shou,Hwee Tou Ng*

Main category: cs.CV

TL;DR: FocusUI is an efficient UI grounding framework that reduces visual token count by selecting relevant patches while preserving positional continuity, achieving faster inference with minimal accuracy loss.


<details>
  <summary>Details</summary>
Motivation: Current Vision-Language Models for UI grounding use high-resolution screenshots that create thousands of visual tokens, causing computational overhead and diluted attention, unlike humans who focus on relevant regions.

Method: FocusUI addresses two challenges: 1) Eliminates redundant tokens using patch-level supervision combining instruction-conditioned scores with rule-based UI-graph scores, 2) Preserves positional continuity with PosPad strategy that compresses dropped token sequences into special markers at their last index.

Result: FocusUI surpasses GUI-specific baselines on four grounding benchmarks, achieving 3.7% improvement over GUI-Actor-7B on ScreenSpot-Pro. With only 30% visual token retention, it drops by only 3.2% while achieving 1.44x faster inference and 17% lower peak GPU memory.

Conclusion: FocusUI demonstrates that efficient UI grounding is achievable through intelligent visual token selection that preserves positional continuity, offering significant computational benefits with minimal performance degradation.

Abstract: Vision-Language Models (VLMs) have shown remarkable performance in User Interface (UI) grounding tasks, driven by their ability to process increasingly high-resolution screenshots. However, screenshots are tokenized into thousands of visual tokens (e.g., about 4700 for 2K resolution), incurring significant computational overhead and diluting attention. In contrast, humans typically focus on regions of interest when interacting with UI. In this work, we pioneer the task of efficient UI grounding. Guided by practical analysis of the task's characteristics and challenges, we propose FocusUI, an efficient UI grounding framework that selects patches most relevant to the instruction while preserving positional continuity for precise grounding. FocusUI addresses two key challenges: (1) Eliminating redundant tokens in visual encoding. We construct patch-level supervision by fusing an instruction-conditioned score with a rule-based UI-graph score that down-weights large homogeneous regions to select distinct and instruction-relevant visual tokens. (2) Preserving positional continuity during visual token selection. We find that general visual token pruning methods suffer from severe accuracy degradation on UI grounding tasks due to broken positional information. We introduce a novel PosPad strategy, which compresses each contiguous sequence of dropped visual tokens into a single special marker placed at the sequence's last index to preserve positional continuity. Comprehensive experiments on four grounding benchmarks demonstrate that FocusUI surpasses GUI-specific baselines. On the ScreenSpot-Pro benchmark, FocusUI-7B achieves a performance improvement of 3.7% over GUI-Actor-7B. Even with only 30% visual token retention, FocusUI-7B drops by only 3.2% while achieving up to 1.44x faster inference and 17% lower peak GPU memory.

</details>


### [58] [ResTok: Learning Hierarchical Residuals in 1D Visual Tokenizers for Autoregressive Image Generation](https://arxiv.org/abs/2601.03955)
*Xu Zhang,Cheng Da,Huan Yang,Kun Gai,Ming Lu,Zhan Ma*

Main category: cs.CV

TL;DR: ResTok introduces a hierarchical residual tokenizer for autoregressive image generation that restores vision-specific priors, achieving state-of-the-art results with fewer sampling steps.


<details>
  <summary>Details</summary>
Motivation: Existing 1D visual tokenizers follow language modeling principles, treating visual data as flat sequential tokens and overlooking key vision properties like hierarchical and residual network designs essential for convergence and efficiency in visual models.

Method: Proposes Residual Tokenizer (ResTok) that builds hierarchical residuals for both image tokens and latent tokens, enabling cross-level feature fusion and preventing information overlap. Also introduces hierarchical AR generator that predicts entire levels of latent tokens at once to reduce sampling steps.

Result: Achieves gFID of 2.34 on ImageNet-256 with only 9 sampling steps, significantly improving AR image generation performance while reducing computational requirements.

Conclusion: Restoring hierarchical residual priors in visual tokenization brings "vision" back to vision modeling, enabling more efficient and effective autoregressive image generation with better representational capacity and concentrated latent distributions.

Abstract: Existing 1D visual tokenizers for autoregressive (AR) generation largely follow the design principles of language modeling, as they are built directly upon transformers whose priors originate in language, yielding single-hierarchy latent tokens and treating visual data as flat sequential token streams. However, this language-like formulation overlooks key properties of vision, particularly the hierarchical and residual network designs that have long been essential for convergence and efficiency in visual models. To bring "vision" back to vision, we propose the Residual Tokenizer (ResTok), a 1D visual tokenizer that builds hierarchical residuals for both image tokens and latent tokens. The hierarchical representations obtained through progressively merging enable cross-level feature fusion at each layer, substantially enhancing representational capacity. Meanwhile, the semantic residuals between hierarchies prevent information overlap, yielding more concentrated latent distributions that are easier for AR modeling. Cross-level bindings consequently emerge without any explicit constraints. To accelerate the generation process, we further introduce a hierarchical AR generator that substantially reduces sampling steps by predicting an entire level of latent tokens at once rather than generating them strictly token-by-token. Extensive experiments demonstrate that restoring hierarchical residual priors in visual tokenization significantly improves AR image generation, achieving a gFID of 2.34 on ImageNet-256 with only 9 sampling steps. Code is available at https://github.com/Kwai-Kolors/ResTok.

</details>


### [59] [FUSION: Full-Body Unified Motion Prior for Body and Hands via Diffusion](https://arxiv.org/abs/2601.03959)
*Enes Duran,Nikos Athanasiou,Muhammed Kocabas,Michael J. Black,Omid Taheri*

Main category: cs.CV

TL;DR: FUSION is a diffusion-based unconditional full-body motion prior that jointly models body and hand motion, addressing the lack of large-scale datasets capturing both.


<details>
  <summary>Details</summary>
Motivation: Existing motion synthesis methods either ignore hands or are limited to narrow tasks. There's a lack of large-scale datasets capturing both diverse full-body motion and detailed hand articulation simultaneously.

Method: Curated and unified existing hand motion datasets with large-scale body motion data. Proposed FUSION, a diffusion-based unconditional full-body motion prior that jointly models body and hand motion. Developed optimization pipeline to refine diffusion model's latent space for task-specific motions.

Result: FUSION surpasses state-of-the-art skeletal control models on Keypoint Tracking in HumanML3D dataset and achieves superior motion naturalness. Successfully generates detailed full-body motion with fingers during object interaction and Self-Interaction motions using LLM-transformed natural language cues.

Conclusion: FUSION enables precise hand motion control while maintaining plausible full-body coordination, going beyond typical motion prior uses through novel applications in object interaction and language-guided self-interaction.

Abstract: Hands are central to interacting with our surroundings and conveying gestures, making their inclusion essential for full-body motion synthesis. Despite this, existing human motion synthesis methods fall short: some ignore hand motions entirely, while others generate full-body motions only for narrowly scoped tasks under highly constrained settings. A key obstacle is the lack of large-scale datasets that jointly capture diverse full-body motion with detailed hand articulation. While some datasets capture both, they are limited in scale and diversity. Conversely, large-scale datasets typically focus either on body motion without hands or on hand motions without the body. To overcome this, we curate and unify existing hand motion datasets with large-scale body motion data to generate full-body sequences that capture both hand and body. We then propose the first diffusion-based unconditional full-body motion prior, FUSION, which jointly models body and hand motion. Despite using a pose-based motion representation, FUSION surpasses state-of-the-art skeletal control models on the Keypoint Tracking task in the HumanML3D dataset and achieves superior motion naturalness. Beyond standard benchmarks, we demonstrate that FUSION can go beyond typical uses of motion priors through two applications: (1) generating detailed full-body motion including fingers during interaction given the motion of an object, and (2) generating Self-Interaction motions using an LLM to transform natural language cues into actionable motion constraints. For these applications, we develop an optimization pipeline that refines the latent space of our diffusion model to generate task-specific motions. Experiments on these tasks highlight precise control over hand motion while maintaining plausible full-body coordination. The code will be public.

</details>


### [60] [PosterVerse: A Full-Workflow Framework for Commercial-Grade Poster Generation with HTML-Based Scalable Typography](https://arxiv.org/abs/2601.03993)
*Junle Liu,Peirong Zhang,Yuyi Zhang,Pengyu Yan,Hui Zhou,Xinyue Zhou,Fengjun Guo,Lianwen Jin*

Main category: cs.CV

TL;DR: PosterVerse is a full-workflow commercial poster generation system that automates design with LLM-based blueprint creation, diffusion-based background generation, and MLLM-powered HTML layout rendering, using a novel HTML-based Chinese dataset called PosterDNA.


<details>
  <summary>Details</summary>
Motivation: Current automated poster generation systems have significant limitations: incomplete design workflows, poor text rendering accuracy, and insufficient flexibility for commercial applications. Commercial-grade posters require seamless integration of aesthetic appeal with precise, informative content delivery.

Method: Three-stage workflow: (1) Blueprint creation using fine-tuned LLMs to extract key design elements from user requirements; (2) Graphical background generation via customized diffusion models for visual appeal; (3) Unified layout-text rendering with MLLM-powered HTML engine for high text accuracy and flexible customization. Also introduces PosterDNA dataset - first Chinese poster generation dataset with HTML typography files.

Result: PosterVerse consistently produces commercial-grade posters with appealing visuals, accurate text alignment, and customizable layouts. The HTML-based approach fundamentally solves challenges of rendering small and high-density text.

Conclusion: PosterVerse is a promising solution for automating commercial poster design, offering a full-workflow approach that addresses key limitations of existing systems through its three-stage pipeline and novel HTML-based dataset.

Abstract: Commercial-grade poster design demands the seamless integration of aesthetic appeal with precise, informative content delivery. Current automated poster generation systems face significant limitations, including incomplete design workflows, poor text rendering accuracy, and insufficient flexibility for commercial applications. To address these challenges, we propose PosterVerse, a full-workflow, commercial-grade poster generation method that seamlessly automates the entire design process while delivering high-density and scalable text rendering. PosterVerse replicates professional design through three key stages: (1) blueprint creation using fine-tuned LLMs to extract key design elements from user requirements, (2) graphical background generation via customized diffusion models to create visually appealing imagery, and (3) unified layout-text rendering with an MLLM-powered HTML engine to guarantee high text accuracy and flexible customization. In addition, we introduce PosterDNA, a commercial-grade, HTML-based dataset tailored for training and validating poster design models. To the best of our knowledge, PosterDNA is the first Chinese poster generation dataset to introduce HTML typography files, enabling scalable text rendering and fundamentally solving the challenges of rendering small and high-density text. Experimental results demonstrate that PosterVerse consistently produces commercial-grade posters with appealing visuals, accurate text alignment, and customizable layouts, making it a promising solution for automating commercial poster design. The code and model are available at https://github.com/wuhaer/PosterVerse.

</details>


### [61] [Pad Neurons for Efficient Neural Models](https://arxiv.org/abs/2601.04005)
*Onur Kele,A. Murat Tekalp*

Main category: cs.CV

TL;DR: Pad neurons (Paons) are a novel non-linear neuron model inspired by Pad approximants that provide stronger non-linearity than traditional McCulloch-Pitts neurons, enabling better performance with fewer layers.


<details>
  <summary>Details</summary>
Motivation: Traditional neural networks use McCulloch-Pitts neurons with linear models followed by point-wise non-linear activations. Existing non-linear neuron models (quadratic neurons, generalized operational neurons, etc.) offer stronger non-linearity but the authors propose an even better model inspired by Pad approximants.

Method: Introduce Pad neurons (Paons) based on Pad approximants, which learn different non-linear functions of inputs. Paons replace classic neurons in existing architectures (ResNet-based models for image super-resolution, compression, and classification) while maintaining the same network structure.

Result: Experimental results show neural models built with Paons provide better or equal performance than classic counterparts with fewer layers. Paons include all previously proposed neuron models as special cases and offer diversity of non-linearity and layer efficiency.

Conclusion: Pad neurons represent a superior non-linear neuron model that enhances neural network performance and efficiency, with open-source PyTorch implementation available for broader adoption.

Abstract: Neural networks commonly employ the McCulloch-Pitts neuron model, which is a linear model followed by a point-wise non-linear activation. Various researchers have already advanced inherently non-linear neuron models, such as quadratic neurons, generalized operational neurons, generative neurons, and super neurons, which offer stronger non-linearity compared to point-wise activation functions. In this paper, we introduce a novel and better non-linear neuron model called Pad neurons (Paons), inspired by Pad approximants. Paons offer several advantages, such as diversity of non-linearity, since each Paon learns a different non-linear function of its inputs, and layer efficiency, since Paons provide stronger non-linearity in much fewer layers compared to piecewise linear approximation. Furthermore, Paons include all previously proposed neuron models as special cases, thus any neuron model in any network can be replaced by Paons. We note that there has been a proposal to employ the Pad approximation as a generalized point-wise activation function, which is fundamentally different from our model. To validate the efficacy of Paons, in our experiments, we replace classic neurons in some well-known neural image super-resolution, compression, and classification models based on the ResNet architecture with Paons. Our comprehensive experimental results and analyses demonstrate that neural models built by Paons provide better or equal performance than their classic counterparts with a smaller number of layers. The PyTorch implementation code for Paon is open-sourced at https://github.com/onur-keles/Paon.

</details>


### [62] [Thinking with Frames: Generative Video Distortion Evaluation via Frame Reward Model](https://arxiv.org/abs/2601.04033)
*Yuan Wang,Borui Liao,Huijuan Huang,Jinda Lu,Ouxiang Li,Kuien Liu,Meng Wang,Xiang Wang*

Main category: cs.CV

TL;DR: REACT is a frame-level reward model for evaluating structural distortions in generative videos, trained on a large human-annotated dataset and using a two-stage training framework with supervised fine-tuning and reinforcement learning.


<details>
  <summary>Details</summary>
Motivation: Existing video reward models focus on visual quality, motion quality, and text alignment but overlook structural distortions like abnormal object appearances and interactions, which degrade generative video quality.

Method: 1) Construct large-scale human preference dataset with structural distortion taxonomy; 2) Use Chain-of-Thought synthesis pipeline for additional data; 3) Two-stage training: supervised fine-tuning with masked loss for domain knowledge, then reinforcement learning with Group Relative Policy Optimization and pairwise rewards; 4) Dynamic sampling during inference to focus on distortion-prone frames.

Result: REACT effectively complements existing reward models in assessing structural distortions, achieving accurate quantitative evaluations and interpretable attribution analysis. Also introduces REACT-Bench benchmark for generative video distortion evaluation.

Conclusion: REACT addresses the gap in structural distortion evaluation for generative videos, providing a specialized reward model that improves assessment of abnormal object appearances and interactions, with both quantitative and interpretable capabilities.

Abstract: Recent advances in video reward models and post-training strategies have improved text-to-video (T2V) generation. While these models typically assess visual quality, motion quality, and text alignment, they often overlook key structural distortions, such as abnormal object appearances and interactions, which can degrade the overall quality of the generative video. To address this gap, we introduce REACT, a frame-level reward model designed specifically for structural distortions evaluation in generative videos. REACT assigns point-wise scores and attribution labels by reasoning over video frames, focusing on recognizing distortions. To support this, we construct a large-scale human preference dataset, annotated based on our proposed taxonomy of structural distortions, and generate additional data using a efficient Chain-of-Thought (CoT) synthesis pipeline. REACT is trained with a two-stage framework: ((1) supervised fine-tuning with masked loss for domain knowledge injection, followed by (2) reinforcement learning with Group Relative Policy Optimization (GRPO) and pairwise rewards to enhance reasoning capability and align output scores with human preferences. During inference, a dynamic sampling mechanism is introduced to focus on frames most likely to exhibit distortion. We also present REACT-Bench, a benchmark for generative video distortion evaluation. Experimental results demonstrate that REACT complements existing reward models in assessing structutal distortion, achieving both accurate quantitative evaluations and interpretable attribution analysis.

</details>


### [63] [Unsupervised Modular Adaptive Region Growing and RegionMix Classification for Wind Turbine Segmentation](https://arxiv.org/abs/2601.04065)
*Ral Prez-Gonzalo,Riccardo Magro,Andreas Espersen,Antonio Agudo*

Main category: cs.CV

TL;DR: The paper introduces an annotation-efficient segmentation approach for wind turbine blades that reframes pixel-level segmentation into binary region classification using unsupervised region growing and novel augmentation techniques.


<details>
  <summary>Details</summary>
Motivation: Wind turbines require frequent inspections to detect surface damages that degrade performance. Current deep learning segmentation methods demand extensive annotated datasets, creating scalability challenges for automating inspections.

Method: The approach reframes pixel-level segmentation into binary region classification using: 1) Modular Adaptive Region Growing (unsupervised, interpretable region generation), 2) Adaptive Thresholding (image-specific guidance), 3) Region Merging (consolidates fragmented areas), and 4) RegionMix augmentation (synthesizes training samples by combining distinct regions).

Result: The framework achieves state-of-the-art segmentation accuracy and demonstrates strong cross-site generalization by consistently segmenting turbine blades across different windfarms.

Conclusion: The proposed annotation-efficient segmentation approach successfully addresses the scalability limitations of traditional dense annotation methods while maintaining high accuracy and generalization capabilities for wind turbine blade inspection.

Abstract: Reliable operation of wind turbines requires frequent inspections, as even minor surface damages can degrade aerodynamic performance, reduce energy output, and accelerate blade wear. Central to automating these inspections is the accurate segmentation of turbine blades from visual data. This task is traditionally addressed through dense, pixel-wise deep learning models. However, such methods demand extensive annotated datasets, posing scalability challenges. In this work, we introduce an annotation-efficient segmentation approach that reframes the pixel-level task into a binary region classification problem. Image regions are generated using a fully unsupervised, interpretable Modular Adaptive Region Growing technique, guided by image-specific Adaptive Thresholding and enhanced by a Region Merging process that consolidates fragmented areas into coherent segments. To improve generalization and classification robustness, we introduce RegionMix, an augmentation strategy that synthesizes new training samples by combining distinct regions. Our framework demonstrates state-of-the-art segmentation accuracy and strong cross-site generalization by consistently segmenting turbine blades across distinct windfarms.

</details>


### [64] [Mind the Generative Details: Direct Localized Detail Preference Optimization for Video Diffusion Models](https://arxiv.org/abs/2601.04068)
*Zitong Huang,Kaidong Zhang,Yukang Ding,Chao Gao,Rui Ding,Ying Chen,Wangmeng Zuo*

Main category: cs.CV

TL;DR: LocalDPO: A novel post-training framework that aligns text-to-video diffusion models using localized preference pairs from real videos, optimizing at spatio-temporal region level for more efficient and fine-grained alignment.


<details>
  <summary>Details</summary>
Motivation: Existing DPO methods for text-to-video models are inefficient (relying on multi-sample ranking and critic models) and provide ambiguous global supervision. There's a need for more efficient and fine-grained alignment methods.

Method: Proposes LocalDPO with automated pipeline: uses real videos as positives, generates negatives by locally corrupting them with random spatio-temporal masks and restoring only masked regions using frozen base model. Uses region-aware DPO loss that restricts learning to corrupted areas.

Result: Experiments on Wan2.1 and CogVideoX show LocalDPO consistently improves video fidelity, temporal coherence and human preference scores over other post-training approaches.

Conclusion: LocalDPO establishes a more efficient and fine-grained paradigm for video generator alignment by enabling localized preference learning at spatio-temporal region level with single inference per prompt.

Abstract: Aligning text-to-video diffusion models with human preferences is crucial for generating high-quality videos. Existing Direct Preference Otimization (DPO) methods rely on multi-sample ranking and task-specific critic models, which is inefficient and often yields ambiguous global supervision. To address these limitations, we propose LocalDPO, a novel post-training framework that constructs localized preference pairs from real videos and optimizes alignment at the spatio-temporal region level. We design an automated pipeline to efficiently collect preference pair data that generates preference pairs with a single inference per prompt, eliminating the need for external critic models or manual annotation. Specifically, we treat high-quality real videos as positive samples and generate corresponding negatives by locally corrupting them with random spatio-temporal masks and restoring only the masked regions using the frozen base model. During training, we introduce a region-aware DPO loss that restricts preference learning to corrupted areas for rapid convergence. Experiments on Wan2.1 and CogVideoX demonstrate that LocalDPO consistently improves video fidelity, temporal coherence and human preference scores over other post-training approaches, establishing a more efficient and fine-grained paradigm for video generator alignment.

</details>


### [65] [Analyzing Reasoning Consistency in Large Multimodal Models under Cross-Modal Conflicts](https://arxiv.org/abs/2601.04073)
*Zhihao Zhu,Jiafeng Liang,Shixin Jiang,Jinlan Fu,Ming Liu,Guanglu Sun,See-Kiong Ng,Bing Qin*

Main category: cs.CV

TL;DR: LMMs suffer from "textual inertia" where they blindly follow erroneous text in reasoning chains despite visual evidence. Proposed LogicGraph Perturbation Protocol reveals poor self-correction (<10% success), and Active Visual-Context Refinement improves robustness.


<details>
  <summary>Details</summary>
Motivation: Large Multimodal Models show impressive video reasoning via Chain-of-Thought, but their reasoning chain robustness is questionable. The paper identifies "textual inertia" - models blindly adhere to erroneous text while ignoring conflicting visual evidence.

Method: 1) LogicGraph Perturbation Protocol: structurally injects perturbations into reasoning chains of diverse LMMs to evaluate self-reflection capabilities. 2) Active Visual-Context Refinement: training-free inference paradigm with active visual re-grounding for fine-grained verification and adaptive context refinement to summarize/denoise reasoning history.

Result: Models successfully self-correct in less than 10% of cases and predominantly succumb to blind textual error propagation. The proposed Active Visual-Context Refinement approach significantly stifles hallucination propagation and enhances reasoning robustness.

Conclusion: Current LMMs have critical failure mode of textual inertia in reasoning chains. The proposed evaluation protocol reveals poor self-correction capabilities, and the training-free Active Visual-Context Refinement method effectively mitigates hallucination propagation for more robust multimodal reasoning.

Abstract: Large Multimodal Models (LMMs) have demonstrated impressive capabilities in video reasoning via Chain-of-Thought (CoT). However, the robustness of their reasoning chains remains questionable. In this paper, we identify a critical failure mode termed textual inertia, where once a textual hallucination occurs in the thinking process, models tend to blindly adhere to the erroneous text while neglecting conflicting visual evidence. To systematically investigate this, we propose the LogicGraph Perturbation Protocol that structurally injects perturbations into the reasoning chains of diverse LMMs spanning both native reasoning architectures and prompt-driven paradigms to evaluate their self-reflection capabilities. The results reveal that models successfully self-correct in less than 10% of cases and predominantly succumb to blind textual error propagation. To mitigate this, we introduce Active Visual-Context Refinement, a training-free inference paradigm which orchestrates an active visual re-grounding mechanism to enforce fine-grained verification coupled with an adaptive context refinement strategy to summarize and denoise the reasoning history. Experiments demonstrate that our approach significantly stifles hallucination propagation and enhances reasoning robustness.

</details>


### [66] [Gen3R: 3D Scene Generation Meets Feed-Forward Reconstruction](https://arxiv.org/abs/2601.04090)
*Jiaxin Huang,Yuanbo Yang,Bangbang Yang,Lin Ma,Yuewen Ma,Yiyi Liao*

Main category: cs.CV

TL;DR: Gen3R bridges foundational 3D reconstruction models with video diffusion models for scene-level 3D generation, producing both RGB videos and corresponding 3D geometry from single or multiple images.


<details>
  <summary>Details</summary>
Motivation: To combine the strengths of reconstruction models (geometric priors) and generative models (appearance priors) for improved 3D scene generation, enabling mutual enhancement between reconstruction and generation capabilities.

Method: Repurposes VGGT reconstruction model to produce geometric latents by training an adapter on its tokens, regularized to align with appearance latents from pre-trained video diffusion models. Jointly generates disentangled yet aligned latents for both RGB videos and 3D geometry (camera poses, depth maps, point clouds).

Result: Achieves state-of-the-art results in single- and multi-image conditioned 3D scene generation. Demonstrates enhanced reconstruction robustness by leveraging generative priors, showing mutual benefit from coupling reconstruction and generative models.

Conclusion: Gen3R successfully bridges reconstruction and generative models for scene-level 3D generation, showing that tight coupling between these approaches provides mutual benefits and enables high-quality 3D scene generation from limited image inputs.

Abstract: We present Gen3R, a method that bridges the strong priors of foundational reconstruction models and video diffusion models for scene-level 3D generation. We repurpose the VGGT reconstruction model to produce geometric latents by training an adapter on its tokens, which are regularized to align with the appearance latents of pre-trained video diffusion models. By jointly generating these disentangled yet aligned latents, Gen3R produces both RGB videos and corresponding 3D geometry, including camera poses, depth maps, and global point clouds. Experiments demonstrate that our approach achieves state-of-the-art results in single- and multi-image conditioned 3D scene generation. Additionally, our method can enhance the robustness of reconstruction by leveraging generative priors, demonstrating the mutual benefit of tightly coupling reconstruction and generative models.

</details>


### [67] [GeoReason: Aligning Thinking And Answering In Remote Sensing Vision-Language Models Via Logical Consistency Reinforcement Learning](https://arxiv.org/abs/2601.04118)
*Wenshuai Li,Xiantai Xiang,Zixiao Wen,Guangyao Zhou,Ben Niu,Feng Wang,Lijia Huang,Qiantong Wang,Yuxin Hu*

Main category: cs.CV

TL;DR: GeoReason framework enhances RS-VLMs by synchronizing internal reasoning with final decisions to reduce logical hallucinations, using a two-stage training approach with a novel logical consistency reward.


<details>
  <summary>Details</summary>
Motivation: Current Remote Sensing Vision-Language Models suffer from logical hallucinations where correct answers come from flawed reasoning or positional shortcuts rather than spatial logic, undermining reliability in strategic spatial decision-making.

Method: 1) Construct GeoReason-Bench dataset with 4,000 reasoning trajectories from geometric primitives and expert knowledge. 2) Two-stage training: Supervised Knowledge Initialization for reasoning syntax and domain expertise, and Consistency-Aware Reinforcement Learning with Logical Consistency Reward using option permutation strategy.

Result: The framework significantly enhances cognitive reliability and interpretability of RS-VLMs, achieving state-of-the-art performance compared to other advanced methods.

Conclusion: GeoReason successfully addresses logical hallucinations in RS-VLMs by synchronizing internal thinking with final decisions through logic-driven training, improving reliability for complex spatial tasks.

Abstract: The evolution of Remote Sensing Vision-Language Models(RS-VLMs) emphasizes the importance of transitioning from perception-centric recognition toward high-level deductive reasoning to enhance cognitive reliability in complex spatial tasks. However, current models often suffer from logical hallucinations, where correct answers are derived from flawed reasoning chains or rely on positional shortcuts rather than spatial logic. This decoupling undermines reliability in strategic spatial decision-making. To address this, we present GeoReason, a framework designed to synchronize internal thinking with final decisions. We first construct GeoReason-Bench, a logic-driven dataset containing 4,000 reasoning trajectories synthesized from geometric primitives and expert knowledge. We then formulate a two-stage training strategy: (1) Supervised Knowledge Initialization to equip the model with reasoning syntax and domain expertise, and (2) Consistency-Aware Reinforcement Learning to refine deductive reliability. This second stage integrates a novel Logical Consistency Reward, which penalizes logical drift via an option permutation strategy to anchor decisions in verifiable reasoning traces. Experimental results demonstrate that our framework significantly enhances the cognitive reliability and interpretability of RS-VLMs, achieving state-of-the-art performance compared to other advanced methods.

</details>


### [68] [Pixel-Wise Multimodal Contrastive Learning for Remote Sensing Images](https://arxiv.org/abs/2601.04127)
*Leandro Stival,Ricardo da Silva Torres,Helio Pedrini*

Main category: cs.CV

TL;DR: Proposes PIMC, a multimodal self-supervision approach using 2D recurrence plots from pixel time series and remote sensing imagery for Earth observation tasks.


<details>
  <summary>Details</summary>
Motivation: Satellites generate massive Earth observation data (SITS), but most deep learning models process entire images or complete time series, missing pixel-level temporal patterns. Need more effective feature extraction from pixel-wise time series.

Method: 1) Generate recurrence plots from pixel-based vegetation index time series (NDVI, EVI, SAVI) as 2D representations instead of raw values. 2) Propose PIxel-wise Multimodal Contrastive (PIMC) - multimodal self-supervision that learns effective encoders from both 2D pixel time series representations and remote sensing imagery.

Result: Evaluated on PASTIS (pixel-level forecasting/classification) and EuroSAT (land cover classification). Outperforms SOTA methods on all downstream tasks. 2D representations enhance SITS feature extraction, contrastive learning improves representation quality for both modalities.

Conclusion: Multimodal method using 2D recurrence plots and contrastive learning outperforms existing models, establishing robust self-supervision framework for processing SITS and remote sensing imagery.

Abstract: Satellites continuously generate massive volumes of data, particularly for Earth observation, including satellite image time series (SITS). However, most deep learning models are designed to process either entire images or complete time series sequences to extract meaningful features for downstream tasks. In this study, we propose a novel multimodal approach that leverages pixel-wise two-dimensional (2D) representations to encode visual property variations from SITS more effectively. Specifically, we generate recurrence plots from pixel-based vegetation index time series (NDVI, EVI, and SAVI) as an alternative to using raw pixel values, creating more informative representations. Additionally, we introduce PIxel-wise Multimodal Contrastive (PIMC), a new multimodal self-supervision approach that produces effective encoders based on two-dimensional pixel time series representations and remote sensing imagery (RSI). To validate our approach, we assess its performance on three downstream tasks: pixel-level forecasting and classification using the PASTIS dataset, and land cover classification on the EuroSAT dataset. Moreover, we compare our results to state-of-the-art (SOTA) methods on all downstream tasks. Our experimental results show that the use of 2D representations significantly enhances feature extraction from SITS, while contrastive learning improves the quality of representations for both pixel time series and RSI. These findings suggest that our multimodal method outperforms existing models in various Earth observation tasks, establishing it as a robust self-supervision framework for processing both SITS and RSI. Code avaliable on

</details>


### [69] [Klear: Unified Multi-Task Audio-Video Joint Generation](https://arxiv.org/abs/2601.04151)
*Jun Wang,Chunyu Qiang,Yuxin Guo,Yiran Wang,Xijuan Zeng,Chen Zhang,Pengfei Wan*

Main category: cs.CV

TL;DR: Klear is a unified audio-video generation model that addresses synchronization, alignment, and quality issues through architectural innovations, progressive training strategies, and a novel large-scale dense-caption dataset.


<details>
  <summary>Details</summary>
Motivation: Current audio-video generation approaches suffer from audio-visual asynchrony, poor lip-speech alignment, unimodal degradation, weak correspondence modeling, limited generalization, and scarcity of high-quality dense-caption data.

Method: Three-pronged approach: 1) Single-tower architecture with unified DiT blocks and Omni-Full Attention for tight alignment; 2) Progressive multitask training with random modality masking and multistage curriculum; 3) Novel automated pipeline for creating large-scale audio-video dataset with dense captions.

Result: Klear achieves high-fidelity, semantically and temporally aligned generation in both joint and unimodal settings, generalizes robustly to out-of-distribution scenarios, substantially outperforms prior methods, and achieves performance comparable to Veo 3.

Conclusion: Klear offers a unified, scalable path toward next-generation audio-video synthesis by addressing core challenges through integrated architectural, training, and data innovations.

Abstract: Audio-video joint generation has progressed rapidly, yet substantial challenges still remain. Non-commercial approaches still suffer audio-visual asynchrony, poor lip-speech alignment, and unimodal degradation, which can be stemmed from weak audio-visual correspondence modeling, limited generalization, and scarce high-quality dense-caption data. To address these issues, we introduce Klear and delve into three axes--model architecture, training strategy, and data curation. Architecturally, we adopt a single-tower design with unified DiT blocks and an Omni-Full Attention mechanism, achieving tight audio-visual alignment and strong scalability. Training-wise, we adopt a progressive multitask regime--random modality masking to joint optimization across tasks, and a multistage curriculum, yielding robust representations, strengthening A-V aligned world knowledge, and preventing unimodal collapse. For datasets, we present the first large-scale audio-video dataset with dense captions, and introduce a novel automated data-construction pipeline which annotates and filters millions of diverse, high-quality, strictly aligned audio-video-caption triplets. Building on this, Klear scales to large datasets, delivering high-fidelity, semantically and temporally aligned, instruction-following generation in both joint and unimodal settings while generalizing robustly to out-of-distribution scenarios. Across tasks, it substantially outperforms prior methods by a large margin and achieves performance comparable to Veo 3, offering a unified, scalable path toward next-generation audio-video synthesis.

</details>


### [70] [Diffusion-DRF: Differentiable Reward Flow for Video Diffusion Fine-Tuning](https://arxiv.org/abs/2601.04153)
*Yifan Wang,Yanyu Li,Sergey Tulyakov,Yun Fu,Anil Kag*

Main category: cs.CV

TL;DR: Diffusion-DRF: A differentiable reward flow method that uses frozen VLMs as training-free critics to fine-tune video diffusion models without needing human annotations or learned reward models.


<details>
  <summary>Details</summary>
Motivation: Current DPO methods for T2V generation rely on non-differentiable preference signals from human annotations or learned reward models, which makes training label-intensive, bias-prone, easy-to-game, and prone to reward hacking and unstable training.

Method: Uses a frozen, off-the-shelf Vision-Language Model as a training-free critic, directly backpropagates VLM feedback through the diffusion denoising chain, converts logit-level responses into token-aware gradients, employs automated aspect-structured prompting for reliable multi-dimensional feedback, and uses gradient checkpointing for efficient updates through final denoising steps.

Result: Improves video quality and semantic alignment while mitigating reward hacking and collapse, without needing additional reward models or preference datasets. The method is model-agnostic and generalizes to other diffusion-based generative tasks.

Conclusion: Diffusion-DRF provides an effective alternative to current DPO methods by using frozen VLMs as differentiable critics, addressing key limitations of existing approaches while maintaining efficiency and generalizability.

Abstract: Direct Preference Optimization (DPO) has recently improved Text-to-Video (T2V) generation by enhancing visual fidelity and text alignment. However, current methods rely on non-differentiable preference signals from human annotations or learned reward models. This reliance makes training label-intensive, bias-prone, and easy-to-game, which often triggers reward hacking and unstable training. We propose Diffusion-DRF, a differentiable reward flow for fine-tuning video diffusion models using a frozen, off-the-shelf Vision-Language Model (VLM) as a training-free critic. Diffusion-DRF directly backpropagates VLM feedback through the diffusion denoising chain, converting logit-level responses into token-aware gradients for optimization. We propose an automated, aspect-structured prompting pipeline to obtain reliable multi-dimensional VLM feedback, while gradient checkpointing enables efficient updates through the final denoising steps. Diffusion-DRF improves video quality and semantic alignment while mitigating reward hacking and collapse -- without additional reward models or preference datasets. It is model-agnostic and readily generalizes to other diffusion-based generative tasks.

</details>


### [71] [ToTMNet: FFT-Accelerated Toeplitz Temporal Mixing Network for Lightweight Remote Photoplethysmography](https://arxiv.org/abs/2601.04159)
*Vladimir Frants,Sos Agaian,Karen Panetta*

Main category: cs.CV

TL;DR: ToTMNet: Lightweight rPPG model using FFT-accelerated Toeplitz temporal mixing instead of attention, achieving strong heart-rate accuracy with only 63k parameters.


<details>
  <summary>Details</summary>
Motivation: Recent deep rPPG models improve robustness but increase computational cost and parameters; attention-based temporal modeling has quadratic scaling with temporal length, making it inefficient for long sequences.

Method: Proposes ToTMNet with FFT-accelerated Toeplitz temporal mixing layer that provides full-sequence temporal receptive field with linear parameters. Uses compact gated temporal mixer combining local depthwise temporal convolution with gated global Toeplitz mixing.

Result: On UBFC-rPPG: 1.055 bpm MAE with Pearson correlation 0.996. Synthetic-to-real (SCAMPS to UBFC-rPPG): 1.582 bpm MAE with Pearson correlation 0.994. Achieves strong accuracy with only 63k parameters.

Conclusion: Toeplitz-structured temporal mixing is a practical and efficient alternative to attention for rPPG, enabling efficient long-range temporal filtering with minimal parameters. Main limitation is evaluation on only two datasets.

Abstract: Remote photoplethysmography (rPPG) estimates a blood volume pulse (BVP) waveform from facial videos captured by commodity cameras. Although recent deep models improve robustness compared to classical signal-processing approaches, many methods increase computational cost and parameter count, and attention-based temporal modeling introduces quadratic scaling with respect to the temporal length. This paper proposes ToTMNet, a lightweight rPPG architecture that replaces temporal attention with an FFT-accelerated Toeplitz temporal mixing layer. The Toeplitz operator provides full-sequence temporal receptive field using a linear number of parameters in the clip length and can be applied in near-linear time using circulant embedding and FFT-based convolution. ToTMNet integrates the global Toeplitz temporal operator into a compact gated temporal mixer that combines a local depthwise temporal convolution branch with gated global Toeplitz mixing, enabling efficient long-range temporal filtering while only having 63k parameters. Experiments on two datasets, UBFC-rPPG (real videos) and SCAMPS (synthetic videos), show that ToTMNet achieves strong heart-rate estimation accuracy with a compact design. On UBFC-rPPG intra-dataset evaluation, ToTMNet reaches 1.055 bpm MAE with Pearson correlation 0.996. In a synthetic-to-real setting (SCAMPS to UBFC-rPPG), ToTMNet reaches 1.582 bpm MAE with Pearson correlation 0.994. Ablation results confirm that the gating mechanism is important for effectively using global Toeplitz mixing, especially under domain shift. The main limitation of this preprint study is the use of only two datasets; nevertheless, the results indicate that Toeplitz-structured temporal mixing is a practical and efficient alternative to attention for rPPG.

</details>


### [72] [ImLoc: Revisiting Visual Localization with Image-based Representation](https://arxiv.org/abs/2601.04185)
*Xudong Jiang,Fangjinhua Wang,Silvano Galliani,Christoph Vogel,Marc Pollefeys*

Main category: cs.CV

TL;DR: 2D image-based localization augmented with depth maps achieves state-of-the-art accuracy while maintaining easy maintenance and efficiency.


<details>
  <summary>Details</summary>
Motivation: Existing visual localization methods have trade-offs: 2D image-based methods are easy to build/maintain but limited in geometric reasoning, while 3D structure-based methods are accurate but require centralized reconstruction and are hard to update.

Method: Augment 2D images with estimated depth maps to capture geometric structure, use dense matchers effectively, implement compact compression and GPU-accelerated LO-RANSAC for efficiency.

Result: Achieves new state-of-the-art accuracy on various standard benchmarks and outperforms existing memory-efficient methods at comparable map sizes.

Conclusion: The proposed depth-augmented 2D image representation provides the best of both worlds: easy maintenance like 2D methods with high accuracy approaching 3D methods, while being efficient in storage and computation.

Abstract: Existing visual localization methods are typically either 2D image-based, which are easy to build and maintain but limited in effective geometric reasoning, or 3D structure-based, which achieve high accuracy but require a centralized reconstruction and are difficult to update. In this work, we revisit visual localization with a 2D image-based representation and propose to augment each image with estimated depth maps to capture the geometric structure. Supported by the effective use of dense matchers, this representation is not only easy to build and maintain, but achieves highest accuracy in challenging conditions. With compact compression and a GPU-accelerated LO-RANSAC implementation, the whole pipeline is efficient in both storage and computation and allows for a flexible trade-off between accuracy and highest memory efficiency. Our method achieves a new state-of-the-art accuracy on various standard benchmarks and outperforms existing memory-efficient methods at comparable map sizes. Code will be available at https://github.com/cvg/Hierarchical-Localization.

</details>


### [73] [Choreographing a World of Dynamic Objects](https://arxiv.org/abs/2601.04194)
*Yanzhe Lyu,Chen Geng,Karthik Dharmarajan,Yunzhi Zhang,Hadi Alzayer,Shangzhe Wu,Jiajun Wu*

Main category: cs.CV

TL;DR: CHORD is a universal generative pipeline for synthesizing 4D (3D+time) dynamic scenes by distilling Lagrangian motion information from 2D videos, enabling category-agnostic generation of diverse multi-body dynamics.


<details>
  <summary>Details</summary>
Motivation: Traditional rule-based graphics pipelines for creating dynamic scenes are labor-intensive and not scalable, while learning-based methods require large datasets that may not cover all object categories of interest.

Method: A distillation-based pipeline that extracts Lagrangian motion information from Eulerian representations of 2D videos, leveraging the universality of video generative models.

Result: Demonstrates effectiveness in generating diverse multi-body 4D dynamics, shows advantages over existing methods, and proves applicability in generating robotics manipulation policies.

Conclusion: CHORD provides a universal, versatile, and category-agnostic approach to generating 4D dynamic scenes, overcoming limitations of traditional rule-based and data-intensive learning methods.

Abstract: Dynamic objects in our physical 4D (3D + time) world are constantly evolving, deforming, and interacting with other objects, leading to diverse 4D scene dynamics. In this paper, we present a universal generative pipeline, CHORD, for CHOReographing Dynamic objects and scenes and synthesizing this type of phenomena. Traditional rule-based graphics pipelines to create these dynamics are based on category-specific heuristics, yet are labor-intensive and not scalable. Recent learning-based methods typically demand large-scale datasets, which may not cover all object categories in interest. Our approach instead inherits the universality from the video generative models by proposing a distillation-based pipeline to extract the rich Lagrangian motion information hidden in the Eulerian representations of 2D videos. Our method is universal, versatile, and category-agnostic. We demonstrate its effectiveness by conducting experiments to generate a diverse range of multi-body 4D dynamics, show its advantage compared to existing methods, and demonstrate its applicability in generating robotics manipulation policies. Project page: https://yanzhelyu.github.io/chord

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [74] [Staged Voxel-Level Deep Reinforcement Learning for 3D Medical Image Segmentation with Noisy Annotations](https://arxiv.org/abs/2601.03875)
*Yuyang Fu,Xiuzhen Guo,Ji Shi*

Main category: eess.IV

TL;DR: SVL-DRL: A staged voxel-level deep reinforcement learning framework for robust medical image segmentation under noisy annotations, achieving SOTA performance with 3% average improvement in Dice/IoU scores.


<details>
  <summary>Details</summary>
Motivation: Noisy annotations in medical image segmentation are common due to complex organ structures and inter-annotator variations, which limit segmentation model efficacy. The paper is motivated by how medical imaging annotators can correct labeling errors using prior knowledge.

Method: End-to-end Staged Voxel-Level Deep Reinforcement Learning (SVL-DRL) framework with: 1) Formulating noisy annotations as voxel-dependent problem addressed through staged RL framework; 2) Voxel-level asynchronous advantage actor-critic (vA3C) module treating each voxel as autonomous agent; 3) Novel action space and composite reward function combining Dice value and spatial continuity metric.

Result: State-of-the-art performance on three public medical image datasets under various experimental settings, with average improvement of over 3% in both Dice and IoU scores.

Conclusion: SVL-DRL provides an effective end-to-end framework for robust medical image segmentation under noisy annotations, automatically mitigating erroneous label impact without manual intervention through dynamic iterative update strategy.

Abstract: Deep learning has achieved significant advancements in medical image segmentation. Currently, obtaining accurate segmentation outcomes is critically reliant on large-scale datasets with high-quality annotations. However, noisy annotations are frequently encountered owing to the complex morphological structures of organs in medical images and variations among different annotators, which can substantially limit the efficacy of segmentation models. Motivated by the fact that medical imaging annotator can correct labeling errors during segmentation based on prior knowledge, we propose an end-to-end Staged Voxel-Level Deep Reinforcement Learning (SVL-DRL) framework for robust medical image segmentation under noisy annotations. This framework employs a dynamic iterative update strategy to automatically mitigate the impact of erroneous labels without requiring manual intervention. The key advancements of SVL-DRL over existing works include: i) formulating noisy annotations as a voxel-dependent problem and addressing it through a novel staged reinforcement learning framework which guarantees robust model convergence; ii) incorporating a voxel-level asynchronous advantage actor-critic (vA3C) module that conceptualizes each voxel as an autonomous agent, which allows each agent to dynamically refine its own state representation during training, thereby directly mitigating the influence of erroneous labels; iii) designing a novel action space for the agents, along with a composite reward function that strategically combines the Dice value and a spatial continuity metric to significantly boost segmentation accuracy while maintain semantic integrity. Experiments on three public medical image datasets demonstrates State-of-The-Art (SoTA) performance under various experimental settings, with an average improvement of over 3\% in both Dice and IoU scores.

</details>
