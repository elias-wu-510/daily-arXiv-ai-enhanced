<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 266]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Domain-Specific Self-Supervised Pre-training for Agricultural Disease Classification: A Hierarchical Vision Transformer Study](https://arxiv.org/abs/2601.11612)
*Arnav S. Sonavane*

Main category: cs.CV

TL;DR: Domain-specific self-supervised pre-training on just 3,000 agricultural images provides greater accuracy gains (+4.57%) than hierarchical architecture design (+3.70%) for agricultural disease classification, with benefits being architecture-agnostic across ViT, Swin, and HierarchicalViT.


<details>
  <summary>Details</summary>
Motivation: To understand the relative importance of domain-specific data versus architectural innovations for agricultural disease classification, particularly comparing self-supervised pre-training benefits against hierarchical transformer design improvements.

Method: Used SimCLR self-supervised pre-training on 3,000 unlabeled agricultural images, applied to multiple architectures (ViT-Base, Swin-Base, HierarchicalViT). HierarchicalViT (HVT) is a Swin-style hierarchical transformer evaluated on three datasets: Cotton Leaf Disease, PlantVillage, and PlantDoc. Also conducted calibration analysis with Expected Calibration Error (ECE).

Result: Domain-specific SSL pre-training provided +4.57% accuracy improvement vs +3.70% from hierarchical architecture. SSL benefits were architecture-agnostic: +4.08% for Swin-Base, +4.20% for ViT-Base. HVT-Base (78M) achieved 88.91% vs Swin-Base (88M) at 87.23% (+1.68%). HVT achieved 3.56% ECE (1.52% after temperature scaling).

Conclusion: Practitioners should prioritize domain-specific data collection and self-supervised pre-training over architectural choices for agricultural disease classification, as SSL provides greater, architecture-agnostic benefits. HierarchicalViT offers modest improvements but data-centric approaches yield superior performance gains.

Abstract: We investigate the impact of domain-specific self-supervised pre-training on agricultural disease classification using hierarchical vision transformers. Our key finding is that SimCLR pre-training on just 3,000 unlabeled agricultural images provides a +4.57% accuracy improvement--exceeding the +3.70% gain from hierarchical architecture design. Critically, we show this SSL benefit is architecture-agnostic: applying the same pre-training to Swin-Base yields +4.08%, to ViT-Base +4.20%, confirming practitioners should prioritize domain data collection over architectural choices. Using HierarchicalViT (HVT), a Swin-style hierarchical transformer, we evaluate on three datasets: Cotton Leaf Disease (7 classes, 90.24%), PlantVillage (38 classes, 96.3%), and PlantDoc (27 classes, 87.1%). At matched parameter counts, HVT-Base (78M) achieves 88.91% vs. Swin-Base (88M) at 87.23%, a +1.68% improvement. For deployment reliability, we report calibration analysis showing HVT achieves 3.56% ECE (1.52% after temperature scaling). Code: https://github.com/w2sg-arnav/HierarchicalViT

</details>


### [2] [Multi-modal MRI-Based Alzheimer's Disease Diagnosis with Transformer-based Image Synthesis and Transfer Learning](https://arxiv.org/abs/2601.11614)
*Jason Qiu*

Main category: cs.CV

TL;DR: A 3D TransUNet model synthesizes diffusion MRI metrics (FA/MD) from T1w MRI, improving Alzheimer's disease and mild cognitive impairment classification accuracy without requiring actual dMRI scans.


<details>
  <summary>Details</summary>
Motivation: Early detection of Alzheimer's disease is crucial, but T1w MRI shows late macroscopic changes while dMRI detects earlier microstructural abnormalities. However, dMRI is time-consuming and prone to motion artifacts, limiting clinical use. The goal is to extract dMRI-like information from routinely available T1w scans.

Method: Proposes a 3D TransUNet image synthesis framework that predicts fractional anisotropy (FA) and mean diffusivity (MD) maps directly from T1-weighted MRI. The model generates synthetic diffusion metrics without requiring actual dMRI acquisitions.

Result: The model achieves high-fidelity synthesis with SSIM >0.93 and Pearson correlation >0.94 with ground-truth dMRI. When integrated into a diagnostic model, synthetic features boost AD classification accuracy by 5% (78.75%→83.75%) and improve MCI detection by 12.5%.

Conclusion: High-quality diffusion microstructural information can be inferred from routine T1w MRI, enabling multi-modality benefits without additional scans. This approach improves accessibility, efficiency, and accuracy of AD diagnosis in clinical practice by reducing scan time while preserving complementary information.

Abstract: Alzheimer's disease (AD) is a progressive neurodegenerative disorder in which pathological changes begin many years before the onset of clinical symptoms, making early detection essential for timely intervention. T1-weighted (T1w) Magnetic Resonance Imaging (MRI) is routinely used in clinical practice to identify macroscopic brain alterations, but these changes typically emerge relatively late in the disease course. Diffusion MRI (dMRI), in contrast, is sensitive to earlier microstructural abnormalities by probing water diffusion in brain tissue. dMRI metrics, including fractional anisotropy (FA) and mean diffusivity (MD), provide complementary information about white matter integrity and neurodegeneration. However, dMRI acquisitions are time-consuming and susceptible to motion artifacts, limiting their routine use in clinical populations. To bridge this gap, I propose a 3D TransUNet image synthesis framework that predicts FA and MD maps directly from T1w MRI. My model generates high-fidelity maps, achieving a structural similarity index (SSIM) exceeding 0.93 and a strong Pearson correlation (>0.94) with ground-truth dMRI. When integrated into a multi-modal diagnostic model, these synthetic features boost AD classification accuracy by 5% (78.75%->83.75%) and, most importantly, improve mild cognitive impairment (MCI) detection by 12.5%. This study demonstrates that high-quality diffusion microstructural information can be inferred from routinely acquired T1w MRI, effectively transferring the benefits of multi-modality imaging to settings where diffusion data are unavailable. By reducing scan time while preserving complementary structural and microstructural information, the proposed approach has the potential to improve the accessibility, efficiency, and accuracy of AD diagnosis in clinical practice.

</details>


### [3] [PointSLAM++: Robust Dense Neural Gaussian Point Cloud-based SLAM](https://arxiv.org/abs/2601.11617)
*Xu Wang,Boyao Han,Xiaojun Chen,Ying Liu,Ruihui Li*

Main category: cs.CV

TL;DR: PointSLAM++ is an RGB-D SLAM system that uses neural Gaussian representation with hierarchical constraints and progressive pose optimization to achieve high-precision 3D mapping and photorealistic rendering, outperforming existing 3DGS-based methods.


<details>
  <summary>Details</summary>
Motivation: Current SLAM approaches struggle with maintaining structural consistency and robust pose estimation in the presence of depth noise, which is crucial for robotics and augmented reality applications requiring real-time 3D reconstruction.

Method: Uses hierarchically constrained neural Gaussian representation to preserve structural relationships while generating Gaussian primitives; employs progressive pose optimization to mitigate depth sensor noise; and utilizes a dynamic neural representation graph that adjusts Gaussian node distribution based on local geometric complexity.

Result: Outperforms existing 3DGS-based SLAM methods in both reconstruction accuracy and rendering quality, demonstrating advantages for large-scale AR and robotics applications.

Conclusion: PointSLAM++ successfully addresses key challenges in real-time 3D reconstruction by combining neural Gaussian representation with adaptive optimization techniques, enabling high-precision mapping and photorealistic rendering suitable for demanding robotics and AR applications.

Abstract: Real-time 3D reconstruction is crucial for robotics and augmented reality, yet current simultaneous localization and mapping(SLAM) approaches often struggle to maintain structural consistency and robust pose estimation in the presence of depth noise. This work introduces PointSLAM++, a novel RGB-D SLAM system that leverages a hierarchically constrained neural Gaussian representation to preserve structural relationships while generating Gaussian primitives for scene mapping. It also employs progressive pose optimization to mitigate depth sensor noise, significantly enhancing localization accuracy. Furthermore, it utilizes a dynamic neural representation graph that adjusts the distribution of Gaussian nodes based on local geometric complexity, enabling the map to adapt to intricate scene details in real time. This combination yields high-precision 3D mapping and photorealistic scene rendering. Experimental results show PointSLAM++ outperforms existing 3DGS-based SLAM methods in reconstruction accuracy and rendering quality, demonstrating its advantages for large-scale AR and robotics.

</details>


### [4] [Handcrafted Feature-Assisted One-Class Learning for Artist Authentication in Historical Drawings](https://arxiv.org/abs/2601.11627)
*Hassan Ugail,Jan Ritch-Frel,Irina Matuzava*

Main category: cs.CV

TL;DR: One-class autoencoder framework using handcrafted features for historical drawing authentication achieves 83.3% true acceptance with 9.5% false acceptance, designed to complement traditional connoisseurship.


<details>
  <summary>Details</summary>
Motivation: Authentication of historical drawings is challenging due to small reference corpora and limited stylistic cues expressed through line and tonal variation, requiring computational methods that work with scarce data.

Method: Verification-based framework using one-class autoencoders trained on handcrafted features (Fourier-domain energy, Shannon entropy, global contrast, GLCM homogeneity, fractal complexity) from authenticated sketches across multiple museum collections.

Result: Pooled system achieves 83.3% True Acceptance Rate with 9.5% False Acceptance Rate across 900 verification decisions; performance varies by artist with structured error patterns reflecting stylistic proximity.

Conclusion: The methodology provides reproducible quantitative evidence for historical sketch attribution in data-scarce settings, designed to complement rather than replace traditional connoisseurship approaches.

Abstract: Authentication and attribution of works on paper remain persistent challenges in cultural heritage, particularly when the available reference corpus is small and stylistic cues are primarily expressed through line and limited tonal variation. We present a verification-based computational framework for historical drawing authentication using one-class autoencoders trained on a compact set of interpretable handcrafted features. Ten artist-specific verifiers are trained using authenticated sketches from the Metropolitan Museum of Art open-access collection, the Ashmolean Collections Catalogue, the Morgan Library and Museum, the Royal Collection Trust (UK), the Victoria and Albert Museum Collections, and an online catalogue of the Casa Buonarroti collection and evaluated under a biometric-style protocol with genuine and impostor trials. Feature vectors comprise Fourier-domain energy, Shannon entropy, global contrast, GLCM-based homogeneity, and a box-counting estimate of fractal complexity. Across 900 verification decisions (90 genuine and 810 impostor trials), the pooled system achieves a True Acceptance Rate of 83.3% with a False Acceptance Rate of 9.5% at the chosen operating point. Performance varies substantially by artist, with near-zero false acceptance for some verifiers and elevated confusability for others. A pairwise attribution of false accepts indicates structured error pathways consistent with stylistic proximity and shared drawing conventions, whilst also motivating tighter control of digitisation artefacts and threshold calibration. The proposed methodology is designed to complement, rather than replace, connoisseurship by providing reproducible, quantitative evidence suitable for data-scarce settings common in historical sketch attribution.

</details>


### [5] [A one-step generation model with a Single-Layer Transformer: Layer number re-distillation of FreeFlow](https://arxiv.org/abs/2601.11630)
*Haonan Wei,Linyuan Wang,Nuolin Sun,Zhizhong Zheng,Lei Li,Bin Yan*

Main category: cs.CV

TL;DR: SLT distills FreeFlow's 28-layer Transformer into a single shared DiT block, reducing parameters from 675M to 4.3M, enabling efficient noise screening to improve one-step generation quality.


<details>
  <summary>Details</summary>
Motivation: To compress one-step generation models like FreeFlow while maintaining quality, and to address quality fluctuations caused by low-quality initial noise in limited sampling calls.

Method: Distill FreeFlow's 28-layer Transformer into a single shared DiT block (SLT) by matching teacher's intermediate features at depth patches, fusing representations, and aligning velocity prediction.

Result: Compressed parameters from 675M to 4.3M, enables 100+ noise screenings within time of 2 teacher samplings, improves generation stability and quality by selecting better initial points.

Conclusion: SLT effectively compresses one-step generation models while enhancing quality through efficient noise screening, making high-quality generation more stable and accessible.

Abstract: Currently, Flow matching methods aim to compress the iterative generation process of diffusion models into a few or even a single step, with MeanFlow and FreeFlow being representative achievements of one-step generation based on Ordinary Differential Equations (ODEs). We observe that the 28-layer Transformer architecture of FreeFlow can be characterized as an Euler discretization scheme for an ODE along the depth axis, where the layer index serves as the discrete time step. Therefore, we distill the number of layers of the FreeFlow model, following the same derivation logic as FreeFlow, and propose SLT (Single-Layer Transformer), which uses a single shared DiT block to approximate the depth-wise feature evolution of the 28-layer teacher. During training, it matches the teacher's intermediate features at several depth patches, fuses those patch-level representations, and simultaneously aligns the teacher's final velocity prediction. Through distillation training, we compress the 28 independent Transformer Blocks of the teacher model DiT-XL/2 into a single Transformer Block, reducing the parameter count from 675M to 4.3M. Furthermore, leveraging its minimal parameters and rapid sampling speed, SLT can screen more candidate points in the noise space within the same timeframe, thereby selecting higher-quality initial points for the teacher model FreeFlow and ultimately enhancing the quality of generated images. Experimental results demonstrate that within a time budget comparable to two random samplings of the teacher model, our method performs over 100 noise screenings and produces a high-quality sample through the teacher model using the selected points. Quality fluctuations caused by low-quality initial noise under a limited number of FreeFlow sampling calls are effectively avoided, substantially improving the stability and average generation quality of one-step generation.

</details>


### [6] [Compress to Focus: Efficient Coordinate Compression for Policy Optimization in Multi-Turn GUI Agents](https://arxiv.org/abs/2601.11631)
*Yurun Song,Jiong Yin,Rongjunchen Zhang,Ian G. Harris*

Main category: cs.CV

TL;DR: CCPO is a policy optimization framework for multi-turn GUI agents that combines visual compression with policy learning, achieving 55% token compression and 3.8× training speedup while maintaining SOTA performance.


<details>
  <summary>Details</summary>
Motivation: Multi-turn GUI agents suffer from severe context inflation as interaction history accumulates. Existing solutions either sacrifice long-term context through truncation or compromise spatial structure via token pruning, creating a need for more efficient approaches.

Method: CCPO introduces Coordinate-Aware Spatial Compression (CASC) that aggregates coordinates from multiple rollouts to identify target-relevant regions and progressively narrows historical attention around key visual areas. It also uses Distance-Based Advantage for fine-grained learning signals based on distance rather than binary correctness.

Result: CCPO achieves state-of-the-art performance across four benchmarks with up to 55% token compression and 3.8× training speedup, demonstrating both efficiency and effectiveness improvements.

Conclusion: CCPO successfully addresses context inflation in multi-turn GUI agents by coupling visual compression with policy optimization, enabling efficient long-term context management while preserving spatial structure and improving learning signals.

Abstract: Multi-turn GUI agents enable complex task completion through sequential decision-making, but suffer from severe context inflation as interaction history accumulates. Existing strategies either sacrifice long-term context via truncation or compromise spatial structure through token pruning. In this paper, we propose Coordinate Compression Policy Optimization (CCPO), an efficient policy optimization framework that couples visual compression with policy optimization for multi-turn GUI agents. CCPO introduces Coordinate-Aware Spatial Compression (CASC), which aggregates coordinates from multiple rollouts to capture target-relevant regions and progressively narrow historical attention around key visual areas. From interactions across rollouts, CASC adaptively constructs attention boundaries that concentrate computation on the most informative regions of the scene. We further design a Distance-Based Advantage that provides fine-grained learning signals based on distance rather than binary correctness, improving both grounding accuracy and compression quality. Extensive experiments demonstrate that CCPO achieves SOTA performance across four benchmarks with up to 55% token compression and 3.8$\times$ training speedup.

</details>


### [7] [KG-ViP: Bridging Knowledge Grounding and Visual Perception in Multi-modal LLMs for Visual Question Answering](https://arxiv.org/abs/2601.11632)
*Zhiyang Li,Ao Ke,Yukun Cao,Xike Xie*

Main category: cs.CV

TL;DR: KG-ViP is a unified framework that fuses scene graphs and commonsense graphs to address knowledge hallucination and insufficient visual perception in MLLMs for VQA.


<details>
  <summary>Details</summary>
Motivation: MLLMs for VQA suffer from two key limitations: knowledge hallucination (making up facts) and insufficient fine-grained visual perception. While commonsense graphs provide external knowledge to address hallucination, and scene graphs capture fine-grained visual details, prior works treat them separately, missing their synergistic potential.

Method: KG-ViP proposes a unified framework with a novel retrieval-and-fusion pipeline that uses the query as a semantic bridge to progressively integrate both scene graphs and commonsense graphs, synthesizing a unified structured context for multi-modal reasoning.

Result: Extensive experiments on FVQA 2.0+ and MVQA benchmarks show that KG-ViP significantly outperforms existing VQA methods.

Conclusion: Fusing scene graphs and commonsense graphs in a unified framework effectively addresses the dual limitations of MLLMs in VQA, demonstrating the importance of leveraging their complementary strengths through structured integration.

Abstract: Multi-modal Large Language Models (MLLMs) for Visual Question Answering (VQA) often suffer from dual limitations: knowledge hallucination and insufficient fine-grained visual perception. Crucially, we identify that commonsense graphs and scene graphs provide precisely complementary solutions to these respective deficiencies by providing rich external knowledge and capturing fine-grained visual details. However, prior works typically treat them in isolation, overlooking their synergistic potential. To bridge this gap, we propose KG-ViP, a unified framework that empowers MLLMs by fusing scene graphs and commonsense graphs. The core of the KG-ViP framework is a novel retrieval-and-fusion pipeline that utilizes the query as a semantic bridge to progressively integrate both graphs, synthesizing a unified structured context that facilitates reliable multi-modal reasoning. Extensive experiments on FVQA 2.0+ and MVQA benchmarks demonstrate that KG-ViP significantly outperforms existing VQA methods.

</details>


### [8] [Beyond Accuracy: Evaluating Grounded Visual Evidence in Thinking with Images](https://arxiv.org/abs/2601.11633)
*Xuchen Li,Xuzhao Li,Renjie Pi,Shiyu Hu,Jian Zhao,Jiahui Gao*

Main category: cs.CV

TL;DR: ViEBench is a process-verifiable benchmark for evaluating faithful visual reasoning in VLMs, using 200 multi-scenario images with expert-annotated visual evidence and a dual-axis matrix for fine-grained diagnosis.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks rely on outcome-oriented accuracy and cannot assess whether VLMs accurately leverage fine-grained visual cues for multi-step reasoning, lacking evaluation of authentic reasoning processes.

Method: Created ViEBench with 200 multi-scenario high-resolution images with expert-annotated visual evidence, categorized tasks by difficulty into perception and reasoning dimensions, and introduced a dual-axis matrix with four diagnostic quadrants for fine-grained evaluation.

Result: Experiments revealed that VLMs can sometimes produce correct answers despite grounding on irrelevant regions, and may successfully locate correct evidence but still fail to use it for accurate conclusions.

Conclusion: ViEBench serves as a more explainable and practical benchmark for comprehensively evaluating the effectiveness of agentic VLMs, providing transparent diagnosis of model behavior across varying task complexities.

Abstract: Despite the remarkable progress of Vision-Language Models (VLMs) in adopting "Thinking-with-Images" capabilities, accurately evaluating the authenticity of their reasoning process remains a critical challenge. Existing benchmarks mainly rely on outcome-oriented accuracy, lacking the capability to assess whether models can accurately leverage fine-grained visual cues for multi-step reasoning. To address these limitations, we propose ViEBench, a process-verifiable benchmark designed to evaluate faithful visual reasoning. Comprising 200 multi-scenario high-resolution images with expert-annotated visual evidence, ViEBench uniquely categorizes tasks by difficulty into perception and reasoning dimensions, where reasoning tasks require utilizing localized visual details with prior knowledge. To establish comprehensive evaluation criteria, we introduce a dual-axis matrix that provides fine-grained metrics through four diagnostic quadrants, enabling transparent diagnosis of model behavior across varying task complexities. Our experiments yield several interesting observations: (1) VLMs can sometimes produce correct final answers despite grounding on irrelevant regions, and (2) they may successfully locate the correct evidence but still fail to utilize it to reach accurate conclusions. Our findings demonstrate that ViEBench can serve as a more explainable and practical benchmark for comprehensively evaluating the effectiveness agentic VLMs. The codes will be released at: https://github.com/Xuchen-Li/ViEBench.

</details>


### [9] [When Rules Fall Short: Agent-Driven Discovery of Emerging Content Issues in Short Video Platforms](https://arxiv.org/abs/2601.11634)
*Chenghui Yu,Hongwei Wang,Junwen Chen,Zixuan Wang,Bingfeng Deng,Zhuolin Hao,Hongyu Xiong,Yang Song*

Main category: cs.CV

TL;DR: Proposes an automatic issue discovery method using multimodal LLM agents to detect emerging content issues on short-video platforms, improving discovery effectiveness by 20% F1 score and reducing problematic video views by 15%.


<details>
  <summary>Details</summary>
Motivation: Trends on short-video platforms evolve rapidly with new content issues emerging daily that fall outside existing annotation policies. Traditional human-driven discovery is too slow, leading to delayed policy updates and challenges for effective content governance.

Method: Uses multimodal LLM agents to automatically recall short videos containing potential new issues, applies two-stage clustering to group them (each cluster corresponds to a newly discovered issue), then generates updated annotation policies from these clusters.

Result: Offline and online experiments show the agent-based method improves emerging-issue discovery effectiveness by over 20% F1 score and enhances subsequent issue governance by reducing problematic video view count by approximately 15%. Greatly reduces time costs and accelerates policy iteration compared to manual discovery.

Conclusion: The proposed multimodal LLM agent approach effectively addresses the challenge of rapidly evolving content issues on short-video platforms, enabling faster policy updates and better content governance through automated issue discovery and policy generation.

Abstract: Trends on short-video platforms evolve at a rapid pace, with new content issues emerging every day that fall outside the coverage of existing annotation policies. However, traditional human-driven discovery of emerging issues is too slow, which leads to delayed updates of annotation policies and poses a major challenge for effective content governance. In this work, we propose an automatic issue discovery method based on multimodal LLM agents. Our approach automatically recalls short videos containing potential new issues and applies a two-stage clustering strategy to group them, with each cluster corresponding to a newly discovered issue. The agent then generates updated annotation policies from these clusters, thereby extending coverage to these emerging issues. Our agent has been deployed in the real system. Both offline and online experiments demonstrate that this agent-based method significantly improves the effectiveness of emerging-issue discovery (with an F1 score improvement of over 20%) and enhances the performance of subsequent issue governance (reducing the view count of problematic videos by approximately 15%). More importantly, compared to manual issue discovery, it greatly reduces time costs and substantially accelerates the iteration of annotation policies.

</details>


### [10] [Now You See Me, Now You Don't: A Unified Framework for Expression Consistent Anonymization in Talking Head Videos](https://arxiv.org/abs/2601.11635)
*Anil Egin,Andrea Tangherloni,Antitza Dantcheva*

Main category: cs.CV

TL;DR: Anon-NET is a unified framework for face video anonymization that preserves demographic attributes and facial dynamics while obfuscating identity.


<details>
  <summary>Details</summary>
Motivation: Privacy preservation in video analysis while maintaining utility for downstream computer vision tasks like expression recognition, tracking, and action recognition.

Method: Two-stage approach: 1) Diffusion-based generative model for face inpainting guided by attribute recognition and motion-aware expression transfer, 2) Video-driven animation to animate de-identified faces using original video motion.

Result: Effective identity obfuscation while preserving visual realism and temporal consistency across diverse datasets (VoxCeleb2, CelebV-HQ, HDTF).

Conclusion: Anon-NET provides a practical solution for privacy-preserving video analysis with code to be publicly released.

Abstract: Face video anonymization is aimed at privacy preservation while allowing for the analysis of videos in a number of computer vision downstream tasks such as expression recognition, people tracking, and action recognition. We propose here a novel unified framework referred to as Anon-NET, streamlined to de-identify facial videos, while preserving age, gender, race, pose, and expression of the original video. Specifically, we inpaint faces by a diffusion-based generative model guided by high-level attribute recognition and motion-aware expression transfer. We then animate deidentified faces by video-driven animation, which accepts the de-identified face and the original video as input. Extensive experiments on the datasets VoxCeleb2, CelebV-HQ, and HDTF, which include diverse facial dynamics, demonstrate the effectiveness of AnonNET in obfuscating identity while retaining visual realism and temporal consistency. The code of AnonNet will be publicly released.

</details>


### [11] [Evaluating Self-Correcting Vision Agents Through Quantitative and Qualitative Metrics](https://arxiv.org/abs/2601.11637)
*Aradhya Dixit*

Main category: cs.CV

TL;DR: A diagnostic benchmark reveals VLAs have poor self-correction ability (25-33% success) despite decent initial task performance (62% success), with semantic drift being a major failure cause.


<details>
  <summary>Details</summary>
Motivation: While VLAs can decompose visual tasks into tool-based plans, their iterative self-correction capabilities are poorly understood. Current benchmarks don't quantify the limits of correction or identify dominant reasoning bottlenecks.

Method: Introduces a Diagnostic Micro-Benchmark that decouples Task Success Rate from Correction Success Rate, quantifies diminishing returns of correction attempts, and develops a failure taxonomy to identify reasoning bottlenecks.

Result: VLAs achieve 62% initial task success but only 25-33% correction success, showing initial competence doesn't predict repair ability. Correction attempts show diminishing returns, saturating after 3 retries. Semantic drift (loss of contextual state) accounts for ~28% of failures.

Conclusion: The benchmark isolates semantic drift as a key reasoning bottleneck and provides a reproducible framework for developing stateful, trustworthy multimodal agents with better self-correction capabilities.

Abstract: Recent progress in multimodal foundation models has enabled Vision-Language Agents (VLAs) to decompose complex visual tasks into executable tool-based plans. While recent benchmarks have begun to evaluate iterative self-correction, its quantitative limits and dominant reasoning bottlenecks remain poorly characterized. This work introduces a Diagnostic Micro-Benchmark. Our analysis decouples Task Success Rate (TSR = 62 percent) from Correction Success Rate (CSR = 25 to 33 percent), revealing that initial competence does not predict repair ability. We explicitly quantify the diminishing returns of correction, which saturates after three retries. Our Failure Taxonomy reveals a frequent factor is Semantic Drift (about 28 percent of failures), a loss of contextual state. By isolating this reasoning bottleneck, this benchmark defines a reproducible framework toward stateful, trustworthy multimodal agents.

</details>


### [12] [Confident Learning for Object Detection under Model Constraints](https://arxiv.org/abs/2601.11640)
*Yingda Yu,Jiaqi Xuan,Shuhui Shi,Xuanyu Teng,Shuyang Xu,Guanchao Tong*

Main category: cs.CV

TL;DR: MDDC framework improves weed detection on edge devices by systematically fixing data quality issues instead of scaling models, achieving 5-25% mAP gains with fixed lightweight detectors.


<details>
  <summary>Details</summary>
Motivation: Edge devices for agricultural weed detection face strict constraints on model capacity, computation, and latency, preventing performance improvements through traditional methods like model scaling or ensembling. There's a need for alternative approaches that work within these fixed constraints.

Method: Proposes Model-Driven Data Correction (MDDC), a data-centric framework that iteratively diagnoses and corrects data quality deficiencies. Uses automated error analysis to categorize detection failures into four types (false negatives, false positives, class confusion, localization errors), then applies a structured train-fix-retrain pipeline with version-controlled data management.

Result: Experimental results on multiple weed detection datasets show consistent improvements of 5-25% in mAP at 0.5 using a fixed lightweight detector (YOLOv8n). Demonstrates that systematic data quality optimization can effectively alleviate performance bottlenecks under fixed model capacity constraints.

Conclusion: When model scaling is not feasible due to edge device constraints, systematic data quality optimization through frameworks like MDDC provides an effective alternative for improving detection performance, achieving significant gains without changing model architecture.

Abstract: Agricultural weed detection on edge devices is subject to strict constraints on model capacity, computational resources, and real-time inference latency, which prevent performance improvements through model scaling or ensembling. This paper proposes Model-Driven Data Correction (MDDC), a data-centric framework that enhances detection performance by iteratively diagnosing and correcting data quality deficiencies. An automated error analysis procedure categorizes detection failures into four types: false negatives, false positives, class confusion, and localization errors. These error patterns are systematically addressed through a structured train-fix-retrain pipeline with version-controlled data management. Experimental results on multiple weed detection datasets demonstrate consistent improvements of 5-25 percent in mAP at 0.5 using a fixed lightweight detector (YOLOv8n), indicating that systematic data quality optimization can effectively alleviate performance bottlenecks under fixed model capacity constraints.

</details>


### [13] [Mixture of Distributions Matters: Dynamic Sparse Attention for Efficient Video Diffusion Transformers](https://arxiv.org/abs/2601.11641)
*Yuxi Liu,Yipeng Hu,Zekun Zhang,Kunze Jiang,Kun Yuan*

Main category: cs.CV

TL;DR: MOD-DiT: A sampling-free dynamic attention framework for efficient video generation that overcomes quadratic complexity of self-attention in Diffusion Transformers through mixture-of-distribution modeling and online block masking.


<details>
  <summary>Details</summary>
Motivation: Current Diffusion Transformers for video generation suffer from quadratic complexity of self-attention, creating practical deployment barriers. Existing sparse attention methods either use oversimplified static patterns or require expensive sampling operations, leading to inaccurate predictions and degraded quality.

Method: Two-stage approach: 1) Uses prior information from early denoising steps with distributed mixing to model efficient linear approximation for predicting mask patterns, 2) Implements online block masking strategy that dynamically applies predicted masks while maintaining historical sparsity information, eliminating repetitive sampling.

Result: Extensive evaluations show consistent acceleration and quality improvements across multiple benchmarks and model architectures, validating effectiveness for efficient, high-quality video generation.

Conclusion: MOD-DiT overcomes computational limitations of traditional sparse attention approaches while maintaining generation quality, enabling practical deployment of Diffusion Transformers for video generation.

Abstract: While Diffusion Transformers (DiTs) have achieved notable progress in video generation, this long-sequence generation task remains constrained by the quadratic complexity inherent to self-attention mechanisms, creating significant barriers to practical deployment. Although sparse attention methods attempt to address this challenge, existing approaches either rely on oversimplified static patterns or require computationally expensive sampling operations to achieve dynamic sparsity, resulting in inaccurate pattern predictions and degraded generation quality. To overcome these limitations, we propose a \underline{\textbf{M}}ixtrue-\underline{\textbf{O}}f-\underline{\textbf{D}}istribution \textbf{DiT} (\textbf{MOD-DiT}), a novel sampling-free dynamic attention framework that accurately models evolving attention patterns through a two-stage process. First, MOD-DiT leverages prior information from early denoising steps and adopts a {distributed mixing approach} to model an efficient linear approximation model, which is then used to predict mask patterns for a specific denoising interval. Second, an online block masking strategy dynamically applies these predicted masks while maintaining historical sparsity information, eliminating the need for repetitive sampling operations. Extensive evaluations demonstrate consistent acceleration and quality improvements across multiple benchmarks and model architectures, validating MOD-DiT's effectiveness for efficient, high-quality video generation while overcoming the computational limitations of traditional sparse attention approaches.

</details>


### [14] [PSSF: Early osteoarthritis detection using physical synthetic knee X-ray scans and AI radiomics models](https://arxiv.org/abs/2601.11642)
*Abbas Alzubaidi,Ali Al-Bayaty*

Main category: cs.CV

TL;DR: Researchers developed a physics-based synthetic simulation framework (PSSF) to generate controllable knee X-ray scans for OA assessment without real patient data, addressing privacy and data scarcity issues.


<details>
  <summary>Details</summary>
Motivation: Knee OA assessment relies on subjective radiographic grading (KL scale), while AI/radiomics approaches need large annotated datasets that are difficult to obtain due to privacy, governance, and resource constraints.

Method: Created a 2D X-ray projection simulator (PSSF) from parametric anatomical models of distal femur and proximal tibia. Generated virtual cohort of 180 subjects (260 knees) with three imaging protocols. Used IBSI for feature extraction and trained three ML models (logistic regression, random forest, gradient boosting) for binary and three-class OA prediction.

Result: Successfully generated synthetic X-ray scans and evaluated ML model robustness across IBSI protocol, cross-protocol, and multi-protocol scenarios. Assessed feature stability using intraclass correlation coefficients across acquisition changes.

Conclusion: The PSSF provides a privacy-preserving solution for generating synthetic knee X-ray data, enabling OA assessment research without real patient data constraints while maintaining feature stability across imaging variations.

Abstract: Knee osteoarthritis (OA) is a major cause of disability worldwide and is still largely assessed using subjective radiographic grading, most commonly the Kellgren-Lawrence (KL) scale. Artificial intelligence (AI) and radiomics offer quantitative tools for OA assessment but depend on large, well-annotated image datasets, mainly X-ray scans, that are often difficult to obtain because of privacy, governance and resourcing constraints. In this research, we introduce a physics-based synthetic simulation framework (PSSF) to fully generate controllable X-ray scans without patients' involvement and violating their privacy and institutional constraints. This PSSF is a 2D X-ray projection simulator of anteroposterior knee radiographs from a parametric anatomical model of the distal femur and proximal tibia. Using PSSF, we create a virtual cohort of 180 subjects (260 knees), each is imaged under three protocols (reference, low-dose, and geometry-shift). Medial joint regions are automatically localized, preprocessed, and processed with the Image Biomarker Standardisation Initiative (IBSI). Practically, three machine learning (ML) models are utilized, logistic regression, random forest, and gradient boosting, to train binary (KL-like "0" vs. "2") and three-class (0-2) prediction radiographic images. Robustness is assessed within IBSI protocol, cross-protocol, and multi-protocol scenarios. Finally, features stability is then evaluated using intraclass correlation coefficients across acquisition changes.

</details>


### [15] [Predicting When to Trust Vision-Language Models for Spatial Reasoning](https://arxiv.org/abs/2601.11644)
*Muhammad Imran,Yugyung Lee*

Main category: cs.CV

TL;DR: Vision-based confidence estimation framework improves trust in VLM spatial predictions through geometric verification, achieving 34% AUROC improvement over text-based baselines.


<details>
  <summary>Details</summary>
Motivation: VLMs show poor spatial reasoning (49-54% accuracy on basic directional relationships), creating safety risks for robotics/autonomous systems deployment. Need to predict when to trust VLM spatial predictions rather than accepting all outputs.

Method: Propose vision-based confidence estimation that validates VLM predictions through independent geometric verification using object detection. Fuses four signals via gradient boosting: geometric alignment between VLM claims and coordinates, spatial ambiguity from overlap, detection quality, and VLM internal uncertainty.

Result: Achieved 0.674 AUROC on BLIP-2 (34.0% improvement over text-based baselines) and 0.583 AUROC on CLIP (16.1% improvement). At 60% target accuracy, achieved 61.9% coverage vs 27.6% baseline (2.2x improvement) on BLIP-2. Vision-based signals contribute 87.4% of model importance vs 12.7% from VLM confidence.

Conclusion: External geometric verification outperforms self-assessment for VLM spatial reasoning. Framework enables reliable scene graph construction where confidence-based pruning improves precision from 52.1% to 78.3% while retaining 68.2% of edges.

Abstract: Vision-Language Models (VLMs) demonstrate impressive capabilities across multimodal tasks, yet exhibit systematic spatial reasoning failures, achieving only 49% (CLIP) to 54% (BLIP-2) accuracy on basic directional relationships. For safe deployment in robotics and autonomous systems, we need to predict when to trust VLM spatial predictions rather than accepting all outputs. We propose a vision-based confidence estimation framework that validates VLM predictions through independent geometric verification using object detection. Unlike text-based approaches relying on self-assessment, our method fuses four signals via gradient boosting: geometric alignment between VLM claims and coordinates, spatial ambiguity from overlap, detection quality, and VLM internal uncertainty. We achieve 0.674 AUROC on BLIP-2 (34.0% improvement over text-based baselines) and 0.583 AUROC on CLIP (16.1% improvement), generalizing across generative and classification architectures. Our framework enables selective prediction: at 60% target accuracy, we achieve 61.9% coverage versus 27.6% baseline (2.2x improvement) on BLIP-2. Feature analysis reveals vision-based signals contribute 87.4% of model importance versus 12.7% from VLM confidence, validating that external geometric verification outperforms self-assessment. We demonstrate reliable scene graph construction where confidence-based pruning improves precision from 52.1% to 78.3% while retaining 68.2% of edges.

</details>


### [16] [IMSAHLO: Integrating Multi-Scale Attention and Hybrid Loss Optimization Framework for Robust Neuronal Brain Cell Segmentation](https://arxiv.org/abs/2601.11645)
*Ujjwal Jain,Oshin Misra,Roshni Chakraborty,Mahua Bhattacharya*

Main category: cs.CV

TL;DR: IMSAHLO framework combines multi-scale attention and hybrid loss optimization for robust neuronal cell segmentation in fluorescence microscopy, outperforming state-of-the-art methods on challenging dense and sparse cell distributions.


<details>
  <summary>Details</summary>
Motivation: Neuronal cell segmentation in fluorescence microscopy faces challenges including densely packed vs. sparsely distributed cells, complex overlapping morphologies, and severe class imbalance. Conventional deep learning models fail to preserve fine topological details and accurate boundaries under these conditions.

Method: Proposes IMSAHLO framework with Multi-Scale Dense Blocks (MSDBs) to capture features at various receptive fields, Hierarchical Attention mechanism to focus on salient morphological features, and a hybrid loss function combining Tversky and Focal loss for class imbalance, plus topology-aware Centerline Dice loss and Contour-Weighted Boundary loss for topological continuity.

Result: Outperforms state-of-the-art on Fluorescent Neuronal Cells dataset with precision 81.4%, macro F1 82.7%, micro F1 83.3%, and balanced accuracy 99.5% on difficult dense and sparse cases. Ablation studies validate benefits of multi-scale attention and hybrid loss terms.

Conclusion: Establishes foundation for generalizable segmentation models applicable to various biomedical imaging modalities, pushing AI-assisted analysis toward high-throughput neurobiological pipelines.

Abstract: Accurate segmentation of neuronal cells in fluorescence microscopy is a fundamental task for quantitative analysis in computational neuroscience. However, it is significantly impeded by challenges such as the coexistence of densely packed and sparsely distributed cells, complex overlapping morphologies, and severe class imbalance. Conventional deep learning models often fail to preserve fine topological details or accurately delineate boundaries under these conditions. To address these limitations, we propose a novel deep learning framework, IMSAHLO (Integrating Multi-Scale Attention and Hybrid Loss Optimization), for robust and adaptive neuronal segmentation. The core of our model features Multi-Scale Dense Blocks (MSDBs) to capture features at various receptive fields, effectively handling variations in cell density, and a Hierarchical Attention (HA) mechanism that adaptively focuses on salient morphological features to preserve Region of Interest (ROI) boundary details. Furthermore, we introduce a novel hybrid loss function synergistically combining Tversky and Focal loss to combat class imbalance, alongside a topology-aware Centerline Dice (clDice) loss and a Contour-Weighted Boundary loss to ensure topological continuity and precise separation of adjacent cells. Large-scale experiments on the public Fluorescent Neuronal Cells (FNC) dataset demonstrate that our framework outperforms state-of-the-art architectures, achieving precision of 81.4%, macro F1 score of 82.7%, micro F1 score of 83.3%, and balanced accuracy of 99.5% on difficult dense and sparse cases. Ablation studies validate the synergistic benefits of multi-scale attention and hybrid loss terms. This work establishes a foundation for generalizable segmentation models applicable to a wide range of biomedical imaging modalities, pushing AI-assisted analysis toward high-throughput neurobiological pipelines.

</details>


### [17] [Aesthetics as Structural Harm: Algorithmic Lookism Across Text-to-Image Generation and Classification](https://arxiv.org/abs/2601.11651)
*Miriam Doh,Aditya Gulati,Corina Canali,Nuria Oliver*

Main category: cs.CV

TL;DR: Study reveals algorithmic lookism in text-to-image AI: Stable Diffusion systematically links facial attractiveness with positive attributes, and gender classification algorithms show bias against women's faces, especially those with negative attributes.


<details>
  <summary>Details</summary>
Motivation: To investigate algorithmic lookism - systematic preferential treatment based on physical appearance - in text-to-image generative AI and downstream gender classification tasks, examining how AI models encode and amplify social biases.

Method: Analyzed 26,400 synthetic faces created with Stable Diffusion 2.1 and 3.5 Medium, examining associations between facial attractiveness and attributes, and testing three gender classification algorithms on these generated faces.

Result: Found systematic attractiveness-positive attribute associations in T2I models; significant gender bias in classification algorithms with women's faces (especially negative-attribute ones) having higher misclassification rates; and intensifying aesthetic constraints in newer models through age homogenization, gendered exposure patterns, and geographic reductionism.

Conclusion: Algorithmic lookism operates as systematic infrastructure across AI vision systems, compounding existing inequalities through both representation (in generation) and recognition (in classification), revealing how AI models mirror and amplify socially constructed biases rather than evidence-based correlations.

Abstract: This paper examines algorithmic lookism-the systematic preferential treatment based on physical appearance-in text-to-image (T2I) generative AI and a downstream gender classification task. Through the analysis of 26,400 synthetic faces created with Stable Diffusion 2.1 and 3.5 Medium, we demonstrate how generative AI models systematically associate facial attractiveness with positive attributes and vice-versa, mirroring socially constructed biases rather than evidence-based correlations. Furthermore, we find significant gender bias in three gender classification algorithms depending on the attributes of the input faces. Our findings reveal three critical harms: (1) the systematic encoding of attractiveness-positive attribute associations in T2I models; (2) gender disparities in classification systems, where women's faces, particularly those generated with negative attributes, suffer substantially higher misclassification rates than men's; and (3) intensifying aesthetic constraints in newer models through age homogenization, gendered exposure patterns, and geographic reductionism. These convergent patterns reveal algorithmic lookism as systematic infrastructure operating across AI vision systems, compounding existing inequalities through both representation and recognition.
  Disclaimer: This work includes visual and textual content that reflects stereotypical associations between physical appearance and socially constructed attributes, including gender, race, and traits associated with social desirability. Any such associations found in this study emerge from the biases embedded in generative AI systems-not from empirical truths or the authors' views.

</details>


### [18] [PSSI-MaxST: An Efficient Pixel-Segment Similarity Index Using Intensity and Smoothness Features for Maximum Spanning Tree Based Segmentation](https://arxiv.org/abs/2601.11654)
*Kaustubh Shivshankar Shejole,Gaurav Mishra*

Main category: cs.CV

TL;DR: Proposes PSSI-MaxST: a novel interactive graph-based segmentation method using Pixel Segment Similarity Index (PSSI) with harmonic mean of inter-channel similarities, MeanShift for low-level segmentation, and Maximum Spanning Tree (MaxST) for partitioning, achieving superior performance over existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing interactive graph-based segmentation methods suffer from high computational costs, sensitivity to user interactions, and degraded performance when foreground/background share similar color distributions. The similarity measure used for edge weights significantly impacts segmentation performance.

Method: 1) Low-level segmentation using MeanShift to capture color, texture, and segment shape. 2) Construct pixel-segment graph with edge weights determined by novel PSSI (Pixel Segment Similarity Index) using harmonic mean of inter-channel similarities incorporating pixel intensity and spatial smoothness. 3) Partition using Maximum Spanning Tree (MaxST) to capture strongly connected local neighborhoods. Computational complexity of PSSI is O(B) where B is number of histogram bins.

Result: Experimental evaluations on GrabCut and Images250 datasets show the method consistently outperforms current graph-based interactive segmentation methods (AMOE, OneCut, SSNCut) in segmentation quality measured by Jaccard Index (IoU), F1 score, execution time, and Mean Error (ME).

Conclusion: The integration of PSSI, MeanShift, and MaxST effectively captures color similarity, smoothness, texture, shape, and strong local connectivity, addressing limitations of existing interactive segmentation methods. Code is publicly available.

Abstract: Interactive graph-based segmentation methods partition an image into foreground and background regions with the aid of user inputs. However, existing approaches often suffer from high computational costs, sensitivity to user interactions, and degraded performance when the foreground and background share similar color distributions. A key factor influencing segmentation performance is the similarity measure used for assigning edge weights in the graph. To address these challenges, we propose a novel Pixel Segment Similarity Index (PSSI), which leverages the harmonic mean of inter-channel similarities by incorporating both pixel intensity and spatial smoothness features. The harmonic mean effectively penalizes dissimilarities in any individual channel, enhancing robustness. The computational complexity of PSSI is $\mathcal{O}(B)$, where $B$ denotes the number of histogram bins. Our segmentation framework begins with low-level segmentation using MeanShift, which effectively captures color, texture, and segment shape. Based on the resulting pixel segments, we construct a pixel-segment graph with edge weights determined by PSSI. For partitioning, we employ the Maximum Spanning Tree (MaxST), which captures strongly connected local neighborhoods beneficial for precise segmentation. The integration of the proposed PSSI, MeanShift, and MaxST allows our method to jointly capture color similarity, smoothness, texture, shape, and strong local connectivity. Experimental evaluations on the GrabCut and Images250 datasets demonstrate that our method consistently outperforms current graph-based interactive segmentation methods such as AMOE, OneCut, and SSNCut in terms of segmentation quality, as measured by Jaccard Index (IoU), $F_1$ score, execution time and Mean Error (ME). Code is publicly available at: https://github.com/KaustubhShejole/PSSI-MaxST.

</details>


### [19] [Zeros can be Informative: Masked Binary U-Net for Image Segmentation on Tensor Cores](https://arxiv.org/abs/2601.11660)
*Chunshu Wu,Ruibing Song,Sushant Kondguli,Tong Geng,Ang Li*

Main category: cs.CV

TL;DR: MBU-Net: A masked binary U-Net architecture with GPU execution framework that achieves near full-precision accuracy with 2.04x speedup and 3.54x energy reduction for real-time image segmentation on edge devices.


<details>
  <summary>Details</summary>
Motivation: Real-time image segmentation on resource-constrained edge devices requires meeting tight accuracy, latency, and energy budgets. While U-Net offers good accuracy-efficiency balance, achieving real-time performance on high-resolution inputs is challenging. Binary networks are hardware-friendly but suffer from severe accuracy degradation and lack practical end-to-end GPU implementations.

Method: Masked Binary U-Net (MBU-Net) uses a cost-aware masking strategy based on two empirical observations: (1) explicit zero state is essential for sparsity in binary U-Net weights, and (2) quantization sensitivity is uniform across layers. The method prioritizes masking where it yields highest accuracy-per-cost. A GPU execution framework maps MBU-Net to Tensor Cores using subtractive bit-encoding scheme, leveraging native binary Tensor Core BMMA instructions.

Result: Across 3 segmentation benchmarks, MBU-Net achieves near full-precision accuracy with only 3% average accuracy drop, while delivering 2.04x speedup and 3.54x energy reduction compared to 16-bit floating point U-Net.

Conclusion: MBU-Net successfully reconciles accuracy with near-binary efficiency for real-time image segmentation, making extreme quantization practical for edge devices through masked binary weights and efficient GPU implementation using Tensor Cores.

Abstract: Real-time image segmentation is a key enabler for AR/VR, robotics, drones, and autonomous systems, where tight accuracy, latency, and energy budgets must be met on resource-constrained edge devices. While U-Net offers a favorable balance of accuracy and efficiency compared to large transformer-based models, achieving real-time performance on high-resolution input remains challenging due to compute, memory, and power limits. Extreme quantization, particularly binary networks, is appealing for its hardware-friendly operations. However, two obstacles limit practicality: (1) severe accuracy degradation, and (2) a lack of end-to-end implementations that deliver efficiency on general-purpose GPUs.
  We make two empirical observations that guide our design. (1) An explicit zero state is essential: training with zero masking to binary U-Net weights yields noticeable sparsity. (2) Quantization sensitivity is uniform across layers. Motivated by these findings, we introduce Masked Binary U-Net (MBU-Net), obtained through a cost-aware masking strategy that prioritizes masking where it yields the highest accuracy-per-cost, reconciling accuracy with near-binary efficiency.
  To realize these gains in practice, we develop a GPU execution framework that maps MBU-Net to Tensor Cores via a subtractive bit-encoding scheme, efficiently implementing masked binary weights with binary activations. This design leverages native binary Tensor Core BMMA instructions, enabling high throughput and energy savings on widely available GPUs. Across 3 segmentation benchmarks, MBU-Net attains near full-precision accuracy (3% average drop) while delivering 2.04x speedup and 3.54x energy reductions over a 16-bit floating point U-Net.

</details>


### [20] [LTV-YOLO: A Lightweight Thermal Object Detector for Young Pedestrians in Adverse Conditions](https://arxiv.org/abs/2601.11662)
*Abdullah Jirjees,Ryan Myers,Muhammad Haris Ikram,Mohamed H. Zaki*

Main category: cs.CV

TL;DR: LTV-YOLO: A lightweight thermal-only object detection model based on YOLO11 architecture, optimized for detecting young pedestrians (children and adolescents) in low-light and adverse weather conditions using LWIR cameras, with real-time edge device performance.


<details>
  <summary>Details</summary>
Motivation: Detecting vulnerable road users (VRUs), especially children and adolescents, remains challenging in low-light and adverse weather conditions where traditional RGB cameras fail. There's a need for reliable pedestrian detection systems for safety in school zones, autonomous vehicles, and smart city infrastructure.

Method: Based on YOLO11 architecture customized for thermal detection using LWIR cameras. Integrates depthwise separable convolutions and feature pyramid network (FPN) for computational efficiency and handling small-scale, partially occluded, thermally distinct VRUs. Designed as a thermal-only edge-capable model specifically for young/small VRUs.

Result: LTV-YOLO achieves strong performance in detecting small-scale, partially occluded, and thermally distinct VRUs while maintaining compact architecture. The model is optimized for computational efficiency, accuracy, and real-time performance on edge devices.

Conclusion: The paper presents a practical and scalable solution for improving pedestrian safety in intelligent transportation systems. The novel integration of FPN and depthwise separable convolutions into a thermal-only pipeline optimized for short/occluded VRUs under adverse conditions represents a task-specific advancement over prior thermal detectors.

Abstract: Detecting vulnerable road users (VRUs), particularly children and adolescents, in low light and adverse weather conditions remains a critical challenge in computer vision, surveillance, and autonomous vehicle systems. This paper presents a purpose-built lightweight object detection model designed to identify young pedestrians in various environmental scenarios. To address these challenges, our approach leverages thermal imaging from long-wave infrared (LWIR) cameras, which enhances detection reliability in conditions where traditional RGB cameras operating in the visible spectrum fail. Based on the YOLO11 architecture and customized for thermal detection, our model, termed LTV-YOLO (Lightweight Thermal Vision YOLO), is optimized for computational efficiency, accuracy and real-time performance on edge devices. By integrating separable convolutions in depth and a feature pyramid network (FPN), LTV-YOLO achieves strong performance in detecting small-scale, partially occluded, and thermally distinct VRUs while maintaining a compact architecture. This work contributes a practical and scalable solution to improve pedestrian safety in intelligent transportation systems, particularly in school zones, autonomous navigation, and smart city infrastructure. Unlike prior thermal detectors, our contribution is task-specific: a thermally only edge-capable design designed for young and small VRUs (children and distant adults). Although FPN and depthwise separable convolutions are standard components, their integration into a thermal-only pipeline optimized for short/occluded VRUs under adverse conditions is, to the best of our knowledge, novel.

</details>


### [21] [UAV-Based Infrastructure Inspections: A Literature Review and Proposed Framework for AEC+FM](https://arxiv.org/abs/2601.11665)
*Amir Farzin Nikkhah,Dong Chen,Bradford Campbell,Somayeh Asadi,Arsalan Heydarian*

Main category: cs.CV

TL;DR: This review paper synthesizes over 150 studies on UAV-based infrastructure inspection in AEC+FM, covering data acquisition, photogrammetric modeling, defect detection, and decision support, while proposing a multimodal fusion framework and identifying future research directions.


<details>
  <summary>Details</summary>
Motivation: UAVs are transforming infrastructure inspections in AEC+FM, but there are challenges in real-time processing, multimodal data fusion, and generalizability that need to be addressed to improve inspection accuracy and reliability.

Method: The paper synthesizes insights from over 150 studies and proposes a workflow framework that integrates RGB imagery, LiDAR, and thermal sensing with transformer-based architectures. The framework includes path optimization, multimodal data fusion, and dynamic adaptation for complex environments.

Result: UAVs have demonstrated value in structural health monitoring, disaster response, urban infrastructure management, energy efficiency evaluations, and cultural heritage preservation. The proposed framework aims to improve accuracy in detecting structural defects, thermal anomalies, and geometric inconsistencies.

Conclusion: Future research should focus on lightweight AI models, adaptive flight planning, synthetic datasets, and richer modality fusion to streamline modern infrastructure inspections and address current limitations in real-time processing and generalizability.

Abstract: Unmanned Aerial Vehicles (UAVs) are transforming infrastructure inspections in the Architecture, Engineering, Construction, and Facility Management (AEC+FM) domain. By synthesizing insights from over 150 studies, this review paper highlights UAV-based methodologies for data acquisition, photogrammetric modeling, defect detection, and decision-making support. Key innovations include path optimization, thermal integration, and advanced machine learning (ML) models such as YOLO and Faster R-CNN for anomaly detection. UAVs have demonstrated value in structural health monitoring (SHM), disaster response, urban infrastructure management, energy efficiency evaluations, and cultural heritage preservation. Despite these advancements, challenges in real-time processing, multimodal data fusion, and generalizability remain. A proposed workflow framework, informed by literature and a case study, integrates RGB imagery, LiDAR, and thermal sensing with transformer-based architectures to improve accuracy and reliability in detecting structural defects, thermal anomalies, and geometric inconsistencies. The proposed framework ensures precise and actionable insights by fusing multimodal data and dynamically adapting path planning for complex environments, presented as a comprehensive step-by-step guide to address these challenges effectively. This paper concludes with future research directions emphasizing lightweight AI models, adaptive flight planning, synthetic datasets, and richer modality fusion to streamline modern infrastructure inspections.

</details>


### [22] [MATEX: Multi-scale Attention and Text-guided Explainability of Medical Vision-Language Models](https://arxiv.org/abs/2601.11666)
*Muhammad Imran,Chi Lee,Yugyung Lee*

Main category: cs.CV

TL;DR: MATEX is a new interpretability framework for medical vision-language models that uses multi-scale attention, text-guided spatial priors, and layer consistency to produce precise, anatomically grounded attribution maps for radiological AI.


<details>
  <summary>Details</summary>
Motivation: The paper addresses limitations in current medical vision-language model interpretability methods, specifically spatial imprecision, lack of anatomical grounding, and limited attention granularity, which hinder clinical trust and transparency in radiological AI applications.

Method: MATEX combines three key components: 1) multi-layer attention rollout for comprehensive attention analysis, 2) text-guided spatial priors to incorporate anatomical knowledge, and 3) layer consistency analysis to ensure stable and reliable explanations across model layers.

Result: On the MS-CXR dataset, MATEX outperforms the state-of-the-art M2IB approach in both spatial precision and alignment with expert-annotated findings, demonstrating superior interpretability performance.

Conclusion: MATEX represents a significant advancement in medical AI interpretability, offering more faithful and clinically meaningful explanations that can enhance trust and transparency in radiological applications, potentially bridging the gap between AI models and clinical practice.

Abstract: We introduce MATEX (Multi-scale Attention and Text-guided Explainability), a novel framework that advances interpretability in medical vision-language models by incorporating anatomically informed spatial reasoning. MATEX synergistically combines multi-layer attention rollout, text-guided spatial priors, and layer consistency analysis to produce precise, stable, and clinically meaningful gradient attribution maps. By addressing key limitations of prior methods, such as spatial imprecision, lack of anatomical grounding, and limited attention granularity, MATEX enables more faithful and interpretable model explanations. Evaluated on the MS-CXR dataset, MATEX outperforms the state-of-the-art M2IB approach in both spatial precision and alignment with expert-annotated findings. These results highlight MATEX's potential to enhance trust and transparency in radiological AI applications.

</details>


### [23] [Generating metamers of human scene understanding](https://arxiv.org/abs/2601.11675)
*Ritik Raina,Abe Leite,Alexandros Graikos,Seoyoung Ahn,Dimitris Samaras,Gregory J. Zelinsky*

Main category: cs.CV

TL;DR: MetamerGen is a latent diffusion model that generates image metamers aligned with human scene representations by combining peripheral gist information with high-resolution fixation data.


<details>
  <summary>Details</summary>
Motivation: Human vision constructs coherent scene understanding by combining low-resolution peripheral gist with high-resolution fixation information. The paper aims to create a tool that generates scenes aligned with these latent human scene representations.

Method: Developed MetamerGen, a latent diffusion model using dual-stream DINOv2 tokens that fuse detailed features from fixated areas with peripherally degraded context features. Evaluated perceptual alignment through same-different behavioral experiments comparing generated vs. original images.

Result: Successfully generated image metamers that align with human scene representations. Found that high-level semantic alignment most strongly predicts metamerism when scenes are conditioned on viewers' own fixations, though metamers can also be generated from random fixations.

Conclusion: MetamerGen is a powerful tool for understanding human scene perception, revealing specific visual processing features that contribute to human judgments. It provides insights into how humans construct coherent scene understanding from foveated visual input.

Abstract: Human vision combines low-resolution "gist" information from the visual periphery with sparse but high-resolution information from fixated locations to construct a coherent understanding of a visual scene. In this paper, we introduce MetamerGen, a tool for generating scenes that are aligned with latent human scene representations. MetamerGen is a latent diffusion model that combines peripherally obtained scene gist information with information obtained from scene-viewing fixations to generate image metamers for what humans understand after viewing a scene. Generating images from both high and low resolution (i.e. "foveated") inputs constitutes a novel image-to-image synthesis problem, which we tackle by introducing a dual-stream representation of the foveated scenes consisting of DINOv2 tokens that fuse detailed features from fixated areas with peripherally degraded features capturing scene context. To evaluate the perceptual alignment of MetamerGen generated images to latent human scene representations, we conducted a same-different behavioral experiment where participants were asked for a "same" or "different" response between the generated and the original image. With that, we identify scene generations that are indeed metamers for the latent scene representations formed by the viewers. MetamerGen is a powerful tool for understanding scene understanding. Our proof-of-concept analyses uncovered specific features at multiple levels of visual processing that contributed to human judgments. While it can generate metamers even conditioned on random fixations, we find that high-level semantic alignment most strongly predicts metamerism when the generated scenes are conditioned on viewers' own fixated regions.

</details>


### [24] [Conformal Point and the Calibrated Conic](https://arxiv.org/abs/2601.11679)
*Richard Hartley*

Main category: cs.CV

TL;DR: The paper explores conformal points and calibrating conics for visualizing image geometry and computing geometric properties like angles and directions.


<details>
  <summary>Details</summary>
Motivation: To develop intuitive methods for visualizing and computing image geometry by leveraging relationships between conformal points and calibrating conics.

Method: Analyzes the relationship between conformal points and calibrating conics, using these concepts as tools for geometric visualization and computation in images.

Result: Provides insights into how conformal points and calibrating conics relate to each other and demonstrates their utility for intuitive geometric computations in images.

Conclusion: Conformal points and calibrating conics offer valuable geometric tools for image analysis, enabling intuitive visualization and computation of angles, directions, and other geometric properties.

Abstract: This gives some information about the conformal point and the calibrating conic, and their relationship one to the other. These concepts are useful for visualizing image geometry, and lead to intuitive ways to compute geometry, such as angles and directions in an image.

</details>


### [25] [Telling Human and Machine Handwriting Apart](https://arxiv.org/abs/2601.11700)
*Luis A. Leiva,Moises Diaz,Nuwan T. Attygalle,Miguel A. Ferrer,Rejean Plamondon*

Main category: cs.CV

TL;DR: Paper proposes using handwriting movement analysis as behavioral biometrics to distinguish human vs. AI-generated inputs, achieving 98.3% AUC with shallow RNN on trajectory data.


<details>
  <summary>Details</summary>
Motivation: Need for reliable human verification systems to prevent automated attacks, framing handwriting analysis as a reverse Turing test to detect AI-generated inputs.

Method: Used 10 public datasets of handwritten symbols with 7 different AI synthesizers (Kinematic Theory, GANs, Transformers, Diffusion models), trained shallow recurrent neural network on non-featurized trajectory data.

Result: Achieved excellent performance (98.3% AUC, 1.4% equal error rate) across all synthesizers and datasets, maintained strong performance with only 10% training data, and performed well in out-of-domain settings.

Conclusion: Handwriting movement analysis provides effective human verification with implications for security systems, adding additional layer against automated attacks while maintaining high accuracy with minimal training data.

Abstract: Handwriting movements can be leveraged as a unique form of behavioral biometrics, to verify whether a real user is operating a device or application. This task can be framed as a reverse Turing test in which a computer has to detect if an input instance has been generated by a human or artificially. To tackle this task, we study ten public datasets of handwritten symbols (isolated characters, digits, gestures, pointing traces, and signatures) that are artificially reproduced using seven different synthesizers, including, among others, the Kinematic Theory (Sigma h model), generative adversarial networks, Transformers, and Diffusion models. We train a shallow recurrent neural network that achieves excellent performance (98.3 percent Area Under the ROC Curve (AUC) score and 1.4 percent equal error rate on average across all synthesizers and datasets) using nonfeaturized trajectory data as input. In few-shot settings, we show that our classifier achieves such an excellent performance when trained on just 10 percent of the data, as evaluated on the remaining 90% of the data as a test set. We further challenge our classifier in out-of-domain settings, and observe very competitive results as well. Our work has implications for computerized systems that need to verify human presence, and adds an additional layer of security to keep attackers at bay.

</details>


### [26] [SemAlign: Language Guided Semi-supervised Domain Generalization](https://arxiv.org/abs/2601.11724)
*Muditha Fernando,Kajhanan Kailainathan,Krishnakanth Nagaratnam,Isuranga Udaravi Bandara Senavirathne,Ranga Rodrigo*

Main category: cs.CV

TL;DR: Proposes a novel SSDG method that aligns model features with Vision Language Model embeddings for domain-invariant representations, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: Existing SSDG methods overemphasize pseudo-labeling accuracy without maximizing data utilization, limiting performance improvements. Need for better domain generalization with limited labeled data.

Method: Aligns intermediate model features with VLM's semantically rich feature space for domain-invariance, enhanced with image-level augmentation and output-level regularization to improve data utilization and prevent overfitting.

Result: Achieves state-of-the-art results across four benchmarks, outperforming existing SSDG baselines both qualitatively and quantitatively.

Conclusion: Aligning with VLM feature space while maximizing data utilization through augmentation and regularization effectively addresses SSDG challenges, offering superior generalization to unseen domains with limited labeled data.

Abstract: Semi-supervised Domain Generalization (SSDG) addresses the challenge of generalizing to unseen target domains with limited labeled data. Existing SSDG methods highlight the importance of achieving high pseudo-labeling (PL) accuracy and preventing model overfitting as the main challenges in SSDG. In this light, we show that the SSDG literature's excessive focus on PL accuracy, without consideration for maximum data utilization during training, limits potential performance improvements. We propose a novel approach to the SSDG problem by aligning the intermediate features of our model with the semantically rich and generalized feature space of a Vision Language Model (VLM) in a way that promotes domain-invariance. The above approach is enhanced with effective image-level augmentation and output-level regularization strategies to improve data utilization and minimize overfitting. Extensive experimentation across four benchmarks against existing SSDG baselines suggests that our method achieves SOTA results both qualitatively and quantitatively. The code will be made publicly available.

</details>


### [27] [SpaRRTa: A Synthetic Benchmark for Evaluating Spatial Intelligence in Visual Foundation Models](https://arxiv.org/abs/2601.11729)
*Turhan Can Kargin,Wojciech Jasiński,Adam Pardyl,Bartosz Zieliński,Marcin Przewięźlikowski*

Main category: cs.CV

TL;DR: SpaRRTa benchmark evaluates spatial reasoning in Visual Foundation Models, revealing significant gaps in their ability to understand relative object positions despite semantic understanding capabilities.


<details>
  <summary>Details</summary>
Motivation: Visual Foundation Models (VFMs) like DINO and CLIP have strong semantic understanding but limited spatial reasoning, which is crucial for embodied systems. Current VFMs show inconsistent performance on spatial tasks, raising questions about whether they truly have spatial awareness or just overfit to specific 3D objectives.

Method: Introduces the Spatial Relation Recognition Task (SpaRRTa) benchmark that generates photorealistic images with diverse scenes and fully controllable object arrangements, along with accessible spatial annotations. Unlike traditional 3D tasks focused on precise metric prediction, SpaRRTa evaluates fundamental spatial understanding of relative object positions.

Result: Evaluation of state-of-the-art VFMs reveals significant disparities in their spatial reasoning abilities. The analysis provides insights into mechanisms that support or hinder spatial awareness in modern VFMs.

Conclusion: SpaRRTa serves as a tool for guiding development of future spatially aware visual models, addressing the gap between semantic understanding and spatial reasoning in Visual Foundation Models.

Abstract: Visual Foundation Models (VFMs), such as DINO and CLIP, excel in semantic understanding of images but exhibit limited spatial reasoning capabilities, which limits their applicability to embodied systems. As a result, recent work incorporates some 3D tasks (such as depth estimation) into VFM training. However, VFM performance remains inconsistent across other spatial tasks, raising the question of whether these models truly have spatial awareness or overfit to specific 3D objectives. To address this question, we introduce the Spatial Relation Recognition Task (SpaRRTa) benchmark, which evaluates the ability of VFMs to identify relative positions of objects in the image. Unlike traditional 3D objectives that focus on precise metric prediction (e.g., surface normal estimation), SpaRRTa probes a fundamental capability underpinning more advanced forms of human-like spatial understanding. SpaRRTa generates an arbitrary number of photorealistic images with diverse scenes and fully controllable object arrangements, along with freely accessible spatial annotations. Evaluating a range of state-of-the-art VFMs, we reveal significant disparities between their spatial reasoning abilities. Through our analysis, we provide insights into the mechanisms that support or hinder spatial awareness in modern VFMs. We hope that SpaRRTa will serve as a useful tool for guiding the development of future spatially aware visual models.

</details>


### [28] [From Pixels to Purchase: Building and Evaluating a Taxonomy-Decoupled Visual Search Engine for Home Goods E-commerce](https://arxiv.org/abs/2601.11769)
*Cheng Lyu,Jingyue Zhang,Ryan Maunu,Mengwei Li,Vinny DeGenova,Yuanli Pei*

Main category: cs.CV

TL;DR: A taxonomy-decoupled visual search system with classification-free region proposals and LLM-based evaluation improves e-commerce retrieval quality and customer engagement.


<details>
  <summary>Details</summary>
Motivation: Existing e-commerce visual search systems rely on object detection with taxonomy-based classification and noisy catalog data for evaluation, limiting robustness and scalability for subjective, style-driven domains.

Method: Proposed taxonomy-decoupled architecture using classification-free region proposals and unified embeddings for similarity retrieval, plus an LLM-as-a-Judge framework for zero-shot evaluation of visual similarity and category relevance.

Result: Deployed at scale on a global home goods platform, the system improves retrieval quality, yields measurable uplift in customer engagement, and offline evaluation metrics strongly correlate with real-world outcomes.

Conclusion: The proposed approach enables more flexible and generalizable visual search by decoupling from rigid taxonomies and using LLM-based evaluation, overcoming limitations of traditional industrial systems.

Abstract: Visual search is critical for e-commerce, especially in style-driven domains where user intent is subjective and open-ended. Existing industrial systems typically couple object detection with taxonomy-based classification and rely on catalog data for evaluation, which is prone to noise that limits robustness and scalability. We propose a taxonomy-decoupled architecture that uses classification-free region proposals and unified embeddings for similarity retrieval, enabling a more flexible and generalizable visual search. To overcome the evaluation bottleneck, we propose an LLM-as-a-Judge framework that assesses nuanced visual similarity and category relevance for query-result pairs in a zero-shot manner, removing dependence on human annotations or noise-prone catalog data. Deployed at scale on a global home goods platform, our system improves retrieval quality and yields a measurable uplift in customer engagement, while our offline evaluation metrics strongly correlate with real-world outcomes.

</details>


### [29] [studentSplat: Your Student Model Learns Single-view 3D Gaussian Splatting](https://arxiv.org/abs/2601.11772)
*Yimu Pan,Hongda Mao,Qingshuang Chen,Yelin Kim*

Main category: cs.CV

TL;DR: studentSplat: Single-view 3D Gaussian splatting for scene reconstruction using teacher-student architecture and extrapolation network to overcome scale ambiguity and missing context.


<details>
  <summary>Details</summary>
Motivation: Single-view 3D scene reconstruction remains under-explored due to inherent ambiguity in single-view inputs, while feed-forward 3D Gaussian splatting has shown success in multi-view reconstruction and single-view object reconstruction.

Method: 1) Teacher-student architecture where a multi-view teacher model provides geometric supervision to address scale ambiguity. 2) Extrapolation network that completes missing scene context for high-quality extrapolation.

Result: Achieves state-of-the-art single-view novel-view reconstruction quality, comparable performance to multi-view methods at scene level, and competitive performance as self-supervised single-view depth estimation method.

Conclusion: studentSplat demonstrates strong potential for general single-view 3D understanding tasks by effectively overcoming scale ambiguity and extrapolation challenges inherent in single-view scene reconstruction.

Abstract: Recent advance in feed-forward 3D Gaussian splatting has enable remarkable multi-view 3D scene reconstruction or single-view 3D object reconstruction but single-view 3D scene reconstruction remain under-explored due to inherited ambiguity in single-view. We present \textbf{studentSplat}, a single-view 3D Gaussian splatting method for scene reconstruction. To overcome the scale ambiguity and extrapolation problems inherent in novel-view supervision from a single input, we introduce two techniques: 1) a teacher-student architecture where a multi-view teacher model provides geometric supervision to the single-view student during training, addressing scale ambiguity and encourage geometric validity; and 2) an extrapolation network that completes missing scene context, enabling high-quality extrapolation. Extensive experiments show studentSplat achieves state-of-the-art single-view novel-view reconstruction quality and comparable performance to multi-view methods at the scene level. Furthermore, studentSplat demonstrates competitive performance as a self-supervised single-view depth estimation method, highlighting its potential for general single-view 3D understanding tasks.

</details>


### [30] [Cross-Domain Object Detection Using Unsupervised Image Translation](https://arxiv.org/abs/2601.11779)
*Vinicius F. Arruda,Rodrigo F. Berriel,Thiago M. Paixão,Claudine Badue,Alberto F. De Souza,Nicu Sebe,Thiago Oliveira-Santos*

Main category: cs.CV

TL;DR: Proposes using unsupervised image translation (CycleGAN and AdaIN) to generate artificial target domain datasets for training object detectors, achieving SOTA results with improved simplicity and interpretability.


<details>
  <summary>Details</summary>
Motivation: Current unsupervised domain adaptation methods for object detection are complex, hard to implement/interpret, and still have performance gaps compared to training with target data. Need simpler, more effective approaches.

Method: Generate artificial target domain datasets using unsupervised image translators (CycleGAN and AdaIN-based model) with only annotated source data and unannotated target data, then train object detector on generated dataset.

Result: Significant improvements in real-world autonomous driving scenarios, outperforming state-of-the-art methods in most cases, further closing gap toward upper-bound (training with target data).

Conclusion: Proposed method is less complex yet more effective with improved interpretability, demonstrating strong performance for unsupervised domain adaptation in object detection.

Abstract: Unsupervised domain adaptation for object detection addresses the adaption of detectors trained in a source domain to work accurately in an unseen target domain. Recently, methods approaching the alignment of the intermediate features proven to be promising, achieving state-of-the-art results. However, these methods are laborious to implement and hard to interpret. Although promising, there is still room for improvements to close the performance gap toward the upper-bound (when training with the target data). In this work, we propose a method to generate an artificial dataset in the target domain to train an object detector. We employed two unsupervised image translators (CycleGAN and an AdaIN-based model) using only annotated data from the source domain and non-annotated data from the target domain. Our key contributions are the proposal of a less complex yet more effective method that also has an improved interpretability. Results on real-world scenarios for autonomous driving show significant improvements, outperforming state-of-the-art methods in most cases, further closing the gap toward the upper-bound.

</details>


### [31] [Digital FAST: An AI-Driven Multimodal Framework for Rapid and Early Stroke Screening](https://arxiv.org/abs/2601.11896)
*Ngoc-Khai Hoang,Thi-Nhu-Mai Nguyen,Huy-Hieu Pham*

Main category: cs.CV

TL;DR: A multimodal deep learning framework using facial expressions, speech, and upper-body movements achieves 95.83% accuracy for automatic stroke screening during F.A.S.T. assessments.


<details>
  <summary>Details</summary>
Motivation: Early stroke identification is crucial for timely intervention, especially in prehospital settings where rapid screening can significantly improve patient outcomes.

Method: Multimodal deep learning framework integrating facial expressions (Transformer on landmark features), speech (Audio Spectrogram Transformer on mel spectrograms), and upper-body movements (MLP-Mixer on pose sequences) with attention-based fusion.

Result: Achieved 95.83% accuracy and 96.00% F1-score on a dataset of 222 videos from 37 subjects, outperforming unimodal baselines and detecting all stroke cases in test set.

Conclusion: Multimodal learning shows strong potential for early stroke screening but requires larger, clinically representative datasets for reliable real-world deployment.

Abstract: Early identification of stroke symptoms is essential for enabling timely intervention and improving patient outcomes, particularly in prehospital settings. This study presents a fast, non-invasive multimodal deep learning framework for automatic binary stroke screening based on data collected during the F.A.S.T. assessment. The proposed approach integrates complementary information from facial expressions, speech signals, and upper-body movements to enhance diagnostic robustness. Facial dynamics are represented using landmark based features and modeled with a Transformer architecture to capture temporal dependencies. Speech signals are converted into mel spectrograms and processed using an Audio Spectrogram Transformer, while upper-body pose sequences are analyzed with an MLP-Mixer network to model spatiotemporal motion patterns. The extracted modality specific representations are combined through an attention-based fusion mechanism to effectively learn cross modal interactions. Experiments conducted on a self-collected dataset of 222 videos from 37 subjects demonstrate that the proposed multimodal model consistently outperforms unimodal baselines, achieving 95.83% accuracy and a 96.00% F1-score. The model attains a strong balance between sensitivity and specificity and successfully detects all stroke cases in the test set. These results highlight the potential of multimodal learning and transfer learning for early stroke screening, while emphasizing the need for larger, clinically representative datasets to support reliable real-world deployment.

</details>


### [32] [RemoteVAR: Autoregressive Visual Modeling for Remote Sensing Change Detection](https://arxiv.org/abs/2601.11898)
*Yilmaz Korkmaz,Vishal M. Patel*

Main category: cs.CV

TL;DR: RemoteVAR is a new visual autoregressive model framework for remote sensing change detection that addresses limitations of existing VARs through multi-resolution feature conditioning and specialized training for change map prediction.


<details>
  <summary>Details</summary>
Motivation: Current visual autoregressive models (VARs) have impressive image generation capabilities but face limitations for pixel-level discriminative tasks like change detection due to weak controllability, suboptimal dense prediction performance, and exposure bias.

Method: RemoteVAR conditions autoregressive prediction on multi-resolution fused bi-temporal features via cross-attention and employs an autoregressive training strategy specifically designed for change map prediction.

Result: Extensive experiments on standard change detection benchmarks show RemoteVAR delivers consistent and significant improvements over strong diffusion-based and transformer-based baselines.

Conclusion: RemoteVAR establishes a competitive autoregressive alternative for remote sensing change detection, addressing previous limitations of VARs for dense prediction tasks.

Abstract: Remote sensing change detection aims to localize and characterize scene changes between two time points and is central to applications such as environmental monitoring and disaster assessment. Meanwhile, visual autoregressive models (VARs) have recently shown impressive image generation capability, but their adoption for pixel-level discriminative tasks remains limited due to weak controllability, suboptimal dense prediction performance and exposure bias. We introduce RemoteVAR, a new VAR-based change detection framework that addresses these limitations by conditioning autoregressive prediction on multi-resolution fused bi-temporal features via cross-attention, and by employing an autoregressive training strategy designed specifically for change map prediction. Extensive experiments on standard change detection benchmarks show that RemoteVAR delivers consistent and significant improvements over strong diffusion-based and transformer-based baselines, establishing a competitive autoregressive alternative for remote sensing change detection. Code will be available \href{https://github.com/yilmazkorkmaz1/RemoteVAR}{\underline{here}}.

</details>


### [33] [Towards Airborne Object Detection: A Deep Learning Analysis](https://arxiv.org/abs/2601.11907)
*Prosenjit Chatterjee,ANK Zaman*

Main category: cs.CV

TL;DR: Dual-task EfficientNetB4 model achieves 96% classification and 90% threat prediction accuracy for airborne objects using new AODTA Dataset.


<details>
  <summary>Details</summary>
Motivation: Need for real-time automated threat assessment due to proliferation of airborne platforms (aircraft, drones, UAVs), as current manual monitoring lacks scalability and efficiency.

Method: Dual-task model based on EfficientNetB4 for simultaneous airborne object classification and threat-level prediction. Created AODTA Dataset by aggregating/refining public sources to address data scarcity. Benchmarked against AVD Dataset and ResNet-50 baseline.

Result: EfficientNetB4 achieved 96% accuracy in object classification and 90% accuracy in threat-level prediction, outperforming ResNet-50 baseline. Note: Focus is on classification/threat inference using pre-localized images, not detection.

Conclusion: EfficientNetB4 dual-task model shows promise for surveillance, defense, and airspace management applications, though title references detection while actual work focuses on classification/threat inference.

Abstract: The rapid proliferation of airborne platforms, including commercial aircraft, drones, and UAVs, has intensified the need for real-time, automated threat assessment systems. Current approaches depend heavily on manual monitoring, resulting in limited scalability and operational inefficiencies. This work introduces a dual-task model based on EfficientNetB4 capable of performing airborne object classification and threat-level prediction simultaneously. To address the scarcity of clean, balanced training data, we constructed the AODTA Dataset by aggregating and refining multiple public sources. We benchmarked our approach on both the AVD Dataset and the newly developed AODTA Dataset and further compared performance against a ResNet-50 baseline, which consistently underperformed EfficientNetB4. Our EfficientNetB4 model achieved 96% accuracy in object classification and 90% accuracy in threat-level prediction, underscoring its promise for applications in surveillance, defense, and airspace management. Although the title references detection, this study focuses specifically on classification and threat-level inference using pre-localized airborne object images provided by existing datasets.

</details>


### [34] [Effects of the retina-inspired light intensity encoding on color discrimination performance](https://arxiv.org/abs/2601.11909)
*Io Yamada,Hirotsugu Okuno*

Main category: cs.CV

TL;DR: The study investigates how different light intensity encoding functions (logarithmic vs Naka-Rushton) affect color constancy performance in a center/surround retinex model, finding that Naka-Rushton function with double opponent color plane representation provides best discrimination of target colors under varying illumination.


<details>
  <summary>Details</summary>
Motivation: Color constancy is crucial for reliable object recognition since color perception is heavily influenced by illumination color. The study aims to improve color constancy models by examining different light intensity encoding functions inspired by biological vision systems.

Method: Used center/surround retinex model with two light intensity encoding functions: logarithmic (original model) and Naka-Rushton (retinal photoreceptor model). Tested with color-variable LEDs illuminating targets under various lighting colors. Evaluated color discrimination using HSV color space and opponent color theory-based color plane representations.

Result: The Naka-Rushton function combined with double opponent color plane representation achieved superior discrimination performance for identifying target colors under different illumination conditions compared to other combinations.

Conclusion: Biological photoreceptor response modeling (Naka-Rushton function) combined with opponent color processing provides better color constancy performance than traditional logarithmic encoding, suggesting biologically-inspired approaches can enhance computer vision systems.

Abstract: Color is an important source of information for visual functions such as object recognition, but it is greatly affected by the color of illumination. The ability to perceive the color of a visual target independent of illumination color is called color constancy (CC), and is an important feature for vision systems that use color information. In this study, we investigated the effects of the light intensity encoding function on the performance of CC of the center/surround (C/S) retinex model, which is a well-known model inspired by CC of the visual nervous system. The functions used to encode light intensity are the logarithmic function used in the original C/S retinex model and the Naka-Rushton (N-R) function, which is a model of retinal photoreceptor response. Color-variable LEDs were used to illuminate visual targets with various lighting colors, and color information computed by each model was used to evaluate the degree to which the color of visual targets illuminated with different lighting colors could be discriminated. Color information was represented using the HSV color space and a color plane based on the classical opponent color theory. The results showed that the combination of the N-R function and the double opponent color plane representation provided superior discrimination performance.

</details>


### [35] [A Training-Free Guess What Vision Language Model from Snippets to Open-Vocabulary Object Detection](https://arxiv.org/abs/2601.11910)
*Guiying Zhu,Bowen Yang,Yin Zhuang,Tong Zhang,Guanqun Wang,Zhihao Che,He Chen,Lianlin Li*

Main category: cs.CV

TL;DR: GW-VLM is a training-free approach for Open-Vocabulary Object Detection that uses pre-trained Vision Language Models and Large Language Models in a "guess what" game format with multi-scale visual-language alignment and contextual concept prompts.


<details>
  <summary>Details</summary>
Motivation: Existing foundation models have impressive zero-shot capabilities for OVOD, but they lack universal understanding for any object cognition. The paper addresses the overlooked need for creating such universal understanding based on already pretrained foundation models.

Method: Proposes GW-VLM with two key components: 1) Multi-Scale Visual Language Searching (MS-VLS) for VLM to generate snippets from class-agnostic object detection results using multi-scale visual-language soft-alignment, and 2) Contextual Concept Prompt (CCP) to form concept flow and help LLM understand snippets for OVOD.

Result: Extensive experiments on natural (COCO val, Pascal VOC) and remote sensing (DIOR, NWPU-10) datasets show GW-VLM achieves superior OVOD performance compared to state-of-the-art methods without any training steps.

Conclusion: GW-VLM successfully creates a universal understanding paradigm for OVOD by leveraging pre-trained VLMs and LLMs in a training-free manner, demonstrating that effective object detection can be achieved without additional training through careful prompt design and multi-scale alignment.

Abstract: Open-Vocabulary Object Detection (OVOD) aims to develop the capability to detect anything. Although myriads of large-scale pre-training efforts have built versatile foundation models that exhibit impressive zero-shot capabilities to facilitate OVOD, the necessity of creating a universal understanding for any object cognition according to already pretrained foundation models is usually overlooked. Therefore, in this paper, a training-free Guess What Vision Language Model, called GW-VLM, is proposed to form a universal understanding paradigm based on our carefully designed Multi-Scale Visual Language Searching (MS-VLS) coupled with Contextual Concept Prompt (CCP) for OVOD. This approach can engage a pre-trained Vision Language Model (VLM) and a Large Language Model (LLM) in the game of "guess what". Wherein, MS-VLS leverages multi-scale visual-language soft-alignment for VLM to generate snippets from the results of class-agnostic object detection, while CCP can form the concept of flow referring to MS-VLS and then make LLM understand snippets for OVOD. Finally, the extensive experiments are carried out on natural and remote sensing datasets, including COCO val, Pascal VOC, DIOR, and NWPU-10, and the results indicate that our proposed GW-VLM can achieve superior OVOD performance compared to the-state-of-the-art methods without any training step.

</details>


### [36] [Reliable Deep Learning for Small-Scale Classifications: Experiments on Real-World Image Datasets from Bangladesh](https://arxiv.org/abs/2601.11911)
*Muhammad Ibrahim,Alfe Suny,MD Sakib Ul Islam,Md. Imran Hossain*

Main category: cs.CV

TL;DR: Compact CNN achieves high accuracy on diverse Bangladeshi image datasets with efficient convergence and low computational cost, demonstrating suitability for small-class classification tasks.


<details>
  <summary>Details</summary>
Motivation: Standard CNNs often have complex architectures that can overfit on small datasets, creating a need for more streamlined approaches that maintain performance while reducing complexity.

Method: Evaluation of a compact convolutional neural network across five real-world image datasets from Bangladesh covering urban encroachment, vehicle detection, road damage, and agricultural crops.

Result: The network demonstrates high classification accuracy, efficient convergence, low computational overhead, effectively captures discriminative features, and generalizes robustly across diverse scenarios.

Conclusion: Streamlined CNN architectures are well-suited for small-class image classification tasks, offering strong performance with reduced complexity and computational requirements.

Abstract: Convolutional neural networks (CNNs) have achieved state-of-the-art performance in image recognition tasks but often involve complex architectures that may overfit on small datasets. In this study, we evaluate a compact CNN across five publicly available, real-world image datasets from Bangladesh, including urban encroachment, vehicle detection, road damage, and agricultural crops. The network demonstrates high classification accuracy, efficient convergence, and low computational overhead. Quantitative metrics and saliency analyses indicate that the model effectively captures discriminative features and generalizes robustly across diverse scenarios, highlighting the suitability of streamlined CNN architectures for small-class image classification tasks.

</details>


### [37] [From Spurious to Causal: Low-rank Orthogonal Subspace Intervention for Generalizable Face Forgery Detection](https://arxiv.org/abs/2601.11915)
*Chi Wang,Xinjue Hu,Boyu Wang,Ziwen He,Zhangjie Fu*

Main category: cs.CV

TL;DR: A novel intervention paradigm for face forgery detection that models spurious correlations as a low-rank subspace and removes it via orthogonal projection, achieving SOTA performance with minimal parameters.


<details>
  <summary>Details</summary>
Motivation: Face forgery detection suffers from generalization issues due to spurious correlations (forgery-irrelevant information) that create biased learning. Previous methods address specific spurious correlations individually, but this is impractical since spurious correlations arise from unobservable confounding factors.

Method: Proposes an intervention paradigm that uniformly models spurious correlations as a low-rank subspace. Uses orthogonal low-rank projection to decompose spurious correlation features, removes this subspace from original representations, and trains the orthogonal complement to capture authentic forgery-related features.

Result: Achieves state-of-the-art performance across several benchmarks with only 0.43M trainable parameters, demonstrating excellent robustness and generalization capabilities.

Conclusion: The proposed low-rank subspace intervention effectively eliminates spurious correlation factors, ensuring classification decisions are based on authentic forgery cues rather than biased correlations, providing a practical solution to the generalization problem in face forgery detection.

Abstract: The generalization problem remains a critical challenge in face forgery detection. Some researches have discovered that ``a backdoor path" in the representations from forgery-irrelevant information to labels induces biased learning, thereby hindering the generalization. In this paper, these forgery-irrelevant information are collectively termed spurious correlations factors. Previous methods predominantly focused on identifying concrete, specific spurious correlation and designing corresponding solutions to address them. However, spurious correlations arise from unobservable confounding factors, making it impractical to identify and address each one individually. To address this, we propose an intervention paradigm for representation space. Instead of tracking and blocking various instance-level spurious correlation one by one, we uniformly model them as a low-rank subspace and intervene in them. Specifically, we decompose spurious correlation features into a low-rank subspace via orthogonal low-rank projection, subsequently removing this subspace from the original representation and training its orthogonal complement to capture forgery-related features. This low-rank projection removal effectively eliminates spurious correlation factors, ensuring that classification decision is based on authentic forgery cues. With only 0.43M trainable parameters, our method achieves state-of-the-art performance across several benchmarks, demonstrating excellent robustness and generalization.

</details>


### [38] [Effects of Gabor Filters on Classification Performance of CNNs Trained on a Limited Number of Conditions](https://arxiv.org/abs/2601.11918)
*Akito Morita,Hirotsugu Okuno*

Main category: cs.CV

TL;DR: Using Gabor filters (inspired by visual nervous system) as CNN preprocessing improves generalization and reduces model size for edge-based robot vision with limited training data.


<details>
  <summary>Details</summary>
Motivation: Edge devices need small, efficient CNNs for robot vision, but training with limited data under constrained conditions is challenging. The visual nervous system provides inspiration as it learns effectively from few visual experiences.

Method: Used Gabor filters (modeling visual nervous system feature extraction) as CNN preprocessing. Created dataset with images from different camera positions, trained CNNs on images from specific distances, and compared architectures with/without Gabor preprocessing.

Result: Gabor filter preprocessing improves CNN generalization performance and contributes to reducing CNN size, especially when trained with small amounts of data from limited conditions.

Conclusion: Biologically-inspired Gabor filter preprocessing is effective for developing compact, generalizable CNNs suitable for edge-based robot vision applications with limited training data.

Abstract: In this study, we propose a technique to improve the accuracy and reduce the size of convolutional neural networks (CNNs) running on edge devices for real-world robot vision applications. CNNs running on edge devices must have a small architecture, and CNNs for robot vision applications involving on-site object recognition must be able to be trained efficiently to identify specific visual targets from data obtained under a limited variation of conditions. The visual nervous system (VNS) is a good example that meets the above requirements because it learns from few visual experiences. Therefore, we used a Gabor filter, a model of the feature extractor of the VNS, as a preprocessor for CNNs to investigate the accuracy of the CNNs trained with small amounts of data. To evaluate how well CNNs trained on image data acquired under a limited variation of conditions generalize to data acquired under other conditions, we created an image dataset consisting of images acquired from different camera positions, and investigated the accuracy of the CNNs that trained using images acquired at a certain distance. The results were compared after training on multiple CNN architectures with and without Gabor filters as preprocessing. The results showed that preprocessing with Gabor filters improves the generalization performance of CNNs and contributes to reducing the size of CNNs.

</details>


### [39] [SupScene: Learning Overlap-Aware Global Descriptor for Unconstrained SfM](https://arxiv.org/abs/2601.11930)
*Xulei Shi,Maoyu Wang,Yuning Peng,Guanbo Wang,Xin Wang,Qi Chen,Pengjie Tao*

Main category: cs.CV

TL;DR: SupScene learns global descriptors for finding overlapping image pairs in Structure-from-Motion using subgraph-based training and DINO-inspired VLAD aggregation with attention-based gating.


<details>
  <summary>Details</summary>
Motivation: Existing image retrieval methods for SfM focus too much on semantic similarity rather than geometric matchability, and current deep learning approaches using binary overlapping/non-overlapping pairs fail to capture the nuanced geometric relationships needed for effective SfM.

Method: 1) Subgraph-based training strategy using ground-truth geometric overlapping relationships with various weights via soft supervised contrastive loss; 2) DiVLAD - DINO-inspired VLAD aggregator leveraging multi-head attention maps from ViT; 3) Learnable gating mechanism to adaptively combine semantic cues with visual features.

Result: Achieves state-of-the-art performance on GL3D dataset, significantly outperforming NetVLAD while adding negligible trainable parameters. The training strategy provides consistent gains across different aggregation techniques.

Conclusion: SupScene effectively addresses the geometric matchability problem in SfM image retrieval through novel subgraph-based training and attention-aware aggregation, demonstrating superior performance with minimal parameter overhead.

Abstract: Image retrieval is a critical step for alleviating the quadratic complexity of image matching in unconstrained Structure-from-Motion (SfM). However, in this context, image retrieval typically focuses more on the image pairs of geometric matchability than on those of semantic similarity, a nuance that most existing deep learning-based methods guided by batched binaries (overlapping vs. non-overlapping pairs) fail to capture. In this paper, we introduce SupScene, a novel solution that learns global descriptors tailored for finding overlapping image pairs of similar geometric nature for SfM. First, to better underline co-visible regions, we employ a subgraph-based training strategy that moves beyond equally important isolated pairs, leveraging ground-truth geometric overlapping relationships with various weights to provide fine-grained supervision via a soft supervised contrastive loss. Second, we introduce DiVLAD, a DINO-inspired VLAD aggregator that leverages the inherent multi-head attention maps from the last block of ViT. And then, a learnable gating mechanism is designed to adaptively utilize these semantically salient cues with visual features, enabling a more discriminative global descriptor. Extensive experiments on the GL3D dataset demonstrate that our method achieves state-of-the-art performance, significantly outperforming NetVLAD while introducing a negligible number of additional trainable parameters. Furthermore, we show that the proposed training strategy brings consistent gains across different aggregation techniques. Code and models are available at https://anonymous.4open.science/r/SupScene-5B73.

</details>


### [40] [Language-Guided and Motion-Aware Gait Representation for Generalizable Recognition](https://arxiv.org/abs/2601.11931)
*Zhengxian Wu,Chuanrui Zhang,Shenao Jiang,Hangrui Xu,Zirui Liao,Luyuan Zhang,Huaqiu Li,Peng Jiao,Haoqian Wang*

Main category: cs.CV

TL;DR: LMGait: A language-guided, motion-aware gait recognition framework that uses gait-related language cues to capture key motion features, addressing overfitting to static noise like clothing.


<details>
  <summary>Details</summary>
Motivation: Existing gait recognition methods overfit to static noise (e.g., clothing) while failing to effectively capture dynamic motion regions due to complex architectures that directly extract features from images and use pooling operations for sequence-level representations.

Method: LMGait uses designed gait-related language cues to capture key motion features in gait sequences, creating a language-guided and motion-aware framework for gait recognition.

Result: The abstract doesn't provide specific quantitative results, but the proposed method addresses the limitations of existing approaches by focusing on motion features rather than static noise.

Conclusion: LMGait represents an innovative approach to gait recognition that leverages language guidance to better capture motion dynamics and reduce overfitting to static appearance features like clothing.

Abstract: Gait recognition is emerging as a promising technology and an innovative field within computer vision. However, existing methods typically rely on complex architectures to directly extract features from images and apply pooling operations to obtain sequence-level representations. Such designs often lead to overfitting on static noise (e.g., clothing), while failing to effectively capture dynamic motion regions.To address the above challenges, we present a Language guided and Motion-aware gait recognition framework, named LMGait.In particular, we utilize designed gait-related language cues to capture key motion features in gait sequences.

</details>


### [41] [Deep learning-based neurodevelopmental assessment in preterm infants](https://arxiv.org/abs/2601.11944)
*Lexin Ren,Jiamiao Lu,Weichuan Zhang,Benqing Wu,Tuo Wang,Yi Liao,Jiapan Guo,Changming Sun,Liang Guo*

Main category: cs.CV

TL;DR: A novel Hierarchical Dense Attention Network improves white and gray matter segmentation in preterm infant brain MRI by addressing isointense tissue challenges, showing preterm infants have significantly lower brain volumes than term infants.


<details>
  <summary>Details</summary>
Motivation: Preterm infants face high neurodevelopmental risks, but accurate MRI segmentation of white and gray matter is difficult due to their similar signal intensities (isointense appearance) during early brain development, limiting effective assessment.

Method: Proposed Hierarchical Dense Attention Network with 3D spatial-channel attention mechanism and attention-guided dense upsampling strategy to enhance feature discrimination in low-contrast volumetric MRI data.

Result: Method achieves superior segmentation performance compared to state-of-the-art baselines and confirms that white and gray matter volumes in preterm infants are significantly lower than in term infants.

Conclusion: The proposed network effectively addresses isointense tissue differentiation challenges and provides imaging evidence of neurodevelopmental delays in preterm infants, with code publicly available for research use.

Abstract: Preterm infants (born between 28 and 37 weeks of gestation) face elevated risks of neurodevelopmental delays, making early identification crucial for timely intervention. While deep learning-based volumetric segmentation of brain MRI scans offers a promising avenue for assessing neonatal neurodevelopment, achieving accurate segmentation of white matter (WM) and gray matter (GM) in preterm infants remains challenging due to their comparable signal intensities (isointense appearance) on MRI during early brain development. To address this, we propose a novel segmentation neural network, named Hierarchical Dense Attention Network. Our architecture incorporates a 3D spatial-channel attention mechanism combined with an attention-guided dense upsampling strategy to enhance feature discrimination in low-contrast volumetric data. Quantitative experiments demonstrate that our method achieves superior segmentation performance compared to state-of-the-art baselines, effectively tackling the challenge of isointense tissue differentiation. Furthermore, application of our algorithm confirms that WM and GM volumes in preterm infants are significantly lower than those in term infants, providing additional imaging evidence of the neurodevelopmental delays associated with preterm birth. The code is available at: https://github.com/ICL-SUST/HDAN.

</details>


### [42] [Decoder Gradient Shields: A Family of Provable and High-Fidelity Methods Against Gradient-Based Box-Free Watermark Removal](https://arxiv.org/abs/2601.11952)
*Haonan An,Guang Hua,Wei Du,Hangcheng Cao,Yihang Tao,Guowen Xu,Susanto Rahardja,Yuguang Fang*

Main category: cs.CV

TL;DR: Proposed Decoder Gradient Shields (DGS) to defend against gradient-based attacks on box-free watermark decoders, achieving 100% defense success rate.


<details>
  <summary>Details</summary>
Motivation: Existing box-free watermarking research focuses on encoder robustness but overlooks decoder vulnerabilities, leaving watermarks exposed to gradient-based attacks where attackers use query responses to train watermark removers.

Method: Proposed DGS family: DGS-O (output), DGS-I (input), and DGS-L (layers) defenses that reorient and rescale gradients from watermark channel gradient leaking queries. DGS-O has closed-form solution, all DGS have provable performance.

Result: DGS achieved 100% defense success rate across all settings in deraining and image generation tasks with state-of-the-art box-free watermarking, preventing watermark remover training convergence while preserving decoder output image quality.

Conclusion: Decoder Gradient Shields effectively protect box-free watermarks against gradient-based attacks by manipulating decoder gradients, addressing a critical vulnerability in existing watermarking systems.

Abstract: Box-free model watermarking has gained significant attention in deep neural network (DNN) intellectual property protection due to its model-agnostic nature and its ability to flexibly manage high-entropy image outputs from generative models. Typically operating in a black-box manner, it employs an encoder-decoder framework for watermark embedding and extraction. While existing research has focused primarily on the encoders for the robustness to resist various attacks, the decoders have been largely overlooked, leading to attacks against the watermark. In this paper, we identify one such attack against the decoder, where query responses are utilized to obtain backpropagated gradients to train a watermark remover. To address this issue, we propose Decoder Gradient Shields (DGSs), a family of defense mechanisms, including DGS at the output (DGS-O), at the input (DGS-I), and in the layers (DGS-L) of the decoder, with a closed-form solution for DGS-O and provable performance for all DGS. Leveraging the joint design of reorienting and rescaling of the gradients from watermark channel gradient leaking queries, the proposed DGSs effectively prevent the watermark remover from achieving training convergence to the desired low-loss value, while preserving image quality of the decoder output. We demonstrate the effectiveness of our proposed DGSs in diverse application scenarios. Our experimental results on deraining and image generation tasks with the state-of-the-art box-free watermarking show that our DGSs achieve a defense success rate of 100% under all settings.

</details>


### [43] [Real-Time Multi-Modal Embedded Vision Framework for Object Detection Facial Emotion Recognition and Biometric Identification on Low-Power Edge Platforms](https://arxiv.org/abs/2601.11970)
*S. M. Khalid Bin Zahid,Md. Rakibul Hasan Nishat,Abdul Hasib,Md. Rakibul Hasan,Md. Ashiqussalehin,Md. Sahadat Hossen Sajib,A. S. M. Ahsanul Sarkar Akib*

Main category: cs.CV

TL;DR: A real-time multi-modal vision framework with adaptive scheduling for edge devices that integrates object detection, face recognition, and emotion analysis, reducing computational load by 65% while maintaining good performance metrics.


<details>
  <summary>Details</summary>
Motivation: Current intelligent surveillance systems handle perceptual tasks independently without unified adaptive scheduling, limiting holistic understanding and efficiency on low-power edge devices. There's a need for context-aware resource allocation to enable complex multi-modal AI on cost-effective hardware.

Method: Developed a unified pipeline integrating YOLOv8n for object detection, custom FaceNet-based embedding for facial recognition, and DeepFace's CNN for emotion classification. The core innovation is an adaptive scheduling mechanism that selectively activates modules based on contextual triggers to reduce computational load.

Result: Achieved 65% reduction in computational load compared to continuous processing. Object detection AP: 0.861, facial recognition accuracy: 88%, emotion detection AUC up to 0.97 for specific emotions, operating at 5.6 FPS on Raspberry Pi 5.

Conclusion: Context-aware scheduling is essential for enabling complex multi-modal AI on cost-effective edge hardware, making intelligent perception more accessible and privacy-preserving while maintaining good performance metrics.

Abstract: Intelligent surveillance systems often handle perceptual tasks such as object detection, facial recognition, and emotion analysis independently, but they lack a unified, adaptive runtime scheduler that dynamically allocates computational resources based on contextual triggers. This limits their holistic understanding and efficiency on low-power edge devices. To address this, we present a real-time multi-modal vision framework that integrates object detection, owner-specific face recognition, and emotion detection into a unified pipeline deployed on a Raspberry Pi 5 edge platform. The core of our system is an adaptive scheduling mechanism that reduces computational load by 65\% compared to continuous processing by selectively activating modules such as, YOLOv8n for object detection, a custom FaceNet-based embedding system for facial recognition, and DeepFace's CNN for emotion classification. Experimental results demonstrate the system's efficacy, with the object detection module achieving an Average Precision (AP) of 0.861, facial recognition attaining 88\% accuracy, and emotion detection showing strong discriminatory power (AUC up to 0.97 for specific emotions), while operating at 5.6 frames per second. Our work demonstrates that context-aware scheduling is the key to unlocking complex multi-modal AI on cost-effective edge hardware, making intelligent perception more accessible and privacy-preserving.

</details>


### [44] [AVIR: Adaptive Visual In-Document Retrieval for Efficient Multi-Page Document Question Answering](https://arxiv.org/abs/2601.11976)
*Zongmin Li,Yachuan Li,Lei Kang,Dimosthenis Karatzas,Wenkang Ma*

Main category: cs.CV

TL;DR: AVIR framework uses adaptive visual retrieval to select relevant document pages for MP-DocVQA, reducing page count by 70% while achieving state-of-the-art performance without model fine-tuning.


<details>
  <summary>Details</summary>
Motivation: MP-DocVQA is challenging because long documents strain computational resources and reduce attention mechanism effectiveness in large vision-language models.

Method: Lightweight retrieval model scores page relevance, clusters pages based on score distribution for adaptive selection, uses Top-K screening for compact context, and employs relevance probability threshold for short documents. Selected pages are fed to frozen LVLM for answer generation.

Result: Reduces average page count by 70%, achieves 84.58% ANLS on MP-DocVQA dataset (surpassing previous methods), and shows effectiveness on SlideVQA and DUDE benchmarks with lower computational cost.

Conclusion: AVIR framework effectively addresses MP-DocVQA challenges by adaptively selecting relevant document content, enabling efficient question answering without model fine-tuning while maintaining high accuracy.

Abstract: Multi-page Document Visual Question Answering (MP-DocVQA) remains challenging because long documents not only strain computational resources but also reduce the effectiveness of the attention mechanism in large vision-language models (LVLMs). We tackle these issues with an Adaptive Visual In-document Retrieval (AVIR) framework. A lightweight retrieval model first scores each page for question relevance. Pages are then clustered according to the score distribution to adaptively select relevant content. The clustered pages are screened again by Top-K to keep the context compact. However, for short documents, clustering reliability decreases, so we use a relevance probability threshold to select pages. The selected pages alone are fed to a frozen LVLM for answer generation, eliminating the need for model fine-tuning. The proposed AVIR framework reduces the average page count required for question answering by 70%, while achieving an ANLS of 84.58% on the MP-DocVQA dataset-surpassing previous methods with significantly lower computational cost. The effectiveness of the proposed AVIR is also verified on the SlideVQA and DUDE benchmarks. The code is available at https://github.com/Li-yachuan/AVIR.

</details>


### [45] [Nip Rumors in the Bud: Retrieval-Guided Topic-Level Adaptation for Test-Time Fake News Video Detection](https://arxiv.org/abs/2601.11981)
*Jian Lang,Rongpei Hong,Ting Zhong,Yong Wang,Fan Zhou*

Main category: cs.CV

TL;DR: RADAR is a test-time adaptation framework for fake news video detection that handles unseen topics by using retrieval-guided adaptation with stable source-close videos to align unstable instances and adapt to changing label distributions.


<details>
  <summary>Details</summary>
Motivation: Existing fake news video detection methods fail on emerging events and unseen topics because they assume consistent news topic distribution between training and testing phases. There's a need for methods that can adapt to new, unseen topics during test time.

Method: RADAR introduces a retrieval-guided adaptation paradigm with three key components: 1) Entropy Selection-Based Retrieval to find stable (low-entropy) reference videos, 2) Stable Anchor-Guided Alignment module that aligns unstable instances to source domain via distribution-level matching, and 3) Target-Domain Aware Self-Training that generates informative pseudo-labels using stable references to capture changing label distributions.

Result: Extensive experiments show RADAR achieves superior performance for test-time fake news video detection, enabling strong on-the-fly adaptation to unseen fake news video topics.

Conclusion: RADAR is the first framework that enables test-time adaptation to unseen news videos, bridging the gap in fake news video detection for emerging events and unseen topics through its novel retrieval-guided adaptation approach.

Abstract: Fake News Video Detection (FNVD) is critical for social stability. Existing methods typically assume consistent news topic distribution between training and test phases, failing to detect fake news videos tied to emerging events and unseen topics. To bridge this gap, we introduce RADAR, the first framework that enables test-time adaptation to unseen news videos. RADAR pioneers a new retrieval-guided adaptation paradigm that leverages stable (source-close) videos from the target domain to guide robust adaptation of semantically related but unstable instances. Specifically, we propose an Entropy Selection-Based Retrieval mechanism that provides videos with stable (low-entropy), relevant references for adaptation. We also introduce a Stable Anchor-Guided Alignment module that explicitly aligns unstable instances' representations to the source domain via distribution-level matching with their stable references, mitigating severe domain discrepancies. Finally, our novel Target-Domain Aware Self-Training paradigm can generate informative pseudo-labels augmented by stable references, capturing varying and imbalanced category distributions in the target domain and enabling RADAR to adapt to the fast-changing label distributions. Extensive experiments demonstrate that RADAR achieves superior performance for test-time FNVD, enabling strong on-the-fly adaptation to unseen fake news video topics.

</details>


### [46] [An AI-IoT Based Smart Wheelchair with Gesture-Controlled Mobility, Deep Learning-Based Obstacle Detection, Multi-Sensor Health Monitoring, and Emergency Alert System](https://arxiv.org/abs/2601.11983)
*Md. Asiful Islam,Abdul Hasib,Tousif Mahmud Emon,Khandaker Tabin Hasan,A. S. M. Ahsanul Sarkar Akib*

Main category: cs.CV

TL;DR: AI-IoT smart wheelchair system with gesture control, obstacle detection, and health monitoring achieves high accuracy rates for affordable assistive mobility.


<details>
  <summary>Details</summary>
Motivation: Growing need for affordable, intelligent wheelchairs for differently-abled and elderly individuals; traditional wheelchairs lack dynamic features, and existing smart alternatives are costly, single-modality, and limited in health integration.

Method: Comprehensive AI-IoT system with: 1) glove-based gesture control for hands-free navigation, 2) real-time object detection using YOLOv8 with auditory feedback, 3) ultrasonic sensors for immediate collision avoidance, and 4) continuous vital signs monitoring (heart rate, SpO2, ECG, temperature) with ThingSpeak cloud upload and email alerts.

Result: Gesture control achieved 95.5% success rate; ultrasonic obstacle detection reached 94% accuracy; YOLOv8-based object detection delivered 91.5% Precision, 90.2% Recall, and 90.8% F1-score.

Conclusion: Integrated multi-modal approach offers practical, scalable, affordable solution that enhances user autonomy, safety, and independence, bridging gap between innovative research and real-world deployment.

Abstract: The growing number of differently-abled and elderly individuals demands affordable, intelligent wheelchairs that combine safe navigation with health monitoring. Traditional wheelchairs lack dynamic features, and many smart alternatives remain costly, single-modality, and limited in health integration. Motivated by the pressing demand for advanced, personalized, and affordable assistive technologies, we propose a comprehensive AI-IoT based smart wheelchair system that incorporates glove-based gesture control for hands-free navigation, real-time object detection using YOLOv8 with auditory feedback for obstacle avoidance, and ultrasonic for immediate collision avoidance. Vital signs (heart rate, SpO$_2$, ECG, temperature) are continuously monitored, uploaded to ThingSpeak, and trigger email alerts for critical conditions. Built on a modular and low-cost architecture, the gesture control achieved a 95.5\% success rate, ultrasonic obstacle detection reached 94\% accuracy, and YOLOv8-based object detection delivered 91.5\% Precision, 90.2\% Recall, and a 90.8\% F1-score. This integrated, multi-modal approach offers a practical, scalable, and affordable solution, significantly enhancing user autonomy, safety, and independence by bridging the gap between innovative research and real-world deployment.

</details>


### [47] [Structural Graph Neural Networks with Anatomical Priors for Explainable Chest X-ray Diagnosis](https://arxiv.org/abs/2601.11987)
*Khaled Berkani*

Main category: cs.CV

TL;DR: A graph reasoning framework using anatomical priors for explainable medical diagnosis, converting image patches to graphs with spatial-aware propagation for structured inference.


<details>
  <summary>Details</summary>
Motivation: To create explainable vision-based diagnosis systems that incorporate explicit anatomical structural priors, moving beyond black-box models to enable transparent reasoning about medical images.

Method: Converts convolutional feature maps to patch-level graphs where nodes encode appearance and spatial coordinates, with edges reflecting local adjacency. Introduces custom structural propagation that explicitly models relative spatial relations, enabling graphs to serve as inductive biases for structured inference.

Result: Demonstrated through chest X-ray case study showing how structural priors guide relational reasoning and improve interpretability. The framework supports both node-level lesion predictions and graph-level diagnostic reasoning with intrinsic explainability via learned node importance scores.

Conclusion: Proposes a domain-agnostic graph reasoning framework that contributes to structure-aware and explainable learning in AI systems, particularly valuable for medical imaging where interpretability is crucial.

Abstract: We present a structural graph reasoning framework that incorporates explicit anatomical priors for explainable vision-based diagnosis. Convolutional feature maps are reinterpreted as patch-level graphs, where nodes encode both appearance and spatial coordinates, and edges reflect local structural adjacency. Unlike conventional graph neural networks that rely on generic message passing, we introduce a custom structural propagation mechanism that explicitly models relative spatial relations as part of the reasoning process. This design enables the graph to act as an inductive bias for structured inference rather than a passive relational representation. The proposed model jointly supports node-level lesion-aware predictions and graph-level diagnostic reasoning, yielding intrinsic explainability through learned node importance scores without relying on post-hoc visualization techniques. We demonstrate the approach through a chest X-ray case study, illustrating how structural priors guide relational reasoning and improve interpretability. While evaluated in a medical imaging context, the framework is domain-agnostic and aligns with the broader vision of graph-based reasoning across artificial intelligence systems. This work contributes to the growing body of research exploring graphs as computational substrates for structure-aware and explainable learning.

</details>


### [48] [DAOS: A Multimodal In-cabin Behavior Monitoring with Driver Action-Object Synergy Dataset](https://arxiv.org/abs/2601.11990)
*Yiming Li,Chen Cai,Tianyi Liu,Dan Lin,Wenqian Wang,Wenfei Liang,Bingbing Li,Kim-Hui Yap*

Main category: cs.CV

TL;DR: DAOS dataset with 9,787 video clips, 36 driver actions, 15 object classes, and multi-modal views; AOR-Net model uses multi-level reasoning and chain-of-action prompting for better driver action recognition.


<details>
  <summary>Details</summary>
Motivation: Existing driver-monitoring datasets lack accurate object-location annotations and don't link objects to actions, creating a gap for reliable action recognition since drivers often use objects (phone vs steering wheel) to distinguish similar upper-body movements.

Method: Introduces DAOS dataset with multi-modal (RGB, IR, depth) multi-view data and proposes AOR-Net with multi-level reasoning, chain-of-action prompting to model action-object relations, and Mixture of Thoughts module for dynamic knowledge selection.

Result: AOR-Net outperforms state-of-the-art methods on various datasets, demonstrating effectiveness in handling both object-rich and object-scarce conditions for driver action recognition.

Conclusion: The DAOS dataset addresses critical gaps in driver monitoring, and AOR-Net effectively models human-object relations for fine-grained action recognition, advancing driver activity monitoring through better understanding of action-object synergies.

Abstract: In driver activity monitoring, movements are mostly limited to the upper body, which makes many actions look similar. To tell these actions apart, human often rely on the objects the driver is using, such as holding a phone compared with gripping the steering wheel. However, most existing driver-monitoring datasets lack accurate object-location annotations or do not link objects to their associated actions, leaving a critical gap for reliable action recognition. To address this, we introduce the Driver Action with Object Synergy (DAOS) dataset, comprising 9,787 video clips annotated with 36 fine-grained driver actions and 15 object classes, totaling more than 2.5 million corresponding object instances. DAOS offers multi-modal, multi-view data (RGB, IR, and depth) from front, face, left, and right perspectives. Although DAOS captures a wide range of cabin objects, only a few are directly relevant to each action for prediction, so focusing on task-specific human-object relations is essential. To tackle this challenge, we propose the Action-Object-Relation Network (AOR-Net). AOR-Net comprehends complex driver actions through multi-level reasoning and a chain-of-action prompting mechanism that models the logical relationships among actions, objects, and their relations. Additionally, the Mixture of Thoughts module is introduced to dynamically select essential knowledge at each stage, enhancing robustness in object-rich and object-scarce conditions. Extensive experiments demonstrate that our model outperforms other state-of-the-art methods on various datasets.

</details>


### [49] [SMc2f: Robust Scenario Mining for Robotic Autonomy from Coarse to Fine](https://arxiv.org/abs/2601.12010)
*Yifei Chen,Ross Greer*

Main category: cs.CV

TL;DR: Proposes SMc2f, a coarse-to-fine pipeline for mining safety-critical scenarios from driving logs using vision-language models and contrastive learning to improve retrieval quality and efficiency over existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing methods like RefAV rely on trajectory labels and ignore direct connections between natural language and raw RGB images, which contradicts video retrieval intuition. They also depend on upstream 3D object detection/tracking quality, and trajectory inaccuracies lead to downstream localization errors.

Method: SMc2f uses a coarse-to-fine pipeline: 1) VLMs for coarse image-text filtering, 2) builds a database of successful mining cases on top of RefAV with automatic exemplar retrieval for few-shot LLM conditioning, and 3) introduces text-trajectory contrastive learning to create a fine-grained matcher that refines LLM's candidate trajectories.

Result: Experiments on public datasets demonstrate substantial gains in both retrieval quality and efficiency compared to existing methods.

Conclusion: The proposed SMc2f framework addresses limitations of existing scenario mining approaches by leveraging vision-language models and contrastive learning to create a more robust and efficient pipeline for mining safety-critical scenarios from driving logs.

Abstract: The safety validation of autonomous robotic vehicles hinges on systematically testing their planning and control stacks against rare, safety-critical scenarios. Mining these long-tail events from massive real-world driving logs is therefore a critical step in the robotic development lifecycle. The goal of the Scenario Mining task is to retrieve useful information to enable targeted re-simulation, regression testing, and failure analysis of the robot's decision-making algorithms. RefAV, introduced by the Argoverse team, is an end-to-end framework that uses large language models (LLMs) to spatially and temporally localize scenarios described in natural language. However, this process performs retrieval on trajectory labels, ignoring the direct connection between natural language and raw RGB images, which runs counter to the intuition of video retrieval; it also depends on the quality of upstream 3D object detection and tracking. Further, inaccuracies in trajectory data lead to inaccuracies in downstream spatial and temporal localization. To address these issues, we propose Robust Scenario Mining for Robotic Autonomy from Coarse to Fine (SMc2f), a coarse-to-fine pipeline that employs vision-language models (VLMs) for coarse image-text filtering, builds a database of successful mining cases on top of RefAV and automatically retrieves exemplars to few-shot condition the LLM for more robust retrieval, and introduces text-trajectory contrastive learning to pull matched pairs together and push mismatched pairs apart in a shared embedding space, yielding a fine-grained matcher that refines the LLM's candidate trajectories. Experiments on public datasets demonstrate substantial gains in both retrieval quality and efficiency.

</details>


### [50] [SAR-Based Marine Oil Spill Detection Using the DeepSegFusion Architecture](https://arxiv.org/abs/2601.12015)
*Pavan Kumar Yata,Pediredla Pradeep,Goli Himanish,Swathi M*

Main category: cs.CV

TL;DR: DeepSegFusion: A hybrid deep learning model combining SegNet and DeepLabV3+ with attention-based feature fusion for accurate oil spill segmentation in SAR images, achieving 94.85% accuracy and 64.4% reduction in false detections.


<details>
  <summary>Details</summary>
Motivation: Traditional threshold-based methods for oil spill detection in satellite images suffer from high false alarm rates due to look-alike phenomena like wind slicks and ship wakes, necessitating more robust and accurate segmentation approaches.

Method: A hybrid deep learning model called DeepSegFusion that integrates SegNet and DeepLabV3+ architectures with an attention-based feature fusion mechanism to improve boundary precision and contextual understanding for oil spill segmentation in SAR images.

Result: Achieved 94.85% accuracy, 0.5685 IoU, and 0.9330 ROC-AUC score on SAR oil spill datasets including ALOS PALSAR imagery, with more than three times fewer false detections (64.4% reduction) compared to baseline models and traditional methods.

Conclusion: DeepSegFusion is a stable model under various marine conditions that can be used for near real-time oil spill monitoring, offering significant improvements in accuracy and false detection reduction over existing approaches.

Abstract: Detection of oil spills from satellite images is essential for both environmental surveillance and maritime safety. Traditional threshold-based methods frequently encounter performance degradation due to very high false alarm rates caused by look-alike phenomena such as wind slicks and ship wakes. Here, a hybrid deep learning model, DeepSegFusion, is presented for oil spill segmentation in Synthetic Aperture Radar (SAR) images. The model uses SegNet and DeepLabV3+ integrated with an attention-based feature fusion mechanism to achieve better boundary precision as well as improved contextual understanding. Results obtained on SAR oil spill datasets, including ALOS PALSAR imagery, confirm that the proposed DeepSegFusion model achieves an accuracy of 94.85%, an Intersection over Union (IoU) of 0.5685, and a ROC-AUC score of 0.9330. The proposed method delivers more than three times fewer false detections compared to individual baseline models and traditional non-segmentation methods, achieving a reduction of 64.4%. These results indicate that DeepSegFusion is a stable model under various marine conditions and can therefore be used in near real-time oil spill monitoring scenarios.

</details>


### [51] [DIAMOND-SSS: Diffusion-Augmented Multi-View Optimization for Data-efficient SubSurface Scattering](https://arxiv.org/abs/2601.12020)
*Guillermo Figueroa-Araneda,Iris Diana Jimenez,Florian Hofherr,Manny Ko,Hector Andrade-Loarca,Daniel Cremers*

Main category: cs.CV

TL;DR: DIAMOND-SSS enables high-fidelity translucent material reconstruction from extremely sparse data (as few as 10 images) using diffusion models for data augmentation and illumination-independent geometric priors.


<details>
  <summary>Details</summary>
Motivation: Current neural rendering methods for subsurface scattering require dense multi-view, multi-light datasets (often 100+ views and 112 OLATs), making data acquisition expensive and time-consuming.

Method: Fine-tune diffusion models for novel-view synthesis and relighting conditioned on estimated geometry, trained on <7% of dataset; introduce illumination-independent geometric priors (multi-view silhouette consistency loss and multi-view depth consistency loss) for stable reconstruction.

Result: Achieves state-of-the-art quality in relightable Gaussian rendering across all sparsity regimes, reducing real capture requirements by up to 90% compared to SSS-3DGS, with photorealistic augmentations replacing up to 95% of missing captures.

Conclusion: DIAMOND-SSS provides a data-efficient framework for high-fidelity translucent reconstruction from extremely sparse supervision, dramatically reducing the data acquisition burden for subsurface scattering modeling.

Abstract: Subsurface scattering (SSS) gives translucent materials -- such as wax, jade, marble, and skin -- their characteristic soft shadows, color bleeding, and diffuse glow. Modeling these effects in neural rendering remains challenging due to complex light transport and the need for densely captured multi-view, multi-light datasets (often more than 100 views and 112 OLATs).
  We present DIAMOND-SSS, a data-efficient framework for high-fidelity translucent reconstruction from extremely sparse supervision -- even as few as ten images. We fine-tune diffusion models for novel-view synthesis and relighting, conditioned on estimated geometry and trained on less than 7 percent of the dataset, producing photorealistic augmentations that can replace up to 95 percent of missing captures. To stabilize reconstruction under sparse or synthetic supervision, we introduce illumination-independent geometric priors: a multi-view silhouette consistency loss and a multi-view depth consistency loss.
  Across all sparsity regimes, DIAMOND-SSS achieves state-of-the-art quality in relightable Gaussian rendering, reducing real capture requirements by up to 90 percent compared to SSS-3DGS.

</details>


### [52] [\textit{FocaLogic}: Logic-Based Interpretation of Visual Model Decisions](https://arxiv.org/abs/2601.12049)
*Chenchen Zhao,Muxi Chen,Qiang Xu*

Main category: cs.CV

TL;DR: FocaLogic is a model-agnostic framework that interprets visual models by identifying minimal visual regions (visual focuses) that influence predictions and translating them into logical expressions, with quantitative metrics for evaluation.


<details>
  <summary>Details</summary>
Motivation: Existing interpretability methods for visual models either require white-box model access or lack quantitative rigor, limiting their applicability in high-stakes scenarios where transparent decision-making is crucial.

Method: FocaLogic identifies minimal interpretable subsets of visual regions (visual focuses) that decisively influence model predictions, then translates these into precise logical expressions. It includes quantitative metrics (focus precision, recall, divergence) for objective evaluation.

Result: Empirical analyses show FocaLogic can uncover critical insights: training-induced concentration, improved focus accuracy through generalization, and anomalous focuses under biases and adversarial attacks.

Conclusion: FocaLogic provides a systematic, scalable, and quantitative solution for interpreting visual models, addressing limitations of existing methods by being model-agnostic and offering rigorous quantitative evaluation.

Abstract: Interpretability of modern visual models is crucial, particularly in high-stakes applications. However, existing interpretability methods typically suffer from either reliance on white-box model access or insufficient quantitative rigor. To address these limitations, we introduce FocaLogic, a novel model-agnostic framework designed to interpret and quantify visual model decision-making through logic-based representations. FocaLogic identifies minimal interpretable subsets of visual regions-termed visual focuses-that decisively influence model predictions. It translates these visual focuses into precise and compact logical expressions, enabling transparent and structured interpretations. Additionally, we propose a suite of quantitative metrics, including focus precision, recall, and divergence, to objectively evaluate model behavior across diverse scenarios. Empirical analyses demonstrate FocaLogic's capability to uncover critical insights such as training-induced concentration, increasing focus accuracy through generalization, and anomalous focuses under biases and adversarial attacks. Overall, FocaLogic provides a systematic, scalable, and quantitative solution for interpreting visual models.

</details>


### [53] [A Unified Masked Jigsaw Puzzle Framework for Vision and Language Models](https://arxiv.org/abs/2601.12051)
*Weixin Ye,Wei Wang,Yahui Liu,Yue Song,Bin Ren,Wei Bi,Rita Cucchiara,Nicu Sebe*

Main category: cs.CV

TL;DR: A Masked Jigsaw Puzzle (MJP) framework improves Transformer security against gradient attacks and boosts performance in CV/NLP tasks by disrupting position embeddings through token shuffling and masking.


<details>
  <summary>Details</summary>
Motivation: Transformers in federated learning are vulnerable to gradient attacks where position embeddings can leak input data. There's a need to improve both security and performance across vision and language tasks.

Method: MJP framework uses random token shuffling to break token order, then applies learnable unknown position embeddings to mask shuffled tokens' position embeddings, disrupting local spatial information and forcing models to learn less position-dependent features.

Result: MJP improves robustness against gradient attacks while boosting performance on ImageNet-1K classification and Yelp/Amazon sentiment analysis tasks, showing effectiveness across different Transformer models in both vision and language domains.

Conclusion: MJP provides a unified framework for enhancing Transformer security and performance in federated learning across CV and NLP applications by disrupting position embedding vulnerabilities.

Abstract: In federated learning, Transformer, as a popular architecture, faces critical challenges in defending against gradient attacks and improving model performance in both Computer Vision (CV) and Natural Language Processing (NLP) tasks. It has been revealed that the gradient of Position Embeddings (PEs) in Transformer contains sufficient information, which can be used to reconstruct the input data. To mitigate this issue, we introduce a Masked Jigsaw Puzzle (MJP) framework. MJP starts with random token shuffling to break the token order, and then a learnable \textit{unknown (unk)} position embedding is used to mask out the PEs of the shuffled tokens. In this manner, the local spatial information which is encoded in the position embeddings is disrupted, and the models are forced to learn feature representations that are less reliant on the local spatial information. Notably, with the careful use of MJP, we can not only improve models' robustness against gradient attacks, but also boost their performance in both vision and text application scenarios, such as classification for images (\textit{e.g.,} ImageNet-1K) and sentiment analysis for text (\textit{e.g.,} Yelp and Amazon). Experimental results suggest that MJP is a unified framework for different Transformer-based models in both vision and language tasks. Code is publicly available via https://github.com/ywxsuperstar/transformerattack

</details>


### [54] [Task-Driven Prompt Learning: A Joint Framework for Multi-modal Cloud Removal and Segmentation](https://arxiv.org/abs/2601.12052)
*Zaiyan Zhang,Jie Li,Shaowei Shi,Qiangqiang Yuan*

Main category: cs.CV

TL;DR: TDP-CR is a task-driven multimodal framework for cloud removal and land-cover segmentation that uses a Prompt-Guided Fusion mechanism to adaptively integrate SAR data only where optical data is corrupted, achieving superior performance with fewer parameters.


<details>
  <summary>Details</summary>
Motivation: Cloud occlusion in optical remote sensing imagery limits downstream utility, and existing cloud removal methods often over-smooth textures and boundaries, creating a mismatch between visually plausible restoration and semantic utility for analysis-ready data.

Method: Proposes TDP-CR with Prompt-Guided Fusion (PGF) mechanism that uses learnable degradation prompts to encode cloud thickness and spatial uncertainty, combining global channel context with local prompt-conditioned spatial bias to adaptively integrate SAR information. Uses parameter-efficient two-phase training that decouples reconstruction and semantic representation learning.

Result: On LuojiaSET-OSFCR dataset: surpasses state-of-the-art baselines by 0.18 dB in PSNR while using only 15% of parameters, and achieves 1.4% improvement in mIoU consistently against multi-task competitors.

Conclusion: TDP-CR effectively bridges the gap between visually plausible restoration and semantic utility, delivering analysis-ready data through task-driven multimodal cloud removal with adaptive SAR fusion.

Abstract: Optical remote sensing imagery is indispensable for Earth observation, yet persistent cloud occlusion limits its downstream utility. Most cloud removal (CR) methods are optimized for low-level fidelity and can over-smooth textures and boundaries that are critical for analysis-ready data (ARD), leading to a mismatch between visually plausible restoration and semantic utility. To bridge this gap, we propose TDP-CR, a task-driven multimodal framework that jointly performs cloud removal and land-cover segmentation. Central to our approach is a Prompt-Guided Fusion (PGF) mechanism, which utilizes a learnable degradation prompt to encode cloud thickness and spatial uncertainty. By combining global channel context with local prompt-conditioned spatial bias, PGF adaptively integrates Synthetic Aperture Radar (SAR) information only where optical data is corrupted. We further introduce a parameter-efficient two-phase training strategy that decouples reconstruction and semantic representation learning. Experiments on the LuojiaSET-OSFCR dataset demonstrate the superiority of our framework: TDP-CR surpasses heavy state-of-the-art baselines by 0.18 dB in PSNR while using only 15\% of the parameters, and achieves a 1.4\% improvement in mIoU consistently against multi-task competitors, effectively delivering analysis-ready data.

</details>


### [55] [Automating Parameter Selection in Deep Image Prior for Fluorescence Microscopy Image Denoising via Similarity-Based Parameter Transfer](https://arxiv.org/abs/2601.12055)
*Lina Meyer,Felix Wissel,Tobias Knopp,Susanne Pfefferle,Ralf Fliegert,Maximilian Sandmann,Liana Uebler,Franziska Möckl,Björn-Philipp Diercks,David Lohr,René Werner*

Main category: cs.CV

TL;DR: AUTO-DIP enables optimization-free unsupervised denoising for fluorescence microscopy by transferring optimal DIP parameters from similar images, outperforming baseline DIP and variational methods.


<details>
  <summary>Details</summary>
Motivation: Unsupervised DIP requires manual optimization of network architecture and stopping points for each image, which is time-consuming when processing many images. The authors hypothesize that similar fluorescence microscopy images share optimal DIP parameters, enabling parameter transfer without per-image optimization.

Method: Generated calibration (n=110) and validation (n=55) sets from open-source fluorescence microscopy data. Performed network architecture search for optimal U-net configurations and stopping points. Developed AUTO-DIP pipeline for automatic parameter transfer based on image similarity criteria (metadata vs. quantitative measures). Compared against baseline DIP and state-of-the-art variational denoising.

Result: Parameter transfer based only on image metadata similarity (microscope type, specimen) performed similarly or better than quantitative image similarity measures. AUTO-DIP outperformed baseline DIP and variational denoising approaches across multiple test datasets, especially for very noisy inputs. Demonstrated superiority on locally acquired fluorescence microscopy images.

Conclusion: AUTO-DIP enables efficient, optimization-free DIP-based denoising for fluorescence microscopy by leveraging parameter transfer from similar images, making unsupervised deep learning more practical for large-scale microscopy applications.

Abstract: Unsupervised deep image prior (DIP) addresses shortcomings of training data requirements and limited generalization associated with supervised deep learning. The performance of DIP depends on the network architecture and the stopping point of its iterative process. Optimizing these parameters for a new image requires time, restricting DIP application in domains where many images need to be processed. Focusing on fluorescence microscopy data, we hypothesize that similar images share comparable optimal parameter configurations for DIP-based denoising, potentially enabling optimization-free DIP for fluorescence microscopy. We generated a calibration (n=110) and validation set (n=55) of semantically different images from an open-source dataset for a network architecture search targeted towards ideal U-net architectures and stopping points. The calibration set represented our transfer basis. The validation set enabled the assessment of which image similarity criterion yields the best results. We then implemented AUTO-DIP, a pipeline for automatic parameter transfer, and compared it to the originally published DIP configuration (baseline) and a state-of-the-art image-specific variational denoising approach. We show that a parameter transfer from the calibration dataset to a test image based on only image metadata similarity (e.g., microscope type, imaged specimen) leads to similar and better performance than a transfer based on quantitative image similarity measures. AUTO-DIP outperforms the baseline DIP (DIP with original DIP parameters) as well as the variational denoising approaches for several open-source test datasets of varying complexity, particularly for very noisy inputs. Applications to locally acquired fluorescence microscopy images further proved superiority of AUTO-DIP.

</details>


### [56] [Learning Language-Driven Sequence-Level Modal-Invariant Representations for Video-Based Visible-Infrared Person Re-Identification](https://arxiv.org/abs/2601.12062)
*Xiaomei Yang,Xizhan Gao,Antai Liu,Kang Wei,Fa Zhu,Guang Feng,Xiaofeng Qu,Sijie Niu*

Main category: cs.CV

TL;DR: LSMRL method uses language-driven sequence-level modal-invariant representation learning for VVI-ReID, addressing spatial-temporal modeling, cross-modal interaction, and modality-level loss guidance limitations.


<details>
  <summary>Details</summary>
Motivation: Current VVI-ReID methods using CLIP language prompts still face limitations in efficient spatial-temporal modeling, sufficient cross-modal interaction, and explicit modality-level loss guidance.

Method: Proposes LSMRL with three modules: STFL for parameter-efficient spatial-temporal modeling on CLIP, SD for diffusing language prompts into visible/infrared features, and CMI for bidirectional cross-modal self-attention to eliminate modality gaps.

Result: Extensive experiments on large-scale VVI-ReID datasets demonstrate superiority over AOTA methods.

Conclusion: LSMRL effectively addresses limitations in spatial-temporal modeling, cross-modal interaction, and modality-level guidance for VVI-ReID, achieving state-of-the-art performance.

Abstract: The core of video-based visible-infrared person re-identification (VVI-ReID) lies in learning sequence-level modal-invariant representations across different modalities. Recent research tends to use modality-shared language prompts generated by CLIP to guide the learning of modal-invariant representations. Despite achieving optimal performance, such methods still face limitations in efficient spatial-temporal modeling, sufficient cross-modal interaction, and explicit modality-level loss guidance. To address these issues, we propose the language-driven sequence-level modal-invariant representation learning (LSMRL) method, which includes spatial-temporal feature learning (STFL) module, semantic diffusion (SD) module and cross-modal interaction (CMI) module. To enable parameter- and computation-efficient spatial-temporal modeling, the STFL module is built upon CLIP with minimal modifications. To achieve sufficient cross-modal interaction and enhance the learning of modal-invariant features, the SD module is proposed to diffuse modality-shared language prompts into visible and infrared features to establish preliminary modal consistency. The CMI module is further developed to leverage bidirectional cross-modal self-attention to eliminate residual modality gaps and refine modal-invariant representations. To explicitly enhance the learning of modal-invariant representations, two modality-level losses are introduced to improve the features' discriminative ability and their generalization to unseen categories. Extensive experiments on large-scale VVI-ReID datasets demonstrate the superiority of LSMRL over AOTA methods.

</details>


### [57] [Learning Stochastic Bridges for Video Object Removal via Video-to-Video Translation](https://arxiv.org/abs/2601.12066)
*Zijie Lou,Xiangwei Feng,Jiaxin Wang,Xiaochao Qu,Luoqi Liu,Ting Liu*

Main category: cs.CV

TL;DR: Video object removal reformulated as video-to-video translation using stochastic bridge model instead of noise-to-data diffusion, with adaptive mask modulation for better results.


<details>
  <summary>Details</summary>
Motivation: Existing diffusion-based video object removal methods discard structural priors from input video, leading to incomplete removal or implausible content. Need to leverage input video as strong prior for better guidance.

Method: Reformulates video object removal as video-to-video translation via stochastic bridge model that establishes direct path from source video (with objects) to target video (objects removed). Uses adaptive mask modulation to balance background fidelity with generative flexibility for large object removal.

Result: Extensive experiments show significant outperformance over existing methods in both visual quality and temporal consistency.

Conclusion: Stochastic bridge formulation effectively leverages input video as structural prior for precise object removal while maintaining logical consistency with surrounding environment.

Abstract: Existing video object removal methods predominantly rely on diffusion models following a noise-to-data paradigm, where generation starts from uninformative Gaussian noise. This approach discards the rich structural and contextual priors present in the original input video. Consequently, such methods often lack sufficient guidance, leading to incomplete object erasure or the synthesis of implausible content that conflicts with the scene's physical logic. In this paper, we reformulate video object removal as a video-to-video translation task via a stochastic bridge model. Unlike noise-initialized methods, our framework establishes a direct stochastic path from the source video (with objects) to the target video (objects removed). This bridge formulation effectively leverages the input video as a strong structural prior, guiding the model to perform precise removal while ensuring that the filled regions are logically consistent with the surrounding environment. To address the trade-off where strong bridge priors hinder the removal of large objects, we propose a novel adaptive mask modulation strategy. This mechanism dynamically modulates input embeddings based on mask characteristics, balancing background fidelity with generative flexibility. Extensive experiments demonstrate that our approach significantly outperforms existing methods in both visual quality and temporal consistency.

</details>


### [58] [ARMARecon: An ARMA Convolutional Filter based Graph Neural Network for Neurodegenerative Dementias Classification](https://arxiv.org/abs/2601.12067)
*VSS Tejaswi Abburi,Ananya Singhal,Saurabh J. Shigwan,Nitin Kumar*

Main category: cs.CV

TL;DR: ARMARecon: A graph learning framework using ARMA filtering and reconstruction objectives for early detection of Alzheimer's and Frontotemporal Dementia from white-matter diffusion MRI data.


<details>
  <summary>Details</summary>
Motivation: Early detection of neurodegenerative diseases like Alzheimer's Disease (AD) and Frontotemporal Dementia (FTD) is crucial to reduce progression risk. Since these diseases propagate along white-matter regions in a graph-dependent manner, graph-based neural networks are well-suited to capture these patterns.

Method: ARMARecon integrates Autoregressive Moving Average (ARMA) graph filtering with a reconstruction-driven objective to enhance feature representation. It models both local and global connectivity using 20-bin Fractional Anisotropy (FA) histogram features from white-matter regions while mitigating over-smoothing issues.

Result: ARMARecon achieves superior performance compared to state-of-the-art methods on multi-site dMRI datasets ADNI and NIFD.

Conclusion: The proposed unified graph learning framework effectively captures disease propagation patterns in white-matter regions and improves classification accuracy for early detection of neurodegenerative diseases.

Abstract: Early detection of neurodegenerative diseases such as Alzheimer's Disease (AD) and Frontotemporal Dementia (FTD) is essential for reducing the risk of progression to severe disease stages. As AD and FTD propagate along white-matter regions in a global, graph-dependent manner, graph-based neural networks are well suited to capture these patterns. Hence, we introduce ARMARecon, a unified graph learning framework that integrates Autoregressive Moving Average (ARMA) graph filtering with a reconstruction-driven objective to enhance feature representation and improve classification accuracy. ARMARecon effectively models both local and global connectivity by leveraging 20-bin Fractional Anisotropy (FA) histogram features extracted from white-matter regions, while mitigating over-smoothing. Overall, ARMARecon achieves superior performance compared to state-of-the-art methods on the multi-site dMRI datasets ADNI and NIFD.

</details>


### [59] [CroBIM-V: Memory-Quality Controlled Remote Sensing Referring Video Object Segmentation](https://arxiv.org/abs/2601.12076)
*H. Jiang,Y. Sun,Z. Dong,T. Liu,Y. Gu*

Main category: cs.CV

TL;DR: Proposes RS-RVOS Bench benchmark and MQC-SAM framework for remote sensing video referring object segmentation, addressing weak target saliency and error propagation issues.


<details>
  <summary>Details</summary>
Motivation: Remote sensing RVOS faces challenges with weak target saliency, visual truncation, biased memory construction, and error propagation. Lack of large-scale benchmarks hinders progress.

Method: 1) Creates RS-RVOS Bench with 111 videos, 25K frames, 213K annotations using causality-aware annotation. 2) Proposes MQC-SAM with temporal motion consistency module for memory calibration and decoupled attention-based memory integration with dynamic quality assessment.

Result: MQC-SAM achieves state-of-the-art performance on RS-RVOS Bench through extensive experiments.

Conclusion: The paper advances RS-RVOS research through dual contributions: a large-scale benchmark and a memory-quality-aware framework that effectively addresses error propagation and improves segmentation accuracy.

Abstract: Remote sensing video referring object segmentation (RS-RVOS) is challenged by weak target saliency and severe visual information truncation in dynamic scenes, making it extremely difficult to maintain discriminative target representations during segmentation. Moreover, progress in this field is hindered by the absence of large-scale dedicated benchmarks, while existing models are often affected by biased initial memory construction that impairs accurate instance localization in complex scenarios, as well as indiscriminate memory accumulation that encodes noise from occlusions or misclassifications, leading to persistent error propagation. This paper advances RS-RVOS research through dual contributions in data and methodology. First, we construct RS-RVOS Bench, the first large-scale benchmark comprising 111 video sequences, about 25,000 frames, and 213,000 temporal referring annotations. Unlike common RVOS benchmarks where many expressions are written with access to the full video context, our dataset adopts a strict causality-aware annotation strategy in which linguistic references are generated solely from the target state in the initial frame. Second, we propose a memory-quality-aware online referring segmentation framework, termed Memory Quality Control with Segment Anything Model (MQC-SAM). MQC-SAM introduces a temporal motion consistency module for initial memory calibration, leveraging short-term motion trajectory priors to correct structural deviations and establish accurate memory anchoring. Furthermore, it incorporates a decoupled attention-based memory integration mechanism with dynamic quality assessment, selectively updating high-confidence semantic features while filtering unreliable information, thereby effectively preventing error accumulation and propagation. Extensive experiments on RS-RVOS Bench demonstrate that MQC-SAM achieves state-of-the-art performance.

</details>


### [60] [EmoLat: Text-driven Image Sentiment Transfer via Emotion Latent Space](https://arxiv.org/abs/2601.12079)
*Jing Zhang,Bingjie Fan,Jixiang Zhu,Zhe Wang*

Main category: cs.CV

TL;DR: EmoLat is a novel emotion latent space for text-driven image sentiment transfer that models cross-modal correlations between text and visual emotion features using an emotion semantic graph and adversarial regularization.


<details>
  <summary>Details</summary>
Motivation: Existing methods lack fine-grained control over image sentiment transfer using textual guidance. There's a need for better modeling of cross-modal correlations between textual semantics and visual emotion features for controllable sentiment editing.

Method: 1) Construct EmoLat emotion latent space with emotion semantic graph capturing relationships among emotions, objects, and visual attributes. 2) Use adversarial regularization to align latent emotion distributions across modalities. 3) Build cross-modal sentiment transfer framework with joint text-EmoLat embedding. 4) Optimize with multi-objective loss (semantic consistency, emotion alignment, adversarial regularization). 5) Create EmoSpace Set benchmark dataset with dense emotion, object, and attribute annotations.

Result: Extensive experiments on EmoSpace Set show the approach significantly outperforms state-of-the-art methods in both quantitative metrics and qualitative transfer fidelity, establishing a new paradigm for controllable image sentiment editing.

Conclusion: EmoLat enables fine-grained, text-driven image sentiment transfer through effective cross-modal emotion modeling. The proposed framework and benchmark dataset advance the field of controllable image sentiment editing guided by textual input.

Abstract: We propose EmoLat, a novel emotion latent space that enables fine-grained, text-driven image sentiment transfer by modeling cross-modal correlations between textual semantics and visual emotion features. Within EmoLat, an emotion semantic graph is constructed to capture the relational structure among emotions, objects, and visual attributes. To enhance the discriminability and transferability of emotion representations, we employ adversarial regularization, aligning the latent emotion distributions across modalities. Building upon EmoLat, a cross-modal sentiment transfer framework is proposed to manipulate image sentiment via joint embedding of text and EmoLat features. The network is optimized using a multi-objective loss incorporating semantic consistency, emotion alignment, and adversarial regularization. To support effective modeling, we construct EmoSpace Set, a large-scale benchmark dataset comprising images with dense annotations on emotions, object semantics, and visual attributes. Extensive experiments on EmoSpace Set demonstrate that our approach significantly outperforms existing state-of-the-art methods in both quantitative metrics and qualitative transfer fidelity, establishing a new paradigm for controllable image sentiment editing guided by textual input. The EmoSpace Set and all the code are available at http://github.com/JingVIPLab/EmoLat.

</details>


### [61] [Toward Real-World High-Precision Image Matting and Segmentation](https://arxiv.org/abs/2601.12080)
*Haipeng Zhou,Zhaohu Xing,Hongqiu Wang,Jun Ma,Ping Li,Lei Zhu*

Main category: cs.CV

TL;DR: FCLM is a Foreground Consistent Learning model for high-precision scene parsing that addresses limitations in existing methods through depth-aware distillation, domain-invariant learning for synthetic data, and an object-oriented decoder for interactive prediction.


<details>
  <summary>Details</summary>
Motivation: Existing methods for high-precision scene parsing (image matting and dichotomous segmentation) focus on single foreground objects, have class-agnostic interactive methods with poor generalization, and rely on inharmonious synthetic data that doesn't generalize well to real-world scenarios.

Method: Proposes FCLM with three key components: 1) Depth-Aware Distillation strategy to transfer depth-related knowledge for better foreground representation, 2) Domain-invariant learning strategy to treat synthetic data processing as a domain adaptation problem focusing on foreground learning, and 3) Object-Oriented Decoder that accepts both visual and language prompts for interactive target prediction.

Result: Experimental results show the method quantitatively and qualitatively outperforms state-of-the-art methods.

Conclusion: FCLM effectively addresses the limitations of existing high-precision scene parsing methods by improving foreground consistency through depth-aware knowledge transfer, domain adaptation for synthetic data, and flexible interactive prediction capabilities.

Abstract: High-precision scene parsing tasks, including image matting and dichotomous segmentation, aim to accurately predict masks with extremely fine details (such as hair). Most existing methods focus on salient, single foreground objects. While interactive methods allow for target adjustment, their class-agnostic design restricts generalization across different categories. Furthermore, the scarcity of high-quality annotation has led to a reliance on inharmonious synthetic data, resulting in poor generalization to real-world scenarios. To this end, we propose a Foreground Consistent Learning model, dubbed as FCLM, to address the aforementioned issues. Specifically, we first introduce a Depth-Aware Distillation strategy where we transfer the depth-related knowledge for better foreground representation. Considering the data dilemma, we term the processing of synthetic data as domain adaptation problem where we propose a domain-invariant learning strategy to focus on foreground learning. To support interactive prediction, we contribute an Object-Oriented Decoder that can receive both visual and language prompts to predict the referring target. Experimental results show that our method quantitatively and qualitatively outperforms SOTA methods.

</details>


### [62] [Conditional Random Fields for Interactive Refinement of Histopathological Predictions](https://arxiv.org/abs/2601.12082)
*Tiffanie Godelaine,Maxime Zanella,Karim El Khoury,Saïd Mahmoudi,Benoît Macq,Christophe De Vleeschouwer*

Main category: cs.CV

TL;DR: HistoCRF: A CRF-based framework that refines zero-shot predictions from histology foundation models without additional training, using pairwise potentials to promote label diversity and leverage expert annotations.


<details>
  <summary>Details</summary>
Motivation: Vision-Language Models provide strong but imperfect zero-shot predictions for histopathological image analysis, which has high clinical value for cancer detection and staging. There's a need to refine these predictions without requiring additional model training.

Method: Proposes HistoCRF, a Conditional Random Fields framework adapted to histopathology with a novel pairwise potential definition that promotes label diversity and leverages expert annotations. The method works in three scenarios: without annotations, with expert annotations, and with iterative human-in-the-loop annotations that progressively correct misclassified patches.

Result: Experiments on five patch-level classification datasets covering different organs and diseases show average accuracy gains of 16.0% without annotations, 27.5% with only 100 annotations compared to zero-shot predictions. Human-in-the-loop integration reaches a further gain of 32.6% with the same number of annotations.

Conclusion: HistoCRF effectively refines zero-shot predictions from histology foundation models, demonstrating significant accuracy improvements across multiple datasets and annotation scenarios, with particular benefits from human-in-the-loop integration.

Abstract: Assisting pathologists in the analysis of histopathological images has high clinical value, as it supports cancer detection and staging. In this context, histology foundation models have recently emerged. Among them, Vision-Language Models (VLMs) provide strong yet imperfect zero-shot predictions. We propose to refine these predictions by adapting Conditional Random Fields (CRFs) to histopathological applications, requiring no additional model training. We present HistoCRF, a CRF-based framework, with a novel definition of the pairwise potential that promotes label diversity and leverages expert annotations. We consider three experiments: without annotations, with expert annotations, and with iterative human-in-the-loop annotations that progressively correct misclassified patches. Experiments on five patch-level classification datasets covering different organs and diseases demonstrate average accuracy gains of 16.0% without annotations and 27.5% with only 100 annotations, compared to zero-shot predictions. Moreover, integrating a human in the loop reaches a further gain of 32.6% with the same number of annotations. The code will be made available on https://github.com/tgodelaine/HistoCRF.

</details>


### [63] [Detecting 3D Line Segments for 6DoF Pose Estimation with Limited Data](https://arxiv.org/abs/2601.12090)
*Matej Mok,Lukáš Gajdošech,Michal Mesároš,Martin Madaras,Viktor Kocur*

Main category: cs.CV

TL;DR: A novel 6DoF pose estimation method for industrial bins that detects 3D line segments from point clouds and uses geometric reasoning, outperforming state-of-the-art without requiring CAD models during inference.


<details>
  <summary>Details</summary>
Motivation: Traditional deep learning approaches for 6DoF object pose estimation require extensive training data or CAD models, which limits their application in real-world industrial settings where data is scarce and object instances vary. There's a need for methods that work in data-scarce industrial environments.

Method: The method exploits the cuboid geometry of industrial bins by first detecting intermediate 3D line segments corresponding to their top edges. It extends the 2D line segment detection network LeTR to operate on structured point cloud data. The detected 3D line segments are then processed using a simple geometric procedure to robustly determine the bin's 6DoF pose.

Result: The method achieves state-of-the-art performance with 3 cm translation error and 8.2° rotation error. Incorporating synthetic training data significantly improves pose estimation accuracy on real scans. The method outperforms current state-of-the-art 6DoF pose estimation methods while not requiring instance-specific CAD models during inference.

Conclusion: The proposed method provides an effective solution for 6DoF pose estimation of industrial bins that works well in data-scarce environments, leverages geometric priors, and eliminates the need for CAD models during inference, making it practical for real-world industrial applications.

Abstract: The task of 6DoF object pose estimation is one of the fundamental problems of 3D vision with many practical applications such as industrial automation. Traditional deep learning approaches for this task often require extensive training data or CAD models, limiting their application in real-world industrial settings where data is scarce and object instances vary. We propose a novel method for 6DoF pose estimation focused specifically on bins used in industrial settings. We exploit the cuboid geometry of bins by first detecting intermediate 3D line segments corresponding to their top edges. Our approach extends the 2D line segment detection network LeTR to operate on structured point cloud data. The detected 3D line segments are then processed using a simple geometric procedure to robustly determine the bin's 6DoF pose. To evaluate our method, we extend an existing dataset with a newly collected and annotated dataset, which we make publicly available. We show that incorporating synthetic training data significantly improves pose estimation accuracy on real scans. Moreover, we show that our method significantly outperforms current state-of-the-art 6DoF pose estimation methods in terms of the pose accuracy (3 cm translation error, 8.2$^\circ$ rotation error) while not requiring instance-specific CAD models during inference.

</details>


### [64] [Energy-Aware Ensemble Learning for Coffee Leaf Disease Classification](https://arxiv.org/abs/2601.12109)
*Larissa Ferreira Rodrigues Moreira,Rodrigo Moreira,Leonardo Gabriel Ferreira Rodrigues*

Main category: cs.CV

TL;DR: Lightweight AI models for coffee leaf disease diagnosis using knowledge distillation and ensemble learning achieve competitive accuracy with reduced energy consumption for IoT applications.


<details>
  <summary>Details</summary>
Motivation: Coffee yields depend on timely disease diagnosis, but field assessment is challenging. While AI vision models are accurate, their adoption is limited by constrained devices and intermittent connectivity in agricultural settings.

Method: Used knowledge distillation where high-capacity CNNs trained in data centers transfer knowledge to compact CNNs via Ensemble Learning. Integrated dense tiny pairs through simple and optimized ensembling to enhance accuracy while maintaining computational and energy constraints.

Result: On a curated coffee leaf dataset, distilled tiny ensembles achieved competitive accuracy compared to prior work with significantly reduced energy consumption and carbon footprint.

Conclusion: Lightweight models, when properly distilled and ensembled, can provide practical diagnostic solutions for IoT applications in agriculture, enabling sustainable on-device disease diagnosis.

Abstract: Coffee yields are contingent on the timely and accurate diagnosis of diseases; however, assessing leaf diseases in the field presents significant challenges. Although Artificial Intelligence (AI) vision models achieve high accuracy, their adoption is hindered by the limitations of constrained devices and intermittent connectivity. This study aims to facilitate sustainable on-device diagnosis through knowledge distillation: high-capacity Convolutional Neural Networks (CNNs) trained in data centers transfer knowledge to compact CNNs through Ensemble Learning (EL). Furthermore, dense tiny pairs were integrated through simple and optimized ensembling to enhance accuracy while adhering to strict computational and energy constraints. On a curated coffee leaf dataset, distilled tiny ensembles achieved competitive with prior work with significantly reduced energy consumption and carbon footprint. This indicates that lightweight models, when properly distilled and ensembled, can provide practical diagnostic solutions for Internet of Things (IoT) applications.

</details>


### [65] [RCDN: Real-Centered Detection Network for Robust Face Forgery Identification](https://arxiv.org/abs/2601.12111)
*Wyatt McCurdy,Xin Zhang,Yuqi Song,Min Gao*

Main category: cs.CV

TL;DR: RCDN is a frequency-spatial CNN framework that anchors representation space around authentic facial images to improve cross-domain generalization for image forgery detection, achieving state-of-the-art performance and reducing generalization gaps.


<details>
  <summary>Details</summary>
Motivation: Existing forgery detection methods perform well within the same domain but fail in cross-domain scenarios, which is problematic as new forgery techniques continuously emerge and detectors must remain reliable against unseen manipulations.

Method: Proposes Real-Centered Detection Network (RCDN), a frequency-spatial CNN framework with Xception backbone that anchors representation space around authentic facial images. Uses dual-branch architecture and real-centered loss design to emphasize consistency of real images rather than modeling diverse forgery patterns.

Result: Extensive experiments on DiFF dataset with three forgery types (FE, I2I, T2I) show RCDN achieves state-of-the-art in-domain accuracy and significantly stronger cross-domain generalization. Reduces generalization gap compared to leading baselines and achieves highest cross/in-domain stability ratio.

Conclusion: RCDN demonstrates strong potential as a practical solution for defending against evolving and unseen image forgery techniques by focusing on real image consistency rather than modeling diverse forgery patterns, enabling better generalization across domains.

Abstract: Image forgery has become a critical threat with the rapid proliferation of AI-based generation tools, which make it increasingly easy to synthesize realistic but fraudulent facial content. Existing detection methods achieve near-perfect performance when training and testing are conducted within the same domain, yet their effectiveness deteriorates substantially in crossdomain scenarios. This limitation is problematic, as new forgery techniques continuously emerge and detectors must remain reliable against unseen manipulations. To address this challenge, we propose the Real-Centered Detection Network (RCDN), a frequency spatial convolutional neural networks(CNN) framework with an Xception backbone that anchors its representation space around authentic facial images. Instead of modeling the diverse and evolving patterns of forgeries, RCDN emphasizes the consistency of real images, leveraging a dual-branch architecture and a real centered loss design to enhance robustness under distribution shifts. Extensive experiments on the DiFF dataset, focusing on three representative forgery types (FE, I2I, T2I), demonstrate that RCDN achieves both state-of-the-art in-domain accuracy and significantly stronger cross-domain generalization. Notably, RCDN reduces the generalization gap compared to leading baselines and achieves the highest cross/in-domain stability ratio, highlighting its potential as a practical solution for defending against evolving and unseen image forgery techniques.

</details>


### [66] [CARLA-Round: A Multi-Factor Simulation Dataset for Roundabout Trajectory Prediction](https://arxiv.org/abs/2601.12119)
*Xiaotong Zhou,Zhenhui Yuan,Yi Han,Tianhua Xu,Laurence T. Yang*

Main category: cs.CV

TL;DR: CARLA-Round: A systematic simulation dataset for roundabout trajectory prediction with controlled weather and traffic density variations, enabling factor impact analysis and showing effective sim-to-real transfer.


<details>
  <summary>Details</summary>
Motivation: Roundabout trajectory prediction is critical for safety but challenging due to circular geometry and complex interactions. Existing datasets are scarce and real-world data suffers from incomplete observations and entangled factors that are difficult to isolate.

Method: Created CARLA-Round, a systematically designed simulation dataset with 25 controlled scenarios varying weather conditions (5 types) and traffic density levels (LOS A-E). Includes realistic driving behaviors and explicit annotations not available in existing datasets.

Result: Traffic density dominates prediction difficulty with strong monotonic effects, while weather shows non-linear impacts. Best model achieves 0.312m ADE on real-world rounD dataset, demonstrating effective sim-to-real transfer.

Conclusion: The systematic simulation approach enables precise analysis of factor impacts impossible to isolate in confounded real-world datasets, providing a valuable resource for roundabout trajectory prediction research.

Abstract: Accurate trajectory prediction of vehicles at roundabouts is critical for reducing traffic accidents, yet it remains highly challenging due to their circular road geometry, continuous merging and yielding interactions, and absence of traffic signals. Developing accurate prediction algorithms relies on reliable, multimodal, and realistic datasets; however, such datasets for roundabout scenarios are scarce, as real-world data collection is often limited by incomplete observations and entangled factors that are difficult to isolate. We present CARLA-Round, a systematically designed simulation dataset for roundabout trajectory prediction. The dataset varies weather conditions (five types) and traffic density levels (spanning Level-of-Service A-E) in a structured manner, resulting in 25 controlled scenarios. Each scenario incorporates realistic mixtures of driving behaviors and provides explicit annotations that are largely absent from existing datasets. Unlike randomly sampled simulation data, this structured design enables precise analysis of how different conditions influence trajectory prediction performance. Validation experiments using standard baselines (LSTM, GCN, GRU+GCN) reveal traffic density dominates prediction difficulty with strong monotonic effects, while weather shows non-linear impacts. The best model achieves 0.312m ADE on real-world rounD dataset, demonstrating effective sim-to-real transfer. This systematic approach quantifies factor impacts impossible to isolate in confounded real-world datasets. Our CARLA-Round dataset is available at https://github.com/Rebecca689/CARLA-Round.

</details>


### [67] [Segment and Matte Anything in a Unified Model](https://arxiv.org/abs/2601.12147)
*Zezhong Fan,Xiaohan Li,Topojoy Biswas,Kaushiki Nag,Kannan Achan*

Main category: cs.CV

TL;DR: SAMA extends SAM to unify high-quality interactive segmentation and matting in a lightweight framework with minimal extra parameters.


<details>
  <summary>Details</summary>
Motivation: SAM's mask prediction accuracy falls short for real-world applications, and interactive image matting hasn't been explored in SAM context despite strong correlations between segmentation and matting tasks.

Method: Introduces Multi-View Localization Encoder (MVLE) for detailed local features, Localization Adapter for boundary refinement, and dual prediction heads for segmentation and matting tasks simultaneously.

Result: Achieves state-of-the-art performance across multiple segmentation and matting benchmarks, demonstrating adaptability and effectiveness in diverse downstream tasks.

Conclusion: SAMA successfully unifies interactive segmentation and matting in a lightweight extension of SAM, addressing accuracy limitations while maintaining efficiency.

Abstract: Segment Anything (SAM) has recently pushed the boundaries of segmentation by demonstrating zero-shot generalization and flexible prompting after training on over one billion masks. Despite this, its mask prediction accuracy often falls short of the precision required in real-world applications. While several refinement modules have been proposed to boost SAM's segmentation quality, achieving highly accurate object delineation within a single, unified framework remains an open challenge. Furthermore, interactive image matting, which aims to generate fine-grained alpha mattes guided by diverse user hints, has not yet been explored in the context of SAM. Insights from recent studies highlight strong correlations between segmentation and matting, suggesting the feasibility of a unified model capable of both tasks. In this paper, we introduce Segment And Matte Anything (SAMA), a lightweight extension of SAM that delivers high-quality interactive image segmentation and matting with minimal extra parameters. Our Multi-View Localization Encoder (MVLE) captures detailed features from local views, while the Localization Adapter (Local-Adapter) refines mask outputs by recovering subtle boundary details. We also incorporate two prediction heads for each task into the architecture to generate segmentation and matting masks, simultaneously. Trained on a diverse dataset aggregated from publicly available sources, SAMA achieves state-of-the-art performance across multiple segmentation and matting benchmarks, showcasing its adaptability and effectiveness in a wide range of downstream tasks.

</details>


### [68] [Principal Component Analysis-Based Terahertz Self-Supervised Denoising and Deblurring Deep Neural Networks](https://arxiv.org/abs/2601.12149)
*Pengfei Zhu,Xavier Maldague*

Main category: cs.CV

TL;DR: THz-SSDD: A self-supervised network using PCA and Recorrupted-to-Recorrupted learning for simultaneous denoising and deblurring of THz images without labeled data.


<details>
  <summary>Details</summary>
Motivation: THz systems suffer from frequency-dependent degradation - low-frequency blurring and high-frequency noise. Conventional methods can't handle both simultaneously, and manual intervention is needed due to unknown boundaries between denoising and deblurring.

Method: Proposes THz-SSDD network using Recorrupted-to-Recorrupted self-supervised learning to capture noise features through invariance under repeated corruption, combined with PCA decomposition and reconstruction for image restoration across frequency bands.

Result: Evaluated on four sample types, requiring only small unlabeled noisy image sets for training. Effective denoising and deblurring across different material properties and measurement modes, with quantitative analysis validating improved image quality while preserving original signal characteristics.

Conclusion: THz-SSDD provides an effective self-supervised solution for simultaneous denoising and deblurring of THz images, overcoming limitations of conventional methods and reducing need for manual intervention.

Abstract: Terahertz (THz) systems inherently introduce frequency-dependent degradation effects, resulting in low-frequency blurring and high-frequency noise in amplitude images. Conventional image processing techniques cannot simultaneously address both issues, and manual intervention is often required due to the unknown boundary between denoising and deblurring. To tackle this challenge, we propose a principal component analysis (PCA)-based THz self-supervised denoising and deblurring network (THz-SSDD). The network employs a Recorrupted-to-Recorrupted self-supervised learning strategy to capture the intrinsic features of noise by exploiting invariance under repeated corruption. PCA decomposition and reconstruction are then applied to restore images across both low and high frequencies. The performance of the THz-SSDD network was evaluated on four types of samples. Training requires only a small set of unlabeled noisy images, and testing across samples with different material properties and measurement modes demonstrates effective denoising and deblurring. Quantitative analysis further validates the network feasibility, showing improvements in image quality while preserving the physical characteristics of the original signals.

</details>


### [69] [Enhanced Diagnostic Performance via Large-Resolution Inference Optimization for Pathology Foundation Models](https://arxiv.org/abs/2601.12150)
*Mengxuan Hu,Zihan Guan,John Kang,Sheng Li,Zhongliang Zhou*

Main category: cs.CV

TL;DR: A space- and time-efficient inference strategy for pathology foundation models that sparsifies attention using spatially aware neighboring blocks and filters non-informative tokens, enabling efficient high-resolution whole-slide image inference without GPU memory constraints.


<details>
  <summary>Details</summary>
Motivation: Pathology foundation models are constrained by fixed input sizes (e.g., 224x224), creating inefficiencies for whole-slide images spanning thousands of resolutions. Naive approaches (enlarging inputs or downsampling WSIs) either cause prohibitive GPU memory consumption or obscure critical morphological details.

Method: Proposes an efficient inference strategy that sparsifies attention using spatially aware neighboring blocks and filters out non-informative tokens through global attention scores. This reduces GPU memory and runtime while preserving high-resolution information.

Result: Achieves up to 7.67% improvement in ROI classification and compatible results in segmentation. Enables inference at higher resolutions under the same GPU budget by substantially reducing memory and runtime requirements.

Conclusion: The proposed method overcomes limitations of pathology foundation models by enabling efficient high-resolution whole-slide image inference through attention sparsification and token filtering, improving performance while reducing computational costs.

Abstract: Despite their prominent performance on tasks such as ROI classification and segmentation, many pathology foundation models remain constrained by a specific input size e.g. 224 x 224, creating substantial inefficiencies when applied to whole-slide images (WSIs), which span thousands of resolutions. A naive strategy is to either enlarge inputs or downsample the WSIs. However, enlarging inputs results in prohibitive GPU memory consumption, while downsampling alters the microns-per-pixel resolution and obscures critical morphological details. To overcome these limitations, we propose an space- and time- efficient inference strategy that sparsifies attention using spatially aware neighboring blocks and filters out non-informative tokens through global attention scores. This design substantially reduces GPU memory and runtime during high-resolution WSI inference while preserving and even improving the downstream performance, enabling inference at higher resolutions under the same GPU budget. The experimental results show that our method can achieves up to an 7.67% improvement in the ROI classification and compatible results in segmentation.

</details>


### [70] [Inverse Rendering for High-Genus 3D Surface Meshes from Multi-view Images with Persistent Homology Priors](https://arxiv.org/abs/2601.12155)
*Xiang Gao,Xinmu Wang,Yuanpeng Liu,Yue Wang,Junqi Huang,Wei Chen,Xianfeng Gu*

Main category: cs.CV

TL;DR: Collaborative inverse rendering with persistent homology priors improves 3D reconstruction by using topological constraints to resolve ambiguities and handle high-genus surfaces.


<details>
  <summary>Details</summary>
Motivation: 3D reconstruction from images is ill-posed due to ambiguities in geometry, appearance, and topology, especially for complex high-genus surfaces where existing methods often fail by collapsing tunnels or losing topological structure.

Method: Collaborative inverse rendering combines photometric consistency from multi-view images with persistent homology priors that capture topological features like tunnel loops and handle loops. Uses gradient-based optimization within a mesh-based framework (no neural networks) to highlight topological priors' role.

Result: Method achieves lower Chamfer Distance (CD) and higher Volume IoU compared to state-of-the-art mesh-based methods, demonstrating improved geometric accuracy and robustness against topological failures.

Conclusion: Incorporating persistent homology priors effectively resolves reconstruction ambiguities for complex high-genus surfaces, providing a robust solution to topological challenges in 3D reconstruction.

Abstract: Reconstructing 3D objects from images is inherently an ill-posed problem due to ambiguities in geometry, appearance, and topology. This paper introduces collaborative inverse rendering with persistent homology priors, a novel strategy that leverages topological constraints to resolve these ambiguities. By incorporating priors that capture critical features such as tunnel loops and handle loops, our approach directly addresses the difficulty of reconstructing high-genus surfaces. The collaboration between photometric consistency from multi-view images and homology-based guidance enables recovery of complex high-genus geometry while circumventing catastrophic failures such as collapsing tunnels or losing high-genus structure. Instead of neural networks, our method relies on gradient-based optimization within a mesh-based inverse rendering framework to highlight the role of topological priors. Experimental results show that incorporating persistent homology priors leads to lower Chamfer Distance (CD) and higher Volume IoU compared to state-of-the-art mesh-based methods, demonstrating improved geometric accuracy and robustness against topological failure.

</details>


### [71] [VIRTUE: Versatile Video Retrieval Through Unified Embeddings](https://arxiv.org/abs/2601.12193)
*Shaunak Halbe,Bhagyashree Puranik,Jayakrishnan Unnikrishnan,Kushan Thakkar,Vimal Bhat,Toufiq Parag*

Main category: cs.CV

TL;DR: VIRTUE is an MLLM-based video retrieval framework that unifies corpus-level retrieval, moment localization, and multimodal querying in a single architecture, achieving state-of-the-art performance with efficient LoRA training.


<details>
  <summary>Details</summary>
Motivation: Specialized video retrieval architectures excel at specific tasks but cannot handle composed multimodal queries, while MLLM-based methods support rich queries but underperform on retrieval. There's a need for a unified system that combines the strengths of both approaches.

Method: Uses a shared MLLM backbone with contrastive alignment of visual and textual embeddings, trained efficiently with LoRA on 700K paired samples. Combines embedding-based candidate search with reranking, and adapts the same model for multiple retrieval tasks without additional training.

Result: Surpasses other MLLM-based methods on zero-shot video retrieval, achieves competitive results on zero-shot moment retrieval, and state-of-the-art results for zero-shot composed video retrieval. With reranking, outperforms existing MLLM systems and matches specialized models trained on much larger datasets.

Conclusion: VIRTUE demonstrates that a single MLLM-based architecture can effectively unify diverse video retrieval tasks while achieving performance comparable to specialized systems, offering a versatile solution for modern video retrieval needs.

Abstract: Modern video retrieval systems are expected to handle diverse tasks ranging from corpus-level retrieval and fine-grained moment localization to flexible multimodal querying. Specialized architectures achieve strong retrieval performance by training modality-specific encoders on massive datasets, but they lack the ability to process composed multimodal queries. In contrast, multimodal LLM (MLLM)-based methods support rich multimodal search but their retrieval performance remains well below that of specialized systems. We present VIRTUE, an MLLM-based versatile video retrieval framework that integrates corpus and moment-level retrieval capabilities while accommodating composed multimodal queries within a single architecture. We use contrastive alignment of visual and textual embeddings generated using a shared MLLM backbone to facilitate efficient embedding-based candidate search. Our embedding model, trained efficiently using low-rank adaptation (LoRA) on 700K paired visual-text data samples, surpasses other MLLM-based methods on zero-shot video retrieval tasks. Additionally, we demonstrate that the same model can be adapted without further training to achieve competitive results on zero-shot moment retrieval, and state of the art results for zero-shot composed video retrieval. With additional training for reranking candidates identified in the embedding-based search, our model substantially outperforms existing MLLM-based retrieval systems and achieves retrieval performance comparable to state of the art specialized models which are trained on orders of magnitude larger data.

</details>


### [72] [Where It Moves, It Matters: Referring Surgical Instrument Segmentation via Motion](https://arxiv.org/abs/2601.12224)
*Meng Wei,Kun Yuan,Shi Li,Yue Zhou,Long Bai,Nassir Navab,Hongliang Ren,Hong Joo Lee,Tom Vercauteren,Nicolas Padoy*

Main category: cs.CV

TL;DR: SurgRef is a motion-guided framework for surgical instrument segmentation from natural language descriptions, using instrument motion patterns rather than static visual features for better generalization.


<details>
  <summary>Details</summary>
Motivation: Current surgical referring segmentation approaches rely too heavily on static visual cues and predefined instrument names, limiting their ability to generalize across different surgical procedures, handle occlusion, or understand unfamiliar terminology. There's a need for more robust language-driven interaction with surgical scenes for intelligent operating rooms.

Method: SurgRef introduces a motion-guided framework that grounds free-form language expressions in instrument motion patterns. Instead of focusing on what instruments look like, it captures how tools move and interact across time. The approach is trained on Ref-IMotion, a diverse multi-institutional video dataset with dense spatiotemporal masks and motion-centric expressions.

Result: SurgRef achieves state-of-the-art accuracy and generalization across surgical procedures. It demonstrates robust performance even under challenging conditions like occlusion, ambiguity, or unfamiliar terminology, setting a new benchmark for language-driven surgical video segmentation.

Conclusion: By focusing on motion patterns rather than static visual features, SurgRef enables more intuitive, language-driven interaction with surgical scenes, representing an important step toward intelligent operating rooms and autonomous surgical robotic assistance.

Abstract: Enabling intuitive, language-driven interaction with surgical scenes is a critical step toward intelligent operating rooms and autonomous surgical robotic assistance. However, the task of referring segmentation, localizing surgical instruments based on natural language descriptions, remains underexplored in surgical videos, with existing approaches struggling to generalize due to reliance on static visual cues and predefined instrument names. In this work, we introduce SurgRef, a novel motion-guided framework that grounds free-form language expressions in instrument motion, capturing how tools move and interact across time, rather than what they look like. This allows models to understand and segment instruments even under occlusion, ambiguity, or unfamiliar terminology. To train and evaluate SurgRef, we present Ref-IMotion, a diverse, multi-institutional video dataset with dense spatiotemporal masks and rich motion-centric expressions. SurgRef achieves state-of-the-art accuracy and generalization across surgical procedures, setting a new benchmark for robust, language-driven surgical video segmentation.

</details>


### [73] [DiffusionQC: Artifact Detection in Histopathology via Diffusion Model](https://arxiv.org/abs/2601.12233)
*Zhenzhen Wang,Zhongliang Zhou,Zhuoyu Wen,Jeong Hwan Kook,John B Wojcik,John Kang*

Main category: cs.CV

TL;DR: DiffusionQC: A diffusion model-based method for artifact detection in digital pathology that requires only clean images for training, eliminating need for artifact annotations and predefined artifact types.


<details>
  <summary>Details</summary>
Motivation: Histopathology images often contain artifacts from slide preparation/digitization that must be detected and excluded for reliable analysis. Traditional supervised models need large annotated datasets, are resource-intensive, and lack generalization to novel artifact types.

Method: Uses diffusion model to detect artifacts as outliers among clean images. Requires only clean images for training (no pixel-level artifact annotations). Includes contrastive learning module to explicitly enlarge distribution separation between artifact and clean images.

Result: Superior performance to state-of-the-art methods, offers cross-stain generalization capacity, requires significantly less data and annotations.

Conclusion: DiffusionQC provides an effective, annotation-efficient approach for artifact detection in digital pathology with strong generalization capabilities.

Abstract: Digital pathology plays a vital role across modern medicine, offering critical insights for disease diagnosis, prognosis, and treatment. However, histopathology images often contain artifacts introduced during slide preparation and digitization. Detecting and excluding them is essential to ensure reliable downstream analysis. Traditional supervised models typically require large annotated datasets, which is resource-intensive and not generalizable to novel artifact types. To address this, we propose DiffusionQC, which detects artifacts as outliers among clean images using a diffusion model. It requires only a set of clean images for training rather than pixel-level artifact annotations and predefined artifact types. Furthermore, we introduce a contrastive learning module to explicitly enlarge the distribution separation between artifact and clean images, yielding an enhanced version of our method. Empirical results demonstrate superior performance to state-of-the-art and offer cross-stain generalization capacity, with significantly less data and annotations.

</details>


### [74] [Less is More: Label-Guided Summarization of Procedural and Instructional Videos](https://arxiv.org/abs/2601.12243)
*Shreya Rajpal,Michal Golovanesky,Carsten Eickhoff*

Main category: cs.CV

TL;DR: PRISM is a three-stage framework for video summarization that uses adaptive visual sampling, label-driven keyframe anchoring, and LLM validation to create semantically grounded summaries, retaining 84% semantic content with less than 5% of original frames.


<details>
  <summary>Details</summary>
Motivation: Video summarization is crucial for efficient review and analysis in high-stakes domains like surgical training. While prior work has evolved from basic visual features to vision-language models, there's a need for more context-aware, semantically grounded summaries that capture procedural transitions and filter out irrelevant content.

Method: PRISM uses a three-stage framework: 1) Adaptive visual sampling to reduce frame count, 2) Label-driven keyframe anchoring to identify meaningful procedural transitions, and 3) Contextual validation using a large language model (LLM) to filter out generic or hallucinated content and ensure semantic coherence.

Result: The method retains 84% semantic content while sampling fewer than 5% of original frames, improving over baselines by up to 33%. It generalizes well across procedural and domain-specific video tasks, achieving strong performance in both semantic alignment and precision.

Conclusion: PRISM provides an effective framework for producing contextually coherent, semantically grounded video summaries that capture meaningful procedural transitions while filtering irrelevant content, demonstrating strong generalization across different video domains.

Abstract: Video summarization helps turn long videos into clear, concise representations that are easier to review, document, and analyze, especially in high-stakes domains like surgical training. Prior work has progressed from using basic visual features like color, motion, and structural changes to using pre-trained vision-language models that can better understand what's happening in the video (semantics) and capture temporal flow, resulting in more context-aware video summarization. We propose a three-stage framework, PRISM: Procedural Representation via Integrated Semantic and Multimodal analysis, that produces semantically grounded video summaries. PRISM combines adaptive visual sampling, label-driven keyframe anchoring, and contextual validation using a large language model (LLM). Our method ensures that selected frames reflect meaningful and procedural transitions while filtering out generic or hallucinated content, resulting in contextually coherent summaries across both domain-specific and instructional videos. We evaluate our method on instructional and activity datasets, using reference summaries for instructional videos. Despite sampling fewer than 5% of the original frames, our summaries retain 84% semantic content while improving over baselines by as much as 33%. Our approach generalizes across procedural and domain-specific video tasks, achieving strong performance with both semantic alignment and precision.

</details>


### [75] [An Innovative Framework for Breast Cancer Detection Using Pyramid Adaptive Atrous Convolution, Transformer Integration, and Multi-Scale Feature Fusion](https://arxiv.org/abs/2601.12249)
*Ehsan Sadeghi Pour,Mahdi Esmaeili,Morteza Romoozi*

Main category: cs.CV

TL;DR: A novel breast cancer detection framework combining Pyramid Adaptive Atrous Convolution (PAAC) and Transformer architectures achieves state-of-the-art performance in mammographic image analysis with 98.5% accuracy.


<details>
  <summary>Details</summary>
Motivation: Breast cancer is a major global health concern for women, and accurate, timely diagnosis is crucial for improving treatment outcomes. Current methods need improvement in detecting malignant masses in mammographic images, especially in complex scenarios and large datasets.

Method: Proposed framework integrates Pyramid Adaptive Atrous Convolution (PAAC) with Transformer architecture. Uses Multi-Scale Feature Fusion to enhance feature extraction from benign/malignant tissues. Combines Dice Loss and Focal Loss functions to improve learning. Trained on preprocessed INbreast, MIAS, and DDSM datasets (augmented, contrast-enhanced, resized to 227x227). Leverages Transformer's Self-Attention for long-range dependencies.

Result: Achieved state-of-the-art performance: 98.5% accuracy, 97.8% sensitivity, 96.3% specificity, 98.2% F1-score, 97.9% precision. Outperformed benchmark models including BreastNet, DeepMammo, Multi-Scale CNN, Swin-Unet, and SegFormer.

Conclusion: The proposed PAAC-Transformer framework demonstrates significant improvement over traditional methods and shows potential as a reliable, efficient tool for breast cancer diagnosis that can be integrated into medical diagnostic systems.

Abstract: Breast cancer is one of the most common cancers among women worldwide, and its accurate and timely diagnosis plays a critical role in improving treatment outcomes. This thesis presents an innovative framework for detecting malignant masses in mammographic images by integrating the Pyramid Adaptive Atrous Convolution (PAAC) and Transformer architectures. The proposed approach utilizes Multi-Scale Feature Fusion to enhance the extraction of features from benign and malignant tissues and combines Dice Loss and Focal Loss functions to improve the model's learning process, effectively reducing errors in binary breast cancer classification and achieving high accuracy and efficiency. In this study, a comprehensive dataset of breast cancer images from INbreast, MIAS, and DDSM was preprocessed through data augmentation and contrast enhancement and resized to 227x227 pixels for model training. Leveraging the Transformer's ability to manage long-range dependencies with Self-Attention mechanisms, the proposed model achieved high accuracy in detecting cancerous masses, outperforming foundational models such as BreastNet, DeepMammo, Multi-Scale CNN, Swin-Unet, and SegFormer. The final evaluation results for the proposed model include an accuracy of 98.5\%, sensitivity of 97.8\%, specificity of 96.3\%, F1-score of 98.2\%, and overall precision of 97.9\%. These metrics demonstrate a significant improvement over traditional methods and confirm the model's effectiveness in identifying cancerous masses in complex scenarios and large datasets. This model shows potential as a reliable and efficient tool for breast cancer diagnosis and can be effectively integrated into medical diagnostic systems.

</details>


### [76] [Federated Joint Learning for Domain and Class Generalization](https://arxiv.org/abs/2601.12253)
*Haoran Xu,Jiaze Li,Jianzhong Ju,Zhenbo Luo*

Main category: cs.CV

TL;DR: FedDCG is a federated learning approach that jointly addresses both class and domain generalization by training class-generalized networks within domain groups and aggregating results based on domain similarity during inference.


<details>
  <summary>Details</summary>
Motivation: Existing methods typically address either unseen classes or unseen domains in isolation, but not both together. There's a need for a joint framework that handles both class and domain generalization in federated learning settings, especially for large visual-language models like CLIP that have extensive pretraining requirements.

Method: FedDCG introduces a domain grouping strategy where class-generalized networks are trained within each group to prevent decision boundary confusion. It uses a learnable network to enhance class generalization capabilities and a decoupling mechanism to separate general and domain-specific knowledge. During inference, results are aggregated based on domain similarity.

Result: Extensive experiments across various datasets show that FedDCG outperforms state-of-the-art baselines in terms of accuracy and robustness.

Conclusion: FedDCG provides an effective solution for joint class and domain generalization in federated learning, demonstrating superior performance compared to existing methods that address these challenges separately.

Abstract: Efficient fine-tuning of visual-language models like CLIP has become crucial due to their large-scale parameter size and extensive pretraining requirements. Existing methods typically address either the issue of unseen classes or unseen domains in isolation, without considering a joint framework for both. In this paper, we propose \textbf{Fed}erated Joint Learning for \textbf{D}omain and \textbf{C}lass \textbf{G}eneralization, termed \textbf{FedDCG}, a novel approach that addresses both class and domain generalization in federated learning settings. Our method introduces a domain grouping strategy where class-generalized networks are trained within each group to prevent decision boundary confusion. During inference, we aggregate class-generalized results based on domain similarity, effectively integrating knowledge from both class and domain generalization. Specifically, a learnable network is employed to enhance class generalization capabilities, and a decoupling mechanism separates general and domain-specific knowledge, improving generalization to unseen domains. Extensive experiments across various datasets show that \textbf{FedDCG} outperforms state-of-the-art baselines in terms of accuracy and robustness.

</details>


### [77] [Soft Shadow Diffusion (SSD): Physics-inspired Learning for 3D Computational Periscopy](https://arxiv.org/abs/2601.12257)
*Fadlullah Raji,John Murray-Bruce*

Main category: cs.CV

TL;DR: Passive NLOS imaging method reconstructs 3D hidden scenes from ordinary photographs of subtle shadows, using novel light transport decomposition and two solution approaches.


<details>
  <summary>Details</summary>
Motivation: Line-of-sight imaging is often impractical, dangerous, or impossible in certain scenarios. While passive NLOS methods using shadow photographs exist, they are limited to 1D/low-resolution 2D imaging or require known object shapes.

Method: Proposes a novel reformulation of light transport model that decomposes hidden scenes into light-occluding and non-light-occluding components, creating a separable non-linear least squares inverse problem. Develops two solutions: gradient-based optimization and physics-inspired neural network called Soft Shadow diffusion (SSD).

Result: Demonstrates effective 3D reconstruction of hidden scenes from ordinary NLOS photographs in real experimental scenarios. SSD generalizes well from simulation to unseen classes and real-world scenes, showing robustness to noise and ambient illumination.

Conclusion: Successfully generalizes passive NLOS imaging to achieve 3D reconstruction from ordinary shadow photographs, overcoming previous limitations through novel light transport modeling and dual solution approaches.

Abstract: Conventional imaging requires a line of sight to create accurate visual representations of a scene. In certain circumstances, however, obtaining a suitable line of sight may be impractical, dangerous, or even impossible. Non-line-of-sight (NLOS) imaging addresses this challenge by reconstructing the scene from indirect measurements. Recently, passive NLOS methods that use an ordinary photograph of the subtle shadow cast onto a visible wall by the hidden scene have gained interest. These methods are currently limited to 1D or low-resolution 2D color imaging or to localizing a hidden object whose shape is approximately known. Here, we generalize this class of methods and demonstrate a 3D reconstruction of a hidden scene from an ordinary NLOS photograph. To achieve this, we propose a novel reformulation of the light transport model that conveniently decomposes the hidden scene into \textit{light-occluding} and \textit{non-light-occluding} components to yield a separable non-linear least squares (SNLLS) inverse problem. We develop two solutions: A gradient-based optimization method and a physics-inspired neural network approach, which we call Soft Shadow diffusion (SSD). Despite the challenging ill-conditioned inverse problem encountered here, our approaches are effective on numerous 3D scenes in real experimental scenarios. Moreover, SSD is trained in simulation but generalizes well to unseen classes in simulation and real-world NLOS scenes. SSD also shows surprising robustness to noise and ambient illumination.

</details>


### [78] [AgenticPruner: MAC-Constrained Neural Network Compression via LLM-Driven Strategy Search](https://arxiv.org/abs/2601.12272)
*Shahrzad Esmat,Mahdi Banisharif,Ali Jannesari*

Main category: cs.CV

TL;DR: AgenticPruner uses LLM-powered agents to achieve precise MAC-constrained neural network pruning, improving convergence success from 48% to 71% while maintaining or improving accuracy across CNN and Vision Transformer architectures.


<details>
  <summary>Details</summary>
Motivation: Existing neural network pruning methods focus on parameter reduction without directly controlling computational cost (MAC operations), leading to unpredictable inference latency in resource-constrained deployment scenarios where strict MAC budgets must be met.

Method: A framework with three specialized LLM agents: Profiling Agent analyzes model architecture and MAC distributions; Master Agent orchestrates workflow with divergence monitoring; Analysis Agent (powered by Claude 3.5 Sonnet) learns optimal pruning strategies from historical attempts through in-context learning. Builds on isomorphic pruning's graph-based structural grouping with context-aware adaptation by analyzing patterns across pruning iterations.

Result: On ImageNet-1K: ResNet-50 reaches 1.77G MACs with 77.04% accuracy (+0.91% vs baseline); ResNet-101 achieves 4.22G MACs with 78.94% accuracy (+1.56% vs baseline). ConvNeXt-Small pruning to 8.17G MACs yields 1.41x GPU and 1.07x CPU speedup with 45% parameter reduction. For Vision Transformers, achieves MAC-budget compliance within user-defined tolerance bands (+1% to +5% overshoot, -5% to -15% undershoot). Convergence success rate improves from 48% to 71% compared to grid search.

Conclusion: AgenticPruner enables automatic convergence to target MAC budgets within tolerance bands, establishing feasibility for deployment scenarios requiring strict computational guarantees while maintaining or improving model accuracy across diverse architectures.

Abstract: Neural network pruning remains essential for deploying deep learning models on resource-constrained devices, yet existing approaches primarily target parameter reduction without directly controlling computational cost. This yields unpredictable inference latency in deployment scenarios where strict Multiply-Accumulate (MAC) operation budgets must be met. We propose AgenticPruner, a framework utilizing large language models to achieve MAC-constrained optimization through iterative strategy learning. Our approach coordinates three specialized agents: a Profiling Agent that analyzes model architecture and MAC distributions, a Master Agent that orchestrates the workflow with divergence monitoring, and an Analysis Agent powered by Claude 3.5 Sonnet that learns optimal strategies from historical attempts. Through in-context learning, the Analysis Agent improves convergence success rate from 48% to 71% compared to grid search. Building upon isomorphic pruning's graph-based structural grouping, our method adds context-aware adaptation by analyzing patterns across pruning iterations, enabling automatic convergence to target MAC budgets within user-defined tolerance bands.
  We validate our framework on ImageNet-1K across ResNet, ConvNeXt, and DeiT architectures. On CNNs, our approach achieves MAC targeting while maintaining or improving accuracy: ResNet-50 reaches 1.77G MACs with 77.04% accuracy (+0.91% vs baseline); ResNet-101 achieves 4.22G MACs with 78.94% accuracy (+1.56% vs baseline). For ConvNeXt-Small, pruning to 8.17G MACs yields 1.41x GPU and 1.07x CPU speedup with 45% parameter reduction. On Vision Transformers, we demonstrate MAC-budget compliance within user-defined tolerance bands (typically +1% to +5% overshoot, -5% to -15% undershoot), establishing feasibility for deployment scenarios requiring strict computational guarantees.

</details>


### [79] [CytoCLIP: Learning Cytoarchitectural Characteristics in Developing Human Brain Using Contrastive Language Image Pre-Training](https://arxiv.org/abs/2601.12282)
*Pralaypati Ta,Sriram Venkatesaperumal,Keerthi Ram,Mohanasankar Sivaprakasam*

Main category: cs.CV

TL;DR: CytoCLIP: A vision-language model suite using CLIP frameworks to automate brain cytoarchitecture region identification from histological sections, achieving high classification accuracy.


<details>
  <summary>Details</summary>
Motivation: Manual delineation of brain regions by cytoarchitecture is time-consuming and requires specialized expertise. An automated approach is needed to reduce human effort and enable scalable scientific analysis of brain regions.

Method: Developed CytoCLIP, a suite of vision-language models based on pre-trained CLIP frameworks. Two variants: one trained on low-resolution whole-region images for overall patterns, another on high-resolution image tiles for cellular details. Used NISSL-stained fetal brain histological sections with 86 regions for low-res and 384 regions for high-res data.

Result: CytoCLIP outperforms existing methods, achieving F1 scores of 0.87 for whole-region classification and 0.91 for high-resolution image tile classification. Demonstrates strong generalization across different ages and sectioning planes.

Conclusion: CytoCLIP provides an effective automated solution for brain cytoarchitecture analysis, reducing reliance on human experts while maintaining high accuracy in region identification tasks.

Abstract: The functions of different regions of the human brain are closely linked to their distinct cytoarchitecture, which is defined by the spatial arrangement and morphology of the cells. Identifying brain regions by their cytoarchitecture enables various scientific analyses of the brain. However, delineating these areas manually in brain histological sections is time-consuming and requires specialized knowledge. An automated approach is necessary to minimize the effort needed from human experts. To address this, we propose CytoCLIP, a suite of vision-language models derived from pre-trained Contrastive Language-Image Pre-Training (CLIP) frameworks to learn joint visual-text representations of brain cytoarchitecture. CytoCLIP comprises two model variants: one is trained using low-resolution whole-region images to understand the overall cytoarchitectural pattern of an area, and the other is trained on high-resolution image tiles for detailed cellular-level representation. The training dataset is created from NISSL-stained histological sections of developing fetal brains of different gestational weeks. It includes 86 distinct regions for low-resolution images and 384 brain regions for high-resolution tiles. We evaluate the model's understanding of the cytoarchitecture and generalization ability using region classification and cross-modal retrieval tasks. Multiple experiments are performed under various data setups, including data from samples of different ages and sectioning planes. Experimental results demonstrate that CytoCLIP outperforms existing methods. It achieves an F1 score of 0.87 for whole-region classification and 0.91 for high-resolution image tile classification.

</details>


### [80] [SDiT: Semantic Region-Adaptive for Diffusion Transformers](https://arxiv.org/abs/2601.12283)
*Bowen Lin,Fanjiang Ye,Yihua Liu,Zhenghui Guo,Boyuan Zhang,Weijian Zheng,Yufan Xu,Tiancheng Xing,Yuke Wang,Chengming Zhang*

Main category: cs.CV

TL;DR: SDiT accelerates Diffusion Transformers by 3x without retraining, using semantic-aware clustering and selective region updates based on spatial complexity.


<details>
  <summary>Details</summary>
Motivation: DiTs are computationally expensive due to iterative denoising and quadratic attention cost. The authors observed that denoising dynamics are spatially non-uniform - background regions converge quickly while edges/textured areas evolve more actively.

Method: Training-free framework with three components: (1) semantic-aware clustering via fast Quickshift-based segmentation, (2) complexity-driven regional scheduling to selectively update informative areas, and (3) boundary-aware refinement to maintain spatial coherence.

Result: Achieves up to 3.0x acceleration while preserving nearly identical perceptual and semantic quality to full-attention inference. No model retraining or architectural modification required.

Conclusion: SDiT provides an efficient acceleration framework for Diffusion Transformers by exploiting spatial non-uniformity in denoising dynamics, achieving significant speedup without compromising quality.

Abstract: Diffusion Transformers (DiTs) achieve state-of-the-art performance in text-to-image synthesis but remain computationally expensive due to the iterative nature of denoising and the quadratic cost of global attention. In this work, we observe that denoising dynamics are spatially non-uniform-background regions converge rapidly while edges and textured areas evolve much more actively. Building on this insight, we propose SDiT, a Semantic Region-Adaptive Diffusion Transformer that allocates computation according to regional complexity. SDiT introduces a training-free framework combining (1) semantic-aware clustering via fast Quickshift-based segmentation, (2) complexity-driven regional scheduling to selectively update informative areas, and (3) boundary-aware refinement to maintain spatial coherence. Without any model retraining or architectural modification, SDiT achieves up to 3.0x acceleration while preserving nearly identical perceptual and semantic quality to full-attention inference.

</details>


### [81] [LegacyAvatars: Volumetric Face Avatars For Traditional Graphics Pipelines](https://arxiv.org/abs/2601.12285)
*Safa C. Medin,Gengyan Li,Ziqian Bai,Ruofei Du,Leonhard Helminger,Yinda Zhang,Stephan J. Garbin,Philip L. Davidson,Gregory W. Wornell,Thabo Beeler,Abhimitra Meka*

Main category: cs.CV

TL;DR: Novel explicit mesh-based representation for photorealistic 3D face avatars that enables efficient classical rendering on legacy graphics platforms without custom engineering.


<details>
  <summary>Details</summary>
Motivation: To create photorealistic 3D face avatars that can be efficiently rendered using classical mesh and shader-based rendering on existing graphics platforms, eliminating the need for custom engineering or integration.

Method: Leverages radiance fields anchored to parametric face models to learn radiance manifolds in 3D space, extracting an explicit layered mesh with appearance and warp textures. Uses linear blending and alpha compositing for animation control.

Result: Achieves controllable volumetric rendering of complex facial features (hair, skin, eyes) and enables efficient online streaming and rendering on legacy graphics platforms.

Conclusion: The explicit representation enables photorealistic 3D face avatars to be deployed and rendered efficiently on existing graphics infrastructure without requiring custom engineering solutions.

Abstract: We introduce a novel representation for efficient classical rendering of photorealistic 3D face avatars. Leveraging recent advances in radiance fields anchored to parametric face models, our approach achieves controllable volumetric rendering of complex facial features, including hair, skin, and eyes. At enrollment time, we learn a set of radiance manifolds in 3D space to extract an explicit layered mesh, along with appearance and warp textures. During deployment, this allows us to control and animate the face through simple linear blending and alpha compositing of textures over a static mesh. This explicit representation also enables the generated avatar to be efficiently streamed online and then rendered using classical mesh and shader-based rendering on legacy graphics platforms, eliminating the need for any custom engineering or integration.

</details>


### [82] [Concepts from Representations: Post-hoc Concept Bottleneck Models via Sparse Decomposition of Visual Representations](https://arxiv.org/abs/2601.12303)
*Shizhan Gong,Xiaofan Zhang,Qi Dou*

Main category: cs.CV

TL;DR: PCBM-ReD is a novel post-hoc concept bottleneck model that retrofits interpretability onto pretrained models by automatically extracting visual concepts, using MLLMs for labeling/filtering, and decomposing image representations into concept embeddings via CLIP's alignment.


<details>
  <summary>Details</summary>
Motivation: Deep learning models lack interpretability for critical applications. Existing concept-based methods have limitations: unreliable concept relevance, non-visual/labor-intensive concept definitions, and model/data-agnostic assumptions.

Method: PCBM-ReD extracts visual concepts from pretrained encoders, uses multimodal LLMs to label/filter concepts based on visual identifiability and task relevance, selects independent concepts via reconstruction-guided optimization, and decomposes image representations into linear combinations of concept embeddings using CLIP's visual-text alignment.

Result: Extensive experiments across 11 image classification tasks show PCBM-ReD achieves state-of-the-art accuracy, narrows performance gap with end-to-end models, and exhibits better interpretability.

Conclusion: PCBM-ReD provides an effective pipeline for retrofitting interpretability onto pretrained models while maintaining high accuracy and improving transparency through automatically extracted visual concepts.

Abstract: Deep learning has achieved remarkable success in image recognition, yet their inherent opacity poses challenges for deployment in critical domains. Concept-based interpretations aim to address this by explaining model reasoning through human-understandable concepts. However, existing post-hoc methods and ante-hoc concept bottleneck models (CBMs), suffer from limitations such as unreliable concept relevance, non-visual or labor-intensive concept definitions, and model or data-agnostic assumptions. This paper introduces Post-hoc Concept Bottleneck Model via Representation Decomposition (PCBM-ReD), a novel pipeline that retrofits interpretability onto pretrained opaque models. PCBM-ReD automatically extracts visual concepts from a pre-trained encoder, employs multimodal large language models (MLLMs) to label and filter concepts based on visual identifiability and task relevance, and selects an independent subset via reconstruction-guided optimization. Leveraging CLIP's visual-text alignment, it decomposes image representations into linear combination of concept embeddings to fit into the CBMs abstraction. Extensive experiments across 11 image classification tasks show PCBM-ReD achieves state-of-the-art accuracy, narrows the performance gap with end-to-end models, and exhibits better interpretability.

</details>


### [83] [A Two-Stage Globally-Diverse Adversarial Attack for Vision-Language Pre-training Models](https://arxiv.org/abs/2601.12304)
*Wutao Chen,Huaqin Zou,Chen Wan,Lifeng Huang*

Main category: cs.CV

TL;DR: 2S-GDA: A two-stage globally-diverse attack framework that improves adversarial transferability against vision-language pre-training models by enhancing both textual and visual perturbation diversity.


<details>
  <summary>Details</summary>
Motivation: Vision-language pre-training models are vulnerable to adversarial attacks, especially in black-box scenarios. Existing multimodal attacks have limitations: limited perturbation diversity and unstable multi-stage pipelines that reduce effectiveness.

Method: Two-stage globally-diverse attack (2S-GDA) framework: 1) Textual perturbations using globally-diverse strategy combining candidate text expansion with globally-aware replacement, 2) Visual perturbations using multi-scale resizing and block-shuffle rotation to enhance diversity.

Result: Extensive experiments show 2S-GDA consistently improves attack success rates over state-of-the-art methods, achieving gains up to 11.17% in black-box settings. The framework is modular and can be combined with existing methods.

Conclusion: 2S-GDA effectively addresses diversity limitations in multimodal adversarial attacks, demonstrating superior performance in black-box scenarios and offering a modular framework that can enhance existing attack methods.

Abstract: Vision-language pre-training (VLP) models are vulnerable to adversarial examples, particularly in black-box scenarios. Existing multimodal attacks often suffer from limited perturbation diversity and unstable multi-stage pipelines. To address these challenges, we propose 2S-GDA, a two-stage globally-diverse attack framework. The proposed method first introduces textual perturbations through a globally-diverse strategy by combining candidate text expansion with globally-aware replacement. To enhance visual diversity, image-level perturbations are generated using multi-scale resizing and block-shuffle rotation. Extensive experiments on VLP models demonstrate that 2S-GDA consistently improves attack success rates over state-of-the-art methods, with gains of up to 11.17\% in black-box settings. Our framework is modular and can be easily combined with existing methods to further enhance adversarial transferability.

</details>


### [84] [Adaptive Multi-Scale Correlation Meta-Network for Few-Shot Remote Sensing Image Classification](https://arxiv.org/abs/2601.12308)
*Anurag Kaushish,Ayan Sar,Sampurna Roy,Sudeshna Chakraborty,Prashant Trivedi,Tanupriya Choudhury,Kanav Gupta*

Main category: cs.CV

TL;DR: AMC-MetaNet is a lightweight few-shot learning framework for remote sensing that uses correlation-guided feature pyramids, adaptive channel correlation, and correlation-based meta-learning to address data scarcity, domain shifts, and multi-scale objects with only ~600K parameters.


<details>
  <summary>Details</summary>
Motivation: Address three key challenges in remote sensing few-shot learning: 1) scarcity of labeled data, 2) substantial domain shifts between different remote sensing datasets, and 3) multi-scale nature of geospatial objects that vary in size across different resolutions and contexts.

Method: Three key innovations: 1) Correlation-guided feature pyramids for capturing scale-invariant patterns, 2) Adaptive channel correlation module (ACCM) for learning dynamic cross-scale relationships, 3) Correlation-guided meta-learning that leverages correlation patterns instead of conventional prototype averaging. The framework is trained from scratch with only ~600K parameters.

Result: Achieves up to 86.65% accuracy in 5-way 5-shot classification on various remote sensing datasets (EuroSAT, NWPU-RESISC45, UC Merced Land Use, and AID). Offers 20× fewer parameters than ResNet-18 while maintaining high efficiency (<50ms per image inference).

Conclusion: AMC-MetaNet establishes itself as a computationally efficient, scale-aware framework for real-world few-shot remote sensing applications, outperforming prior approaches that rely on heavy pre-trained models or transformers while being much more parameter-efficient.

Abstract: Few-shot learning in remote sensing remains challenging due to three factors: the scarcity of labeled data, substantial domain shifts, and the multi-scale nature of geospatial objects. To address these issues, we introduce Adaptive Multi-Scale Correlation Meta-Network (AMC-MetaNet), a lightweight yet powerful framework with three key innovations: (i) correlation-guided feature pyramids for capturing scale-invariant patterns, (ii) an adaptive channel correlation module (ACCM) for learning dynamic cross-scale relationships, and (iii) correlation-guided meta-learning that leverages correlation patterns instead of conventional prototype averaging. Unlike prior approaches that rely on heavy pre-trained models or transformers, AMC-MetaNet is trained from scratch with only $\sim600K$ parameters, offering $20\times$ fewer parameters than ResNet-18 while maintaining high efficiency ($<50$ms per image inference). AMC-MetaNet achieves up to 86.65\% accuracy in 5-way 5-shot classification on various remote sensing datasets, including EuroSAT, NWPU-RESISC45, UC Merced Land Use, and AID. Our results establish AMC-MetaNet as a computationally efficient, scale-aware framework for real-world few-shot remote sensing.

</details>


### [85] [CurConMix+: A Unified Spatio-Temporal Framework for Hierarchical Surgical Workflow Understanding](https://arxiv.org/abs/2601.12312)
*Yongjun Jeon,Jongmin Shin,Kanggil Park,Seonmin Park,Soyoung Lim,Jung Yong Kim,Jinsoo Rhu,Jongman Kim,Gyu-Seong Choi,Namkee Oh,Kyu-Hwan Jung*

Main category: cs.CV

TL;DR: CurConMix+ is a surgical action triplet recognition framework with curriculum-guided contrastive learning and multi-resolution temporal transformer, achieving state-of-the-art performance and strong cross-level generalization.


<details>
  <summary>Details</summary>
Motivation: Surgical action triplet recognition is clinically important for workflow analysis and skill assessment, but progress has been hindered by severe class imbalance, subtle visual variations, and semantic interdependence among triplet components. Existing approaches address only subsets of these challenges rather than tackling them jointly.

Method: Builds on CurConMix spatial framework with curriculum-guided contrastive learning, structured hard-pair sampling, and feature-level mixup. CurConMix+ adds Multi-Resolution Temporal Transformer (MRTT) that adaptively fuses multi-scale temporal features and dynamically balances spatio-temporal cues. Also introduces LLS48 benchmark dataset with hierarchical annotations.

Result: Outperforms state-of-the-art approaches in triplet recognition on CholecT45 and LLS48 datasets. Exhibits strong cross-level generalization with fine-grained features effectively transferring to higher-level phase and step recognition tasks.

Conclusion: The framework and dataset provide unified foundation for hierarchy-aware, reproducible, and interpretable surgical workflow understanding. Code and dataset will be publicly released to facilitate reproducibility and further research.

Abstract: Surgical action triplet recognition aims to understand fine-grained surgical behaviors by modeling the interactions among instruments, actions, and anatomical targets. Despite its clinical importance for workflow analysis and skill assessment, progress has been hindered by severe class imbalance, subtle visual variations, and the semantic interdependence among triplet components. Existing approaches often address only a subset of these challenges rather than tackling them jointly, which limits their ability to form a holistic understanding. This study builds upon CurConMix, a spatial representation framework. At its core, a curriculum-guided contrastive learning strategy learns discriminative and progressively correlated features, further enhanced by structured hard-pair sampling and feature-level mixup. Its temporal extension, CurConMix+, integrates a Multi-Resolution Temporal Transformer (MRTT) that achieves robust, context-aware understanding by adaptively fusing multi-scale temporal features and dynamically balancing spatio-temporal cues. Furthermore, we introduce LLS48, a new, hierarchically annotated benchmark for complex laparoscopic left lateral sectionectomy, providing step-, task-, and action-level annotations. Extensive experiments on CholecT45 and LLS48 demonstrate that CurConMix+ not only outperforms state-of-the-art approaches in triplet recognition, but also exhibits strong cross-level generalization, as its fine-grained features effectively transfer to higher-level phase and step recognition tasks. Together, the framework and dataset provide a unified foundation for hierarchy-aware, reproducible, and interpretable surgical workflow understanding. The code and dataset will be publicly released on GitHub to facilitate reproducibility and further research.

</details>


### [86] [S^2F-Net:A Robust Spatial-Spectral Fusion Framework for Cross-Model AIGC Detection](https://arxiv.org/abs/2601.12313)
*Xiangyu Hu,Yicheng Hong,Hongchuang Zheng,Wenjun Zeng,Bingyao Liu*

Main category: cs.CV

TL;DR: S²F-Net: A cross-model detection framework that leverages spectral discrepancies between real and synthetic textures to detect AI-generated content with strong generalization across unseen generative models.


<details>
  <summary>Details</summary>
Motivation: Existing AI-generated content detection methods suffer from overfitting to specific source models and poor generalization to unseen generative architectures, creating an urgent need for more robust detection schemes.

Method: Proposes S²F-Net framework focusing on frequency-domain artifacts from upsampling operations. Uses a learnable frequency attention module that adaptively weights discriminative frequency bands by combining spatial texture analysis with spectral dependencies.

Result: Achieves 90.49% detection accuracy on AIGCDetectBenchmark (17 categories of generative models), significantly outperforming existing baseline methods in cross-domain detection scenarios.

Conclusion: Focusing on inherent spectral discrepancies between real and synthetic textures provides a fundamental approach to improve generalization performance in AI-generated content detection across diverse generative models.

Abstract: The rapid development of generative models has imposed an urgent demand for detection schemes with strong generalization capabilities. However, existing detection methods generally suffer from overfitting to specific source models, leading to significant performance degradation when confronted with unseen generative architectures. To address these challenges, this paper proposes a cross-model detection framework called S 2 F-Net, whose core lies in exploring and leveraging the inherent spectral discrepancies between real and synthetic textures. Considering that upsampling operations leave unique and distinguishable frequency fingerprints in both texture-poor and texture-rich regions, we focus our research on the detection of frequency-domain artifacts, aiming to fundamentally improve the generalization performance of the model. Specifically, we introduce a learnable frequency attention module that adaptively weights and enhances discriminative frequency bands by synergizing spatial texture analysis and spectral dependencies.On the AIGCDetectBenchmark, which includes 17 categories of generative models, S 2 F-Net achieves a detection accuracy of 90.49%, significantly outperforming various existing baseline methods in cross-domain detection scenarios.

</details>


### [87] [GazeFormer-MoE: Context-Aware Gaze Estimation via CLIP and MoE Transformer](https://arxiv.org/abs/2601.12316)
*Xinyuan Zhao,Xianrui Chen,Ahmad Chaddad*

Main category: cs.CV

TL;DR: Gazeformer: A semantics-modulated, multi-scale Transformer for 3D gaze estimation that achieves state-of-the-art results across multiple datasets with up to 64% relative improvement.


<details>
  <summary>Details</summary>
Motivation: To improve 3D gaze estimation by addressing challenges like illumination variations, head pose changes, background clutter, and gaze direction diversity through semantic modulation and multi-scale feature fusion.

Method: Uses CLIP global features conditioned with learnable prototype banks (illumination, head pose, background, direction), fuses these with CLIP patch tokens and high-resolution CNN tokens in unified attention space, and replaces FFN blocks with routed/shared Mixture of Experts for increased conditional capacity.

Result: Achieves new SOTA angular errors: 2.49° on MPIIFaceGaze, 3.22° on EYEDIAP, 10.16° on Gaze360, and 1.44° on ETH-XGaze, demonstrating up to 64% relative improvement over previous results.

Conclusion: The proposed Gazeformer model effectively addresses key challenges in 3D gaze estimation through semantic modulation, multi-scale fusion, and conditional capacity enhancement, setting new benchmarks across multiple datasets.

Abstract: We present a semantics modulated, multi scale Transformer for 3D gaze estimation. Our model conditions CLIP global features with learnable prototype banks (illumination, head pose, background, direction), fuses these prototype-enriched global vectors with CLIP patch tokens and high-resolution CNN tokens in a unified attention space, and replaces several FFN blocks with routed/shared Mixture of Experts to increase conditional capacity. Evaluated on MPIIFaceGaze, EYEDIAP, Gaze360 and ETH-XGaze, our model achieves new state of the art angular errors of 2.49°, 3.22°, 10.16°, and 1.44°, demonstrating up to a 64% relative improvement over previously reported results. ablations attribute gains to prototype conditioning, cross scale fusion, MoE and hyperparameter. Our code is publicly available at https://github. com/AIPMLab/Gazeformer.

</details>


### [88] [Multi-Sensor Matching with HyperNetworks](https://arxiv.org/abs/2601.12325)
*Eli Passov,Nathan S. Netanyahu,Yosi Keller*

Main category: cs.CV

TL;DR: Hypernetwork-based lightweight descriptor architecture improves multimodal patch matching with adaptive per-channel scaling/shifting and modality-specific normalization, achieving SOTA on VIS-IR benchmarks with efficient inference.


<details>
  <summary>Details</summary>
Motivation: To improve multimodal patch matching (especially visible vs. infrared) while maintaining efficiency of descriptor-based methods and increasing robustness to appearance shifts between modalities.

Method: Augments Siamese CNN with hypernetwork modules for adaptive per-channel scaling/shifting, plus conditional instance normalization for modality-specific adaptation in shallow layers. Trained with triplet loss and hard-negative mining.

Result: Achieves state-of-the-art results on VIS-NIR and other VIS-IR benchmarks, matches/surpasses prior methods on additional datasets despite lower inference cost. Also releases GAP-VIR dataset with 500K ground/aerial VIS-IR pairs.

Conclusion: Hypernetworks provide effective lightweight adaptation for multimodal patch matching, balancing efficiency and robustness. The released GAP-VIR dataset enables better evaluation of cross-domain generalization in VIS-IR matching.

Abstract: Hypernetworks are models that generate or modulate the weights of another network. They provide a flexible mechanism for injecting context and task conditioning and have proven broadly useful across diverse applications without significant increases in model size. We leverage hypernetworks to improve multimodal patch matching by introducing a lightweight descriptor-learning architecture that augments a Siamese CNN with (i) hypernetwork modules that compute adaptive, per-channel scaling and shifting and (ii) conditional instance normalization that provides modality-specific adaptation (e.g., visible vs. infrared, VIS-IR) in shallow layers. This combination preserves the efficiency of descriptor-based methods during inference while increasing robustness to appearance shifts. Trained with a triplet loss and hard-negative mining, our approach achieves state-of-the-art results on VIS-NIR and other VIS-IR benchmarks and matches or surpasses prior methods on additional datasets, despite their higher inference cost. To spur progress on domain shift, we also release GAP-VIR, a cross-platform (ground/aerial) VIS-IR patch dataset with 500K pairs, enabling rigorous evaluation of cross-domain generalization and adaptation.

</details>


### [89] [EmoKGEdit: Training-free Affective Injection via Visual Cue Transformation](https://arxiv.org/abs/2601.12326)
*Jing Zhang,Bingjie Fan*

Main category: cs.CV

TL;DR: EmoKGEdit: A training-free framework for precise image emotion editing using a knowledge graph to disentangle emotional cues from content, preserving visual structure while effectively injecting target emotions.


<details>
  <summary>Details</summary>
Motivation: Existing image emotion editing methods struggle to disentangle emotional cues from content representations, resulting in weak emotional expression and distorted visual structures. There's a need for precise, structure-preserving emotion editing.

Method: Proposes EmoKGEdit with two key components: 1) Multimodal Sentiment Association Knowledge Graph (MSA-KG) that encodes causal chains among objects, attributes, scenes, visual clues and emotions, enabling chain-of-thought reasoning; 2) Disentangled structure-emotion editing module that separates emotional attributes from layout features in latent space to maintain visual coherence while injecting target emotions.

Result: Extensive experiments show EmoKGEdit achieves excellent performance in both emotion fidelity and content preservation, outperforming state-of-the-art methods.

Conclusion: EmoKGEdit successfully bridges the gap in image emotion editing by effectively disentangling emotional cues from content through knowledge graph-based reasoning and explicit latent space separation, enabling precise emotion editing while preserving visual structure.

Abstract: Existing image emotion editing methods struggle to disentangle emotional cues from latent content representations, often yielding weak emotional expression and distorted visual structures. To bridge this gap, we propose EmoKGEdit, a novel training-free framework for precise and structure-preserving image emotion editing. Specifically, we construct a Multimodal Sentiment Association Knowledge Graph (MSA-KG) to disentangle the intricate relationships among objects, scenes, attributes, visual clues and emotion. MSA-KG explicitly encode the causal chain among object-attribute-emotion, and as external knowledge to support chain of thought reasoning, guiding the multimodal large model to infer plausible emotion-related visual cues and generate coherent instructions. In addition, based on MSA-KG, we design a disentangled structure-emotion editing module that explicitly separates emotional attributes from layout features within the latent space, which ensures that the target emotion is effectively injected while strictly maintaining visual spatial coherence. Extensive experiments demonstrate that EmoKGEdit achieves excellent performance in both emotion fidelity and content preservation, and outperforms the state-of-the-art methods.

</details>


### [90] [FlowIID: Single-Step Intrinsic Image Decomposition via Latent Flow Matching](https://arxiv.org/abs/2601.12329)
*Mithlesh Singla,Seema Kumari,Shanmuganathan Raman*

Main category: cs.CV

TL;DR: FlowIID: A parameter-efficient intrinsic image decomposition model using flow matching that separates images into albedo and shading in a single inference step.


<details>
  <summary>Details</summary>
Motivation: Existing IID models achieve good results but use too many parameters, making them costly to combine with other models in real-world applications. There's a need for more parameter-efficient solutions.

Method: Proposed FlowIID architecture based on latent flow matching, combining a VAE-guided latent space with a flow matching module for stable decomposition of albedo and shading components.

Result: FlowIID delivers competitive and superior results compared to existing models across various benchmarks, despite its compact design. It's parameter-efficient and produces results in a single inference step.

Conclusion: FlowIID is well-suited for deployment in resource-constrained and real-time vision applications due to its parameter efficiency and single-step inference capability.

Abstract: Intrinsic Image Decomposition (IID) separates an image into albedo and shading components. It is a core step in many real-world applications, such as relighting and material editing. Existing IID models achieve good results, but often use a large number of parameters. This makes them costly to combine with other models in real-world settings. To address this problem, we propose a flow matching-based solution. For this, we design a novel architecture, FlowIID, based on latent flow matching. FlowIID combines a VAE-guided latent space with a flow matching module, enabling a stable decomposition of albedo and shading. FlowIID is not only parameter-efficient, but also produces results in a single inference step. Despite its compact design, FlowIID delivers competitive and superior results compared to existing models across various benchmarks. This makes it well-suited for deployment in resource-constrained and real-time vision applications.

</details>


### [91] [Turbo-GoDec: Exploiting the Cluster Sparsity Prior for Hyperspectral Anomaly Detection](https://arxiv.org/abs/2601.12337)
*Jiahui Sheng,Xiaorun Li,Shuhan Chen*

Main category: cs.CV

TL;DR: Turbo-GoDec improves hyperspectral anomaly detection by incorporating cluster sparsity prior (anomalies appear as small clustered groups) into the GoDec algorithm using Markov random fields and message passing.


<details>
  <summary>Details</summary>
Motivation: Existing hyperspectral anomaly detection methods rely on low-rank background and sparse anomaly assumptions but rarely expand on the sparsity prior. Observations show anomalies exhibit spatial cluster sparsity (small clustered groups), which current methods don't exploit.

Method: Combines cluster sparsity prior with GoDec algorithm by modeling cluster sparsity using Markov random field, computing anomaly marginal probabilities via message passing on factor graph, and treating high-probability locations as sparse component in Turbo-GoDec.

Result: Superior performance in detecting small-size anomalies compared to vanilla GoDec (LSMAD) and state-of-the-art methods on three real HSI datasets.

Conclusion: Incorporating cluster sparsity prior significantly improves hyperspectral anomaly detection performance, especially for small anomalies, demonstrating the value of exploiting spatial distribution characteristics of anomalies.

Abstract: As a key task in hyperspectral image processing, hyperspectral anomaly detection has garnered significant attention and undergone extensive research. Existing methods primarily relt on two prior assumption: low-rank background and sparse anomaly, along with additional spatial assumptions of the background. However, most methods only utilize the sparsity prior assumption for anomalies and rarely expand on this hypothesis. From observations of hyperspectral images, we find that anomalous pixels exhibit certain spatial distribution characteristics: they often manifest as small, clustered groups in space, which we refer to as cluster sparsity of anomalies. Then, we combined the cluster sparsity prior with the classical GoDec algorithm, incorporating the cluster sparsity prior into the S-step of GoDec. This resulted in a new hyperspectral anomaly detection method, which we called Turbo-GoDec. In this approach, we modeled the cluster sparsity prior of anomalies using a Markov random field and computed the marginal probabilities of anomalies through message passing on a factor graph. Locations with high anomalous probabilities were treated as the sparse component in the Turbo-GoDec. Experiments are conducted on three real hyperspectral image (HSI) datasets which demonstrate the superior performance of the proposed Turbo-GoDec method in detecting small-size anomalies comparing with the vanilla GoDec (LSMAD) and state-of-the-art anomaly detection methods. The code is available at https://github.com/jiahuisheng/Turbo-GoDec.

</details>


### [92] [MMDeepResearch-Bench: A Benchmark for Multimodal Deep Research Agents](https://arxiv.org/abs/2601.12346)
*Peizhou Huang,Zixuan Zhong,Zhongwei Wan,Donghao Zhou,Samiul Alam,Xin Wang,Zexin Li,Zhihao Dou,Li Zhu,Jing Xiong,Chaofan Tao,Yan Xu,Dimitrios Dimitriadis,Tuo Zhang,Mi Zhang*

Main category: cs.CV

TL;DR: MMDR-Bench is a new benchmark for evaluating multimodal deep research agents that generate citation-rich reports using both text and visual evidence, with comprehensive evaluation metrics for report quality, citation grounding, and multimodal integrity.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks focus on text-only settings or short-form multimodal QA, missing end-to-end evaluation of multimodal evidence use in deep research agents that generate citation-rich reports.

Method: Introduce MMDR-Bench with 140 expert-crafted tasks across 21 domains, each providing image-text bundles. Propose three evaluation components: FLAE for report quality, TRACE for citation-grounded evidence alignment, and MOSAIC for text-visual integrity.

Result: Experiments with 25 state-of-the-art models reveal systematic trade-offs between generation quality, citation discipline, and multimodal grounding. Strong prose doesn't guarantee faithful evidence use, and multimodal integrity remains a key bottleneck.

Conclusion: MMDR-Bench addresses the gap in evaluating multimodal deep research agents, providing comprehensive assessment of report generation with explicit evidence use and highlighting the challenges in maintaining multimodal integrity.

Abstract: Deep Research Agents (DRAs) generate citation-rich reports via multi-step search and synthesis, yet existing benchmarks mainly target text-only settings or short-form multimodal QA, missing end-to-end multimodal evidence use. We introduce MMDeepResearch-Bench (MMDR-Bench), a benchmark of 140 expert-crafted tasks across 21 domains, where each task provides an image-text bundle to evaluate multimodal understanding and citation-grounded report generation. Compared to prior setups, MMDR-Bench emphasizes report-style synthesis with explicit evidence use, where models must connect visual artifacts to sourced claims and maintain consistency across narrative, citations, and visual references. We further propose a unified, interpretable evaluation pipeline: Formula-LLM Adaptive Evaluation (FLAE) for report quality, Trustworthy Retrieval-Aligned Citation Evaluation (TRACE) for citation-grounded evidence alignment, and Multimodal Support-Aligned Integrity Check (MOSAIC) for text-visual integrity, each producing fine-grained signals that support error diagnosis beyond a single overall score. Experiments across 25 state-of-the-art models reveal systematic trade-offs between generation quality, citation discipline, and multimodal grounding, highlighting that strong prose alone does not guarantee faithful evidence use and that multimodal integrity remains a key bottleneck for deep research agents.

</details>


### [93] [SimpleMatch: A Simple and Strong Baseline for Semantic Correspondence](https://arxiv.org/abs/2601.12357)
*Hailing Jin,Huiying Li*

Main category: cs.CV

TL;DR: SimpleMatch is a lightweight semantic correspondence framework that achieves strong performance at low resolutions by addressing feature fusion issues from downsampling, using an upsample decoder and multi-scale supervision while reducing memory usage by 51%.


<details>
  <summary>Details</summary>
Motivation: Current semantic correspondence methods rely on high-resolution inputs for optimal performance, causing computational overhead. A key limitation is the irreversible fusion of adjacent keypoint features when semantically distinct keypoints fall within the same downsampled receptive field during deep downsampling operations.

Method: SimpleMatch uses: 1) a lightweight upsample decoder that progressively recovers spatial detail by upsampling deep features to 1/4 resolution, 2) a multi-scale supervised loss to ensure upsampled features retain discriminative features across spatial scales, and 3) sparse matching with window-based localization to optimize training memory usage.

Result: At 252x252 resolution (3.3x smaller than current SOTA), SimpleMatch achieves 84.1% PCK@0.1 on SPair-71k benchmark while reducing training memory usage by 51%.

Conclusion: SimpleMatch provides a practical and efficient baseline for semantic correspondence research by delivering strong performance at low resolutions, addressing feature fusion issues, and significantly reducing computational requirements.

Abstract: Recent advances in semantic correspondence have been largely driven by the use of pre-trained large-scale models. However, a limitation of these approaches is their dependence on high-resolution input images to achieve optimal performance, which results in considerable computational overhead. In this work, we address a fundamental limitation in current methods: the irreversible fusion of adjacent keypoint features caused by deep downsampling operations. This issue is triggered when semantically distinct keypoints fall within the same downsampled receptive field (e.g., 16x16 patches). To address this issue, we present SimpleMatch, a simple yet effective framework for semantic correspondence that delivers strong performance even at low resolutions. We propose a lightweight upsample decoder that progressively recovers spatial detail by upsampling deep features to 1/4 resolution, and a multi-scale supervised loss that ensures the upsampled features retain discriminative features across different spatial scales. In addition, we introduce sparse matching and window-based localization to optimize training memory usage and reduce it by 51%. At a resolution of 252x252 (3.3x smaller than current SOTA methods), SimpleMatch achieves superior performance with 84.1% PCK@0.1 on the SPair-71k benchmark. We believe this framework provides a practical and efficient baseline for future research in semantic correspondence. Code is available at: https://github.com/hailong23-jin/SimpleMatch.

</details>


### [94] [From Prompts to Pavement: LMMs-based Agentic Behavior-Tree Generation Framework for Autonomous Vehicles](https://arxiv.org/abs/2601.12358)
*Omar Y. Goba,Ahmed Y. Gado,Catherine M. Elias,Ahmed Hussein*

Main category: cs.CV

TL;DR: LLM/LVM-based agentic framework generates adaptive behavior trees for autonomous vehicles, enabling on-the-fly navigation around unexpected obstacles without human intervention.


<details>
  <summary>Details</summary>
Motivation: Traditional behavior trees are static and require manual tuning, limiting their applicability for SAE Level 5 autonomy where vehicles need to handle unpredictable real-world environments.

Method: Three-agent framework: Descriptor agent uses chain-of-symbols prompting to assess scene criticality; Planner agent constructs high-level sub-goals via in-context learning; Generator agent synthesizes executable BT sub-trees in XML format. Integrated into CARLA+Nav2 simulation and triggered only when baseline BT fails.

Result: Successfully demonstrated navigation around unexpected obstacles (e.g., street blockage) with no human intervention. The system extends to diverse driving scenarios beyond static BT baseline capabilities.

Conclusion: The agentic framework combining LLMs and LVMs provides a proof-of-concept for adaptive behavior planning in autonomous vehicles, addressing limitations of traditional static behavior trees for SAE Level 5 autonomy.

Abstract: Autonomous vehicles (AVs) require adaptive behavior planners to navigate unpredictable, real-world environments safely. Traditional behavior trees (BTs) offer structured decision logic but are inherently static and demand labor-intensive manual tuning, limiting their applicability at SAE Level 5 autonomy. This paper presents an agentic framework that leverages large language models (LLMs) and multi-modal vision models (LVMs) to generate and adapt BTs on the fly. A specialized Descriptor agent applies chain-of-symbols prompting to assess scene criticality, a Planner agent constructs high-level sub-goals via in-context learning, and a Generator agent synthesizes executable BT sub-trees in XML format. Integrated into a CARLA+Nav2 simulation, our system triggers only upon baseline BT failure, demonstrating successful navigation around unexpected obstacles (e.g., street blockage) with no human intervention. Compared to a static BT baseline, this approach is a proof-of-concept that extends to diverse driving scenarios.

</details>


### [95] [DepthCropSeg++: Scaling a Crop Segmentation Foundation Model With Depth-Labeled Data](https://arxiv.org/abs/2601.12366)
*Jiafei Zhang,Songliang Cao,Binghui Xu,Yanan Li,Weiwei Jia,Tingting Wu,Hao Lu,Weijuan Hu,Zhiguo Han*

Main category: cs.CV

TL;DR: DepthCropSeg++ is a foundation model for crop segmentation that achieves 93.11% mIoU by scaling up dataset to 28,406 images across 30+ species and 15 environments, using enhanced ViT-Adapter with dynamic upsampling and two-stage self-training.


<details>
  <summary>Details</summary>
Motivation: Current crop segmentation models suffer from limited data due to expensive pixel-level labeling, performing well only under specific crop types or controlled environments. The paper aims to create a foundation model that can generalize across diverse crop species and environmental conditions.

Method: 1) Scale up dataset to 28,406 images across 30+ crop species and 15 environmental conditions; 2) Build upon ViT-Adapter architecture with dynamic upsampling for improved detail awareness; 3) Train with two-stage self-training pipeline; 4) Follow almost unsupervised approach from previous DepthCropSeg work.

Result: Achieves 93.11% mIoU on comprehensive testing set, outperforming supervised baselines (+0.36%) and general-purpose vision foundation models like SAM (+48.57%). Excels in challenging scenarios: night-time (86.90% mIoU), high-density canopies (90.09% mIoU), and unseen crop varieties (90.09% mIoU).

Conclusion: DepthCropSeg++ establishes a new state-of-the-art for crop segmentation with strong generalization capabilities across diverse crop species and environmental conditions, demonstrating the effectiveness of scaling up datasets and using enhanced self-training approaches for agricultural vision tasks.

Abstract: DepthCropSeg++: a foundation model for crop segmentation, capable of segmenting different crop species under open in-field environment. Crop segmentation is a fundamental task for modern agriculture, which closely relates to many downstream tasks such as plant phenotyping, density estimation, and weed control. In the era of foundation models, a number of generic large language and vision models have been developed. These models have demonstrated remarkable real world generalization due to significant model capacity and largescale datasets. However, current crop segmentation models mostly learn from limited data due to expensive pixel-level labelling cost, often performing well only under specific crop types or controlled environment. In this work, we follow the vein of our previous work DepthCropSeg, an almost unsupervised approach to crop segmentation, to scale up a cross-species and crossscene crop segmentation dataset, with 28,406 images across 30+ species and 15 environmental conditions. We also build upon a state-of-the-art semantic segmentation architecture ViT-Adapter architecture, enhance it with dynamic upsampling for improved detail awareness, and train the model with a two-stage selftraining pipeline. To systematically validate model performance, we conduct comprehensive experiments to justify the effectiveness and generalization capabilities across multiple crop datasets. Results demonstrate that DepthCropSeg++ achieves 93.11% mIoU on a comprehensive testing set, outperforming both supervised baselines and general-purpose vision foundation models like Segmentation Anything Model (SAM) by significant margins (+0.36% and +48.57% respectively). The model particularly excels in challenging scenarios including night-time environment (86.90% mIoU), high-density canopies (90.09% mIoU), and unseen crop varieties (90.09% mIoU), indicating a new state of the art for crop segmentation.

</details>


### [96] [CD-TWINSAFE: A ROS-enabled Digital Twin for Scene Understanding and Safety Emerging V2I Technology](https://arxiv.org/abs/2601.12373)
*Amro Khaled,Farah Khaled,Omar Riad,Catherine M. Elias*

Main category: cs.CV

TL;DR: CD-TWINSAFE is a V2I-based digital twin system for autonomous vehicles that combines on-board driving stack with digital twin simulation for real-time safety monitoring and alerts.


<details>
  <summary>Details</summary>
Motivation: To enhance autonomous vehicle safety through real-time monitoring and alert systems by creating a digital twin that replicates the real-world driving environment for safety validation and risk assessment.

Method: Two-stack architecture: 1) On-board driving stack with stereo camera for localization and perception (object detection, velocity/yaw estimation, safety metrics), 2) Digital twin stack using Unreal Engine 5 replica that receives real-time data via ROS2/UDP over 4G V2I communication.

Result: The system successfully processes 20-fps stereo images, computes safety metrics (time-to-collision, time-headway), and provides real-time safety alerts through the digital twin cockpit interface.

Conclusion: CD-TWINSAFE demonstrates a valid architecture for real-time autonomous vehicle safety monitoring through V2I-based digital twin technology, enabling enhanced safety assessment and alert systems.

Abstract: In this paper, the CD-TWINSAFE is introduced, a V2I-based digital twin for Autonomous Vehicles. The proposed architecture is composed of two stacks running simultaneously, an on-board driving stack that includes a stereo camera for scene understanding, and a digital twin stack that runs an Unreal Engine 5 replica of the scene viewed by the camera as well as returning safety alerts to the cockpit. The on-board stack is implemented on the vehicle side including 2 main autonomous modules; localization and perception. The position and orientation of the ego vehicle are obtained using on-board sensors. Furthermore, the perception module is responsible for processing 20-fps images from stereo camera and understands the scene through two complementary pipelines. The pipeline are working on object detection and feature extraction including object velocity, yaw and the safety metrics time-to-collision and time-headway. The collected data form the driving stack are sent to the infrastructure side through the ROS-enabled architecture in the form of custom ROS2 messages and sent over UDP links that ride a 4G modem for V2I communication. The environment is monitored via the digital twin through the shared messages which update the information of the spawned ego vehicle and detected objects based on the real-time localization and perception data. Several tests with different driving scenarios to confirm the validity and real-time response of the proposed architecture.

</details>


### [97] [Utilizing the Score of Data Distribution for Hyperspectral Anomaly Detection](https://arxiv.org/abs/2601.12379)
*Jiahui Sheng,Yidan Shi,Shu Xiang,Xiaorun Li,Shuhan Chen*

Main category: cs.CV

TL;DR: ScoreAD: A hyperspectral anomaly detection method using score-based generative models to distinguish background spectra (on low-dimensional manifolds) from anomalous spectra (outliers).


<details>
  <summary>Details</summary>
Motivation: Hyperspectral images contain abundant spectral information that likely satisfies the manifold hypothesis - high-dimensional spectra are determined by few factors, suggesting they lie on low-dimensional manifolds. This provides a basis for distinguishing normal background spectra from anomalous ones.

Method: Proposes ScoreAD: 1) Trains a score-based generative model (SGM) on all spectra from the hyperspectral image; 2) At test time, perturbs each spectrum through a perturbation kernel; 3) Feeds perturbed spectrum to trained SGM to obtain estimated score; 4) Uses the discrepancy between background and anomalous spectra in manifold distributions for detection.

Result: Experiments on four hyperspectral datasets demonstrate the effectiveness of the proposed method. Code is publicly available on GitHub.

Conclusion: The manifold hypothesis of hyperspectral images provides a solid foundation for anomaly detection, and score-based generative models can effectively leverage this property to distinguish background spectra (on manifolds) from anomalous outliers.

Abstract: Hyperspectral images (HSIs) are a type of image that contains abundant spectral information. As a type of real-world data, the high-dimensional spectra in hyperspectral images are actually determined by only a few factors, such as chemical composition and illumination. Thus, spectra in hyperspectral images are highly likely to satisfy the manifold hypothesis. Based on the hyperspectral manifold hypothesis, we propose a novel hyperspectral anomaly detection method (named ScoreAD) that leverages the time-dependent gradient field of the data distribution (i.e., the score), as learned by a score-based generative model (SGM). Our method first trains the SGM on the entire set of spectra from the hyperspectral image. At test time, each spectrum is passed through a perturbation kernel, and the resulting perturbed spectrum is fed into the trained SGM to obtain the estimated score. The manifold hypothesis of HSIs posits that background spectra reside on one or more low-dimensional manifolds. Conversely, anomalous spectra, owing to their unique spectral signatures, are considered outliers that do not conform to the background manifold. Based on this fundamental discrepancy in their manifold distributions, we leverage a generative SGM to achieve hyperspectral anomaly detection. Experiments on the four hyperspectral datasets demonstrate the effectiveness of the proposed method. The code is available at https://github.com/jiahuisheng/ScoreAD.

</details>


### [98] [A Hierarchical Benchmark of Foundation Models for Dermatology](https://arxiv.org/abs/2601.12382)
*Furkan Yuceyalcin,Abdurrahim Yilmaz,Burak Temelkuran*

Main category: cs.CV

TL;DR: Foundation models show a "granularity gap" in dermatology: general medical models excel at high-level screening (97.52% F1 for malignancy detection) but decline on fine-grained classification (65.50% on 40 subtypes), while specialized dermatology models perform better on detailed subtype discrimination.


<details>
  <summary>Details</summary>
Motivation: Current benchmarks oversimplify dermatology diagnosis to binary classification, obscuring models' ability to perform fine-grained differential diagnoses needed for clinical workflow integration. There's a need to evaluate foundation models across hierarchical diagnostic granularity levels.

Method: Evaluated 10 foundation models (general CV, medical imaging, dermatology-specific) using DERM12345 dataset with 40 lesion subclasses. Calculated frozen embeddings and trained lightweight adapter models with 5-fold cross-validation. Introduced hierarchical evaluation framework across 4 granularity levels: 40 Subclasses, 15 Main Classes, 2/4 Superclasses, and Binary Malignancy.

Result: Revealed "granularity gap": MedImageInsights achieved 97.52% weighted F1 on Binary Malignancy but declined to 65.50% on 40-class subtype classification. MedSigLip (69.79%) and dermatology-specific models (Derm Foundation and MONET) excelled at fine-grained subtype discrimination but had lower overall performance on broader tasks.

Conclusion: General medical foundation models are effective for high-level screening, but specialized modeling strategies are necessary for the granular distinctions required in diagnostic support systems. Different model types have complementary strengths across diagnostic granularity levels.

Abstract: Foundation models have transformed medical image analysis by providing robust feature representations that reduce the need for large-scale task-specific training. However, current benchmarks in dermatology often reduce the complex diagnostic taxonomy to flat, binary classification tasks, such as distinguishing melanoma from benign nevi. This oversimplification obscures a model's ability to perform fine-grained differential diagnoses, which is critical for clinical workflow integration. This study evaluates the utility of embeddings derived from ten foundation models, spanning general computer vision, general medical imaging, and dermatology-specific domains, for hierarchical skin lesion classification. Using the DERM12345 dataset, which comprises 40 lesion subclasses, we calculated frozen embeddings and trained lightweight adapter models using a five-fold cross-validation. We introduce a hierarchical evaluation framework that assesses performance across four levels of clinical granularity: 40 Subclasses, 15 Main Classes, 2 and 4 Superclasses, and Binary Malignancy. Our results reveal a "granularity gap" in model capabilities: MedImageInsights achieved the strongest overall performance (97.52% weighted F1-Score on Binary Malignancy detection) but declined to 65.50% on fine-grained 40-class subtype classification. Conversely, MedSigLip (69.79%) and dermatology-specific models (Derm Foundation and MONET) excelled at fine-grained 40-class subtype discrimination while achieving lower overall performance than MedImageInsights on broader classification tasks. Our findings suggest that while general medical foundation models are highly effective for high-level screening, specialized modeling strategies are necessary for the granular distinctions required in diagnostic support systems.

</details>


### [99] [Class-Partitioned VQ-VAE and Latent Flow Matching for Point Cloud Scene Generation](https://arxiv.org/abs/2601.12391)
*Dasith de Silva Edirimuni,Ajmal Saeed Mian*

Main category: cs.CV

TL;DR: A novel method for 3D scene generation that directly produces point clouds without database retrieval, using a class-partitioned VQ-VAE and latent flow matching model.


<details>
  <summary>Details</summary>
Motivation: Current 3D scene generation methods either only produce bounding boxes or rely on retrieving objects from databases. Diffusion-based latents often fail to decode into correct point cloud objects that match target classes, especially for complex scenes with varied objects.

Method: Proposes CPVQ-VAE (Class-Partitioned Vector Quantized Variational Autoencoder) with class-partitioned codebook where codevectors are labeled by class, and class-aware running average update to prevent codebook collapse. Uses Latent-space Flow Matching Model (LFMM) to generate object features and class labels, then CPVQ-VAE performs class-aware inverse look-up to decode to class-specific point cloud shapes.

Result: Achieves pure point cloud generation without external database retrieval. Shows significant improvements: up to 70.4% reduction in Chamfer error and 72.3% reduction in Point2Mesh error on complex living room scenes.

Conclusion: The method reliably recovers plausible point cloud scenes by effectively decoding object latent features into class-specific point clouds, overcoming limitations of current autoencoders and database retrieval approaches.

Abstract: Most 3D scene generation methods are limited to only generating object bounding box parameters while newer diffusion methods also generate class labels and latent features. Using object size or latent feature, they then retrieve objects from a predefined database. For complex scenes of varied, multi-categorical objects, diffusion-based latents cannot be effectively decoded by current autoencoders into the correct point cloud objects which agree with target classes. We introduce a Class-Partitioned Vector Quantized Variational Autoencoder (CPVQ-VAE) that is trained to effectively decode object latent features, by employing a pioneering $\textit{class-partitioned codebook}$ where codevectors are labeled by class. To address the problem of $\textit{codebook collapse}$, we propose a $\textit{class-aware}$ running average update which reinitializes dead codevectors within each partition. During inference, object features and class labels, both generated by a Latent-space Flow Matching Model (LFMM) designed specifically for scene generation, are consumed by the CPVQ-VAE. The CPVQ-VAE's class-aware inverse look-up then maps generated latents to codebook entries that are decoded to class-specific point cloud shapes. Thereby, we achieve pure point cloud generation without relying on an external objects database for retrieval. Extensive experiments reveal that our method reliably recovers plausible point cloud scenes, with up to 70.4% and 72.3% reduction in Chamfer and Point2Mesh errors on complex living room scenes.

</details>


### [100] [Weaknesses of Facial Emotion Recognition Systems](https://arxiv.org/abs/2601.12402)
*Aleksandra Jamróz,Patrycja Wysocka,Piotr Garbat*

Main category: cs.CV

TL;DR: This paper reviews emotion detection from faces for human-computer interaction, selects three top neural network solutions and three diverse datasets, trains the networks, and performs cross-dataset experiments to reveal weaknesses in existing approaches.


<details>
  <summary>Details</summary>
Motivation: The motivation is the need for emotion detection in human-computer interaction and the enormous variety of existing methods, which requires an in-depth review and systematic comparison to understand the current state of the field and identify limitations.

Method: The method involves: 1) conducting an in-depth review of existing emotion detection methods, 2) selecting three best neural network solutions, 3) selecting three diverse datasets with many images, 4) training the selected networks, and 5) performing cross-dataset experiments including testing on different datasets than those used for training.

Result: The experiments reveal several weaknesses in existing solutions: differences between datasets, unequal difficulty levels in recognizing certain emotions, and challenges in differentiating between closely related emotions. Cross-dataset testing exposes these limitations.

Conclusion: The study concludes that current emotion detection methods have significant limitations, particularly regarding dataset inconsistencies and difficulty distinguishing similar emotions. Cross-dataset evaluation is crucial for revealing these weaknesses and advancing the field toward more robust solutions.

Abstract: Emotion detection from faces is one of the machine learning problems needed for human-computer interaction. The variety of methods used is enormous, which motivated an in-depth review of articles and scientific studies. Three of the most interesting and best solutions are selected, followed by the selection of three datasets that stood out for the diversity and number of images in them. The selected neural networks are trained, and then a series of experiments are performed to compare their performance, including testing on different datasets than a model was trained on. This reveals weaknesses in existing solutions, including differences between datasets, unequal levels of difficulty in recognizing certain emotions and the challenges in differentiating between closely related emotions.

</details>


### [101] [HOT-POT: Optimal Transport for Sparse Stereo Matching](https://arxiv.org/abs/2601.12423)
*Antonin Clerc,Michael Quellmalz,Moritz Piening,Philipp Flotho,Gregor Kornhardt,Gabriele Steidl*

Main category: cs.CV

TL;DR: Unsupervised sparse stereo matching using optimal transport with line constraints for facial landmark matching across different conventions.


<details>
  <summary>Details</summary>
Motivation: Stereo vision faces challenges like occlusions, motion, and camera distortions, especially with sparse features like facial landmarks. Traditional methods are parameter-sensitive and ill-posed for sparse matching.

Method: Formulates camera-projected points as lines, uses epipolar distance and 3D ray distance as cost functions in partial optimal transport problems. Extends to hierarchical OT for object matching.

Result: Efficiently solvable assignment algorithms for feature and object matching, demonstrated in numerical experiments for facial landmark matching across different conventions.

Conclusion: Optimal transport with line constraints provides effective unsupervised sparse stereo matching, particularly useful for facial analysis applications requiring cross-convention landmark matching.

Abstract: Stereo vision between images faces a range of challenges, including occlusions, motion, and camera distortions, across applications in autonomous driving, robotics, and face analysis. Due to parameter sensitivity, further complications arise for stereo matching with sparse features, such as facial landmarks. To overcome this ill-posedness and enable unsupervised sparse matching, we consider line constraints of the camera geometry from an optimal transport (OT) viewpoint. Formulating camera-projected points as (half)lines, we propose the use of the classical epipolar distance as well as a 3D ray distance to quantify matching quality. Employing these distances as a cost function of a (partial) OT problem, we arrive at efficiently solvable assignment problems. Moreover, we extend our approach to unsupervised object matching by formulating it as a hierarchical OT problem. The resulting algorithms allow for efficient feature and object matching, as demonstrated in our numerical experiments. Here, we focus on applications in facial analysis, where we aim to match distinct landmarking conventions.

</details>


### [102] [SkeFi: Cross-Modal Knowledge Transfer for Wireless Skeleton-Based Action Recognition](https://arxiv.org/abs/2601.12432)
*Shunyu Huang,Yunjiao Zhou,Jianfei Yang*

Main category: cs.CV

TL;DR: SkeFi introduces a cross-modal knowledge transfer framework that uses RGB data to train accurate skeleton estimation from noisy wireless sensors (LiDAR/mmWave) for action recognition in dark environments while addressing privacy concerns.


<details>
  <summary>Details</summary>
Motivation: RGB-based skeleton recognition fails in dark environments and raises privacy concerns, limiting applications in smart homes/hospitals. Wireless sensors (LiDAR/mmWave) offer non-invasive alternatives but suffer from insufficient training data and noisier skeletal keypoints.

Method: Proposes SkeFi with cross-modal knowledge transfer from data-rich RGB modality to wireless sensors. Uses enhanced Temporal Correlation Adaptive Graph Convolution (TC-AGC) with frame interactive enhancement to handle noise/missing frames, plus dual temporal convolution for multiscale temporal modeling.

Result: SkeFi achieves state-of-the-art performance on both mmWave and LiDAR datasets, demonstrating accurate pose extraction and action recognition from noisy wireless sensor data.

Conclusion: SkeFi successfully addresses the limitations of RGB-based systems by enabling accurate skeleton-based action recognition from wireless sensors through cross-modal transfer and robust noise-handling techniques, expanding applications to privacy-sensitive and dark environments.

Abstract: Skeleton-based action recognition leverages human pose keypoints to categorize human actions, which shows superior generalization and interoperability compared to regular end-to-end action recognition. Existing solutions use RGB cameras to annotate skeletal keypoints, but their performance declines in dark environments and raises privacy concerns, limiting their use in smart homes and hospitals. This paper explores non-invasive wireless sensors, i.e., LiDAR and mmWave, to mitigate these challenges as a feasible alternative. Two problems are addressed: (1) insufficient data on wireless sensor modality to train an accurate skeleton estimation model, and (2) skeletal keypoints derived from wireless sensors are noisier than RGB, causing great difficulties for subsequent action recognition models. Our work, SkeFi, overcomes these gaps through a novel cross-modal knowledge transfer method acquired from the data-rich RGB modality. We propose the enhanced Temporal Correlation Adaptive Graph Convolution (TC-AGC) with frame interactive enhancement to overcome the noise from missing or inconsecutive frames. Additionally, our research underscores the effectiveness of enhancing multiscale temporal modeling through dual temporal convolution. By integrating TC-AGC with temporal modeling for cross-modal transfer, our framework can extract accurate poses and actions from noisy wireless sensors. Experiments demonstrate that SkeFi realizes state-of-the-art performances on mmWave and LiDAR. The code is available at https://github.com/Huang0035/Skefi.

</details>


### [103] [Adversarial Defense in Vision-Language Models: An Overview](https://arxiv.org/abs/2601.12443)
*Xiaowei Fu,Lei Zhang*

Main category: cs.CV

TL;DR: Survey paper reviewing three main defense paradigms against adversarial attacks on Vision Language Models (VLMs): Training-time Defense, Test-time Adaptation Defense, and Training-free Defense, analyzing their strengths, limitations, and ongoing challenges.


<details>
  <summary>Details</summary>
Motivation: The widespread adoption of Vision Language Models (VLMs) like CLIP has raised security concerns about their vulnerability to sophisticated adversarial attacks that could compromise model performance and system security in cross-modal tasks.

Method: The paper conducts a comprehensive survey of three defense paradigms: 1) Training-time Defense (adversarial fine-tuning), 2) Test-time Adaptation Defense (updating parameters at inference), and 3) Training-free Defense (altering adversarial inputs or feature embeddings).

Result: The survey reviews latest advancements in adversarial defense strategies for VLMs, highlighting that each approach has trade-offs: Training-time Defense requires heavy computation, Test-time Adaptation adds inference complexity, and Training-free Defense avoids model modification but may have limited effectiveness.

Conclusion: While significant progress has been made in defending VLMs against adversarial attacks, ongoing challenges remain in enhancing robustness, and the field needs solutions that balance effectiveness, computational efficiency, and generalization across attack types.

Abstract: The widespread use of Vision Language Models (VLMs, e.g. CLIP) has raised concerns about their vulnerability to sophisticated and imperceptible adversarial attacks. These attacks could compromise model performance and system security in cross-modal tasks. To address this challenge, three main defense paradigms have been proposed: Training-time Defense, Test-time Adaptation Defense, and Training-free Defense. Training-time Defense involves modifying the training process, typically through adversarial fine-tuning to improve the robustness to adversarial examples. While effective, this approach requires substantial computational resources and may not generalize across all adversarial attacks. Test-time Adaptation Defense focuses on adapting the model at inference time by updating its parameters to handle unlabeled adversarial examples, offering flexibility but often at the cost of increased complexity and computational overhead. Training-free Defense avoids modifying the model itself, instead focusing on altering the adversarial inputs or their feature embeddings, which enforces input perturbations to mitigate the impact of attacks without additional training. This survey reviews the latest advancements in adversarial defense strategies for VLMs, highlighting the strengths and limitations of such approaches and discussing ongoing challenges in enhancing the robustness of VLMs.

</details>


### [104] [Large-scale EM Benchmark for Multi-Organelle Instance Segmentation in the Wild](https://arxiv.org/abs/2601.12464)
*Yanrui Lu,Danyang Chen,Haowen Xiao,Jiarui Zhu,Fukang Ge,Binqian Zou,Jiali Guan,Jiayin Liang,Yuting Wang,Ziqian Guan,Xiangcheng Bao,Jinhao Bi,Lin Gu,Jun He,Yingying Zhu*

Main category: cs.CV

TL;DR: Large-scale EM benchmark for multi-organelle instance segmentation reveals limitations of current patch-based methods in handling real-world heterogeneity and global morphologies.


<details>
  <summary>Details</summary>
Motivation: Current EM segmentation benchmarks are too small and curated, failing to capture real-world heterogeneity and large spatial context needed for accurate organelle instance segmentation.

Method: Created large-scale multi-source benchmark with 100k+ 2D EM images across cell types and 5 organelle classes, using connectivity-aware 3D Label Propagation Algorithm with expert refinement. Benchmarked U-Net, SAM variants, and Mask2Former.

Result: Current models struggle to generalize across heterogeneous EM data and perform poorly on organelles with distributed morphologies (e.g., Endoplasmic Reticulum), revealing fundamental mismatch between local-context models and long-range structural continuity.

Conclusion: Need for new approaches that can handle real-world variability and global structural continuity in EM organelle segmentation. Benchmark dataset and labeling tool will be publicly released.

Abstract: Accurate instance-level segmentation of organelles in electron microscopy (EM) is critical for quantitative analysis of subcellular morphology and inter-organelle interactions. However, current benchmarks, based on small, curated datasets, fail to capture the inherent heterogeneity and large spatial context of in-the-wild EM data, imposing fundamental limitations on current patch-based methods. To address these limitations, we developed a large-scale, multi-source benchmark for multi-organelle instance segmentation, comprising over 100,000 2D EM images across variety cell types and five organelle classes that capture real-world variability. Dataset annotations were generated by our designed connectivity-aware Label Propagation Algorithm (3D LPA) with expert refinement. We further benchmarked several state-of-the-art models, including U-Net, SAM variants, and Mask2Former. Our results show several limitations: current models struggle to generalize across heterogeneous EM data and perform poorly on organelles with global, distributed morphologies (e.g., Endoplasmic Reticulum). These findings underscore the fundamental mismatch between local-context models and the challenge of modeling long-range structural continuity in the presence of real-world variability. The benchmark dataset and labeling tool will be publicly released soon.

</details>


### [105] [DCAC: Dynamic Class-Aware Cache Creates Stronger Out-of-Distribution Detectors](https://arxiv.org/abs/2601.12468)
*Yanqi Wu,Qichao Chen,Runhe Lai,Xinhua Lu,Jia-Xin Zhuang,Zhilin Zhao,Wei-Shi Zheng,Ruixuan Wang*

Main category: cs.CV

TL;DR: DCAC is a training-free test-time calibration module that uses class-specific caches to collect high-entropy samples and calibrate predictions, reducing overconfidence on OOD samples.


<details>
  <summary>Details</summary>
Motivation: OOD detection is challenging due to neural networks' overconfident predictions on unseen OOD samples. The authors discovered that OOD samples predicted as the same class are visually more similar to each other than to true in-distribution samples.

Method: DCAC maintains separate caches for each ID class to collect high-entropy samples. It uses a lightweight two-layer module that leverages cached visual features and predicted probabilities to calibrate raw predictions at test time without requiring training.

Result: Extensive experiments show DCAC significantly enhances existing OOD detection methods across multiple benchmarks, reducing FPR95 by 6.55% when integrated with ASH-S on ImageNet OOD benchmark.

Conclusion: DCAC is an effective, training-free calibration module that can be seamlessly integrated with various OOD detection methods across unimodal and vision-language models with minimal computational overhead.

Abstract: Out-of-distribution (OOD) detection remains a fundamental challenge for deep neural networks, particularly due to overconfident predictions on unseen OOD samples during testing. We reveal a key insight: OOD samples predicted as the same class, or given high probabilities for it, are visually more similar to each other than to the true in-distribution (ID) samples. Motivated by this class-specific observation, we propose DCAC (Dynamic Class-Aware Cache), a training-free, test-time calibration module that maintains separate caches for each ID class to collect high-entropy samples and calibrate the raw predictions of input samples. DCAC leverages cached visual features and predicted probabilities through a lightweight two-layer module to mitigate overconfident predictions on OOD samples. This module can be seamlessly integrated with various existing OOD detection methods across both unimodal and vision-language models while introducing minimal computational overhead. Extensive experiments on multiple OOD benchmarks demonstrate that DCAC significantly enhances existing methods, achieving substantial improvements, i.e., reducing FPR95 by 6.55% when integrated with ASH-S on ImageNet OOD benchmark.

</details>


### [106] [NeuralFur: Animal Fur Reconstruction From Multi-View Images](https://arxiv.org/abs/2601.12481)
*Vanessa Sklyarova,Berna Kabadayi,Anastasios Yiannakidis,Giorgio Becherini,Michael J. Black,Justus Thies*

Main category: cs.CV

TL;DR: First multi-view method for high-fidelity 3D animal fur reconstruction using strand-based representation guided by vision language models.


<details>
  <summary>Details</summary>
Motivation: Reconstructing realistic animal fur from images is challenging due to fine-scale details, self-occlusion, and view-dependent appearance. Unlike human hairstyles, there are no existing datasets for learning fur priors across different animals.

Method: 1) Reconstruct coarse surface geometry using multi-view stereo; 2) Use VLM to retrieve realistic fur length/structure information for body parts; 3) Construct furless geometry and grow strands atop it; 4) Supervise with geometric and photometric losses; 5) Use VLM to guide strand growth direction and relation to gravity vector to mitigate orientation ambiguities.

Result: The method shows generalization across various animals with different fur types, achieving high-fidelity 3D fur reconstruction from multi-view RGB images.

Conclusion: This work presents a novel approach using vision language models to guide 3D fur reconstruction, enabling realistic animal fur modeling without requiring extensive fur-specific datasets.

Abstract: Reconstructing realistic animal fur geometry from images is a challenging task due to the fine-scale details, self-occlusion, and view-dependent appearance of fur. In contrast to human hairstyle reconstruction, there are also no datasets that can be leveraged to learn a fur prior for different animals. In this work, we present a first multi-view-based method for high-fidelity 3D fur modeling of animals using a strand-based representation, leveraging the general knowledge of a vision language model. Given multi-view RGB images, we first reconstruct a coarse surface geometry using traditional multi-view stereo techniques. We then use a vision language model (VLM) system to retrieve information about the realistic length structure of the fur for each part of the body. We use this knowledge to construct the animal's furless geometry and grow strands atop it. The fur reconstruction is supervised with both geometric and photometric losses computed from multi-view images. To mitigate orientation ambiguities stemming from the Gabor filters that are applied to the input images, we additionally utilize the VLM to guide the strands' growth direction and their relation to the gravity vector that we incorporate as a loss. With this new schema of using a VLM to guide 3D reconstruction from multi-view inputs, we show generalization across a variety of animals with different fur types. For additional results and code, please refer to https://neuralfur.is.tue.mpg.de.

</details>


### [107] [Histopath-C: Towards Realistic Domain Shifts for Histopathology Vision-Language Adaptation](https://arxiv.org/abs/2601.12493)
*Mehrdad Noori,Gustavo Adolfo Vargas Hakim,David Osowiechi,Fereshteh Shakeri,Ali Bahri,Moslem Yazdanpanah,Sahar Dastani,Ismail Ben Ayed,Christian Desrosiers*

Main category: cs.CV

TL;DR: The paper introduces Histopath-C, a benchmark with synthetic corruptions for histopathology images to evaluate Test-Time Adaptation methods, and proposes LATTE, a low-rank adaptation strategy that outperforms existing TTA methods.


<details>
  <summary>Details</summary>
Motivation: Medical VLMs show strong performance in histopathology but degrade significantly when faced with real-world domain shifts like staining variations, contamination, blurring, and noise. There's a need for robust adaptation methods specifically designed for histopathology images.

Method: 1) Created Histopath-C benchmark with realistic synthetic corruptions mimicking real-world distribution shifts in digital histopathology. 2) Proposed LATTE (Low-rank Adaptation with Text Templates) - a transductive, low-rank adaptation strategy that exploits multiple text templates to mitigate sensitivity to diverse text inputs.

Result: LATTE outperforms state-of-the-art TTA methods originally designed for natural images across multiple histopathology datasets, demonstrating effectiveness for robust adaptation in histopathology images.

Conclusion: The proposed Histopath-C benchmark and LATTE adaptation strategy provide effective tools for evaluating and improving the robustness of medical VLMs against real-world domain shifts in histopathology, with code and data publicly available.

Abstract: Medical Vision-language models (VLMs) have shown remarkable performances in various medical imaging domains such as histo\-pathology by leveraging pre-trained, contrastive models that exploit visual and textual information. However, histopathology images may exhibit severe domain shifts, such as staining, contamination, blurring, and noise, which may severely degrade the VLM's downstream performance. In this work, we introduce Histopath-C, a new benchmark with realistic synthetic corruptions designed to mimic real-world distribution shifts observed in digital histopathology. Our framework dynamically applies corruptions to any available dataset and evaluates Test-Time Adaptation (TTA) mechanisms on the fly. We then propose LATTE, a transductive, low-rank adaptation strategy that exploits multiple text templates, mitigating the sensitivity of histopathology VLMs to diverse text inputs. Our approach outperforms state-of-the-art TTA methods originally designed for natural images across a breadth of histopathology datasets, demonstrating the effectiveness of our proposed design for robust adaptation in histopathology images. Code and data are available at https://github.com/Mehrdad-Noori/Histopath-C.

</details>


### [108] [Video Individual Counting and Tracking from Moving Drones: A Benchmark and Methods](https://arxiv.org/abs/2601.12500)
*Yaowu Fan,Jia Wan,Tao Han,Andy J. Ma,Antoni B. Chan*

Main category: cs.CV

TL;DR: Proposed MovingDroneCrowd++ dataset and GD3A/DVTrack methods for drone-based dense crowd counting and tracking, achieving 47.4% counting error reduction and 39.2% tracking improvement.


<details>
  <summary>Details</summary>
Motivation: Existing crowd counting/tracking methods rely on fixed cameras with limited spatial coverage, inadequate for large-scale dense crowd analysis. Moving drones offer flexible scene coverage but lack suitable datasets and methods.

Method: 1) Created MovingDroneCrowd++ dataset (largest video-level dataset for drone-captured dense crowds). 2) Proposed GD3A: density map-based video counting using optimal transport with adaptive dustbin score for pixel-level descriptor correspondences. 3) Developed DVTrack: descriptor voting mechanism converting descriptor-level to instance-level associations for tracking.

Result: Methods significantly outperform existing approaches under dense crowds and complex motion: 47.4% reduction in counting error and 39.2% improvement in tracking performance on the MovingDroneCrowd++ dataset.

Conclusion: Moving drone-based approach with GD3A and DVTrack provides effective solution for large-scale dense crowd analysis, overcoming limitations of fixed camera systems through flexible spatial coverage and robust algorithms.

Abstract: Counting and tracking dense crowds in large-scale scenes is highly challenging, yet existing methods mainly rely on datasets captured by fixed cameras, which provide limited spatial coverage and are inadequate for large-scale dense crowd analysis. To address this limitation, we propose a flexible solution using moving drones to capture videos and perform video-level crowd counting and tracking of unique pedestrians across entire scenes. We introduce MovingDroneCrowd++, the largest video-level dataset for dense crowd counting and tracking captured by moving drones, covering diverse and complex conditions with varying flight altitudes, camera angles, and illumination. Existing methods fail to achieve satisfactory performance on this dataset. To this end, we propose GD3A (Global Density Map Decomposition via Descriptor Association), a density map-based video individual counting method that avoids explicit localization. GD3A establishes pixel-level correspondences between pedestrian descriptors across consecutive frames via optimal transport with an adaptive dustbin score, enabling the decomposition of global density maps into shared, inflow, and outflow components. Building on this framework, we further introduce DVTrack, which converts descriptor-level matching into instance-level associations through a descriptor voting mechanism for pedestrian tracking. Experimental results show that our methods significantly outperform existing approaches under dense crowds and complex motion, reducing counting error by 47.4 percent and improving tracking performance by 39.2 percent.

</details>


### [109] [SDCoNet: Saliency-Driven Multi-Task Collaborative Network for Remote Sensing Object Detection](https://arxiv.org/abs/2601.12507)
*Ruo Qi,Linhui Dai,Yusong Qin,Chaolei Yang,Yanshan Li*

Main category: cs.CV

TL;DR: SDCoNet is a multi-task network that jointly performs super-resolution and object detection for low-quality remote sensing images, using shared encoder features, saliency-based token selection, and gradient routing to optimize both tasks collaboratively.


<details>
  <summary>Details</summary>
Motivation: Remote sensing images present challenges for object detection due to complex backgrounds, weak object signals, and small object scales, especially under low-quality conditions. Existing serial pipelines (SR then detection) suffer from misaligned objectives, feature redundancy, and lack of effective interaction between tasks.

Method: Proposes SDCoNet with: 1) Swin Transformer-based shared encoder for cross-task feature collaboration, 2) Multi-scale saliency prediction module to select key tokens and focus on weak object regions, 3) Gradient routing strategy to stabilize detection semantics and guide SR to generate detection-beneficial high-frequency details.

Result: Significantly outperforms existing mainstream algorithms on public datasets (NWPU VHR-10-Split, DOTAv1.5-Split, HRSSD-Split) for small object detection in low-quality remote sensing images, while maintaining competitive computational efficiency.

Conclusion: SDCoNet effectively addresses the limitations of serial SR+detection pipelines through multi-task collaboration, saliency-driven attention, and gradient routing, achieving superior performance for challenging remote sensing object detection tasks.

Abstract: In remote sensing images, complex backgrounds, weak object signals, and small object scales make accurate detection particularly challenging, especially under low-quality imaging conditions. A common strategy is to integrate single-image super-resolution (SR) before detection; however, such serial pipelines often suffer from misaligned optimization objectives, feature redundancy, and a lack of effective interaction between SR and detection. To address these issues, we propose a Saliency-Driven multi-task Collaborative Network (SDCoNet) that couples SR and detection through implicit feature sharing while preserving task specificity. SDCoNet employs the swin transformer-based shared encoder, where hierarchical window-shifted self-attention supports cross-task feature collaboration and adaptively balances the trade-off between texture refinement and semantic representation. In addition, a multi-scale saliency prediction module produces importance scores to select key tokens, enabling focused attention on weak object regions, suppression of background clutter, and suppression of adverse features introduced by multi-task coupling. Furthermore, a gradient routing strategy is introduced to mitigate optimization conflicts. It first stabilizes detection semantics and subsequently routes SR gradients along a detection-oriented direction, enabling the framework to guide the SR branch to generate high-frequency details that are explicitly beneficial for detection. Experiments on public datasets, including NWPU VHR-10-Split, DOTAv1.5-Split, and HRSSD-Split, demonstrate that the proposed method, while maintaining competitive computational efficiency, significantly outperforms existing mainstream algorithms in small object detection on low-quality remote sensing images. Our code is available at https://github.com/qiruo-ya/SDCoNet.

</details>


### [110] [Fine-Tuning Cycle-GAN for Domain Adaptation of MRI Images](https://arxiv.org/abs/2601.12512)
*Mohd Usama,Belal Ahmad,Faleh Menawer R Althiyabi*

Main category: cs.CV

TL;DR: Proposes a Cycle-GAN-based model for unsupervised medical image domain adaptation to address scanner/institutional domain shifts in MRI data, improving deep learning model performance across domains without paired training data.


<details>
  <summary>Details</summary>
Motivation: MRI scans from different scanners/institutions suffer from domain shifts due to hardware, protocol, and parameter variations, which degrade deep learning model performance when applied to target domain images.

Method: Cycle-GAN-based model for unsupervised domain adaptation that learns bidirectional mappings between source and target domains without paired training data, using content and disparity loss to preserve anatomical content while adapting domains.

Result: Experiments on MRI datasets demonstrate efficacy in bidirectional domain adaptation without labeled data, improving model performance and reducing domain-related variability for more precise medical image analysis.

Conclusion: The approach offers promising avenues for improving diagnostic accuracy in healthcare by enabling domain adaptation while maintaining image integrity, contributing to more consistent medical image analysis across different scanners/institutions.

Abstract: Magnetic Resonance Imaging (MRI) scans acquired from different scanners or institutions often suffer from domain shifts owing to variations in hardware, protocols, and acquisition parameters. This discrepancy degrades the performance of deep learning models trained on source domain data when applied to target domain images. In this study, we propose a Cycle-GAN-based model for unsupervised medical-image domain adaptation. Leveraging CycleGANs, our model learns bidirectional mappings between the source and target domains without paired training data, preserving the anatomical content of the images. By leveraging Cycle-GAN capabilities with content and disparity loss for adaptation tasks, we ensured image-domain adaptation while maintaining image integrity. Several experiments on MRI datasets demonstrated the efficacy of our model in bidirectional domain adaptation without labelled data. Furthermore, research offers promising avenues for improving the diagnostic accuracy of healthcare. The statistical results confirm that our approach improves model performance and reduces domain-related variability, thus contributing to more precise and consistent medical image analysis.

</details>


### [111] [Deep Feature Deformation Weights](https://arxiv.org/abs/2601.12527)
*Richard Liu,Itai Lang,Rana Hanocka*

Main category: cs.CV

TL;DR: A hybrid mesh deformation method combining semantic deep features with classical optimization for real-time, precise, and semantic shape editing.


<details>
  <summary>Details</summary>
Motivation: Classical handle-based mesh deformation offers precise control but requires intuitive handle placement and lacks semantic understanding. Data-driven methods provide semantic edits but are slow and imprecise. There's a need to combine semantic understanding with precise real-time control.

Method: Uses deep feature proximity to compute smooth semantic deformation weights without additional regularization. Introduces barycentric feature distillation pipeline that efficiently uses visual signals from shape renders to minimize distillation cost. Preserves classical method properties through feature space constraints and locality weighting.

Result: Weights can be computed in real-time for any surface point, enabling semantic co-deformation of parts. Method works on high-resolution meshes (up to 1M faces) in under a minute vs. hours for prior methods. Supports automatic semantic symmetry detection and symmetry-preserving deformations.

Conclusion: Successfully fuses semantic data priors with classical optimization frameworks, achieving real-time precise semantic mesh deformation on consumer hardware while overcoming limitations of both classical and data-driven approaches.

Abstract: Handle-based mesh deformation has been a long-standing paradigm in computer graphics, enabling intuitive shape edits from sparse controls. Classic techniques offer precise and rapid deformation control. However, they solve an optimization problem with constraints defined by control handle placement, requiring a user to know apriori the ideal distribution of handles on the shape to accomplish the desired edit. The mapping from handle set to deformation behavior is often unintuitive and, importantly, non-semantic. Modern data-driven methods, on the other hand, leverage a data prior to obtain semantic edits, but are slow and imprecise. We propose a technique that fuses the semantic prior of data with the precise control and speed of traditional frameworks. Our approach is surprisingly simple yet effective: deep feature proximity makes for smooth and semantic deformation weights, with no need for additional regularization. The weights can be computed in real-time for any surface point, whereas prior methods require optimization for new handles. Moreover, the semantic prior from deep features enables co-deformation of semantic parts. We introduce an improved feature distillation pipeline, barycentric feature distillation, which efficiently uses the visual signal from shape renders to minimize distillation cost. This allows our weights to be computed for high resolution meshes in under a minute, in contrast to potentially hours for both classical and neural methods. We preserve and extend properties of classical methods through feature space constraints and locality weighting. Our field representation allows for automatic detection of semantic symmetries, which we use to produce symmetry-preserving deformations. We show a proof-of-concept application which can produce deformations for meshes up to 1 million faces in real-time on a consumer-grade machine.

</details>


### [112] [XRefine: Attention-Guided Keypoint Match Refinement](https://arxiv.org/abs/2601.12530)
*Jan Fabian Schmid,Annika Hagemann*

Main category: cs.CV

TL;DR: XRefine is a detector-agnostic sub-pixel keypoint refinement method that uses cross-attention on image patches to improve spatial accuracy without retraining for each detector.


<details>
  <summary>Details</summary>
Motivation: Current keypoint detectors often produce spatially inaccurate matches, and existing refinement methods are detector-specific, requiring retraining for each keypoint detector, which limits practical application.

Method: XRefine uses a cross-attention-based architecture that operates solely on image patches centered at matched keypoints. It learns to predict refined keypoint coordinates without relying on internal detector representations, making it detector-agnostic. The method can also be extended to handle multi-view feature tracks.

Result: Experiments on MegaDepth, KITTI, and ScanNet show consistent improvement in geometric estimation accuracy, achieving superior performance compared to existing refinement methods while maintaining runtime efficiency.

Conclusion: XRefine provides an effective, generalizable solution for sub-pixel keypoint refinement that works across different detectors without retraining, improving 3D vision task performance.

Abstract: Sparse keypoint matching is crucial for 3D vision tasks, yet current keypoint detectors often produce spatially inaccurate matches. Existing refinement methods mitigate this issue through alignment of matched keypoint locations, but they are typically detector-specific, requiring retraining for each keypoint detector. We introduce XRefine, a novel, detector-agnostic approach for sub-pixel keypoint refinement that operates solely on image patches centered at matched keypoints. Our cross-attention-based architecture learns to predict refined keypoint coordinates without relying on internal detector representations, enabling generalization across detectors. Furthermore, XRefine can be extended to handle multi-view feature tracks. Experiments on MegaDepth, KITTI, and ScanNet demonstrate that the approach consistently improves geometric estimation accuracy, achieving superior performance compared to existing refinement methods while maintaining runtime efficiency. Our code and trained models can be found at https://github.com/boschresearch/xrefine.

</details>


### [113] [BirdsEye-RU: A Dataset For Detecting Faces from Overhead Images](https://arxiv.org/abs/2601.12533)
*Md. Ahanaf Arif Khan,Ariful Islam,Sangeeta Biswas,Md. Iqbal Aziz Khan,Subrata Pramanik,Sanjoy Kumar Chakrabarty,Bimal Kumar Pramanik*

Main category: cs.CV

TL;DR: The paper introduces BirdsEye-RU, a new dataset of 2,978 overhead images with over 8,000 annotated faces, designed to address challenges in detecting small and distant faces in drone and high-altitude smartphone images.


<details>
  <summary>Details</summary>
Motivation: Face detection in overhead images is challenging due to extreme scale variations and environmental clutter. Existing datasets may not adequately address these challenges, particularly for small and distant faces captured from aerial perspectives.

Method: Created the BirdsEye-RU dataset containing 2,978 images with over 8,000 annotated faces. The dataset includes both drone images and smartphone-captured images from high altitude, specifically designed to capture small and distant faces across diverse environments.

Result: A comprehensive dataset of overhead face images is now publicly available on Kaggle. The dataset contains 2,978 images with over 8,000 annotated faces, addressing the specific challenges of scale variation and environmental clutter in overhead face detection.

Conclusion: The BirdsEye-RU dataset provides a valuable resource for researchers working on overhead face detection problems, particularly for small and distant faces. The dataset's public availability enables further research and development in this challenging computer vision domain.

Abstract: Detecting faces in overhead images remains a significant challenge due to extreme scale variations and environmental clutter. To address this, we created the BirdsEye-RU dataset, a comprehensive collection of 2,978 images containing over eight thousand annotated faces. This dataset is specifically designed to capture small and distant faces across diverse environments, containing both drone images and smartphone-captured images from high altitude. We present a detailed description of the BirdsEye-RU dataset in this paper. We made our dataset freely available to the public, and it can be accessed at https://www.kaggle.com/datasets/mdahanafarifkhan/birdseye-ru.

</details>


### [114] [Encoding Emotion Through Self-Supervised Eye Movement Reconstruction](https://arxiv.org/abs/2601.12534)
*Marcus Ma,Jordan Prescott,Emily Zhou,Tiantian Feng,Kleanthis Avramidis,Gabor Mihaly Toth,Shrikanth Narayanan*

Main category: cs.CV

TL;DR: Self-supervised eye movement reconstruction from low-resolution videos predicts multimodal emotional expression markers in Holocaust survivor interviews.


<details>
  <summary>Details</summary>
Motivation: Most emotion-gaze studies use specialized high-resolution eye-tracking equipment, limiting accessibility. Need methods for naturalistic, low-resolution video analysis to broaden reach and applications.

Method: Develop novel gaze detection model using self-supervised eye movement reconstruction (inspired by language model pretraining) on unlabeled video. Use encoder embeddings to fine-tune on two downstream tasks: 1) aligning eye movement with speech emotion estimates, 2) predicting momentary emotional behaviors (laughing, crying/sobbing, sighing).

Result: Model is predictive of emotion outcomes. Positive correlation between pretraining performance and emotion processing performance for both experiments.

Conclusion: Self-supervised eye movement reconstruction is effective for encoding affective signals in eye movements from low-resolution naturalistic videos.

Abstract: The relationship between emotional expression and eye movement is well-documented, with literature establishing gaze patterns are reliable indicators of emotion. However, most studies utilize specialized, high-resolution eye-tracking equipment, limiting the potential reach of findings. We investigate how eye movement can be used to predict multimodal markers of emotional expression from naturalistic, low-resolution videos. We utilize a collection of video interviews from the USC Shoah Foundation's Visual History Archive with Holocaust survivors as they recount their experiences in the Auschwitz concentration camp. Inspired by pretraining methods on language models, we develop a novel gaze detection model that uses self-supervised eye movement reconstruction that can effectively leverage unlabeled video. We use this model's encoder embeddings to fine-tune models on two downstream tasks related to emotional expression. The first is aligning eye movement with directional emotion estimates from speech. The second task is using eye gaze as a predictor of three momentary manifestations of emotional behaviors: laughing, crying/sobbing, and sighing. We find our new model is predictive of emotion outcomes and observe a positive correlation between pretraining performance and emotion processing performance for both experiments. We conclude self-supervised eye movement reconstruction is an effective method for encoding the affective signal they carry.

</details>


### [115] [PISE: Physics-Anchored Semantically-Enhanced Deep Computational Ghost Imaging for Robust Low-Bandwidth Machine Perception](https://arxiv.org/abs/2601.12551)
*Tong Wu*

Main category: cs.CV

TL;DR: PISE is a physics-informed deep ghost imaging framework that improves edge perception with low bandwidth by combining adjoint operator initialization and semantic guidance.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the challenge of achieving accurate edge perception with limited bandwidth, which is crucial for applications like remote sensing and IoT devices where transmission capacity is constrained.

Method: PISE combines adjoint operator initialization (for physics-informed priors) with semantic guidance to enhance ghost imaging reconstruction for edge perception tasks.

Result: The framework improves classification accuracy by 2.57% and reduces variance by 9x at 5% sampling rate compared to baseline methods.

Conclusion: PISE demonstrates that physics-informed deep learning with semantic guidance can significantly enhance low-bandwidth edge perception systems, making them more accurate and stable.

Abstract: We propose PISE, a physics-informed deep ghost imaging framework for low-bandwidth edge perception. By combining adjoint operator initialization with semantic guidance, PISE improves classification accuracy by 2.57% and reduces variance by 9x at 5% sampling.

</details>


### [116] [Camera Pose Revisited](https://arxiv.org/abs/2601.12567)
*Władysław Skarbek,Michał Salomonowicz,Michał Król*

Main category: cs.CV

TL;DR: PnP-ProCay78 algorithm solves planar Perspective-n-Point problem using Cayley parameterization and hybrid cost formulation, achieving accuracy comparable to SQPnP with simpler structure.


<details>
  <summary>Details</summary>
Motivation: Camera pose estimation is fundamental for calibration and multi-sensor systems. Existing PnP solvers often have complex structures or lack geometric transparency. The paper aims to develop a simpler, more intuitive algorithm for planar PnP problems.

Method: Combines classical quadratic reconstruction error formulation with Cayley parameterization of rotations and least-squares optimization. Uses deterministic selection of starting points based on analysis of reconstruction error for two canonical vectors. Features hybrid cost formulation that minimizes projection error while analytically eliminating reconstruction-error surrogate for translation.

Result: Achieves practically same projection accuracy as optimal SQPnP and slightly higher than IPPE (both OpenCV PnP procedures). Maintains significantly simpler algorithmic structure. Provides intuitive insight into convergence through analysis of optimization trajectories in Cayley space.

Conclusion: PnP-ProCay78 offers an effective solution to planar PnP with competitive accuracy, simpler implementation, and geometric transparency. The method is attractive for both practical applications and educational purposes due to its intuitive convergence analysis.

Abstract: Estimating the position and orientation of a camera with respect to an observed scene is one of the central problems in computer vision, particularly in the context of camera calibration and multi-sensor systems. This paper addresses the planar Perspective--$n$--Point problem, with special emphasis on the initial estimation of the pose of a calibration object. As a solution, we propose the \texttt{PnP-ProCay78} algorithm, which combines the classical quadratic formulation of the reconstruction error with a Cayley parameterization of rotations and least-squares optimization. The key component of the method is a deterministic selection of starting points based on an analysis of the reconstruction error for two canonical vectors, allowing costly solution-space search procedures to be avoided. Experimental validation is performed using data acquired also from high-resolution RGB cameras and very low-resolution thermal cameras in an integrated RGB--IR setup. The results demonstrate that the proposed algorithm achieves practically the same projection accuracy as optimal \texttt{SQPnP} and slightly higher than \texttt{IPPE}, both prominent \texttt{PnP-OpenCV} procedures. However, \texttt{PnP-ProCay78} maintains a significantly simpler algorithmic structure. Moreover, the analysis of optimization trajectories in Cayley space provides an intuitive insight into the convergence process, making the method attractive also from a didactic perspective. Unlike existing PnP solvers, the proposed \texttt{PnP-ProCay78} algorithm combines projection error minimization with an analytically eliminated reconstruction-error surrogate for translation, yielding a hybrid cost formulation that is both geometrically transparent and computationally efficient.

</details>


### [117] [Linear Mechanisms for Spatiotemporal Reasoning in Vision Language Models](https://arxiv.org/abs/2601.12626)
*Raphi Kang,Hongqiao Chen,Georgia Gkioxari,Pietro Perona*

Main category: cs.CV

TL;DR: VLMs use linear spatial IDs bound to textual activations for spatio-temporal reasoning, which can be causally manipulated to mediate model beliefs and serve as diagnostic tools.


<details>
  <summary>Details</summary>
Motivation: To understand the opaque mechanisms behind spatio-temporal reasoning in Vision Language Models and identify how visual/geometrical and textual representations combine in VLM computations.

Method: Search for confluence of spatial representations, identify linear binding of spatial IDs to textual activations, perform rigorous causal interventions to demonstrate mediation of model beliefs, and extend analysis to video VLMs for temporal ID mechanisms.

Result: VLMs encode object locations via linear spatial IDs bound to textual activations, perform reasoning through language tokens, and these IDs can systematically mediate model beliefs at intermediate layers. Spatial IDs also serve as diagnostic tools for VLM limitations and learning signals.

Conclusion: The paper elucidates a previously underexplored internal reasoning process in VLMs through spatiotemporal ID mechanisms, contributing to improved interpretability and principled design of more aligned and capable models.

Abstract: Spatio-temporal reasoning is a remarkable capability of Vision Language Models (VLMs), but the underlying mechanisms of such abilities remain largely opaque. We postulate that visual/geometrical and textual representations of spatial structure must be combined at some point in VLM computations. We search for such confluence, and ask whether the identified representation can causally explain aspects of input-output model behavior through a linear model. We show empirically that VLMs encode object locations by linearly binding \textit{spatial IDs} to textual activations, then perform reasoning via language tokens. Through rigorous causal interventions we demonstrate that these IDs, which are ubiquitous across the model, can systematically mediate model beliefs at intermediate VLM layers. Additionally, we find that spatial IDs serve as a diagnostic tool for identifying limitations in existing VLMs, and as a valuable learning signal. We extend our analysis to video VLMs and identify an analogous linear temporal ID mechanism. By characterizing our proposed spatiotemporal ID mechanism, we elucidate a previously underexplored internal reasoning process in VLMs, toward improved interpretability and the principled design of more aligned and capable models. We release our code for reproducibility: https://github.com/Raphoo/linear-mech-vlms.

</details>


### [118] [From Bands to Depth: Understanding Bathymetry Decisions on Sentinel-2](https://arxiv.org/abs/2601.12636)
*Satyaki Roy Chowdhury,Aswathnarayan Radhakrishnan,Hsiao Jou Hsu,Hari Subramoni,Joachim Moortgat*

Main category: cs.CV

TL;DR: Swin-BathyUNet model for satellite-derived bathymetry analyzed for depth inference reliability and trustworthiness, with spectral importance ranking, adapted CAM for regression, attention ablations, and cross-region testing revealing depth-dependent degradation patterns.


<details>
  <summary>Details</summary>
Motivation: Deploying Sentinel-2 satellite derived bathymetry (SDB) robustly across different sites remains challenging, requiring better understanding of how deep learning models infer depth and when their predictions can be trusted.

Method: Used Swin-Transformer based U-Net (Swin-BathyUNet) with leave-one-band out study to rank spectral importance, adapted ablation-based CAM to regression (A-CAM-R) with performance retention test, conducted attention ablations, and performed cross-region inference testing (train on one site, test on another).

Result: Spectral importance aligned with shallow water optics, A-CAM-R reliably localized evidence the model relies on, decoder conditioned cross attention improved robustness to glint/foam, and cross-region testing showed MAE rises nearly linearly with depth with bimodal distributions exacerbating mid/deep errors.

Conclusion: Practical guidance includes maintaining wide receptive fields, preserving radiometric fidelity in green/blue channels, pre-filtering bright high variance near shore, and pairing light target site fine-tuning with depth-aware calibration for cross-region transfer.

Abstract: Deploying Sentinel-2 satellite derived bathymetry (SDB) robustly across sites remains challenging. We analyze a Swin-Transformer based U-Net model (Swin-BathyUNet) to understand how it infers depth and when its predictions are trustworthy. A leave-one-band out study ranks spectral importance to the different bands consistent with shallow water optics. We adapt ablation-based CAM to regression (A-CAM-R) and validate the reliability via a performance retention test: keeping only the top-p% salient pixels while neutralizing the rest causes large, monotonic RMSE increase, indicating explanations localize on evidence the model relies on. Attention ablations show decoder conditioned cross attention on skips is an effective upgrade, improving robustness to glint/foam. Cross-region inference (train on one site, test on another) reveals depth-dependent degradation: MAE rises nearly linearly with depth, and bimodal depth distributions exacerbate mid/deep errors. Practical guidance follows: maintain wide receptive fields, preserve radiometric fidelity in green/blue channels, pre-filter bright high variance near shore, and pair light target site fine tuning with depth aware calibration to transfer across regions.

</details>


### [119] [Mixed Precision PointPillars for Efficient 3D Object Detection with TensorRT](https://arxiv.org/abs/2601.12638)
*Ninnart Fuengfusin,Keisuke Yoneda,Naoki Suganuma*

Main category: cs.CV

TL;DR: The paper proposes a mixed precision quantization framework for PointPillars-based LIDAR 3D object detection to achieve real-time performance while maintaining accuracy, addressing challenges of wide numerical distributions and outliers through layer sensitivity analysis and calibration strategies.


<details>
  <summary>Details</summary>
Motivation: LIDAR 3D object detection needs real-time operation for autonomous vehicles, but direct model quantization causes performance degradation due to LIDAR's wide numerical distributions and extreme outliers.

Method: Proposed mixed precision framework: 1) Searches for sensitive layers using PTQ by quantizing one layer at a time to INT8 and evaluating AP, 2) Assigns top-k sensitive layers as FP, 3) Greedily searches layer combinations for mixed precision models, 4) Finalizes with PTQ or QAT, 5) Uses minimal calibration data to reduce outlier impact.

Result: Mixed precision models achieve competitive performance to FP models with QAT pipeline, while PTQ pipeline works without training. TensorRT deployment reduces latency by up to 2.35x and model sizes by up to 2.26x.

Conclusion: The proposed framework effectively addresses LIDAR quantization challenges, enabling real-time 3D object detection with significant speed and size improvements while maintaining accuracy comparable to floating-point models.

Abstract: LIDAR 3D object detection is one of the important tasks for autonomous vehicles. Ensuring that this task operates in real-time is crucial. Toward this, model quantization can be used to accelerate the runtime. However, directly applying model quantization often leads to performance degradation due to LIDAR's wide numerical distributions and extreme outliers. To address the wide numerical distribution, we proposed a mixed precision framework designed for PointPillars. Our framework first searches for sensitive layers with post-training quantization (PTQ) by quantizing one layer at a time to 8-bit integer (INT8) and evaluating each model for average precision (AP). The top-k most sensitive layers are assigned as floating point (FP). Combinations of these layers are greedily searched to produce candidate mixed precision models, which are finalized with either PTQ or quantization-aware training (QAT). Furthermore, to handle outliers, we observe that using a very small number of calibration data reduces the likelihood of encountering outliers, thereby improving PTQ performance. Our methods provides mixed precision models without training in the PTQ pipeline, while our QAT pipeline achieves the performance competitive to FP models. With TensorRT deployment, our models offer less latency and sizes by up to 2.35 and 2.26 times, respectively.

</details>


### [120] [Generalizable Hyperparameter Optimization for Federated Learning on Non-IID Cancer Images](https://arxiv.org/abs/2601.12664)
*Elisa Gonçalves Ribeiro,Rodrigo Moreira,Larissa Ferreira Rodrigues Moreira,André Ricardo Backes*

Main category: cs.CV

TL;DR: This paper explores whether hyperparameters optimized on one cancer imaging dataset can generalize across non-IID federated learning scenarios for histopathology tasks, and introduces a simple cross-dataset aggregation heuristic.


<details>
  <summary>Details</summary>
Motivation: Deep learning for cancer histopathology faces privacy constraints in clinical settings. Federated Learning helps by keeping data local, but its performance depends on hyperparameter choices under non-IID client datasets. The paper examines whether hyperparameters optimized on one dataset can generalize across different non-IID federated scenarios.

Method: The researchers performed centralized Bayesian hyperparameter optimization for binary histopathology tasks on ovarian and colorectal cancers. They then transferred dataset-specific optima to non-IID FL setups. They introduced a cross-dataset aggregation heuristic that combines configurations by averaging learning rates and considering modal optimizers and batch sizes.

Result: The combined configuration achieved competitive classification performance, demonstrating that hyperparameter optimization can generalize across non-IID federated scenarios for cancer histopathology tasks.

Conclusion: The study shows that hyperparameters optimized on one cancer imaging dataset can generalize to non-IID FL scenarios, and the proposed simple cross-dataset aggregation heuristic provides a practical approach for achieving competitive performance in federated cancer histopathology analysis.

Abstract: Deep learning for cancer histopathology training conflicts with privacy constraints in clinical settings. Federated Learning (FL) mitigates this by keeping data local; however, its performance depends on hyperparameter choices under non-independent and identically distributed (non-IID) client datasets. This paper examined whether hyperparameters optimized on one cancer imaging dataset generalized across non-IID federated scenarios. We considered binary histopathology tasks for ovarian and colorectal cancers. We perform centralized Bayesian hyperparameter optimization and transfer dataset-specific optima to the non-IID FL setup. The main contribution of this study is the introduction of a simple cross-dataset aggregation heuristic by combining configurations by averaging the learning rates and considering the modal optimizers and batch sizes. This combined configuration achieves a competitive classification performance.

</details>


### [121] [Near-Light Color Photometric Stereo for mono-Chromaticity non-lambertian surface](https://arxiv.org/abs/2601.12666)
*Zonglin Li,Jieji Ren,Shuangfan Zhou,Heng Guo,Jinnuo Zhang,Jiang Zhou,Boxin Shi,Zhanyu Ma,Guoying Gu*

Main category: cs.CV

TL;DR: Single-shot color photometric stereo using neural implicit representations for depth and BRDF modeling under mono-chromaticity assumption, validated with optical tactile sensor.


<details>
  <summary>Details</summary>
Motivation: Existing color photometric stereo methods assume ideal distant lighting and Lambertian reflectance, leaving practical near-light conditions and non-Lambertian surfaces underexplored. There's a need for single-shot reconstruction that works in more realistic conditions.

Method: Proposes a framework using neural implicit representations for depth and BRDF modeling under mono-chromaticity assumption (uniform chromaticity and homogeneous material). This alleviates ill-posedness of color photometric stereo. Also designs a compact optical tactile sensor for validation.

Result: Experiments on synthetic and real-world datasets demonstrate accurate and robust surface reconstruction from just one image.

Conclusion: The proposed method successfully extends color photometric stereo to practical near-light conditions and non-Lambertian surfaces using neural implicit representations and mono-chromaticity assumption, enabling single-shot surface reconstruction.

Abstract: Color photometric stereo enables single-shot surface reconstruction, extending conventional photometric stereo that requires multiple images of a static scene under varying illumination to dynamic scenarios. However, most existing approaches assume ideal distant lighting and Lambertian reflectance, leaving more practical near-light conditions and non-Lambertian surfaces underexplored. To overcome this limitation, we propose a framework that leverages neural implicit representations for depth and BRDF modeling under the assumption of mono-chromaticity (uniform chromaticity and homogeneous material), which alleviates the inherent ill-posedness of color photometric stereo and allows for detailed surface recovery from just one image. Furthermore, we design a compact optical tactile sensor to validate our approach. Experiments on both synthetic and real-world datasets demonstrate that our method achieves accurate and robust surface reconstruction.

</details>


### [122] [Exploiting Test-Time Augmentation in Federated Learning for Brain Tumor MRI Classification](https://arxiv.org/abs/2601.12671)
*Thamara Leandra de Deus Melo,Rodrigo Moreira,Larissa Ferreira Rodrigues Moreira,André Ricardo Backes*

Main category: cs.CV

TL;DR: Federated learning with CNNs for brain tumor MRI classification shows that test-time augmentation (TTA) provides significant improvements, with light preprocessing offering additional gains when computational resources allow.


<details>
  <summary>Details</summary>
Motivation: Brain tumor diagnosis is crucial but challenging due to lesion variability and image complexity. The paper aims to improve diagnostic efficiency using federated learning with CNNs on MRI images.

Method: Evaluated CNNs in federated learning setting, comparing models trained on original vs preprocessed MRI images (resizing, grayscale conversion, normalization, filtering, histogram equalization). Tested preprocessing alone and combined with test-time augmentation (TTA).

Result: Preprocessing alone yielded negligible gains, but combined with TTA delivered consistent, statistically significant improvements in federated MRI classification (p<0.001).

Conclusion: TTA should be the default inference strategy in FL-based medical imaging; when computational budget permits, pairing TTA with light preprocessing provides additional reliable gains.

Abstract: Efficient brain tumor diagnosis is crucial for early treatment; however, it is challenging because of lesion variability and image complexity. We evaluated convolutional neural networks (CNNs) in a federated learning (FL) setting, comparing models trained on original versus preprocessed MRI images (resizing, grayscale conversion, normalization, filtering, and histogram equalization). Preprocessing alone yielded negligible gains; combined with test-time augmentation (TTA), it delivered consistent, statistically significant improvements in federated MRI classification (p<0.001). In practice, TTA should be the default inference strategy in FL-based medical imaging; when the computational budget permits, pairing TTA with light preprocessing provides additional reliable gains.

</details>


### [123] [VILTA: A VLM-in-the-Loop Adversary for Enhancing Driving Policy Robustness](https://arxiv.org/abs/2601.12672)
*Qimao Chen,Fang Li,Shaoqing Xu,Zhiyi Lai,Zixun Xie,Yuechen Luo,Shengyin Jiang,Hanbing Li,Long Chen,Bing Wang,Yi Zhang,Zhi-Xin Yang*

Main category: cs.CV

TL;DR: VILTA is a novel framework that integrates Vision Language Models directly into closed-loop autonomous driving training to generate diverse, challenging long-tail scenarios by editing surrounding agents' trajectories, improving AD safety and robustness.


<details>
  <summary>Details</summary>
Motivation: Autonomous driving systems face the long-tail problem where rare but critical scenarios are underrepresented in real-world data. Existing solutions using rule-based heuristics, resampling, or two-stage VLM approaches have limited ability to generate diverse and novel challenges, constraining the generative potential of VLMs.

Method: VILTA integrates a VLM directly into the closed-loop training loop of AD agents. The VLM comprehends the dynamic driving environment and strategically generates challenging scenarios through direct, fine-grained editing of surrounding agents' future trajectories, creating a diverse curriculum of plausible yet challenging scenarios.

Result: The approach substantially enhances the safety and robustness of the resulting AD policy, particularly in its ability to navigate critical long-tail events, outperforming traditional methods that rely on separate downstream models for trajectory generation.

Conclusion: VILTA successfully overcomes limitations of existing approaches by fully leveraging VLM's generalization capabilities through direct trajectory editing in the training loop, creating more diverse and challenging scenarios for improved autonomous driving safety.

Abstract: The safe deployment of autonomous driving (AD) systems is fundamentally hindered by the long-tail problem, where rare yet critical driving scenarios are severely underrepresented in real-world data. Existing solutions including safety-critical scenario generation and closed-loop learning often rely on rule-based heuristics, resampling methods and generative models learned from offline datasets, limiting their ability to produce diverse and novel challenges. While recent works leverage Vision Language Models (VLMs) to produce scene descriptions that guide a separate, downstream model in generating hazardous trajectories for agents, such two-stage framework constrains the generative potential of VLMs, as the diversity of the final trajectories is ultimately limited by the generalization ceiling of the downstream algorithm. To overcome these limitations, we introduce VILTA (VLM-In-the-Loop Trajectory Adversary), a novel framework that integrates a VLM into the closed-loop training of AD agents. Unlike prior works, VILTA actively participates in the training loop by comprehending the dynamic driving environment and strategically generating challenging scenarios through direct, fine-grained editing of surrounding agents' future trajectories. This direct-editing approach fully leverages the VLM's powerful generalization capabilities to create a diverse curriculum of plausible yet challenging scenarios that extend beyond the scope of traditional methods. We demonstrate that our approach substantially enhances the safety and robustness of the resulting AD policy, particularly in its ability to navigate critical long-tail events.

</details>


### [124] [Fusion-Restoration Image Processing Algorithm to Improve the High-Temperature Deformation Measurement](https://arxiv.org/abs/2601.12682)
*Banglei Guan,Dongcai Tan,Jing Tao,Ang Su,Yang Shang,Qifeng Yu*

Main category: cs.CV

TL;DR: Proposed image processing methods to suppress thermal radiation and heat haze interference in high-temperature DIC deformation measurement, improving image quality and reducing measurement errors.


<details>
  <summary>Details</summary>
Motivation: Image degradation from thermal radiation and random errors from heat haze limit accuracy in high-temperature deformation measurement using Digital Image Correlation (DIC).

Method: 1) Multi-exposure image fusion with positive/negative channel decomposition for thermal radiation suppression; 2) FSIM-guided iterative optimization with grayscale averaging for heat haze reduction.

Result: Effective computation area increased from 26% to 50% for under-exposed and 32% to 40% for over-exposed images. Static thermal deformation errors reduced: ε_xx by 85.3%, ε_yy by 36.0%, γ_xy by 36.4%.

Conclusion: The proposed methods effectively suppress thermal radiation and heat haze interference, improve image quality, reduce deformation measurement errors, and have potential application value in thermal deformation measurement.

Abstract: In the deformation measurement of high-temperature structures, image degradation caused by thermal radiation and random errors introduced by heat haze restrict the accuracy and effectiveness of deformation measurement. To suppress thermal radiation and heat haze using fusion-restoration image processing methods, thereby improving the accuracy and effectiveness of DIC in the measurement of high-temperature deformation. For image degradation caused by thermal radiation, based on the image layered representation, the image is decomposed into positive and negative channels for parallel processing, and then optimized for quality by multi-exposure image fusion. To counteract the high-frequency, random errors introduced by heat haze, we adopt the FSIM as the objective function to guide the iterative optimization of model parameters, and the grayscale average algorithm is applied to equalize anomalous gray values, thereby reducing measurement error. The proposed multi-exposure image fusion algorithm effectively suppresses image degradation caused by complex illumination conditions, boosting the effective computation area from 26% to 50% for under-exposed images and from 32% to 40% for over-exposed images without degrading measurement accuracy in the experiment. Meanwhile, the image restoration combined with the grayscale average algorithm reduces static thermal deformation measurement errors. The error in ε_xx is reduced by 85.3%, while the errors in ε_yy and γ_xy are reduced by 36.0% and 36.4%, respectively. We present image processing methods to suppress the interference of thermal radiation and heat haze in high-temperature deformation measurement using DIC. The experimental results verify that the proposed method can effectively improve image quality, reduce deformation measurement errors, and has potential application value in thermal deformation measurement.

</details>


### [125] [GaussianTrimmer: Online Trimming Boundaries for 3DGS Segmentation](https://arxiv.org/abs/2601.12683)
*Liwei Liao,Ronggang Wang*

Main category: cs.CV

TL;DR: GaussianTrimmer is a plug-and-play post-processing method that improves 3D Gaussian segmentation by trimming jagged boundaries through virtual camera-based primitive-level refinement.


<details>
  <summary>Details</summary>
Motivation: Existing 3D Gaussian segmentation methods suffer from jagged object boundaries due to large-sized Gaussians that span foreground and background, caused by the wide scale variation of 3D Gaussians.

Method: 1) Generate uniformly distributed virtual cameras with good coverage; 2) Trim Gaussian primitives at the primitive level based on 2D segmentation results from these virtual cameras.

Result: Extensive experiments show GaussianTrimmer effectively improves segmentation quality of existing 3D Gaussian segmentation methods as a plug-and-play solution.

Conclusion: GaussianTrimmer provides an efficient, online boundary trimming method that addresses boundary quality issues in 3D Gaussian segmentation without requiring modifications to existing segmentation approaches.

Abstract: With the widespread application of 3D Gaussians in 3D scene representation, 3D scene segmentation methods based on 3D Gaussians have also gradually emerged. However, existing 3D Gaussian segmentation methods basically segment on the basis of Gaussian primitives. Due to the large variation range of the scale of 3D Gaussians, large-sized Gaussians that often span the foreground and background lead to jagged boundaries of segmented objects. To this end, we propose an online boundary trimming method, GaussianTrimmer, which is an efficient and plug-and-play post-processing method capable of trimming coarse boundaries for existing 3D Gaussian segmentation methods. Our method consists of two core steps: 1. Generating uniformly and well-covered virtual cameras; 2. Trimming Gaussian at the primitive level based on 2D segmentation results on virtual cameras. Extensive quantitative and qualitative experiments demonstrate that our method can improve the segmentation quality of existing 3D Gaussian segmentation methods as a plug-and-play method.

</details>


### [126] [Fusing in 3D: Free-Viewpoint Fusion Rendering with a 3D Infrared-Visible Scene Representation](https://arxiv.org/abs/2601.12697)
*Chao Yang,Deshui Miao,Chao Tian,Guoqing Zhu,Yameng Gu,Zhenyu He*

Main category: cs.CV

TL;DR: Proposes IVGF framework using 3D Gaussian Splatting for infrared-visible image fusion, addressing limitations of 2D methods by reconstructing scene geometry and enabling direct rendering of fused images.


<details>
  <summary>Details</summary>
Motivation: Existing 2D infrared-visible fusion methods focus on fixed camera viewpoints, lacking comprehensive scene understanding and losing critical information about complex scenarios.

Method: IVGF framework reconstructs scene geometry from multimodal 2D inputs using 3D Gaussian Splatting. Includes cross-modal adjustment (CMA) module to modulate Gaussian opacity for cross-modal conflicts, and fusion loss to preserve distinctive features from both modalities.

Result: Comprehensive qualitative and quantitative experiments demonstrate the effectiveness of the proposed method in producing high-quality fused images.

Conclusion: The IVGF framework successfully addresses limitations of 2D fusion methods by leveraging 3D scene reconstruction, enabling better preservation of critical information from both infrared and visible modalities.

Abstract: Infrared-visible image fusion aims to integrate infrared and visible information into a single fused image. Existing 2D fusion methods focus on fusing images from fixed camera viewpoints, neglecting a comprehensive understanding of complex scenarios, which results in the loss of critical information about the scene. To address this limitation, we propose a novel Infrared-Visible Gaussian Fusion (IVGF) framework, which reconstructs scene geometry from multimodal 2D inputs and enables direct rendering of fused images. Specifically, we propose a cross-modal adjustment (CMA) module that modulates the opacity of Gaussians to solve the problem of cross-modal conflicts. Moreover, to preserve the distinctive features from both modalities, we introduce a fusion loss that guides the optimization of CMA, thus ensuring that the fused image retains the critical characteristics of each modality. Comprehensive qualitative and quantitative experiments demonstrate the effectiveness of the proposed method.

</details>


### [127] [P2L-CA: An Effective Parameter Tuning Framework for Rehearsal-Free Multi-Label Class-Incremental Learning](https://arxiv.org/abs/2601.12714)
*Songlin Dong,Jiangyang Li,Chenhao Ding,Zhiheng Ma,Haoyu Luo,Yuhang He,Yihong Gong*

Main category: cs.CV

TL;DR: P2L-CA is a parameter-efficient framework for multi-label class-incremental learning that uses prompt-to-label modules and continuous adapters to reduce computational costs while improving performance without memory buffers.


<details>
  <summary>Details</summary>
Motivation: Existing multi-label class-incremental learning approaches suffer from high computational costs (full-parameter fine-tuning), substantial storage overhead (memory buffers), and inadequate handling of feature confusion and domain discrepancies.

Method: P2L-CA integrates two modules: 1) Prompt-to-Label (P2L) module uses class-specific prompts to disentangle multi-label representations and incorporate linguistic priors for semantic-visual alignment, and 2) Continuous Adapter (CA) module employs lightweight adapters to mitigate domain gaps between pre-trained models and downstream tasks.

Result: Extensive experiments on MS-COCO and PASCAL VOC show P2L-CA achieves substantial improvements over state-of-the-art methods, demonstrates strong generalization in CIL scenarios, requires minimal trainable parameters, and eliminates the need for memory buffers.

Conclusion: P2L-CA provides an effective parameter-efficient solution for multi-label class-incremental learning that addresses computational and storage limitations while maintaining strong performance and generalization capabilities.

Abstract: Multi-label Class-Incremental Learning aims to continuously recognize novel categories in complex scenes where multiple objects co-occur. However, existing approaches often incur high computational costs due to full-parameter fine-tuning and substantial storage overhead from memory buffers, or they struggle to address feature confusion and domain discrepancies adequately. To overcome these limitations, we introduce P2L-CA, a parameter-efficient framework that integrates a Prompt-to-Label module with a Continuous Adapter module. The P2L module leverages class-specific prompts to disentangle multi-label representations while incorporating linguistic priors to enforce stable semantic-visual alignment. Meanwhile, the CA module employs lightweight adapters to mitigate domain gaps between pre-trained models and downstream tasks, thereby enhancing model plasticity. Extensive experiments across standard and challenging MLCIL settings on MS-COCO and PASCAL VOC show that P2L-CA not only achieves substantial improvements over state-of-the-art methods but also demonstrates strong generalization in CIL scenarios, all while requiring minimal trainable parameters and eliminating the need for memory buffers.

</details>


### [128] [RSOD: Reliability-Guided Sonar Image Object Detection with Extremely Limited Labels](https://arxiv.org/abs/2601.12715)
*Chengzhou Li,Ping Guo,Guanchen Meng,Qi Jia,Jinyuan Liu,Zhu Liu,Xiaokang Liu,Yu Liu,Zhongxuan Luo,Xin Fan*

Main category: cs.CV

TL;DR: RSOD is a teacher-student framework for sonar object detection that uses reliability scoring, object mixed pseudo-labeling, and adaptive constraints to achieve competitive performance with only 5% labeled data.


<details>
  <summary>Details</summary>
Motivation: Sonar images have fewer texture details and more noise than natural images, making them difficult for non-experts to annotate precisely. This creates a need for effective object detection methods that can work with extremely limited labeled data in underwater detection systems.

Method: Proposes RSOD teacher-student framework: 1) Calculates reliability scores by assessing teacher prediction consistency across different views, 2) Introduces object mixed pseudo-label method to address labeled data shortage, 3) Implements reliability-guided adaptive constraint to optimize student performance by leveraging unlabeled data.

Result: On UATD dataset, RSOD using only 5% labeled data achieves results competitive with baseline algorithm trained on 100% labeled data. Also collected a new dataset to provide more valuable sonar research data.

Conclusion: RSOD effectively addresses sonar image object detection with limited labels through a novel teacher-student framework with reliability scoring and pseudo-label strategies, demonstrating strong performance with minimal labeled data while contributing a new dataset to the field.

Abstract: Object detection in sonar images is a key technology in underwater detection systems. Compared to natural images, sonar images contain fewer texture details and are more susceptible to noise, making it difficult for non-experts to distinguish subtle differences between classes. This leads to their inability to provide precise annotation data for sonar images. Therefore, designing effective object detection methods for sonar images with extremely limited labels is particularly important. To address this, we propose a teacher-student framework called RSOD, which aims to fully learn the characteristics of sonar images and develop a pseudo-label strategy suitable for these images to mitigate the impact of limited labels. First, RSOD calculates a reliability score by assessing the consistency of the teacher's predictions across different views. To leverage this score, we introduce an object mixed pseudo-label method to tackle the shortage of labeled data in sonar images. Finally, we optimize the performance of the student by implementing a reliability-guided adaptive constraint. By taking full advantage of unlabeled data, the student can perform well even in situations with extremely limited labels. Notably, on the UATD dataset, our method, using only 5% of labeled data, achieves results that can compete against those of our baseline algorithm trained on 100% labeled data. We also collected a new dataset to provide more valuable data for research in the field of sonar.

</details>


### [129] [S2DiT: Sandwich Diffusion Transformer for Mobile Streaming Video Generation](https://arxiv.org/abs/2601.12719)
*Lin Zhao,Yushu Wu,Aleksei Lebedev,Dishani Lahiri,Meng Dong,Arpit Sahni,Michael Vasilkovsky,Hao Chen,Ju Hu,Aliaksandr Siarohin,Sergey Tulyakov,Yanzhi Wang,Anil Kag,Yanyu Li*

Main category: cs.CV

TL;DR: S2DiT is a Streaming Sandwich Diffusion Transformer that enables efficient, high-fidelity video generation on mobile devices by combining novel attention mechanisms, sandwich architecture design, and distillation from large teacher models.


<details>
  <summary>Details</summary>
Motivation: Current Diffusion Transformers (DiTs) produce high-quality video generation but are computationally expensive, making real-time or on-device generation impractical for mobile hardware.

Method: S2DiT uses: 1) Novel efficient attentions (LinConv Hybrid Attention and Stride Self-Attention), 2) Sandwich design discovered via budget-aware dynamic programming search, and 3) 2-in-1 distillation framework to transfer knowledge from large teacher models to compact few-step models.

Result: S2DiT achieves video quality comparable to state-of-the-art server models while streaming at over 10 FPS on an iPhone, making real-time mobile video generation feasible.

Conclusion: S2DiT demonstrates that efficient video generation on mobile hardware is achievable through careful architectural design and distillation techniques, bridging the gap between server-quality video generation and mobile deployment.

Abstract: Diffusion Transformers (DiTs) have recently improved video generation quality. However, their heavy computational cost makes real-time or on-device generation infeasible. In this work, we introduce S2DiT, a Streaming Sandwich Diffusion Transformer designed for efficient, high-fidelity, and streaming video generation on mobile hardware. S2DiT generates more tokens but maintains efficiency with novel efficient attentions: a mixture of LinConv Hybrid Attention (LCHA) and Stride Self-Attention (SSA). Based on this, we uncover the sandwich design via a budget-aware dynamic programming search, achieving superior quality and efficiency. We further propose a 2-in-1 distillation framework that transfers the capacity of large teacher models (e.g., Wan 2.2-14B) to the compact few-step sandwich model. Together, S2DiT achieves quality on par with state-of-the-art server video models, while streaming at over 10 FPS on an iPhone.

</details>


### [130] [DC-VLAQ: Query-Residual Aggregation for Robust Visual Place Recognition](https://arxiv.org/abs/2601.12729)
*Hanyu Zhu,Zhihao Zhan,Yuhang Ming,Liang Li,Dibo Hou,Javier Civera,Wanzeng Kong*

Main category: cs.CV

TL;DR: DC-VLAQ is a visual place recognition framework that fuses complementary visual foundation models (DINOv2 and CLIP) and uses a novel query-residual global aggregation scheme to create robust representations that handle viewpoint changes, illumination variations, and domain shifts.


<details>
  <summary>Details</summary>
Motivation: Existing VPR methods typically use single visual foundation models, missing complementary cues from different VFMs. However, fusing multiple VFMs disrupts token distributions, challenging the stability of existing query-based global aggregation methods.

Method: Two key components: 1) Residual-guided complementary fusion that anchors representations in DINOv2 space while injecting CLIP semantics via learned residual correction; 2) Vector of Local Aggregated Queries (VLAQ) - a query-residual global aggregation scheme that encodes local tokens by their residual responses to learnable queries for improved stability and fine-grained discriminative preservation.

Result: Extensive experiments on standard VPR benchmarks (Pitts30k, Tokyo24/7, MSLS, Nordland, SPED, AmsterTime) show DC-VLAQ consistently outperforms strong baselines and achieves state-of-the-art performance, especially under challenging domain shifts and long-term appearance changes.

Conclusion: DC-VLAQ successfully addresses the challenges of fusing complementary VFMs while maintaining aggregation stability, resulting in robust global representations for visual place recognition that excel under various challenging conditions.

Abstract: One of the central challenges in visual place recognition (VPR) is learning a robust global representation that remains discriminative under large viewpoint changes, illumination variations, and severe domain shifts. While visual foundation models (VFMs) provide strong local features, most existing methods rely on a single model, overlooking the complementary cues offered by different VFMs. However, exploiting such complementary information inevitably alters token distributions, which challenges the stability of existing query-based global aggregation schemes. To address these challenges, we propose DC-VLAQ, a representation-centric framework that integrates the fusion of complementary VFMs and robust global aggregation. Specifically, we first introduce a lightweight residual-guided complementary fusion that anchors representations in the DINOv2 feature space while injecting complementary semantics from CLIP through a learned residual correction. In addition, we propose the Vector of Local Aggregated Queries (VLAQ), a query--residual global aggregation scheme that encodes local tokens by their residual responses to learnable queries, resulting in improved stability and the preservation of fine-grained discriminative cues. Extensive experiments on standard VPR benchmarks, including Pitts30k, Tokyo24/7, MSLS, Nordland, SPED, and AmsterTime, demonstrate that DC-VLAQ consistently outperforms strong baselines and achieves state-of-the-art performance, particularly under challenging domain shifts and long-term appearance changes.

</details>


### [131] [KaoLRM: Repurposing Pre-trained Large Reconstruction Models for Parametric 3D Face Reconstruction](https://arxiv.org/abs/2601.12736)
*Qingtian Zhu,Xu Cao,Zhixiang Wang,Yinqiang Zheng,Takafumi Taketomi*

Main category: cs.CV

TL;DR: KaoLRM adapts Large Reconstruction Model's 3D prior for parametric face reconstruction using FLAME-based 2D Gaussian Splatting, achieving better cross-view consistency than existing 3DMM methods.


<details>
  <summary>Details</summary>
Motivation: Existing 3D Morphable Model (3DMM) regressors for facial reconstruction often show poor consistency across different viewpoints, limiting their robustness in real-world applications.

Method: Leverages pre-trained 3D prior from Large Reconstruction Model (LRM) and integrates FLAME-based 2D Gaussian Splatting into LRM's rendering pipeline. Projects LRM's triplane features into FLAME parameter space for geometry recovery, and models appearance via 2D Gaussian primitives coupled to FLAME mesh.

Result: Achieves superior reconstruction accuracy and cross-view consistency on both controlled and in-the-wild benchmarks. Outperforms existing methods that remain sensitive to viewpoint variations.

Conclusion: KaoLRM successfully repurposes LRM's learned 3D prior for parametric face reconstruction, enabling accurate and robust reconstructions even under self-occlusions and diverse viewpoints through its FLAME-aware architecture.

Abstract: We propose KaoLRM to re-target the learned prior of the Large Reconstruction Model (LRM) for parametric 3D face reconstruction from single-view images. Parametric 3D Morphable Models (3DMMs) have been widely used for facial reconstruction due to their compact and interpretable parameterization, yet existing 3DMM regressors often exhibit poor consistency across varying viewpoints. To address this, we harness the pre-trained 3D prior of LRM and incorporate FLAME-based 2D Gaussian Splatting into LRM's rendering pipeline. Specifically, KaoLRM projects LRM's pre-trained triplane features into the FLAME parameter space to recover geometry, and models appearance via 2D Gaussian primitives that are tightly coupled to the FLAME mesh. The rich prior enables the FLAME regressor to be aware of the 3D structure, leading to accurate and robust reconstructions under self-occlusions and diverse viewpoints. Experiments on both controlled and in-the-wild benchmarks demonstrate that KaoLRM achieves superior reconstruction accuracy and cross-view consistency, while existing methods remain sensitive to viewpoint variations. The code is released at https://github.com/CyberAgentAILab/KaoLRM.

</details>


### [132] [SSPFormer: Self-Supervised Pretrained Transformer for MRI Images](https://arxiv.org/abs/2601.12747)
*Jingkai Li,Xiaoze Tian,Yuhang Shen,Jia Wang,Dianjie Lu,Guijuan Zhang,Zhuoran Zheng*

Main category: cs.CV

TL;DR: SSPFormer is a self-supervised pretrained transformer for MRI that uses inverse frequency projection masking and frequency-weighted FFT noise enhancement to learn domain-specific, artifact-robust features from unlabeled raw scans.


<details>
  <summary>Details</summary>
Motivation: Direct transfer of pre-trained transformers to MRI faces two challenges: inability to adapt to medical anatomical specificity, and limitations from privacy/scarcity of medical data.

Method: Proposes SSPFormer with two key strategies: 1) Inverse frequency projection masking that prioritizes reconstruction of high-frequency anatomical regions for structure-aware learning, and 2) Frequency-weighted FFT noise enhancement that injects physiologically realistic noise into Fourier domain for artifact robustness.

Result: Achieves state-of-the-art performance on segmentation, super-resolution, and denoising tasks, verifying its ability to capture fine-grained MRI image fidelity and adapt to clinical requirements.

Conclusion: SSPFormer effectively learns domain-invariant and artifact-robust features directly from raw MRI scans, overcoming domain gap and data scarcity challenges in medical imaging.

Abstract: The pre-trained transformer demonstrates remarkable generalization ability in natural image processing. However, directly transferring it to magnetic resonance images faces two key challenges: the inability to adapt to the specificity of medical anatomical structures and the limitations brought about by the privacy and scarcity of medical data. To address these issues, this paper proposes a Self-Supervised Pretrained Transformer (SSPFormer) for MRI images, which effectively learns domain-specific feature representations of medical images by leveraging unlabeled raw imaging data. To tackle the domain gap and data scarcity, we introduce inverse frequency projection masking, which prioritizes the reconstruction of high-frequency anatomical regions to enforce structure-aware representation learning. Simultaneously, to enhance robustness against real-world MRI artifacts, we employ frequency-weighted FFT noise enhancement that injects physiologically realistic noise into the Fourier domain. Together, these strategies enable the model to learn domain-invariant and artifact-robust features directly from raw scans. Through extensive experiments on segmentation, super-resolution, and denoising tasks, the proposed SSPFormer achieves state-of-the-art performance, fully verifying its ability to capture fine-grained MRI image fidelity and adapt to clinical application requirements.

</details>


### [133] [Moaw: Unleashing Motion Awareness for Video Diffusion Models](https://arxiv.org/abs/2601.12761)
*Tianqi Zhang,Ziyi Wang,Wenzhao Zheng,Weiliang Chen,Yuanhui Huang,Zhengyang Huang,Jie Zhou,Jiwen Lu*

Main category: cs.CV

TL;DR: Moaw is a framework that repurposes video diffusion models for motion transfer by training them for motion perception and injecting learned motion features into video generation models.


<details>
  <summary>Details</summary>
Motivation: Video diffusion models naturally capture correspondences across frames, suggesting they have inherent tracking capabilities. The authors investigate whether supervised training can better harness these capabilities for motion understanding and transfer.

Method: 1) Train a diffusion model for motion perception (shifting from image-to-video generation to video-to-dense-tracking), 2) Construct a motion-labeled dataset to identify features encoding strongest motion information, 3) Inject these features into a structurally identical video generation model without additional adapters due to network homogeneity.

Result: The framework enables motion transfer in a zero-shot manner by leveraging the inherent correspondences learned by video diffusion models and adapting motion-aware features between structurally similar networks.

Conclusion: Moaw provides a new paradigm for bridging generative modeling and motion understanding, paving the way for more unified and controllable video learning frameworks that can leverage pre-trained video diffusion models for motion-related tasks.

Abstract: Video diffusion models, trained on large-scale datasets, naturally capture correspondences of shared features across frames. Recent works have exploited this property for tasks such as optical flow prediction and tracking in a zero-shot setting. Motivated by these findings, we investigate whether supervised training can more fully harness the tracking capability of video diffusion models. To this end, we propose Moaw, a framework that unleashes motion awareness for video diffusion models and leverages it to facilitate motion transfer. Specifically, we train a diffusion model for motion perception, shifting its modality from image-to-video generation to video-to-dense-tracking. We then construct a motion-labeled dataset to identify features that encode the strongest motion information, and inject them into a structurally identical video generation model. Owing to the homogeneity between the two networks, these features can be naturally adapted in a zero-shot manner, enabling motion transfer without additional adapters. Our work provides a new paradigm for bridging generative modeling and motion understanding, paving the way for more unified and controllable video learning frameworks.

</details>


### [134] [Towards Unbiased Source-Free Object Detection via Vision Foundation Models](https://arxiv.org/abs/2601.12765)
*Zhi Cai,Yingjie Gao,Yanan Zhang,Xinzhu Ma,Di Huang*

Main category: cs.CV

TL;DR: DSOD is a novel VFM-assisted source-free object detection framework that mitigates source bias through unified feature injection and semantic-aware feature regularization, achieving SOTA performance on multiple cross-domain benchmarks.


<details>
  <summary>Details</summary>
Motivation: Existing Source-Free Object Detection (SFOD) methods suffer from Source Bias problem where adapted models remain skewed towards source domain, leading to poor generalization and error accumulation during self-training.

Method: Proposes Debiased Source-free Object Detection (DSOD) with: 1) Unified Feature Injection (UFI) module integrating VFM features via Simple-Scale Extension (SSE) and Domain-aware Adaptive Weighting (DAAW); 2) Semantic-aware Feature Regularization (SAFR) to prevent overfitting; 3) VFM-free variant DSOD-distill using Dual-Teacher distillation for computation-restricted scenarios.

Result: Outperforms state-of-the-art SFOD methods: 48.1% AP on Normal-to-Foggy weather adaptation, 39.3% AP on Cross-scene adaptation, and 61.4% AP on Synthetic-to-Real adaptation across multiple benchmarks.

Conclusion: DSOD effectively mitigates source bias in SFOD by leveraging VFM features and regularization techniques, demonstrating superior performance across diverse cross-domain adaptation scenarios while offering a computationally efficient variant.

Abstract: Source-Free Object Detection (SFOD) has garnered much attention in recent years by eliminating the need of source-domain data in cross-domain tasks, but existing SFOD methods suffer from the Source Bias problem, i.e. the adapted model remains skewed towards the source domain, leading to poor generalization and error accumulation during self-training. To overcome this challenge, we propose Debiased Source-free Object Detection (DSOD), a novel VFM-assisted SFOD framework that can effectively mitigate source bias with the help of powerful VFMs. Specifically, we propose Unified Feature Injection (UFI) module that integrates VFM features into the CNN backbone through Simple-Scale Extension (SSE) and Domain-aware Adaptive Weighting (DAAW). Then, we propose Semantic-aware Feature Regularization (SAFR) that constrains feature learning to prevent overfitting to source domain characteristics. Furthermore, we propose a VFM-free variant, termed DSOD-distill for computation-restricted scenarios through a novel Dual-Teacher distillation scheme. Extensive experiments on multiple benchmarks demonstrate that DSOD outperforms state-of-the-art SFOD methods, achieving 48.1% AP on Normal-to-Foggy weather adaptation, 39.3% AP on Cross-scene adaptation, and 61.4% AP on Synthetic-to-Real adaptation.

</details>


### [135] [Spatial-VLN: Zero-Shot Vision-and-Language Navigation With Explicit Spatial Perception and Exploration](https://arxiv.org/abs/2601.12766)
*Lu Yue,Yue Fan,Shiwei Lian,Yu Zhao,Jiaxin Yu,Liang Xie,Feitian Zhang*

Main category: cs.CV

TL;DR: Spatial-VLN: A perception-guided exploration framework for zero-shot Vision-and-Language Navigation that addresses spatial perception bottlenecks in complex continuous environments using specialized perception modules and multi-expert reasoning.


<details>
  <summary>Details</summary>
Motivation: Zero-shot VLN agents using LLMs have good generalization but suffer from insufficient spatial perception, particularly in complex continuous environments with challenges like door interaction, multi-room navigation, and ambiguous instruction execution where existing methods have high failure rates.

Method: Two main modules: 1) Spatial Perception Enhancement (SPE) integrates panoramic filtering with specialized door and region experts for spatially coherent, cross-view consistent perceptual representations. 2) Explored Multi-expert Reasoning (EMR) uses parallel LLM experts for waypoint-level semantics and region-level spatial transitions, with a query-and-explore mechanism to resolve perceptual ambiguities when expert predictions disagree.

Result: Achieves state-of-the-art performance on VLN-CE using only low-cost LLMs. Introduces value-based waypoint sampling strategy that effectively bridges Sim2Real gap, with extensive real-world evaluations confirming superior generalization and robustness in complex environments.

Conclusion: Spatial-VLN framework successfully addresses key spatial perception bottlenecks in zero-shot VLN, demonstrating both superior performance in simulation and effective real-world applicability through innovative perception enhancement and exploration mechanisms.

Abstract: Zero-shot Vision-and-Language Navigation (VLN) agents leveraging Large Language Models (LLMs) excel in generalization but suffer from insufficient spatial perception. Focusing on complex continuous environments, we categorize key perceptual bottlenecks into three spatial challenges: door interaction,multi-room navigation, and ambiguous instruction execution, where existing methods consistently suffer high failure rates. We present Spatial-VLN, a perception-guided exploration framework designed to overcome these challenges. The framework consists of two main modules. The Spatial Perception Enhancement (SPE) module integrates panoramic filtering with specialized door and region experts to produce spatially coherent, cross-view consistent perceptual representations. Building on this foundation, our Explored Multi-expert Reasoning (EMR) module uses parallel LLM experts to address waypoint-level semantics and region-level spatial transitions. When discrepancies arise between expert predictions, a query-and-explore mechanism is activated, prompting the agent to actively probe critical areas and resolve perceptual ambiguities. Experiments on VLN-CE demonstrate that Spatial VLN achieves state-of-the-art performance using only low-cost LLMs. Furthermore, to validate real-world applicability, we introduce a value-based waypoint sampling strategy that effectively bridges the Sim2Real gap. Extensive real-world evaluations confirm that our framework delivers superior generalization and robustness in complex environments. Our codes and videos are available at https://yueluhhxx.github.io/Spatial-VLN-web/.

</details>


### [136] [Delving Deeper: Hierarchical Visual Perception for Robust Video-Text Retrieval](https://arxiv.org/abs/2601.12768)
*Zequn Xie,Boyun Zhang,Yuxiao Lin,Tao Jin*

Main category: cs.CV

TL;DR: HVP-Net improves video-text retrieval by extracting hierarchical features from multiple vision encoder layers to reduce video redundancy and enhance semantic matching.


<details>
  <summary>Details</summary>
Motivation: Current video-text retrieval methods using pre-trained models like CLIP suffer from video redundancy and reliance on coarse final-layer features, limiting matching accuracy.

Method: Introduces HVP-Net (Hierarchical Visual Perception Network) that extracts and refines features from multiple intermediate layers of a vision encoder, progressively distilling salient visual concepts from raw patch-tokens at different semantic levels.

Result: Achieves new state-of-the-art performance on challenging benchmarks including MSRVTT, DiDeMo, and ActivityNet.

Conclusion: Validates the effectiveness of exploiting hierarchical features for advancing video-text retrieval.

Abstract: Video-text retrieval (VTR) aims to locate relevant videos using natural language queries. Current methods, often based on pre-trained models like CLIP, are hindered by video's inherent redundancy and their reliance on coarse, final-layer features, limiting matching accuracy. To address this, we introduce the HVP-Net (Hierarchical Visual Perception Network), a framework that mines richer video semantics by extracting and refining features from multiple intermediate layers of a vision encoder. Our approach progressively distills salient visual concepts from raw patch-tokens at different semantic levels, mitigating redundancy while preserving crucial details for alignment. This results in a more robust video representation, leading to new state-of-the-art performance on challenging benchmarks including MSRVTT, DiDeMo, and ActivityNet. Our work validates the effectiveness of exploiting hierarchical features for advancing video-text retrieval. Our codes are available at https://github.com/boyun-zhang/HVP-Net.

</details>


### [137] [Generalizable and Animatable 3D Full-Head Gaussian Avatar from a Single Image](https://arxiv.org/abs/2601.12770)
*Shuling Zhao,Dan Xu*

Main category: cs.CV

TL;DR: One-shot 3D full-head animatable avatar reconstruction from a single image with real-time animation and 360° rendering using Gaussian primitives and 3D GAN priors.


<details>
  <summary>Details</summary>
Motivation: Existing methods fail under large camera pose variations, compromising realism of 3D avatars. Need for efficient one-shot reconstruction that enables real-time animation and full 360° views.

Method: 1) Model 3D head avatars with Gaussian primitives embedded on parametric face model UV space; 2) Leverage pretrained 3D GAN for global full-head feature extraction and multi-view supervision; 3) Fuse local input image features with global textures using UV space symmetry.

Result: Achieves high-quality 3D full-head modeling with real-time animation, improving realism of 3D talking avatars. Works effectively under large pose variations.

Conclusion: Proposed framework successfully tackles one-shot 3D full-head animatable avatar reconstruction, enabling real-time animation and 360° rendering while maintaining high fidelity to input images.

Abstract: Building 3D animatable head avatars from a single image is an important yet challenging problem. Existing methods generally collapse under large camera pose variations, compromising the realism of 3D avatars. In this work, we propose a new framework to tackle the novel setting of one-shot 3D full-head animatable avatar reconstruction in a single feed-forward pass, enabling real-time animation and simultaneous 360$^\circ$ rendering views. To facilitate efficient animation control, we model 3D head avatars with Gaussian primitives embedded on the surface of a parametric face model within the UV space. To obtain knowledge of full-head geometry and textures, we leverage rich 3D full-head priors within a pretrained 3D generative adversarial network (GAN) for global full-head feature extraction and multi-view supervision. To increase the fidelity of the 3D reconstruction of the input image, we take advantage of the symmetric nature of the UV space and human faces to fuse local fine-grained input image features with the global full-head textures. Extensive experiments demonstrate the effectiveness of our method, achieving high-quality 3D full-head modeling as well as real-time animation, thereby improving the realism of 3D talking avatars.

</details>


### [138] [Open Vocabulary Panoptic Segmentation With Retrieval Augmentation](https://arxiv.org/abs/2601.12779)
*Nafis Sadeq,Qingfeng Liu,Mostafa El-Khamy*

Main category: cs.CV

TL;DR: RetCLIP: A retrieval-augmented panoptic segmentation method that improves open-vocabulary performance on unseen classes by combining CLIP-based classification with database retrieval of masked segment features.


<details>
  <summary>Details</summary>
Motivation: Traditional panoptic segmentation systems trained on specific datasets don't generalize well to unseen classes. Open Vocabulary Panoptic Segmentation aims to segment arbitrary classes, but current methods struggle with generalization beyond training data.

Method: Build a masked segment feature database using paired image-text data. At inference, use masked segment features from input images as queries to retrieve similar features and associated class labels from the database. Combine retrieval-based classification scores with CLIP-based scores for final output.

Result: When trained on COCO and tested on ADE20k: 30.9 PQ, 19.3 mAP, 44.0 mIoU, achieving +4.5 PQ, +2.5 mAP, +10.0 mIoU absolute improvement over baseline (FC-CLIP).

Conclusion: RetCLIP effectively improves open-vocabulary panoptic segmentation performance on unseen classes by augmenting CLIP with retrieval mechanisms, demonstrating significant gains over state-of-the-art methods.

Abstract: Given an input image and set of class names, panoptic segmentation aims to label each pixel in an image with class labels and instance labels. In comparison, Open Vocabulary Panoptic Segmentation aims to facilitate the segmentation of arbitrary classes according to user input. The challenge is that a panoptic segmentation system trained on a particular dataset typically does not generalize well to unseen classes beyond the training data. In this work, we propose RetCLIP, a retrieval-augmented panoptic segmentation method that improves the performance of unseen classes. In particular, we construct a masked segment feature database using paired image-text data. At inference time, we use masked segment features from the input image as query keys to retrieve similar features and associated class labels from the database. Classification scores for the masked segment are assigned based on the similarity between query features and retrieved features. The retrieval-based classification scores are combined with CLIP-based scores to produce the final output. We incorporate our solution with a previous SOTA method (FC-CLIP). When trained on COCO, the proposed method demonstrates 30.9 PQ, 19.3 mAP, 44.0 mIoU on the ADE20k dataset, achieving +4.5 PQ, +2.5 mAP, +10.0 mIoU absolute improvement over the baseline.

</details>


### [139] [SKANet: A Cognitive Dual-Stream Framework with Adaptive Modality Fusion for Robust Compound GNSS Interference Classification](https://arxiv.org/abs/2601.12791)
*Zhihan Zeng,Yang Zhao,Kaihe Wang,Dusit Niyato,Hongyuan Shu,Junchu Zhao,Yanjun Huang,Yue Xiu,Zhongpei Zhang,Ning Wei*

Main category: cs.CV

TL;DR: SKANet: A dual-stream deep learning framework using selective kernels and asymmetric convolutions to dynamically adapt receptive fields for classifying compound GNSS jamming interference, achieving 96.99% accuracy.


<details>
  <summary>Details</summary>
Motivation: GNSS faces growing threats from sophisticated jamming interference. While DL works for basic interference, classifying compound interference is difficult due to superposition of diverse jamming sources. Single-domain approaches suffer from performance degradation because transient burst signals and continuous global signals require conflicting feature extraction scales.

Method: Proposes SKANet (Selective Kernel and Asymmetric convolution Network), a cognitive deep learning framework with dual-stream architecture integrating Time-Frequency Images (TFIs) and Power Spectral Density (PSD). Uses Multi-Branch Selective Kernel (SK) module combined with Asymmetric Convolution Blocks (ACBs) to dynamically adjust receptive fields. Also integrates Squeeze-and-Excitation (SE) mechanism at fusion stage to adaptively recalibrate heterogeneous feature contributions.

Result: Evaluated on dataset of 405,000 samples, SKANet achieves overall accuracy of 96.99%, showing superior robustness for compound jamming classification, particularly under low Jamming-to-Noise Ratio (JNR) regimes.

Conclusion: SKANet effectively addresses the challenge of compound interference classification by dynamically adapting receptive fields to capture both micro-scale transient features and macro-scale spectral trends, outperforming conventional approaches.

Abstract: As the electromagnetic environment becomes increasingly complex, Global Navigation Satellite Systems (GNSS) face growing threats from sophisticated jamming interference. Although Deep Learning (DL) effectively identifies basic interference, classifying compound interference remains difficult due to the superposition of diverse jamming sources. Existing single-domain approaches often suffer from performance degradation because transient burst signals and continuous global signals require conflicting feature extraction scales. We propose the Selective Kernel and Asymmetric convolution Network(SKANet), a cognitive deep learning framework built upon a dual-stream architecture that integrates Time-Frequency Images (TFIs) and Power Spectral Density (PSD). Distinct from conventional fusion methods that rely on static receptive fields, the proposed architecture incorporates a Multi-Branch Selective Kernel (SK) module combined with Asymmetric Convolution Blocks (ACBs). This mechanism enables the network to dynamically adjust its receptive fields, acting as an adaptive filter that simultaneously captures micro-scale transient features and macro-scale spectral trends within entangled compound signals. To complement this spatial-temporal adaptation, a Squeeze-and-Excitation (SE) mechanism is integrated at the fusion stage to adaptively recalibrate the contribution of heterogeneous features from each modality. Evaluations on a dataset of 405,000 samples demonstrate that SKANet achieves an overall accuracy of 96.99\%, exhibiting superior robustness for compound jamming classification, particularly under low Jamming-to-Noise Ratio (JNR) regimes.

</details>


### [140] [Combating Noisy Labels through Fostering Self- and Neighbor-Consistency](https://arxiv.org/abs/2601.12795)
*Zeren Sun,Yazhou Yao,Tongliang Liu,Zechao Li,Fumin Shen,Jinhui Tang*

Main category: cs.CV

TL;DR: Jo-SNC: A noise-robust method that jointly performs sample selection and model regularization based on self- and neighbor-consistency to handle imbalanced label noise and out-of-distribution noisy data.


<details>
  <summary>Details</summary>
Motivation: Deep networks are vulnerable to label noise due to memorization effect. Existing methods often neglect imbalances in label noise across mini-batches and insufficiently address out-of-distribution noisy data, which limits their effectiveness in real-world scenarios with pervasive label noise.

Method: Proposes Jo-SNC with three key components: 1) Uses Jensen-Shannon divergence with nearest neighbors to measure sample "cleanliness" likelihood, 2) Self-adaptive, data-driven thresholding scheme for per-class selection, 3) Different training strategies for clean samples (conventional), in-distribution noisy samples (partial label learning), and out-of-distribution noisy samples (negative learning), plus triplet consistency regularization for self-prediction, neighbor-prediction, and feature consistency.

Result: Extensive experiments on various benchmark datasets and comprehensive ablation studies demonstrate the effectiveness and superiority of the approach over existing state-of-the-art methods.

Conclusion: Jo-SNC effectively addresses limitations of previous methods by jointly handling sample selection and model regularization, considering both in-distribution and out-of-distribution noisy data with adaptive thresholding and consistency regularization, making it robust to imbalanced label noise in real-world scenarios.

Abstract: Label noise is pervasive in various real-world scenarios, posing challenges in supervised deep learning. Deep networks are vulnerable to such label-corrupted samples due to the memorization effect. One major stream of previous methods concentrates on identifying clean data for training. However, these methods often neglect imbalances in label noise across different mini-batches and devote insufficient attention to out-of-distribution noisy data. To this end, we propose a noise-robust method named Jo-SNC (\textbf{Jo}int sample selection and model regularization based on \textbf{S}elf- and \textbf{N}eighbor-\textbf{C}onsistency). Specifically, we propose to employ the Jensen-Shannon divergence to measure the ``likelihood'' of a sample being clean or out-of-distribution. This process factors in the nearest neighbors of each sample to reinforce the reliability of clean sample identification. We design a self-adaptive, data-driven thresholding scheme to adjust per-class selection thresholds. While clean samples undergo conventional training, detected in-distribution and out-of-distribution noisy samples are trained following partial label learning and negative learning, respectively. Finally, we advance the model performance further by proposing a triplet consistency regularization that promotes self-prediction consistency, neighbor-prediction consistency, and feature consistency. Extensive experiments on various benchmark datasets and comprehensive ablation studies demonstrate the effectiveness and superiority of our approach over existing state-of-the-art methods.

</details>


### [141] [PhyG-MoE: A Physics-Guided Mixture-of-Experts Framework for Energy-Efficient GNSS Interference Recognition](https://arxiv.org/abs/2601.12798)
*Zhihan Zeng,Yang Zhao,Kaihe Wang,Dusit Niyato,Yue Xiu,Lu Chen,Zhongpei Zhang,Ning Wei*

Main category: cs.CV

TL;DR: PhyG-MoE: A physics-guided mixture-of-experts framework that dynamically adjusts computational resources based on signal complexity for GNSS interference recognition, achieving 97.58% accuracy while reducing computational overhead.


<details>
  <summary>Details</summary>
Motivation: Current deep learning models for GNSS interference recognition use fixed computational topologies regardless of input complexity, causing resource mismatch where simple signals consume the same processing as complex ones. This is inefficient for resource-constrained cognitive receivers in dynamic electromagnetic environments.

Method: PhyG-MoE uses a spectrum-based gating mechanism to route signals based on spectral feature entanglement. It employs a high-capacity TransNeXt expert for complex saturated scenarios and lightweight experts for fundamental signals, dynamically aligning model capacity with signal complexity.

Result: Achieved 97.58% overall accuracy on 21 jamming categories while significantly reducing computational overhead without performance degradation.

Conclusion: PhyG-MoE resolves the conflict between static computing and dynamic electromagnetic environments, offering a viable solution for resource-constrained cognitive receivers by efficiently matching computational resources to signal complexity.

Abstract: Complex electromagnetic interference increasingly compromises Global Navigation Satellite Systems (GNSS), threatening the reliability of Space-Air-Ground Integrated Networks (SAGIN). Although deep learning has advanced interference recognition, current static models suffer from a \textbf{fundamental limitation}: they impose a fixed computational topology regardless of the input's physical entropy. This rigidity leads to severe resource mismatch, where simple primitives consume the same processing cost as chaotic, saturated mixtures. To resolve this, this paper introduces PhyG-MoE (Physics-Guided Mixture-of-Experts), a framework designed to \textbf{dynamically align model capacity with signal complexity}. Unlike static architectures, the proposed system employs a spectrum-based gating mechanism that routes signals based on their spectral feature entanglement. A high-capacity TransNeXt expert is activated on-demand to disentangle complex features in saturated scenarios, while lightweight experts handle fundamental signals to minimize latency. Evaluations on 21 jamming categories demonstrate that PhyG-MoE achieves an overall accuracy of 97.58\%. By resolving the intrinsic conflict between static computing and dynamic electromagnetic environments, the proposed framework significantly reduces computational overhead without performance degradation, offering a viable solution for resource-constrained cognitive receivers.

</details>


### [142] [Left-Right Symmetry Breaking in CLIP-style Vision-Language Models Trained on Synthetic Spatial-Relation Data](https://arxiv.org/abs/2601.12809)
*Takaki Yamamoto,Chihiro Noguchi,Toshihiro Tanizawa*

Main category: cs.CV

TL;DR: CLIP-style models can learn left-right spatial relations through contrastive training, with label diversity being more important than layout diversity for generalization, and attention mechanisms between positional and token embeddings create horizontal attention gradients that enable left-right discrimination.


<details>
  <summary>Details</summary>
Motivation: Spatial understanding in vision-language models remains unclear - whether it's truly acquired and through what mechanisms. The paper aims to probe how left-right relational understanding emerges in Transformer-based encoders trained with CLIP-style objectives.

Method: Created a controllable 1D image-text testbed with lightweight Transformer-based vision and text encoders trained end-to-end on paired descriptions of one- and two-object scenes. Evaluated generalization to unseen object pairs while varying label and layout diversity. Performed attention decomposition to analyze mechanisms.

Result: Contrastive training learns left-right relations, with label diversity being the primary driver of generalization rather than layout diversity. Attention decomposition revealed that interactions between positional and token embeddings induce horizontal attention gradients that break left-right symmetry; ablating this contribution reduces left-right discrimination.

Conclusion: The study provides mechanistic insight into when and how CLIP-style models acquire relational competence, showing that contrastive objectives can learn spatial relations through attention mechanisms that create directional biases in the encoders.

Abstract: Spatial understanding remains a key challenge in vision-language models. Yet it is still unclear whether such understanding is truly acquired, and if so, through what mechanisms. We present a controllable 1D image-text testbed to probe how left-right relational understanding emerges in Transformer-based vision and text encoders trained with a CLIP-style contrastive objective. We train lightweight Transformer-based vision and text encoders end-to-end on paired descriptions of one- and two-object scenes and evaluate generalization to unseen object pairs while systematically varying label and layout diversity. We find that contrastive training learns left-right relations and that label diversity, more than layout diversity, is the primary driver of generalization in this setting. To gain the mechanistic understanding, we perform an attention decomposition and show that interactions between positional and token embeddings induce a horizontal attention gradient that breaks left-right symmetry in the encoders; ablating this contribution substantially reduces left-right discrimination. Our results provide a mechanistic insight of when and how CLIP-style models acquire relational competence.

</details>


### [143] [CSGaussian: Progressive Rate-Distortion Compression and Segmentation for 3D Gaussian Splatting](https://arxiv.org/abs/2601.12814)
*Yu-Jen Tseng,Chia-Hao Kao,Jing-Zhong Chen,Alessandro Gnutti,Shao-Yuan Lo,Yen-Yu Lin,Wen-Hsiao Peng*

Main category: cs.CV

TL;DR: First unified framework for rate-distortion-optimized compression and segmentation of 3D Gaussian Splatting, enabling joint compression and semantic learning for decoder-side applications.


<details>
  <summary>Details</summary>
Motivation: Prior works treated 3DGS compression and segmentation independently, leaving joint optimization unexplored. Need to support decoder-side applications like scene editing and manipulation beyond traditional reconstruction and view synthesis.

Method: Integrates semantic learning into compression pipeline with lightweight implicit neural representation-based hyperprior for efficient entropy coding of color and semantic attributes. Uses compression-guided segmentation learning with quantization-aware training and quality-aware weighting mechanism.

Result: Significantly reduces transmission cost while preserving high rendering quality and strong segmentation performance on LERF and 3D-OVS datasets.

Conclusion: Presents first unified framework for joint rate-distortion-optimized compression and segmentation of 3DGS, enabling efficient transmission while supporting advanced decoder-side applications.

Abstract: We present the first unified framework for rate-distortion-optimized compression and segmentation of 3D Gaussian Splatting (3DGS). While 3DGS has proven effective for both real-time rendering and semantic scene understanding, prior works have largely treated these tasks independently, leaving their joint consideration unexplored. Inspired by recent advances in rate-distortion-optimized 3DGS compression, this work integrates semantic learning into the compression pipeline to support decoder-side applications--such as scene editing and manipulation--that extend beyond traditional scene reconstruction and view synthesis. Our scheme features a lightweight implicit neural representation-based hyperprior, enabling efficient entropy coding of both color and semantic attributes while avoiding costly grid-based hyperprior as seen in many prior works. To facilitate compression and segmentation, we further develop compression-guided segmentation learning, consisting of quantization-aware training to enhance feature separability and a quality-aware weighting mechanism to suppress unreliable Gaussian primitives. Extensive experiments on the LERF and 3D-OVS datasets demonstrate that our approach significantly reduces transmission cost while preserving high rendering quality and strong segmentation performance.

</details>


### [144] [A Generalist Foundation Model for Total-body PET/CT Enables Diagnostic Reporting and System-wide Metabolic Profiling](https://arxiv.org/abs/2601.12820)
*Wei Chen,Liang Wu,Shuyi Lu,Yuanyuan Sun,Wenkai Bi,Zilong Yuan,Yaoyao He,Feng Wang,Junchi Ma,Shuyong Liu,Zhaoping Cheng,Xiaoyan Hu,Jianfeng Qiu*

Main category: cs.CV

TL;DR: SDF-HOLO is a multimodal foundation model for total-body PET/CT that addresses challenges of heterogeneous signals, large axial coverage, and complex semantics through dual-stream encoders, cross-modal interaction, hierarchical context modeling, and voxel-mask-text alignment.


<details>
  <summary>Details</summary>
Motivation: Total-body PET/CT presents unique challenges for medical AI: heterogeneous anatomical/metabolic signals, ~2m axial coverage requiring system-wide analysis, and complex radiology semantics. Existing models assume single-modality inputs, localized fields of view, and coarse image-text alignment, which don't scale to holistic total-body imaging.

Method: 1) Dual-stream encoders for decoupled CT/PET representation learning with cross-modal interaction module; 2) Hierarchical context modeling combining local windows with global attention for long-range dependencies; 3) Anatomical segmentation masks as semantic anchors with voxel-mask-text alignment during pre-training; 4) Pre-trained on >10,000 patients.

Result: Outperforms task-specific and clinical-reference baselines across tumor segmentation, low-dose lesion detection, and multilingual diagnostic report generation. Reduces localization errors and hallucinated findings. Enables system-wide metabolic profiling and reveals tumor-associated fingerprints of inter-organ metabolic network interactions.

Conclusion: SDF-HOLO provides a scalable computational foundation for total-body PET/CT diagnostics and system-level precision oncology, moving beyond focal interpretation to enable holistic analysis of system-wide metabolic interactions.

Abstract: Total-body PET/CT enables system-wide molecular imaging, but heterogeneous anatomical and metabolic signals, approximately 2 m axial coverage, and structured radiology semantics challenge existing medical AI models that assume single-modality inputs, localized fields of view, and coarse image-text alignment. We introduce SDF-HOLO (Systemic Dual-stream Fusion Holo Model), a multimodal foundation model for holistic total-body PET/CT, pre-trained on more than 10,000 patients. SDF-HOLO decouples CT and PET representation learning with dual-stream encoders and couples them through a cross-modal interaction module, allowing anatomical context to refine PET aggregation while metabolic saliency guides subtle morphological reasoning. To model long-range dependencies across the body, hierarchical context modeling combines efficient local windows with global attention. To bridge voxels and clinical language, we use anatomical segmentation masks as explicit semantic anchors and perform voxel-mask-text alignment during pre-training. Across tumor segmentation, low-dose lesion detection, and multilingual diagnostic report generation, SDF-HOLO outperforms strong task-specific and clinical-reference baselines while reducing localization errors and hallucinated findings. Beyond focal interpretation, the model enables system-wide metabolic profiling and reveals tumor-associated fingerprints of inter-organ metabolic network interactions, providing a scalable computational foundation for total-body PET/CT diagnostics and system-level precision oncology.

</details>


### [145] [TreeDGS: Aerial Gaussian Splatting for Distant DBH Measurement](https://arxiv.org/abs/2601.12823)
*Belal Shaheen,Minh-Hieu Nguyen,Bach-Thuan Bui,Shubham,Tim Wu,Michael Fairley,Matthew David Zane,Michael Wu,James Tompkin*

Main category: cs.CV

TL;DR: TreeDGS uses 3D Gaussian Splatting from aerial images to accurately measure tree diameter at breast height (DBH), outperforming LiDAR with 4.79cm RMSE.


<details>
  <summary>Details</summary>
Motivation: Aerial remote sensing struggles with direct object-level measurement in complex natural scenes, especially for tree DBH where trunks are distant and sparsely observed in images, making conventional reconstruction methods inadequate.

Method: Uses 3D Gaussian Splatting as continuous scene representation, extracts dense point set using RaDe-GS's depth-aware cumulative-opacity integration, assigns multi-view opacity reliability scores, then estimates DBH from trunk-isolated points using opacity-weighted solid-circle fitting.

Result: Achieves 4.79cm RMSE on 10 plots with field-measured DBH (about 2.6 pixels at GSD), outperforming state-of-the-art LiDAR baseline (7.91cm RMSE).

Conclusion: Densified splat-based geometry enables accurate, low-cost aerial DBH measurement, demonstrating that learned radiance-field representations can overcome limitations of sparse aerial observations for natural attribute measurement.

Abstract: Aerial remote sensing enables efficient large-area surveying, but accurate direct object-level measurement remains difficult in complex natural scenes. Recent advancements in 3D vision, particularly learned radiance-field representations such as NeRF and 3D Gaussian Splatting, have begun to raise the ceiling on reconstruction fidelity and densifiable geometry from posed imagery. Nevertheless, direct aerial measurement of important natural attributes such as tree diameter at breast height (DBH) remains challenging. Trunks in aerial forest scans are distant and sparsely observed in image views: at typical operating altitudes, stems may span only a few pixels. With these constraints, conventional reconstruction methods leave breast-height trunk geometry weakly constrained. We present TreeDGS, an aerial image reconstruction method that leverages 3D Gaussian Splatting as a continuous, densifiable scene representation for trunk measurement. After SfM-MVS initialization and Gaussian optimization, we extract a dense point set from the Gaussian field using RaDe-GS's depth-aware cumulative-opacity integration and associate each sample with a multi-view opacity reliability score. We then estimate DBH from trunk-isolated points using opacity-weighted solid-circle fitting. Evaluated on 10 plots with field-measured DBH, TreeDGS reaches 4.79,cm RMSE (about 2.6 pixels at this GSD) and outperforms a state-of-the-art LiDAR baseline (7.91,cm RMSE), demonstrating that densified splat-based geometry can enable accurate, low-cost aerial DBH measurement.

</details>


### [146] [Seeing Isn't Always Believing: Analysis of Grad-CAM Faithfulness and Localization Reliability in Lung Cancer CT Classification](https://arxiv.org/abs/2601.12826)
*Teerapong Panboonyuen*

Main category: cs.CV

TL;DR: Grad-CAM explanations for lung cancer classification show significant variability across architectures, with poor faithfulness for Vision Transformers, questioning the reliability of current XAI methods in medical imaging.


<details>
  <summary>Details</summary>
Motivation: Despite widespread use of XAI techniques like Grad-CAM in medical imaging, there's insufficient scrutiny of whether these heatmap-based explanations truly represent the internal decision-making of deep neural networks, particularly for critical applications like lung cancer classification.

Method: Evaluated Grad-CAM interpretability across five architectures (ResNet-50, ResNet-101, DenseNet-161, EfficientNet-B0, ViT-Base-Patch16-224) using IQ-OTH/NCCD dataset. Developed quantitative framework combining localization accuracy, perturbation-based faithfulness, and explanation consistency to assess reliability.

Result: Grad-CAM effectively highlights tumor regions in most convolutional networks but interpretive fidelity significantly degrades for Vision Transformers due to non-local attention behavior. Substantial variability in saliency localization across models suggests Grad-CAM explanations may not correspond to true diagnostic evidence used by networks.

Conclusion: Current saliency-based XAI approaches have critical limitations in medical imaging, requiring model-aware interpretability methods that are both computationally sound and clinically meaningful. Urges cautious adoption of visual explanation tools and rethinking what it means to "trust" model explanations.

Abstract: Explainable Artificial Intelligence (XAI) techniques, such as Gradient-weighted Class Activation Mapping (Grad-CAM), have become indispensable for visualizing the reasoning process of deep neural networks in medical image analysis. Despite their popularity, the faithfulness and reliability of these heatmap-based explanations remain under scrutiny. This study critically investigates whether Grad-CAM truly represents the internal decision-making of deep models trained for lung cancer image classification. Using the publicly available IQ-OTH/NCCD dataset, we evaluate five representative architectures: ResNet-50, ResNet-101, DenseNet-161, EfficientNet-B0, and ViT-Base-Patch16-224, to explore model-dependent variations in Grad-CAM interpretability. We introduce a quantitative evaluation framework that combines localization accuracy, perturbation-based faithfulness, and explanation consistency to assess Grad-CAM reliability across architectures. Experimental findings reveal that while Grad-CAM effectively highlights salient tumor regions in most convolutional networks, its interpretive fidelity significantly degrades for Vision Transformer models due to non-local attention behavior. Furthermore, cross-model comparisons indicate substantial variability in saliency localization, implying that Grad-CAM explanations may not always correspond to the true diagnostic evidence used by the networks. This work exposes critical limitations of current saliency-based XAI approaches in medical imaging and emphasizes the need for model-aware interpretability methods that are both computationally sound and clinically meaningful. Our findings aim to inspire a more cautious and rigorous adoption of visual explanation tools in medical AI, urging the community to rethink what it truly means to "trust" a model's explanation.

</details>


### [147] [FGTBT: Frequency-Guided Task-Balancing Transformer for Unified Facial Landmark Detection](https://arxiv.org/abs/2601.12863)
*Jun Wan,Xinyu Xiong,Ning Chen,Zhihui Lai,Jie Zhou,Wenwen Min*

Main category: cs.CV

TL;DR: FGTBT: A transformer-based facial landmark detection method using frequency-domain modeling and multi-dataset unified training with fine-grained task balancing and frequency-guided structure awareness.


<details>
  <summary>Details</summary>
Motivation: Current deep learning FLD methods struggle with challenging scenarios (large pose variations, illumination changes, expressions) and have difficulty capturing facial geometric structure. Limited dataset size and diversity hinder robust training, reducing detection accuracy.

Method: Proposes Frequency-Guided Task-Balancing Transformer (FGTBT) with two key components: 1) Fine-Grained Multi-Task Balancing loss (FMB-loss) that assigns weights to individual landmarks based on dataset occurrence for better unified training, and 2) Frequency-Guided Structure-Aware (FGSA) model using frequency-guided structure injection and regularization to learn facial structure constraints.

Result: Extensive experiments on popular benchmark datasets show FGTBT achieves performance comparable to state-of-the-art methods. Code is publicly available.

Conclusion: The proposed FGTBT framework effectively addresses FLD challenges through frequency-domain modeling and multi-dataset unified training with fine-grained task balancing, achieving competitive performance with existing state-of-the-art methods.

Abstract: Recently, deep learning based facial landmark detection (FLD) methods have achieved considerable success. However, in challenging scenarios such as large pose variations, illumination changes, and facial expression variations, they still struggle to accurately capture the geometric structure of the face, resulting in performance degradation. Moreover, the limited size and diversity of existing FLD datasets hinder robust model training, leading to reduced detection accuracy. To address these challenges, we propose a Frequency-Guided Task-Balancing Transformer (FGTBT), which enhances facial structure perception through frequency-domain modeling and multi-dataset unified training. Specifically, we propose a novel Fine-Grained Multi-Task Balancing loss (FMB-loss), which moves beyond coarse task-level balancing by assigning weights to individual landmarks based on their occurrence across datasets. This enables more effective unified training and mitigates the issue of inconsistent gradient magnitudes. Additionally, a Frequency-Guided Structure-Aware (FGSA) model is designed to utilize frequency-guided structure injection and regularization to help learn facial structure constraints. Extensive experimental results on popular benchmark datasets demonstrate that the integration of the proposed FMB-loss and FGSA model into our FGTBT framework achieves performance comparable to state-of-the-art methods. The code is available at https://github.com/Xi0ngxinyu/FGTBT.

</details>


### [148] [Proxy Robustness in Vision Language Models is Effortlessly Transferable](https://arxiv.org/abs/2601.12865)
*Xiaowei Fu,Fuxiang Huang,Lei Zhang*

Main category: cs.CV

TL;DR: The paper proposes HPT-GPD, a method for transferring adversarial robustness to vision-language models without expensive adversarial training, by leveraging CLIP's intrinsic defensive capabilities across architectures while maintaining zero-shot generalization.


<details>
  <summary>Details</summary>
Motivation: Adversarial robustness transfer via distillation works well for conventional image classification but faces prohibitive computational costs when applied to large vision-language models like CLIP, which require expensive adversarial training to create robust teachers.

Method: The authors propose Heterogeneous Proxy Transfer (HPT) framework that leverages CLIP's intrinsic defensive capabilities across different architectures, and Generalization-Pivot Decoupling (GPD) that uses learning rate scheduling to decouple the transfer process into generalization-anchored warm-up and generalization-pulled HPT phases to prevent overfitting.

Result: Extensive experiments on 15 zero-shot datasets demonstrate the effectiveness of HPT-GPD in achieving adversarial robustness transfer while maintaining natural generalization capabilities.

Conclusion: The proposed HPT-GPD method successfully enables efficient adversarial robustness transfer for vision-language models without expensive adversarial training, achieving a good balance between natural generalization and adversarial robustness.

Abstract: As a pivotal technique for improving the defense of deep models, adversarial robustness transfer via distillation has demonstrated remarkable success in conventional image classification tasks. However, this paradigm encounters critical challenges when applied to vision-language models (VLM) (e.g., CLIP): constructing adversarially robust teacher for large-scale multi-modal models demands prohibitively high computational resources. We bridge this gap by revealing an interesting phenomenon: vanilla CLIP (without adversarial training) exhibits intrinsic defensive capabilities against adversarial examples generated by another CLIP with different architectures. We formally define this as proxy adversarial robustness, and naturally propose a Heterogeneous Proxy Transfer (HPT) framework that establishes cross-architectural robustness distillation channels between CLIP variants, effortlessly enabling the VLM robustness transfer from proxy to target models. Yet, such proxy transfer paradigm easily induces severe overfitting, leading to a sharp degradation in zero-shot natural generalization. To resolve that, we design Generalization-Pivot Decoupling (GPD) by leveraging the difference in learning rate scheduling. This decouples the proxy transfer process into a generalization-anchored warm-up that maintains generalization and a generalization-pulled HPT that promotes adversarial robustness, to achieve an equilibrium between natural generalization and adversarial robustness. Extensive experiments on 15 zero-shot datasets demonstrate the effectiveness of our HPT-GPD method. The code is available at the website of github.com/fxw13/HPT-GPD.

</details>


### [149] [Exploring Talking Head Models With Adjacent Frame Prior for Speech-Preserving Facial Expression Manipulation](https://arxiv.org/abs/2601.12876)
*Zhenxuan Lu,Zhihua Xu,Zhijing Yang,Feng Gao,Yongyi Lu,Keze Wang,Tianshui Chen*

Main category: cs.CV

TL;DR: THFEM integrates audio-driven talking head generation with speech-preserving facial expression manipulation to maintain accurate lip sync while altering expressions.


<details>
  <summary>Details</summary>
Motivation: SPFEM struggles with accurate lip synchronization due to complex interplay between facial expressions and mouth shapes. Audio-driven talking head models excel at lip sync but aren't designed for expression manipulation.

Method: THFEM framework combines AD-THG models with SPFEM: AD-THG generates frames with accurate lip sync from audio, SPFEM alters expressions. Adjacent frame learning strategy finetunes AD-THG models to predict consecutive frames, improving quality.

Result: Framework effectively preserves mouth shapes during expression manipulations, with adjacent frame learning significantly improving image quality and expression fidelity.

Conclusion: Integration of AD-THG with SPFEM provides substantial benefits for speech-preserving facial expression manipulation, successfully addressing lip synchronization challenges.

Abstract: Speech-Preserving Facial Expression Manipulation (SPFEM) is an innovative technique aimed at altering facial expressions in images and videos while retaining the original mouth movements. Despite advancements, SPFEM still struggles with accurate lip synchronization due to the complex interplay between facial expressions and mouth shapes. Capitalizing on the advanced capabilities of audio-driven talking head generation (AD-THG) models in synthesizing precise lip movements, our research introduces a novel integration of these models with SPFEM. We present a new framework, Talking Head Facial Expression Manipulation (THFEM), which utilizes AD-THG models to generate frames with accurately synchronized lip movements from audio inputs and SPFEM-altered images. However, increasing the number of frames generated by AD-THG models tends to compromise the realism and expression fidelity of the images. To counter this, we develop an adjacent frame learning strategy that finetunes AD-THG models to predict sequences of consecutive frames. This strategy enables the models to incorporate information from neighboring frames, significantly improving image quality during testing. Our extensive experimental evaluations demonstrate that this framework effectively preserves mouth shapes during expression manipulations, highlighting the substantial benefits of integrating AD-THG with SPFEM.

</details>


### [150] [YOLO26: An Analysis of NMS-Free End to End Framework for Real-Time Object Detection](https://arxiv.org/abs/2601.12882)
*Sudip Chakrabarty*

Main category: cs.CV

TL;DR: YOLO26 eliminates NMS post-processing through end-to-end learning, achieving superior speed-accuracy trade-off compared to previous YOLO versions and state-of-the-art competitors.


<details>
  <summary>Details</summary>
Motivation: Traditional YOLO frameworks (v1-v11) are limited by the latency and hyperparameter sensitivity of Non-Maximum Suppression (NMS) post-processing, creating a bottleneck for real-time object detection performance.

Method: YOLO26 introduces three key innovations: 1) MuSGD optimizer for stabilizing lightweight backbones, 2) STAL (small-target-aware assignment) for better small object detection, and 3) ProgLoss for dynamic supervision. The architecture eliminates NMS in favor of native end-to-end learning.

Result: YOLO26 establishes a new Pareto front, outperforming predecessors (YOLOv1-v11) and state-of-the-art competitors (RTMDet, DAMO-YOLO) in both inference speed and detection accuracy.

Conclusion: By decoupling representation learning from heuristic post-processing, YOLO26 successfully resolves the historical trade-off between latency and precision, representing the next evolutionary step in edge-based computer vision.

Abstract: The "You Only Look Once" (YOLO) framework has long served as the benchmark for real-time object detection, yet traditional iterations (YOLOv1 through YOLO11) remain constrained by the latency and hyperparameter sensitivity of Non-Maximum Suppression (NMS) post-processing. This paper analyzes a comprehensive analysis of YOLO26, an architecture that fundamentally redefines this paradigm by eliminating NMS in favor of a native end-to-end learning strategy. This study examines the critical innovations that enable this transition, specifically the introduction of the MuSGD optimizer for stabilizing lightweight backbones, STAL for small-target-aware assignment, and ProgLoss for dynamic supervision. Through a systematic review of official performance benchmarks, the results demonstrate that YOLO26 establishes a new Pareto front, outperforming a comprehensive suite of predecessors and state-of-the-art competitors (including RTMDet and DAMO-YOLO) in both inference speed and detection accuracy. The analysis confirms that by decoupling representation learning from heuristic post-processing, YOLOv26 successfully resolves the historical trade-off between latency and precision, signaling the next evolutionary step in edge-based computer vision.

</details>


### [151] [Simultaneous Detection of LSD and FMD in Cattle Using Ensemble Deep Learning](https://arxiv.org/abs/2601.12889)
*Nazibul Basar Ayon,Abdul Hasib,Md. Faishal Ahmed,Md. Sadiqur Rahman,Kamrul Islam,T. M. Mehrab Hasan,A. S. M. Ahsanul Sarkar Akib*

Main category: cs.CV

TL;DR: Novel ensemble deep learning framework achieves 98.2% accuracy for simultaneous detection of Lumpy Skin Disease and Foot-and-Mouth Disease in cattle using optimized weighted averaging of VGG16, ResNet50, and InceptionV3 models.


<details>
  <summary>Details</summary>
Motivation: LSD and FMD are highly contagious viral diseases causing significant economic losses, but visual diagnosis is complicated by symptom overlap with each other and benign conditions like insect bites or chemical burns, hindering timely control measures.

Method: Developed an ensemble deep learning framework integrating VGG16, ResNet50, and InceptionV3 with optimized weighted averaging, trained on 10,516 expert-annotated images from 18 farms across India, Brazil, and the USA for simultaneous LSD and FMD detection.

Result: Achieved state-of-the-art accuracy of 98.2%, with macro-averaged precision of 98.2%, recall of 98.1%, F1-score of 98.1%, and AUC-ROC of 99.5%, uniquely addressing symptom overlap challenge in multi-disease detection.

Conclusion: The approach enables early, precise, and automated diagnosis, has potential to enhance disease management and support global agricultural sustainability, and is designed for future deployment in resource-limited settings.

Abstract: Lumpy Skin Disease (LSD) and Foot-and-Mouth Disease (FMD) are highly contagious viral diseases affecting cattle, causing significant economic losses and welfare challenges. Their visual diagnosis is complicated by significant symptom overlap with each other and with benign conditions like insect bites or chemical burns, hindering timely control measures. Leveraging a comprehensive dataset of 10,516 expert-annotated images from 18 farms across India, Brazil, and the USA, this study presents a novel Ensemble Deep Learning framework integrating VGG16, ResNet50, and InceptionV3 with optimized weighted averaging for simultaneous LSD and FMD detection. The model achieves a state-of-the-art accuracy of 98.2\%, with macro-averaged precision of 98.2\%, recall of 98.1\%, F1-score of 98.1\%, and an AUC-ROC of 99.5\%. This approach uniquely addresses the critical challenge of symptom overlap in multi-disease detection, enabling early, precise, and automated diagnosis. This tool has the potential to enhance disease management, support global agricultural sustainability, and is designed for future deployment in resource-limited settings.

</details>


### [152] [TwoHead-SwinFPN: A Unified DL Architecture for Synthetic Manipulation, Detection and Localization in Identity Documents](https://arxiv.org/abs/2601.12895)
*Chan Naseeb,Adeel Ashraf Cheema,Hassan Sami,Tayyab Afzal,Muhammad Omair,Usman Habib*

Main category: cs.CV

TL;DR: TwoHead-SwinFPN is a unified deep learning model that simultaneously detects and localizes synthetic manipulations in ID documents using a dual-head architecture with Swin Transformer backbone, FPN, UNet decoder, and CBAM attention.


<details>
  <summary>Details</summary>
Motivation: The increasing threat of sophisticated generative AI models enabling face swapping and text inpainting attacks on identity documents requires robust detection and localization methods to combat synthetic manipulations.

Method: A unified architecture integrating Swin Transformer backbone with Feature Pyramid Network and UNet-style decoder, enhanced with Convolutional Block Attention Module. Uses dual-head architecture for joint binary classification and segmentation tasks with uncertainty-weighted multi-task learning.

Result: Achieves 84.31% accuracy, 90.78% AUC for classification, 57.24% mean Dice score for localization, and 88.61% F1-score on FantasyIDiap dataset. Demonstrated computational efficiency suitable for real-world deployment via FastAPI.

Conclusion: TwoHead-SwinFPN provides an effective unified solution for detecting and localizing synthetic manipulations in ID documents, with strong performance across multiple languages and devices, making it suitable for practical deployment.

Abstract: The proliferation of sophisticated generative AI models has significantly escalated the threat of synthetic manipulations in identity documents, particularly through face swapping and text inpainting attacks. This paper presents TwoHead-SwinFPN, a unified deep learning architecture that simultaneously performs binary classification and precise localization of manipulated regions in ID documents. Our approach integrates a Swin Transformer backbone with Feature Pyramid Network (FPN) and UNet-style decoder, enhanced with Convolutional Block Attention Module (CBAM) for improved feature representation. The model employs a dual-head architecture for joint optimization of detection and segmentation tasks, utilizing uncertainty-weighted multi-task learning. Extensive experiments on the FantasyIDiap dataset demonstrate superior performance with 84.31\% accuracy, 90.78\% AUC for classification, and 57.24\% mean Dice score for localization. The proposed method achieves an F1-score of 88.61\% for binary classification while maintaining computational efficiency suitable for real-world deployment through FastAPI implementation. Our comprehensive evaluation includes ablation studies, cross-device generalization analysis, and detailed performance assessment across 10 languages and 3 acquisition devices.

</details>


### [153] [Supervision-by-Hallucination-and-Transfer: A Weakly-Supervised Approach for Robust and Precise Facial Landmark Detection](https://arxiv.org/abs/2601.12919)
*Jun Wan,Yuanzhi Yao,Zhihui Lai,Jie Zhou,Xianxu Hou,Wenwen Min*

Main category: cs.CV

TL;DR: A weakly-supervised framework called SHT improves facial landmark detection by combining face hallucination and facial pose transfer to handle low-resolution inputs and limited training data.


<details>
  <summary>Details</summary>
Motivation: High-precision facial landmark detection suffers from low-resolution inputs, image compression, insufficient training data, and imprecise annotations, which degrade performance.

Method: Proposes Supervision-by-Hallucination-and-Transfer (SHT) with two modules: Dual Hallucination Learning Network (DHLN) for learning high-resolution representations from low-resolution inputs, and Facial Pose Transfer Network (FPTN) for improving landmark heatmaps through pose transformation.

Result: Experimental results show the method surpasses state-of-the-art techniques in both face hallucination and facial landmark detection tasks.

Conclusion: This is the first study to explore weakly-supervised facial landmark detection by integrating face hallucination and facial pose transfer, demonstrating improved robustness and precision.

Abstract: High-precision facial landmark detection (FLD) relies on high-resolution deep feature representations. However, low-resolution face images or the compression (via pooling or strided convolution) of originally high-resolution images hinder the learning of such features, thereby reducing FLD accuracy. Moreover, insufficient training data and imprecise annotations further degrade performance. To address these challenges, we propose a weakly-supervised framework called Supervision-by-Hallucination-and-Transfer (SHT) for more robust and precise FLD. SHT contains two novel mutually enhanced modules: Dual Hallucination Learning Network (DHLN) and Facial Pose Transfer Network (FPTN). By incorporating FLD and face hallucination tasks, DHLN is able to learn high-resolution representations with low-resolution inputs for recovering both facial structures and local details and generating more effective landmark heatmaps. Then, by transforming faces from one pose to another, FPTN can further improve landmark heatmaps and faces hallucinated by DHLN for detecting more accurate landmarks. To the best of our knowledge, this is the first study to explore weakly-supervised FLD by integrating face hallucination and facial pose transfer tasks. Experimental results of both face hallucination and FLD demonstrate that our method surpasses state-of-the-art techniques.

</details>


### [154] [Dual-Stream Collaborative Transformer for Image Captioning](https://arxiv.org/abs/2601.12926)
*Jun Wan,Jun Liu,Zhihui lai,Jie Zhou*

Main category: cs.CV

TL;DR: DSCT uses dual-stream region and segmentation features with mutual attention and dynamic nomination to generate more accurate image captions by addressing semantic inconsistencies.


<details>
  <summary>Details</summary>
Motivation: Existing region feature-based captioning methods generate irrelevant descriptions due to lack of contextual information and over-reliance on partial descriptions. They need better feature integration to handle semantic inconsistencies and spatial misalignment.

Method: Dual-Stream Collaborative Transformer (DSCT) with Pattern-Specific Mutual Attention Encoders (PSMAEs) to consolidate region and segmentation features, and Dynamic Nomination Decoders (DNDs) to dynamically select relevant learning blocks for caption generation.

Result: DSCT outperforms state-of-the-art image captioning models on popular benchmark datasets, demonstrating improved accuracy and descriptiveness.

Conclusion: The proposed DSCT effectively fuses different pattern-specific features dynamically, addressing semantic inconsistencies and spatial misalignment to generate more accurate and descriptive image captions.

Abstract: Current region feature-based image captioning methods have progressed rapidly and achieved remarkable performance. However, they are still prone to generating irrelevant descriptions due to the lack of contextual information and the over-reliance on generated partial descriptions for predicting the remaining words. In this paper, we propose a Dual-Stream Collaborative Transformer (DSCT) to address this issue by introducing the segmentation feature. The proposed DSCT consolidates and then fuses the region and segmentation features to guide the generation of caption sentences. It contains multiple Pattern-Specific Mutual Attention Encoders (PSMAEs) and Dynamic Nomination Decoders (DNDs). The PSMAE effectively highlights and consolidates the private information of two representations by querying each other. The DND dynamically searches for the most relevant learning blocks to the input textual representations and exploits the homogeneous features between the consolidated region and segmentation features to generate more accurate and descriptive caption sentences. To the best of our knowledge, this is the first study to explore how to fuse different pattern-specific features in a dynamic way to bypass their semantic inconsistencies and spatial misalignment issues for image captioning. The experimental results from popular benchmark datasets demonstrate that our DSCT outperforms the state-of-the-art image captioning models in the literature.

</details>


### [155] [Membership Inference Test: Auditing Training Data in Object Classification Models](https://arxiv.org/abs/2601.12929)
*Gonzalo Mancera,Daniel DeAlcala,Aythami Morales,Ruben Tolosana,Julian Fierrez*

Main category: cs.CV

TL;DR: Researchers developed specialized architectures for Membership Inference Tests (MINT) in object recognition to determine if data was used during training, achieving 70-80% precision across three public databases with over 174K images.


<details>
  <summary>Details</summary>
Motivation: The research addresses the need to determine whether specific data was utilized during the training phase of object recognition models, focusing on developing specialized MINT architectures to tackle the complexities of this domain and enable more transparent training processes.

Method: Proposed and developed tailored MINT architectures for object recognition using convolutional layers to capture activation patterns. Experiments involved object detection models, embedding extractors, and MINT modules tested across three public databases. The approach analyzes how different detection module layer depths affect MINT performance.

Result: Achieved precision rates between 70% and 80% for identifying whether data was used in training, with performance dependent on the depth of the detection module layer chosen as input to the MINT module. The study also analyzed factors influencing MINT module performance for more transparent training processes.

Conclusion: Specialized MINT architectures can effectively determine data membership in object recognition training with reasonable precision (70-80%), though performance varies with detection layer depth. The research contributes to understanding factors that influence membership inference and promotes transparency in training processes.

Abstract: In this research, we analyze the performance of Membership Inference Tests (MINT), focusing on determining whether given data were utilized during the training phase, specifically in the domain of object recognition. Within the area of object recognition, we propose and develop architectures tailored for MINT models. These architectures aim to optimize performance and efficiency in data utilization, offering a tailored solution to tackle the complexities inherent in the object recognition domain. We conducted experiments involving an object detection model, an embedding extractor, and a MINT module. These experiments were performed in three public databases, totaling over 174K images. The proposed architecture leverages convolutional layers to capture and model the activation patterns present in the data during the training process. Through our analysis, we are able to identify given data used for testing and training, achieving precision rates ranging between 70% and 80%, contingent upon the depth of the detection module layer chosen for input to the MINT module. Additionally, our studies entail an analysis of the factors influencing the MINT Module, delving into the contributing elements behind more transparent training processes.

</details>


### [156] [QASA: Quality-Guided K-Adaptive Slot Attention for Unsupervised Object-Centric Learning](https://arxiv.org/abs/2601.12936)
*Tianran Ouyang,Xingping Dong,Jing Zhang,Mang Ye,Jun Chen,Bo Du*

Main category: cs.CV

TL;DR: QASA introduces a quality-guided K-adaptive slot attention method that decouples slot selection from reconstruction and uses unsupervised slot-quality metrics to dynamically select high-quality slots, outperforming both K-adaptive and K-fixed baselines.


<details>
  <summary>Details</summary>
Motivation: Existing K-adaptive slot attention methods suffer from two key limitations: 1) lack of explicit constraints on slot-binding quality leading to ambiguous feature attribution, and 2) conflicting optimization goals between reducing active slots and maintaining reconstruction fidelity when using slot-count penalties.

Method: QASA decouples slot selection from reconstruction to eliminate mutual constraints. It introduces an unsupervised Slot-Quality metric to assess per-slot quality, uses Quality-Guided Slot Selection to dynamically select high-quality slots, and feeds them to a gated decoder for reconstruction. At inference, token-wise competition yields K-adaptive outcomes.

Result: QASA substantially outperforms existing K-adaptive methods on both real and synthetic datasets. On real-world datasets, QASA even surpasses K-fixed methods.

Conclusion: The proposed quality-guided approach effectively addresses the limitations of existing K-adaptive slot attention methods by providing principled quality assessment and decoupling selection from reconstruction, achieving state-of-the-art performance in unsupervised object-centric learning.

Abstract: Slot Attention, an approach that binds different objects in a scene to a set of "slots", has become a leading method in unsupervised object-centric learning. Most methods assume a fixed slot count K, and to better accommodate the dynamic nature of object cardinality, a few works have explored K-adaptive variants. However, existing K-adaptive methods still suffer from two limitations. First, they do not explicitly constrain slot-binding quality, so low-quality slots lead to ambiguous feature attribution. Second, adding a slot-count penalty to the reconstruction objective creates conflicting optimization goals between reducing the number of active slots and maintaining reconstruction fidelity. As a result, they still lag significantly behind strong K-fixed baselines. To address these challenges, we propose Quality-Guided K-Adaptive Slot Attention (QASA). First, we decouple slot selection from reconstruction, eliminating the mutual constraints between the two objectives. Then, we propose an unsupervised Slot-Quality metric to assess per-slot quality, providing a principled signal for fine-grained slot--object binding. Based on this metric, we design a Quality-Guided Slot Selection scheme that dynamically selects a subset of high-quality slots and feeds them into our newly designed gated decoder for reconstruction during training. At inference, token-wise competition on slot attention yields a K-adaptive outcome. Experiments show that QASA substantially outperforms existing K-adaptive methods on both real and synthetic datasets. Moreover, on real-world datasets QASA surpasses K-fixed methods.

</details>


### [157] [GazeD: Context-Aware Diffusion for Accurate 3D Gaze Estimation](https://arxiv.org/abs/2601.12948)
*Riccardo Catalini,Davide Di Nucci,Guido Borghi,Davide Davoli,Lorenzo Garattoni,Giampiero Francesca,Yuki Kawana,Roberto Vezzani*

Main category: cs.CV

TL;DR: GazeD is a diffusion-based method that jointly estimates 3D gaze and human pose from a single RGB image by treating gaze as an additional body joint and generating multiple plausible hypotheses.


<details>
  <summary>Details</summary>
Motivation: Existing methods often struggle with 3D gaze estimation from single images due to ambiguity and uncertainty. The paper aims to leverage diffusion models' ability to handle uncertainty while exploiting the relationship between gaze and body pose for more accurate joint estimation.

Method: Uses diffusion models conditioned on 2D pose, subject surroundings, and scene context. Introduces novel representation of 3D gaze as an additional body joint at fixed distance from eyes, allowing joint denoising with pose during diffusion process.

Result: Achieves state-of-the-art performance on three benchmark datasets for 3D gaze estimation, even surpassing methods that use temporal information.

Conclusion: GazeD demonstrates that diffusion models can effectively handle uncertainty in 3D gaze estimation and that joint modeling of gaze and pose leads to superior performance from single RGB images.

Abstract: We introduce GazeD, a new 3D gaze estimation method that jointly provides 3D gaze and human pose from a single RGB image. Leveraging the ability of diffusion models to deal with uncertainty, it generates multiple plausible 3D gaze and pose hypotheses based on the 2D context information extracted from the input image. Specifically, we condition the denoising process on the 2D pose, the surroundings of the subject, and the context of the scene. With GazeD we also introduce a novel way of representing the 3D gaze by positioning it as an additional body joint at a fixed distance from the eyes. The rationale is that the gaze is usually closely related to the pose, and thus it can benefit from being jointly denoised during the diffusion process. Evaluations across three benchmark datasets demonstrate that GazeD achieves state-of-the-art performance in 3D gaze estimation, even surpassing methods that rely on temporal information. Project details will be available at https://aimagelab.ing.unimore.it/go/gazed.

</details>


### [158] [StyMam: A Mamba-Based Generator for Artistic Style Transfer](https://arxiv.org/abs/2601.12954)
*Zhou Hong,Rongsheng Hu,Yicheng Di,Xiaolong Xu,Ning Dong,Yihua Shao,Run Ling,Yun Wang,Juqin Wang,Zhanjie Zhang,Ao Ma*

Main category: cs.CV

TL;DR: Proposes StyMam, a mamba-based generator for image style transfer that addresses artifacts and disharmonious patterns in GAN/SD methods while preserving content structure and achieving fast inference.


<details>
  <summary>Details</summary>
Motivation: Existing GAN-based methods struggle with local/global dependency capture causing artifacts, while SD-based methods have content preservation issues and slow inference. Need for high-quality stylization without artifacts while maintaining speed.

Method: Uses mamba-based generator with residual dual-path strip scanning mechanism for local texture features and channel-reweighted spatial attention module for global dependencies.

Result: Extensive experiments show the method outperforms state-of-the-art algorithms in both quality and speed, producing high-quality stylized images without artifacts.

Conclusion: StyMam successfully addresses limitations of existing GAN/SD methods by leveraging mamba architecture to achieve artifact-free style transfer with preserved content structure and fast inference.

Abstract: Image style transfer aims to integrate the visual patterns of a specific artistic style into a content image while preserving its content structure. Existing methods mainly rely on the generative adversarial network (GAN) or stable diffusion (SD). GAN-based approaches using CNNs or Transformers struggle to jointly capture local and global dependencies, leading to artifacts and disharmonious patterns. SD-based methods reduce such issues but often fail to preserve content structures and suffer from slow inference. To address these issues, we revisit GAN and propose a mamba-based generator, termed as StyMam, to produce high-quality stylized images without introducing artifacts and disharmonious patterns. Specifically, we introduce a mamba-based generator with a residual dual-path strip scanning mechanism and a channel-reweighted spatial attention module. The former efficiently captures local texture features, while the latter models global dependencies. Finally, extensive qualitative and quantitative experiments demonstrate that the proposed method outperforms state-of-the-art algorithms in both quality and speed.

</details>


### [159] [Cross-Scale Pretraining: Enhancing Self-Supervised Learning for Low-Resolution Satellite Imagery for Semantic Segmentation](https://arxiv.org/abs/2601.12964)
*John Waithaka,Gustave Bwirayesu,Moise Busogi*

Main category: cs.CV

TL;DR: Adding high-resolution imagery to self-supervised pretraining via spatial affinity component improves mid-resolution image representations and downstream segmentation performance.


<details>
  <summary>Details</summary>
Motivation: Most self-supervised pretraining in remote sensing uses mid-resolution (MR) datasets due to availability, but high-resolution (HR) datasets are now emerging. The paper explores how to incorporate HR data to enhance MR image representation learning and downstream segmentation performance on MR tasks.

Method: Design a spatial affinity component that can be integrated into existing self-supervised learning frameworks. This component uses HR imagery to learn better representations of MR imagery. The approach is tested on two different self-supervised learning frameworks.

Result: The spatial affinity component outperforms models pretrained on HR or MR images alone. It demonstrates that incorporating HR imagery through the proposed component enhances MR image representation learning and downstream segmentation performance.

Conclusion: High-resolution datasets can be effectively incorporated into self-supervised pretraining through a spatial affinity component to improve mid-resolution image representations and downstream task performance, rather than using HR or MR data alone.

Abstract: Self-supervised pretraining in remote sensing is mostly done using mid-spatial resolution (MR) image datasets due to their high availability. Given the release of high-resolution (HR) datasets, we ask how HR datasets can be included in self-supervised pretraining to enhance MR image representation learning and downstream segmentation performance on MR tasks. We design a spatial affinity component that can be added to existing self-supervised learning frameworks and that uses HR imagery to learn better representations of MR imagery. We test the spatial affinity component on two self-supervised learning frameworks and show that it outperforms models pretrained on HR or MR images alone.

</details>


### [160] [Early Prediction of Type 2 Diabetes Using Multimodal data and Tabular Transformers](https://arxiv.org/abs/2601.12981)
*Sulaiman Khan,Md. Rafiul Biswas,Zubair Shah*

Main category: cs.CV

TL;DR: TabTrans transformer model outperforms generative AI and conventional ML for early T2DM prediction using longitudinal EHR and DXA data, achieving ≥79.7% ROC AUC and identifying key bone-related risk factors.


<details>
  <summary>Details</summary>
Motivation: Existing methods often miss complex long-range dependencies in longitudinal patient data for T2DM prediction. There's a need for better models that can integrate multimodal healthcare data (EHR + DXA) to enable early diabetes risk assessment.

Method: Tabular transformer (TabTrans) architecture processes longitudinal health records and bone-related DXA data. Applied to Qatar BioBank cohort (1,382 subjects) with SMOTE/SMOTE-ENN resampling for class imbalance. Compared against conventional ML and generative AI models (Claude 3.5 Sonnet, GPT-4, Gemini Pro).

Result: TabTrans achieved superior performance with ROC AUC ≥79.7% for T2DM prediction, outperforming both generative AI and conventional ML. Key predictors identified: visceral adipose tissue (VAT) mass/volume, ward BMD/BMC, T/Z-scores, and L1-L4 scores.

Conclusion: TabTrans demonstrates significant potential for analyzing complex tabular healthcare data, providing a powerful tool for proactive T2DM management and personalized clinical interventions in the Qatari population.

Abstract: This study introduces a novel approach for early Type 2 Diabetes Mellitus (T2DM) risk prediction using a tabular transformer (TabTrans) architecture to analyze longitudinal patient data. By processing patients` longitudinal health records and bone-related tabular data, our model captures complex, long-range dependencies in disease progression that conventional methods often overlook. We validated our TabTrans model on a retrospective Qatar BioBank (QBB) cohort of 1,382 subjects, comprising 725 men (146 diabetic, 579 healthy) and 657 women (133 diabetic, 524 healthy). The study integrated electronic health records (EHR) with dual-energy X-ray absorptiometry (DXA) data. To address class imbalance, we employed SMOTE and SMOTE-ENN resampling techniques. The proposed model`s performance is evaluated against conventional machine learning (ML) and generative AI models, including Claude 3.5 Sonnet (Anthropic`s constitutional AI), GPT-4 (OpenAI`s generative pre-trained transformer), and Gemini Pro (Google`s multimodal language model). Our TabTrans model demonstrated superior predictive performance, achieving ROC AUC $\geq$ 79.7 % for T2DM prediction compared to both generative AI models and conventional ML approaches. Feature interpretation analysis identified key risk indicators, with visceral adipose tissue (VAT) mass and volume, ward bone mineral density (BMD) and bone mineral content (BMC), T and Z-scores, and L1-L4 scores emerging as the most important predictors associated with diabetes development in Qatari adults. These findings demonstrate the significant potential of TabTrans for analyzing complex tabular healthcare data, providing a powerful tool for proactive T2DM management and personalized clinical interventions in the Qatari population.
  Index Terms: tabular transformers, multimodal data, DXA data, diabetes, T2DM, feature interpretation, tabular data

</details>


### [161] [AsyncBEV: Cross-modal Flow Alignment in Asynchronous 3D Object Detection](https://arxiv.org/abs/2601.12994)
*Shiming Wang,Holger Caesar,Liangliang Nan,Julian F. P. Kooij*

Main category: cs.CV

TL;DR: AsyncBEV is a trainable module that improves 3D BEV object detection robustness against sensor asynchrony by estimating feature flow and aligning feature maps across modalities.


<details>
  <summary>Details</summary>
Motivation: In autonomous driving, multi-modal perception relies on synchronized sensors, but perfect synchrony is rarely guaranteed due to different sensor frequencies, network latency, hardware failures, and processing bottlenecks. This asynchrony degrades perception performance, especially for dynamic objects.

Method: AsyncBEV estimates 2D flow from BEV features of two different sensor modalities, considering the known time offset between measurements. The predicted feature flow is used to warp and spatially align feature maps, which can be integrated into different BEV detector architectures (grid-based and token-based).

Result: Extensive experiments show AsyncBEV improves robustness against both small and large asynchrony between LiDAR or camera sensors in both token-based CMT and grid-based UniBEV architectures. It significantly outperforms ego motion compensated baselines, notably by 16.6% and 11.9% NDS on dynamic objects in worst-case scenario of 0.5s time offset.

Conclusion: AsyncBEV provides a lightweight, trainable solution to address sensor asynchrony in multi-modal 3D object detection, improving robustness especially for dynamic objects across different BEV detector architectures.

Abstract: In autonomous driving, multi-modal perception tasks like 3D object detection typically rely on well-synchronized sensors, both at training and inference. However, despite the use of hardware- or software-based synchronization algorithms, perfect synchrony is rarely guaranteed: Sensors may operate at different frequencies, and real-world factors such as network latency, hardware failures, or processing bottlenecks often introduce time offsets between sensors. Such asynchrony degrades perception performance, especially for dynamic objects. To address this challenge, we propose AsyncBEV, a trainable lightweight and generic module to improve the robustness of 3D Birds' Eye View (BEV) object detection models against sensor asynchrony. Inspired by scene flow estimation, AsyncBEV first estimates the 2D flow from the BEV features of two different sensor modalities, taking into account the known time offset between these sensor measurements. The predicted feature flow is then used to warp and spatially align the feature maps, which we show can easily be integrated into different current BEV detector architectures (e.g., BEV grid-based and token-based). Extensive experiments demonstrate AsyncBEV improves robustness against both small and large asynchrony between LiDAR or camera sensors in both the token-based CMT and grid-based UniBEV, especially for dynamic objects. We significantly outperform the ego motion compensated CMT and UniBEV baselines, notably by $16.6$ % and $11.9$ % NDS on dynamic objects in the worst-case scenario of a $0.5 s$ time offset. Code will be released upon acceptance.

</details>


### [162] [Think3D: Thinking with Space for Spatial Reasoning](https://arxiv.org/abs/2601.13029)
*Zaibin Zhang,Yuhan Wu,Lianjie Jia,Yifan Wang,Zhongbo Zhang,Yijiang Li,Binghao Ran,Fuxi Zhang,Zhuohan Sun,Zhenfei Yin,Lijun Wang,Huchuan Lu*

Main category: cs.CV

TL;DR: Think3D enables vision-language models to perform 3D spatial reasoning by integrating 3D reconstruction tools and interactive camera operations, achieving significant performance gains without additional training.


<details>
  <summary>Details</summary>
Motivation: Current vision-language models are fundamentally 2D perceivers and struggle with genuine 3D spatial reasoning, which is essential for understanding and interacting with the physical world.

Method: Think3D framework leverages 3D reconstruction models to recover point clouds and camera poses from images/videos, enabling agents to actively manipulate space through camera operations and viewpoint switching, creating an interactive 3D chain-of-thought process.

Result: Think3D improves spatial reasoning performance of advanced models (GPT-4.1, Gemini 2.5 Pro) by +7.8% on BLINK Multi-view and MindCube, and +4.7% on VSI-Bench. Smaller models benefit from RL policy for viewpoint selection, increasing tool usage benefit from +0.7% to +6.8%.

Conclusion: Training-free, tool-augmented spatial exploration is a viable path toward more flexible and human-like 3D reasoning in multimodal agents, establishing a new dimension of multimodal intelligence.

Abstract: Understanding and reasoning about the physical world requires spatial intelligence: the ability to interpret geometry, perspective, and spatial relations beyond 2D perception. While recent vision large models (VLMs) excel at visual understanding, they remain fundamentally 2D perceivers and struggle with genuine 3D reasoning. We introduce Think3D, a framework that enables VLM agents to think with 3D space. By leveraging 3D reconstruction models that recover point clouds and camera poses from images or videos, Think3D allows the agent to actively manipulate space through camera-based operations and ego/global-view switching, transforming spatial reasoning into an interactive 3D chain-of-thought process. Without additional training, Think3D significantly improves the spatial reasoning performance of advanced models such as GPT-4.1 and Gemini 2.5 Pro, yielding average gains of +7.8% on BLINK Multi-view and MindCube, and +4.7% on VSI-Bench. We further show that smaller models, which struggle with spatial exploration, benefit significantly from a reinforcement learning policy that enables the model to select informative viewpoints and operations. With RL, the benefit from tool usage increases from +0.7% to +6.8%. Our findings demonstrate that training-free, tool-augmented spatial exploration is a viable path toward more flexible and human-like 3D reasoning in multimodal agents, establishing a new dimension of multimodal intelligence. Code and weights are released at https://github.com/zhangzaibin/spagent.

</details>


### [163] [GridNet-HD: A High-Resolution Multi-Modal Dataset for LiDAR-Image Fusion on Power Line Infrastructure](https://arxiv.org/abs/2601.13052)
*Antoine Carreaud,Shanci Li,Malo De Lacour,Digre Frinde,Jan Skaloud,Adrien Gressin*

Main category: cs.CV

TL;DR: GridNet-HD is a new multi-modal dataset for 3D semantic segmentation of overhead electrical infrastructure, combining high-density LiDAR with high-resolution oblique imagery, with 7,694 images and 2.5 billion points across 11 classes.


<details>
  <summary>Details</summary>
Motivation: There is currently no public dataset that jointly provides high-density LiDAR and high-resolution oblique imagery with 3D semantic labels for power-line assets, creating a gap in available resources for multi-modal 3D semantic segmentation research.

Method: The dataset pairs high-density LiDAR point clouds with high-resolution oblique imagery, annotated into 11 semantic classes. The authors provide predefined splits, mIoU metrics, and establish baselines including unimodal (LiDAR-only, image-only) and multi-modal fusion approaches.

Result: On GridNet-HD, fusion models outperform the best unimodal baseline by +5.55 mIoU, demonstrating the complementarity of geometry (from LiDAR) and appearance (from imagery) for 3D semantic segmentation of electrical infrastructure.

Conclusion: GridNet-HD fills an important gap in available datasets for power-line asset analysis, showing that multi-modal fusion significantly improves performance over unimodal approaches, and the dataset, baselines, and codes are publicly available.

Abstract: This paper presents GridNet-HD, a multi-modal dataset for 3D semantic segmentation of overhead electrical infrastructures, pairing high-density LiDAR with high-resolution oblique imagery. The dataset comprises 7,694 images and 2.5 billion points annotated into 11 classes, with predefined splits and mIoU metrics. Unimodal (LiDAR-only, image-only) and multi-modal fusion baselines are provided. On GridNet-HD, fusion models outperform the best unimodal baseline by +5.55 mIoU, highlighting the complementarity of geometry and appearance. As reviewed in Sec. 2, no public dataset jointly provides high-density LiDAR and high-resolution oblique imagery with 3D semantic labels for power-line assets. Dataset, baselines, and codes are available: https://huggingface.co/collections/heig-vd-geo/gridnet-hd.

</details>


### [164] [Prototype Learning-Based Few-Shot Segmentation for Low-Light Crack on Concrete Structures](https://arxiv.org/abs/2601.13059)
*Yulun Guo*

Main category: cs.CV

TL;DR: A dual-branch prototype learning network combining Retinex theory with few-shot learning for low-light crack segmentation, achieving SOTA performance with minimal annotation requirements.


<details>
  <summary>Details</summary>
Motivation: Real-world cracks often appear in low-light environments (tunnels, bridge undersides) where computer vision segmentation accuracy degrades. Pixel-level annotation of low-light crack images is extremely time-consuming, but most deep learning methods require large, well-illuminated datasets.

Method: Dual-branch prototype learning network integrating Retinex theory with few-shot learning. Retinex-based reflectance components guide illumination-invariant global representation learning. Metric learning reduces dependence on large annotated datasets. Includes cross-similarity prior mask generation module that computes high-dimensional similarities between query and support features to capture crack location and structure, and multi-scale feature enhancement module that fuses multi-scale features with prior mask to alleviate spatial inconsistency.

Result: Extensive experiments on multiple benchmarks demonstrate consistent state-of-the-art performance under low-light conditions.

Conclusion: The proposed method effectively addresses low-light crack segmentation with minimal annotation requirements by combining Retinex theory for illumination invariance with few-shot learning for data efficiency, achieving superior performance in challenging low-light environments.

Abstract: Crack detection is critical for concrete infrastructure safety, but real-world cracks often appear in low-light environments like tunnels and bridge undersides, degrading computer vision segmentation accuracy. Pixel-level annotation of low-light crack images is extremely time-consuming, yet most deep learning methods require large, well-illuminated datasets. We propose a dual-branch prototype learning network integrating Retinex theory with few-shot learning for low-light crack segmentation. Retinex-based reflectance components guide illumination-invariant global representation learning, while metric learning reduces dependence on large annotated datasets. We introduce a cross-similarity prior mask generation module that computes high-dimensional similarities between query and support features to capture crack location and structure, and a multi-scale feature enhancement module that fuses multi-scale features with the prior mask to alleviate spatial inconsistency. Extensive experiments on multiple benchmarks demonstrate consistent state-of-the-art performance under low-light conditions. Code: https://github.com/YulunGuo/CrackFSS.

</details>


### [165] [Patient-Conditioned Adaptive Offsets for Reliable Diagnosis across Subgroups](https://arxiv.org/abs/2601.13094)
*Gelei Xu,Yuying Duan,Jun Xia,Ruining Deng,Wei Jin,Yiyu Shi*

Main category: cs.CV

TL;DR: HyperAdapt: A patient-conditioned adaptation framework that improves subgroup reliability in medical AI while maintaining a shared diagnostic model, outperforming baselines on medical imaging benchmarks.


<details>
  <summary>Details</summary>
Motivation: Medical AI models often show uneven performance across patient populations due to heterogeneity in disease prevalence, imaging appearance, and clinical risk profiles. Traditional fairness approaches suppress sensitive attributes, but in medical settings these attributes carry essential diagnostic information, and removing them can degrade accuracy and reliability in high-stakes applications.

Method: HyperAdapt encodes clinically relevant attributes (age, sex) into compact embeddings that condition a hypernetwork-style module. This generates small residual modulation parameters for selected layers of a shared backbone model, preserving general medical knowledge while enabling targeted adjustments for patient-specific variability. Adaptations are constrained through low-rank and bottlenecked parameterizations for efficiency and robustness.

Result: Experiments across multiple public medical imaging benchmarks show consistent improvement in subgroup-level performance without sacrificing overall accuracy. On PAD-UFES-20 dataset, the method outperforms the strongest competing baseline by 4.1% in recall and 4.4% in F1 score, with larger gains for underrepresented patient populations.

Conclusion: HyperAdapt provides a clinically-inspired approach to subgroup-aware medical AI that leverages rather than suppresses patient attributes, improving reliability across diverse populations while maintaining model efficiency and preserving learned medical knowledge.

Abstract: AI models for medical diagnosis often exhibit uneven performance across patient populations due to heterogeneity in disease prevalence, imaging appearance, and clinical risk profiles. Existing algorithmic fairness approaches typically seek to reduce such disparities by suppressing sensitive attributes. However, in medical settings these attributes often carry essential diagnostic information, and removing them can degrade accuracy and reliability, particularly in high-stakes applications. In contrast, clinical decision making explicitly incorporates patient context when interpreting diagnostic evidence, suggesting a different design direction for subgroup-aware models. In this paper, we introduce HyperAdapt, a patient-conditioned adaptation framework that improves subgroup reliability while maintaining a shared diagnostic model. Clinically relevant attributes such as age and sex are encoded into a compact embedding and used to condition a hypernetwork-style module, which generates small residual modulation parameters for selected layers of a shared backbone. This design preserves the general medical knowledge learned by the backbone while enabling targeted adjustments that reflect patient-specific variability. To ensure efficiency and robustness, adaptations are constrained through low-rank and bottlenecked parameterizations, limiting both model complexity and computational overhead. Experiments across multiple public medical imaging benchmarks demonstrate that the proposed approach consistently improves subgroup-level performance without sacrificing overall accuracy. On the PAD-UFES-20 dataset, our method outperforms the strongest competing baseline by 4.1% in recall and 4.4% in F1 score, with larger gains observed for underrepresented patient populations.

</details>


### [166] [A Streamlined Attention-Based Network for Descriptor Extraction](https://arxiv.org/abs/2601.13126)
*Mattia D'Urso,Emanuele Santellani,Christian Sormann,Mattia Rossi,Andreas Kuhn,Friedrich Fraundorfer*

Main category: cs.CV

TL;DR: SANDesc is a lightweight attention-based descriptor network that improves matching performance without modifying keypoint detectors, achieving better results with only 2.4M parameters.


<details>
  <summary>Details</summary>
Motivation: To create an efficient descriptor extraction network that can improve matching performance when used with existing keypoint detectors, without requiring detector modifications.

Method: Uses a revised U-Net-like architecture with Convolutional Block Attention Modules and residual paths (Residual U-Net Blocks with Attention), trained with modified triplet loss and curriculum learning-inspired hard negative mining.

Result: Outperforms original keypoint descriptors on HPatches, MegaDepth-1500, and Image Matching Challenge 2021 benchmarks. Also introduces new urban 4K dataset where SANDesc achieves substantial gains with limited computational resources.

Conclusion: SANDesc provides an effective, computationally efficient solution for descriptor extraction that can enhance existing keypoint detectors without modification, demonstrating strong performance across multiple benchmarks with minimal parameters.

Abstract: We introduce SANDesc, a Streamlined Attention-Based Network for Descriptor extraction that aims to improve on existing architectures for keypoint description.
  Our descriptor network learns to compute descriptors that improve matching without modifying the underlying keypoint detector. We employ a revised U-Net-like architecture enhanced with Convolutional Block Attention Modules and residual paths, enabling effective local representation while maintaining computational efficiency. We refer to the building blocks of our model as Residual U-Net Blocks with Attention. The model is trained using a modified triplet loss in combination with a curriculum learning-inspired hard negative mining strategy, which improves training stability.
  Extensive experiments on HPatches, MegaDepth-1500, and the Image Matching Challenge 2021 show that training SANDesc on top of existing keypoint detectors leads to improved results on multiple matching tasks compared to the original keypoint descriptors. At the same time, SANDesc has a model complexity of just 2.4 million parameters.
  As a further contribution, we introduce a new urban dataset featuring 4K images and pre-calibrated intrinsics, designed to evaluate feature extractors. On this benchmark, SANDesc achieves substantial performance gains over the existing descriptors while operating with limited computational resources.

</details>


### [167] [PhaseMark: A Post-hoc, Optimization-Free Watermarking of AI-generated Images in the Latent Frequency Domain](https://arxiv.org/abs/2601.13128)
*Sung Ju Lee,Nam Ik Cho*

Main category: cs.CV

TL;DR: PhaseMark is a fast, optimization-free watermarking framework for Latent Diffusion Models that modulates phase in VAE latent frequency domain, achieving state-of-the-art resilience with thousands of times speed improvement.


<details>
  <summary>Details</summary>
Motivation: Existing watermarking methods for Latent Diffusion Models are too slow due to iterative optimization or inversion processes, creating a need for faster yet robust watermarking solutions.

Method: Single-shot, optimization-free framework that directly modulates the phase in the VAE latent frequency domain, analyzing four modulation variants to explore performance-quality trade-offs.

Result: Thousands of times faster than optimization-based techniques while achieving state-of-the-art resilience against severe attacks (including regeneration) without degrading image quality.

Conclusion: PhaseMark demonstrates a new paradigm where efficient, resilient watermarking is achieved by exploiting intrinsic latent properties rather than iterative optimization.

Abstract: The proliferation of hyper-realistic images from Latent Diffusion Models (LDMs) demands robust watermarking, yet existing post-hoc methods are prohibitively slow due to iterative optimization or inversion processes. We introduce PhaseMark, a single-shot, optimization-free framework that directly modulates the phase in the VAE latent frequency domain. This approach makes PhaseMark thousands of times faster than optimization-based techniques while achieving state-of-the-art resilience against severe attacks, including regeneration, without degrading image quality. We analyze four modulation variants, revealing a clear performance-quality trade-off. PhaseMark demonstrates a new paradigm where efficient, resilient watermarking is achieved by exploiting intrinsic latent properties.

</details>


### [168] [GaussExplorer: 3D Gaussian Splatting for Embodied Exploration and Reasoning](https://arxiv.org/abs/2601.13132)
*Kim Yu-Ji,Dahye Lee,Kim Jun-Seong,GeonU Kim,Nam Hyeon-Woo,Yongjin Kwon,Yu-Chiang Frank Wang,Jaesung Choe,Tae-Hyun Oh*

Main category: cs.CV

TL;DR: GaussExplorer is a framework that combines 3D Gaussian Splatting with Vision-Language Models to enable embodied exploration and reasoning in 3D scenes, outperforming existing methods on complex compositional queries.


<details>
  <summary>Details</summary>
Motivation: Prior approaches using language-embedded 3DGS struggle with complex compositional queries, while object-centric RGB-D methods are limited by pre-fixed viewpoints. There's a need for better embodied exploration and reasoning in 3D scenes.

Method: Integrates Vision-Language Models on top of 3D Gaussian Splatting. First identifies pre-captured images most relevant to query questions, then adjusts them into novel viewpoints to capture better visual information for VLM reasoning.

Result: Outperforms existing methods on several benchmarks, demonstrating effectiveness of integrating VLM-based reasoning with 3DGS for embodied tasks.

Conclusion: GaussExplorer successfully enables question-driven exploration and reasoning in 3D scenes by combining 3DGS with VLMs, addressing limitations of previous approaches for complex compositional queries.

Abstract: We present GaussExplorer, a framework for embodied exploration and reasoning built on 3D Gaussian Splatting (3DGS). While prior approaches to language-embedded 3DGS have made meaningful progress in aligning simple text queries with Gaussian embeddings, they are generally optimized for relatively simple queries and struggle to interpret more complex, compositional language queries. Alternative studies based on object-centric RGB-D structured memories provide spatial grounding but are constrained by pre-fixed viewpoints. To address these issues, GaussExplorer introduces Vision-Language Models (VLMs) on top of 3DGS to enable question-driven exploration and reasoning within 3D scenes. We first identify pre-captured images that are most correlated with the query question, and subsequently adjust them into novel viewpoints to more accurately capture visual information for better reasoning by VLMs. Experiments show that ours outperforms existing methods on several benchmarks, demonstrating the effectiveness of integrating VLM-based reasoning with 3DGS for embodied tasks.

</details>


### [169] [CLIP-Guided Adaptable Self-Supervised Learning for Human-Centric Visual Tasks](https://arxiv.org/abs/2601.13133)
*Mingshuang Luo,Ruibing Hou,Bo Chao,Hong Chang,Zimo Liu,Yaowei Wang,Shiguang Shan*

Main category: cs.CV

TL;DR: CLASP is an unsupervised pre-training framework for human-centric visual tasks that uses CLIP to generate multi-level semantic pseudo-labels and a Prompt-Controlled MoE module for adaptive feature extraction.


<details>
  <summary>Details</summary>
Motivation: With large-scale unlabeled human image datasets available, there's a need for a general unsupervised pre-training model that can support diverse human-centric downstream tasks like surveillance, healthcare, and human-computer interaction.

Method: CLASP leverages CLIP to generate low-level (body parts) and high-level (attributes) semantic pseudo-labels, integrates them into visual representations, and uses a Prompt-Controlled Mixture-of-Experts module to dynamically adapt feature extraction based on task-specific prompts.

Result: Extensive experiments across multiple benchmarks show that CLASP consistently outperforms existing unsupervised pre-training methods for human-centric visual analysis.

Conclusion: CLASP advances the field of human-centric visual analysis by providing an effective unsupervised pre-training framework that can adapt to different downstream tasks with varying semantic granularity requirements.

Abstract: Human-centric visual analysis plays a pivotal role in diverse applications, including surveillance, healthcare, and human-computer interaction. With the emergence of large-scale unlabeled human image datasets, there is an increasing need for a general unsupervised pre-training model capable of supporting diverse human-centric downstream tasks. To achieve this goal, we propose CLASP (CLIP-guided Adaptable Self-suPervised learning), a novel framework designed for unsupervised pre-training in human-centric visual tasks. CLASP leverages the powerful vision-language model CLIP to generate both low-level (e.g., body parts) and high-level (e.g., attributes) semantic pseudo-labels. These multi-level semantic cues are then integrated into the learned visual representations, enriching their expressiveness and generalizability. Recognizing that different downstream tasks demand varying levels of semantic granularity, CLASP incorporates a Prompt-Controlled Mixture-of-Experts (MoE) module. MoE dynamically adapts feature extraction based on task-specific prompts, mitigating potential feature conflicts and enhancing transferability. Furthermore, CLASP employs a multi-task pre-training strategy, where part- and attribute-level pseudo-labels derived from CLIP guide the representation learning process. Extensive experiments across multiple benchmarks demonstrate that CLASP consistently outperforms existing unsupervised pre-training methods, advancing the field of human-centric visual analysis.

</details>


### [170] [TVWorld: Foundations for Remote-Control TV Agents](https://arxiv.org/abs/2601.13142)
*Zhantao Ma,Quanfeng Lu,Shuai Zhong,Dahai Yu,Ping Luo,Michael K. Ng*

Main category: cs.CV

TL;DR: TVWorld: A graph-based abstraction for evaluating TV navigation capabilities in vision-language models, with two benchmarks revealing topology awareness limitations and a proposed training framework achieving SOTA performance.


<details>
  <summary>Details</summary>
Motivation: Existing LVLMs focus on point-and-click interaction but neglect remote-control interaction common in everyday TV usage, creating a gap in evaluating TV navigation capabilities.

Method: 1) Introduce TVWorld - offline graph-based abstraction of real-world TV navigation; 2) Create two benchmarks: TVWorld-N for topology-aware navigation and TVWorld-G for focus-aware grounding; 3) Propose Topology-Aware Training framework to inject topology awareness into LVLMs; 4) Develop TVTheseus as a specialized foundation model.

Result: TVTheseus achieves 68.3% success rate on TVWorld-N, surpassing strong closed-source baselines like Gemini 3 Flash and establishing state-of-the-art performance. Benchmarks reveal key limitation: insufficient topology awareness for focus-based, long-horizon TV navigation.

Conclusion: The proposed TVWorld framework and Topology-Aware Training effectively address the underexplored remote-control interaction problem, with TVTheseus demonstrating superior TV navigation capabilities and providing valuable insights for developing effective TV-use agents.

Abstract: Recent large vision-language models (LVLMs) have demonstrated strong potential for device control. However, existing research has primarily focused on point-and-click (PnC) interaction, while remote-control (RC) interaction commonly encountered in everyday TV usage remains largely underexplored. To fill this gap, we introduce \textbf{TVWorld}, an offline graph-based abstraction of real-world TV navigation that enables reproducible and deployment-free evaluation. On this basis, we derive two complementary benchmarks that comprehensively assess TV-use capabilities: \textbf{TVWorld-N} for topology-aware navigation and \textbf{TVWorld-G} for focus-aware grounding. These benchmarks expose a key limitation of existing agents: insufficient topology awareness for focus-based, long-horizon TV navigation. Motivated by this finding, we propose a \emph{Topology-Aware Training} framework that injects topology awareness into LVLMs. Using this framework, we develop \textbf{TVTheseus}, a foundation model specialized for TV navigation. TVTheseus achieves a success rate of $68.3\%$ on TVWorld-N, surpassing strong closed-source baselines such as Gemini 3 Flash and establishing state-of-the-art (SOTA) performance. Additional analyses further provide valuable insights into the development of effective TV-use agents.

</details>


### [171] [ICo3D: An Interactive Conversational 3D Virtual Human](https://arxiv.org/abs/2601.13148)
*Richard Shaw,Youngkyoon Jang,Athanasios Papaioannou,Arthur Moreau,Helisa Dhamo,Zhensong Zhang,Eduardo Pérez-Pellitero*

Main category: cs.CV

TL;DR: ICo3D creates interactive, photorealistic 3D human avatars with conversational AI, using Gaussian splatting for face/body animation and LLM integration for real-time interaction.


<details>
  <summary>Details</summary>
Motivation: To create fully integrated virtual human avatars that are interactive, conversational, and photorealistic for applications in gaming, virtual assistance, and personalized education.

Method: Combines multi-view captures to create animatable 3D face (HeadGaS++) and dynamic 3D body (SWinGS++) models using Gaussian splatting, merges them without artifacts, and integrates LLM for conversation with audio-driven facial animation.

Result: A real-time conversational 3D avatar system with precise audio-visual synchronization, enhanced photorealism, and support for both oral and written interactions in immersive environments.

Conclusion: ICo3D provides a complete virtual avatar experience applicable to diverse fields, demonstrating practical use cases for real-time human-AI interaction.

Abstract: This work presents Interactive Conversational 3D Virtual Human (ICo3D), a method for generating an interactive, conversational, and photorealistic 3D human avatar. Based on multi-view captures of a subject, we create an animatable 3D face model and a dynamic 3D body model, both rendered by splatting Gaussian primitives. Once merged together, they represent a lifelike virtual human avatar suitable for real-time user interactions. We equip our avatar with an LLM for conversational ability. During conversation, the audio speech of the avatar is used as a driving signal to animate the face model, enabling precise synchronization. We describe improvements to our dynamic Gaussian models that enhance photorealism: SWinGS++ for body reconstruction and HeadGaS++ for face reconstruction, and provide as well a solution to merge the separate face and body models without artifacts. We also present a demo of the complete system, showcasing several use cases of real-time conversation with the 3D avatar. Our approach offers a fully integrated virtual avatar experience, supporting both oral and written form interactions in immersive environments. ICo3D is applicable to a wide range of fields, including gaming, virtual assistance, and personalized education, among others. Project page: https://ico3d.github.io/

</details>


### [172] [From 100,000+ images to winning the first brain MRI foundation model challenges: Sharing lessons and models](https://arxiv.org/abs/2601.13166)
*Pedro M. Gordaliza,Jaume Banus,Benoît Gérin,Maxence Wynen,Nataliia Molchanova,Jonas Richiardi,Meritxell Bach Cuadra*

Main category: cs.CV

TL;DR: A U-Net CNN solution won both SSL3D and FOMO25 brain MRI challenges at MICCAI 2025, training much faster and being smaller than transformer approaches.


<details>
  <summary>Details</summary>
Motivation: To develop efficient Foundation Models for medical image analysis that overcome unique challenges in radiological tasks, particularly for 3D brain MRI.

Method: U-Net CNN architecture combined with strategies leveraging anatomical priors and neuroimaging domain knowledge.

Result: Ranked first in both SSL3D and FOMO25 challenge tracks at MICCAI 2025. Models trained 1-2 orders of magnitude faster and were 10 times smaller than competing transformer-based approaches.

Conclusion: The proposed U-Net CNN approach with domain-specific strategies provides an efficient and effective alternative to transformer-based methods for 3D brain MRI analysis, achieving top performance with significantly reduced computational requirements.

Abstract: Developing Foundation Models for medical image analysis is essential to overcome the unique challenges of radiological tasks. The first challenges of this kind for 3D brain MRI, SSL3D and FOMO25, were held at MICCAI 2025. Our solution ranked first in tracks of both contests. It relies on a U-Net CNN architecture combined with strategies leveraging anatomical priors and neuroimaging domain knowledge. Notably, our models trained 1-2 orders of magnitude faster and were 10 times smaller than competing transformer-based approaches. Models are available here: https://github.com/jbanusco/BrainFM4Challenges.

</details>


### [173] [GTPred: Benchmarking MLLMs for Interpretable Geo-localization and Time-of-capture Prediction](https://arxiv.org/abs/2601.13207)
*Jinnao Li,Zijian Chen,Tingzhu Chen,Changbo Wang*

Main category: cs.CV

TL;DR: GTPred is a new benchmark for geo-temporal prediction that evaluates multi-modal LLMs on jointly predicting location and temporal information from images, revealing current models' limitations in world knowledge and temporal reasoning despite strong visual perception.


<details>
  <summary>Details</summary>
Motivation: Existing geo-localization benchmarks ignore temporal information in images, which can provide additional constraints for location prediction. There's a gap in evaluating models' ability to reason about both geographic and temporal aspects simultaneously.

Method: Created GTPred benchmark with 370 globally distributed images spanning 120+ years. Evaluated 15 MLLMs (8 proprietary, 7 open-source) using joint year and hierarchical location sequence matching, plus assessment of intermediate reasoning chains against annotated ground-truth reasoning processes.

Result: Current MLLMs show strong visual perception but limited world knowledge and geo-temporal reasoning capabilities. Incorporating temporal information significantly enhances location inference performance compared to location-only prediction.

Conclusion: The GTPred benchmark highlights the need for improved geo-temporal reasoning in MLLMs and demonstrates the value of temporal information for enhancing geo-localization accuracy, pointing to future research directions in multi-modal temporal reasoning.

Abstract: Geo-localization aims to infer the geographic location where an image was captured using observable visual evidence. Traditional methods achieve impressive results through large-scale training on massive image corpora. With the emergence of multi-modal large language models (MLLMs), recent studies have explored their applications in geo-localization, benefiting from improved accuracy and interpretability. However, existing benchmarks largely ignore the temporal information inherent in images, which can further constrain the location. To bridge this gap, we introduce GTPred, a novel benchmark for geo-temporal prediction. GTPred comprises 370 globally distributed images spanning over 120 years. We evaluate MLLM predictions by jointly considering year and hierarchical location sequence matching, and further assess intermediate reasoning chains using meticulously annotated ground-truth reasoning processes. Experiments on 8 proprietary and 7 open-source MLLMs show that, despite strong visual perception, current models remain limited in world knowledge and geo-temporal reasoning. Results also demonstrate that incorporating temporal information significantly enhances location inference performance.

</details>


### [174] [Rethinking Skip Connections: Additive U-Net for Robust and Interpretable Denoising](https://arxiv.org/abs/2601.13208)
*Vikram R Lakkavalli*

Main category: cs.CV

TL;DR: Additive U-Net replaces concatenative skip connections with gated additive connections using learnable non-negative scalars, providing interpretable control over encoder contributions while avoiding channel inflation, achieving competitive denoising performance with natural feature progression learning.


<details>
  <summary>Details</summary>
Motivation: Standard concatenation skip connections in U-Net architectures double channel dimensionality and obscure information flow, allowing uncontrolled noise transfer between encoder and decoder pathways.

Method: Proposes Additive U-Net which replaces concatenative skip connections with gated additive connections. Each skip pathway is scaled by a learnable non-negative scalar, offering explicit and interpretable control over encoder contributions while avoiding channel inflation.

Result: Evaluations on Kodak-17 denoising benchmark show competitive PSNR/SSIM at noise levels σ = 15, 25, 50, with robustness across kernel schedules and depths. Effective denoising achieved even without explicit down/up-sampling or forced hierarchies.

Conclusion: Additive skips serve as a lightweight and interpretable alternative to concatenation, enabling efficient design and clearer understanding of multi-scale information transfer in reconstruction networks, with the model naturally learning a progression from high-frequency to band-pass to low-frequency features.

Abstract: Skip connections are central to U-Net architectures for image denoising, but standard concatenation doubles channel dimensionality and obscures information flow, allowing uncontrolled noise transfer. We propose the Additive U-Net, which replaces concatenative skips with gated additive connections. Each skip pathway is scaled by a learnable non-negative scalar, offering explicit and interpretable control over encoder contributions while avoiding channel inflation. Evaluations on the Kodak-17 denoising benchmark show that Additive U-Net achieves competitive PSNR/SSIM at noise levels σ = 15, 25, 50, with robustness across kernel schedules and depths. Notably, effective denoising is achieved even without explicit down/up-sampling or forced hierarchies, as the model naturally learns a progression from high-frequency to band-pass to low-frequency features. These results position additive skips as a lightweight and interpretable alternative to concatenation, enabling both efficient design and a clearer understanding of multi-scale information transfer in reconstruction networks.

</details>


### [175] [ObjectVisA-120: Object-based Visual Attention Prediction in Interactive Street-crossing Environments](https://arxiv.org/abs/2601.13218)
*Igor Vozniak,Philipp Mueller,Nils Lipp,Janis Sprenger,Konstantin Poddubnyy,Davit Hovhannisyan,Christian Mueller,Andreas Bulling,Philipp Slusallek*

Main category: cs.CV

TL;DR: A new VR dataset (StreetNav) for object-based visual attention research with novel oSIM metric and SUMGraph model that outperforms SOTA methods.


<details>
  <summary>Details</summary>
Motivation: Object-based attention is well-established in cognitive science but underrepresented in computational models due to lack of suitable datasets and evaluation metrics for object-based attention assessment.

Method: Created a 120-participant VR dataset of street-crossing navigation with gaze data, complete object state-space, variable complexities, and rich annotations. Proposed object-based similarity (oSIM) metric and developed SUMGraph - a Mamba U-Net-based model with graph representation of critical scene objects (vehicles).

Result: Explicit optimization for object-based attention improves oSIM performance and boosts performance on common metrics. SUMGraph outperforms several state-of-the-art visual attention prediction methods.

Conclusion: The dataset addresses ethical/safety challenges of real-world data collection, enabling object-based attention research. The proposed oSIM metric and SUMGraph model advance computational visual attention modeling toward more human-like object-based processing.

Abstract: The object-based nature of human visual attention is well-known in cognitive science, but has only played a minor role in computational visual attention models so far. This is mainly due to a lack of suitable datasets and evaluation metrics for object-based attention. To address these limitations, we present \dataset~ -- a novel 120-participant dataset of spatial street-crossing navigation in virtual reality specifically geared to object-based attention evaluations. The uniqueness of the presented dataset lies in the ethical and safety affiliated challenges that make collecting comparable data in real-world environments highly difficult. \dataset~ not only features accurate gaze data and a complete state-space representation of objects in the virtual environment, but it also offers variable scenario complexities and rich annotations, including panoptic segmentation, depth information, and vehicle keypoints. We further propose object-based similarity (oSIM) as a novel metric to evaluate the performance of object-based visual attention models, a previously unexplored performance characteristic. Our evaluations show that explicitly optimising for object-based attention not only improves oSIM performance but also leads to an improved model performance on common metrics. In addition, we present SUMGraph, a Mamba U-Net-based model, which explicitly encodes critical scene objects (vehicles) in a graph representation, leading to further performance improvements over several state-of-the-art visual attention prediction methods. The dataset, code and models will be publicly released.

</details>


### [176] [Not all Blends are Equal: The BLEMORE Dataset of Blended Emotion Expressions with Relative Salience Annotations](https://arxiv.org/abs/2601.13225)
*Tim Lachmann,Alexandra Israelsson,Christina Tornberg,Teimuraz Saghinadze,Michal Balazia,Philipp Müller,Petri Laukka*

Main category: cs.CV

TL;DR: BLEMORE is a novel multimodal dataset for blended emotion recognition with relative salience annotations, enabling research beyond single-emotion classification.


<details>
  <summary>Details</summary>
Motivation: Current emotion recognition systems focus on single emotions, ignoring blended emotions with varying salience levels, due to lack of appropriate datasets with salience annotations.

Method: Created BLEMORE dataset with 3,000+ multimodal clips from 58 actors, covering 6 basic emotions and 10 blends with 3 salience configurations (50/50, 70/30, 30/70). Evaluated state-of-the-art video classification approaches on presence and salience prediction tasks.

Result: Unimodal classifiers achieved 29% presence accuracy and 13% salience accuracy; multimodal methods improved to 35% presence (ImageBind+WavLM) and 18% salience (HiCMAE). Best test set results: 33% presence (VideoMAEv2+HuBERT) and 18% salience (HiCMAE).

Conclusion: BLEMORE provides a valuable resource for advancing emotion recognition systems that account for the complexity of blended emotions, showing multimodal approaches outperform unimodal ones for this challenging task.

Abstract: Humans often experience not just a single basic emotion at a time, but rather a blend of several emotions with varying salience. Despite the importance of such blended emotions, most video-based emotion recognition approaches are designed to recognize single emotions only. The few approaches that have attempted to recognize blended emotions typically cannot assess the relative salience of the emotions within a blend. This limitation largely stems from the lack of datasets containing a substantial number of blended emotion samples annotated with relative salience. To address this shortcoming, we introduce BLEMORE, a novel dataset for multimodal (video, audio) blended emotion recognition that includes information on the relative salience of each emotion within a blend. BLEMORE comprises over 3,000 clips from 58 actors, performing 6 basic emotions and 10 distinct blends, where each blend has 3 different salience configurations (50/50, 70/30, and 30/70). Using this dataset, we conduct extensive evaluations of state-of-the-art video classification approaches on two blended emotion prediction tasks: (1) predicting the presence of emotions in a given sample, and (2) predicting the relative salience of emotions in a blend. Our results show that unimodal classifiers achieve up to 29% presence accuracy and 13% salience accuracy on the validation set, while multimodal methods yield clear improvements, with ImageBind + WavLM reaching 35% presence accuracy and HiCMAE 18% salience accuracy. On the held-out test set, the best models achieve 33% presence accuracy (VideoMAEv2 + HuBERT) and 18% salience accuracy (HiCMAE). In sum, the BLEMORE dataset provides a valuable resource to advancing research on emotion recognition systems that account for the complexity and significance of blended emotion expressions.

</details>


### [177] [ConvMambaNet: A Hybrid CNN-Mamba State Space Architecture for Accurate and Real-Time EEG Seizure Detection](https://arxiv.org/abs/2601.13234)
*Md. Nishan Khan,Kazi Shahriar Sanjid,Md. Tanzim Hossain,Asib Mostakim Fony,Istiak Ahmed,M. Monir Uddin*

Main category: cs.CV

TL;DR: ConvMambaNet: Hybrid CNN-Mamba model achieves 99% accuracy for EEG seizure detection on CHB-MIT dataset, addressing temporal complexity and class imbalance challenges.


<details>
  <summary>Details</summary>
Motivation: Epilepsy monitoring via EEG is challenging due to temporal complexity of signals and limitations of current automated analysis methods. There's a need for better temporal feature extraction to enable precise seizure detection for real-time clinical applications.

Method: ConvMambaNet integrates Convolutional Neural Networks (CNNs) with Mamba Structured State Space Model (SSM) blocks. This hybrid approach combines CNN's spatial feature extraction with Mamba-SSM's long-range temporal dynamics modeling, embedded within a CNN framework.

Result: Achieved 99% accuracy on CHB-MIT Scalp EEG dataset with robust performance under severe class imbalance conditions, demonstrating superior seizure detection capability.

Conclusion: ConvMambaNet shows strong potential for precise and efficient seizure detection, offering a viable path toward real-time, automated epilepsy monitoring in clinical environments by effectively capturing both spatial and long-range temporal dynamics.

Abstract: Epilepsy is a chronic neurological disorder marked by recurrent seizures that can severely impact quality of life. Electroencephalography (EEG) remains the primary tool for monitoring neural activity and detecting seizures, yet automated analysis remains challenging due to the temporal complexity of EEG signals. This study introduces ConvMambaNet, a hybrid deep learning model that integrates Convolutional Neural Networks (CNNs) with the Mamba Structured State Space Model (SSM) to enhance temporal feature extraction. By embedding the Mamba-SSM block within a CNN framework, the model effectively captures both spatial and long-range temporal dynamics. Evaluated on the CHB-MIT Scalp EEG dataset, ConvMambaNet achieved a 99% accuracy and demonstrated robust performance under severe class imbalance. These results underscore the model's potential for precise and efficient seizure detection, offering a viable path toward real-time, automated epilepsy monitoring in clinical environments.

</details>


### [178] [A Semantic Decoupling-Based Two-Stage Rainy-Day Attack for Revealing Weather Robustness Deficiencies in Vision-Language Models](https://arxiv.org/abs/2601.13238)
*Chengyin Hu,Xiang Chen,Zhe Jia,Weiwen Shi,Fengyu Zhang,Jiujiang Guo,Yiwei Wei*

Main category: cs.CV

TL;DR: The paper introduces the first adversarial framework using realistic weather perturbations to attack Vision-Language Models, showing that physically plausible rain conditions can cause significant semantic misalignment in VLMs.


<details>
  <summary>Details</summary>
Motivation: VLMs are trained on canonical visual conditions but their robustness to real-world weather conditions and stability of cross-modal semantic alignment under structured perturbations remain insufficiently studied. The paper focuses on rainy scenarios to address this gap.

Method: Two-stage parameterized perturbation model: Stage 1 applies low-dimensional global modulation to weaken semantic decision boundaries; Stage 2 introduces structured rain variations by modeling multi-scale raindrop appearance and illumination changes, optimizing the non-differentiable weather space to induce semantic shifts.

Result: Experiments show that physically plausible, highly constrained weather perturbations can induce substantial semantic misalignment in mainstream VLMs, posing safety and reliability risks. Ablations confirm illumination modeling and multi-scale raindrop structures are key drivers of semantic shifts.

Conclusion: The framework demonstrates that realistic weather conditions can significantly degrade VLM performance, highlighting potential safety risks in real-world deployment and emphasizing the need for more robust VLMs that account for environmental variations.

Abstract: Vision-Language Models (VLMs) are trained on image-text pairs collected under canonical visual conditions and achieve strong performance on multimodal tasks. However, their robustness to real-world weather conditions, and the stability of cross-modal semantic alignment under such structured perturbations, remain insufficiently studied. In this paper, we focus on rainy scenarios and introduce the first adversarial framework that exploits realistic weather to attack VLMs, using a two-stage, parameterized perturbation model based on semantic decoupling to analyze rain-induced shifts in decision-making. In Stage 1, we model the global effects of rainfall by applying a low-dimensional global modulation to condition the embedding space and gradually weaken the original semantic decision boundaries. In Stage 2, we introduce structured rain variations by explicitly modeling multi-scale raindrop appearance and rainfall-induced illumination changes, and optimize the resulting non-differentiable weather space to induce stable semantic shifts. Operating in a non-pixel parameter space, our framework generates perturbations that are both physically grounded and interpretable. Experiments across multiple tasks show that even physically plausible, highly constrained weather perturbations can induce substantial semantic misalignment in mainstream VLMs, posing potential safety and reliability risks in real-world deployment. Ablations further confirm that illumination modeling and multi-scale raindrop structures are key drivers of these semantic shifts.

</details>


### [179] [Deep Learning for Semantic Segmentation of 3D Ultrasound Data](https://arxiv.org/abs/2601.13263)
*Chenyu Liu,Marco Cecotti,Harikrishnan Vijayakumar,Patrick Robinson,James Barson,Mihai Caleap*

Main category: cs.CV

TL;DR: A novel framework for 3D semantic segmentation using solid-state ultrasound sensors (Calyo Pulse) as a complementary modality for autonomous vehicles in harsh environments.


<details>
  <summary>Details</summary>
Motivation: Current perception systems (LiDAR and cameras) have cost, robustness, and performance trade-offs, especially under adverse conditions. There's a need for cost-efficient and reliable alternatives that work well in harsh, cluttered environments.

Method: Introduces a learning-based framework using Calyo Pulse (modular solid-state 3D ultrasound sensors) with a 3D U-Net architecture trained on spatial ultrasound data for volumetric semantic segmentation.

Result: Demonstrates robust segmentation performance from Calyo Pulse sensors, with potential for further improvement through larger datasets, refined ground truth, and weighted loss functions.

Conclusion: 3D ultrasound sensing shows promise as a complementary modality for reliable autonomy, offering potential advantages over traditional LiDAR and camera systems in certain conditions.

Abstract: Developing cost-efficient and reliable perception systems remains a central challenge for automated vehicles. LiDAR and camera-based systems dominate, yet they present trade-offs in cost, robustness and performance under adverse conditions. This work introduces a novel framework for learning-based 3D semantic segmentation using Calyo Pulse, a modular, solid-state 3D ultrasound sensor system for use in harsh and cluttered environments. A 3D U-Net architecture is introduced and trained on the spatial ultrasound data for volumetric segmentation. Results demonstrate robust segmentation performance from Calyo Pulse sensors, with potential for further improvement through larger datasets, refined ground truth, and weighted loss functions. Importantly, this study highlights 3D ultrasound sensing as a promising complementary modality for reliable autonomy.

</details>


### [180] [Enginuity: Building an Open Multi-Domain Dataset of Complex Engineering Diagrams](https://arxiv.org/abs/2601.13299)
*Ethan Seefried,Prahitha Movva,Naga Harshita Marupaka,Tilak Kasturi,Tirthankar Ghosal*

Main category: cs.CV

TL;DR: Enginuity is the first open, large-scale, multi-domain engineering diagram dataset with comprehensive structural annotations for automated diagram parsing.


<details>
  <summary>Details</summary>
Motivation: Current AI systems cannot fully participate in scientific workflows because they lack the ability to comprehend and manipulate visual-structural knowledge in engineering diagrams, which is essential for hypothesis generation, experimental design, and discovery.

Method: Create a comprehensive dataset with hierarchical component relationships, connections, and semantic elements across diverse engineering domains, designed specifically for multimodal large language models.

Result: The proposed Enginuity dataset would enable critical downstream tasks including structured diagram parsing, cross-modal information retrieval, and AI-assisted engineering simulation.

Conclusion: Enginuity would be transformative for AI for Scientific Discovery by breaking down the fundamental barrier preventing AI from fully participating in scientific workflows that require diagram interpretation and visual reasoning.

Abstract: We propose Enginuity - the first open, large-scale, multi-domain engineering diagram dataset with comprehensive structural annotations designed for automated diagram parsing. By capturing hierarchical component relationships, connections, and semantic elements across diverse engineering domains, our proposed dataset would enable multimodal large language models to address critical downstream tasks including structured diagram parsing, cross-modal information retrieval, and AI-assisted engineering simulation. Enginuity would be transformative for AI for Scientific Discovery by enabling artificial intelligence systems to comprehend and manipulate the visual-structural knowledge embedded in engineering diagrams, breaking down a fundamental barrier that currently prevents AI from fully participating in scientific workflows where diagram interpretation, technical drawing analysis, and visual reasoning are essential for hypothesis generation, experimental design, and discovery.

</details>


### [181] [CausalSpatial: A Benchmark for Object-Centric Causal Spatial Reasoning](https://arxiv.org/abs/2601.13304)
*Wenxin Ma,Chenlong Wang,Ruisheng Yuan,Hao Chen,Nanru Dai,S. Kevin Zhou,Yijun Yang,Alan Yuille,Jieneng Chen*

Main category: cs.CV

TL;DR: MLLMs fail at causal spatial reasoning (predicting consequences of object motions), scoring only 54% vs human 84%. The paper introduces CausalSpatial benchmark and proposes COW framework that generates videos of hypothetical dynamics to ground reasoning in visual evidence.


<details>
  <summary>Details</summary>
Motivation: Current multimodal large language models (MLLMs) are limited to static spatial perception and cannot answer "what-if" questions about object motions in 3D scenes, while humans can instantly predict consequences like collisions. There's a need to evaluate and improve models' causal spatial reasoning abilities.

Method: 1) Created CausalSpatial benchmark with four tasks (Collision, Compatibility, Occlusion, Trajectory) to evaluate causal spatial reasoning. 2) Analyzed why MLLMs fail (over-reliance on textual chain-of-thought reasoning that drifts from visual evidence). 3) Proposed Causal Object World model (COW) framework that externalizes simulation by generating videos of hypothetical dynamics to provide explicit visual cues of causality.

Result: Humans score 84% on CausalSpatial benchmark while GPT-5 achieves only 54%, revealing a severe performance gap. Analysis shows models produce fluent but spatially ungrounded hallucinations due to over-reliance on textual reasoning disconnected from visual evidence.

Conclusion: MLLMs fundamentally lack causal spatial reasoning capabilities. The proposed COW framework addresses this by grounding reasoning in physical reality through explicit visual simulation, moving beyond linguistic priors. The benchmark and code are publicly available for further research.

Abstract: Humans can look at a static scene and instantly predict what happens next -- will moving this object cause a collision? We call this ability Causal Spatial Reasoning. However, current multimodal large language models (MLLMs) cannot do this, as they remain largely restricted to static spatial perception, struggling to answer "what-if" questions in a 3D scene. We introduce CausalSpatial, a diagnostic benchmark evaluating whether models can anticipate consequences of object motions across four tasks: Collision, Compatibility, Occlusion, and Trajectory. Results expose a severe gap: humans score 84% while GPT-5 achieves only 54%. Why do MLLMs fail? Our analysis uncovers a fundamental deficiency: models over-rely on textual chain-of-thought reasoning that drifts from visual evidence, producing fluent but spatially ungrounded hallucinations. To address this, we propose the Causal Object World model (COW), a framework that externalizes the simulation process by generating videos of hypothetical dynamics. With explicit visual cues of causality, COW enables models to ground their reasoning in physical reality rather than linguistic priors. We make the dataset and code publicly available here: https://github.com/CausalSpatial/CausalSpatial

</details>


### [182] [MultiST: A Cross-Attention-Based Multimodal Model for Spatial Transcriptomic](https://arxiv.org/abs/2601.13331)
*Wei Wang,Quoc-Toan Ly,Chong Yu,Jun Bai*

Main category: cs.CV

TL;DR: MultiST is a multimodal framework that integrates spatial topology, gene expression, and tissue morphology using cross-attention fusion to improve spatial domain boundary resolution in spatial transcriptomics data.


<details>
  <summary>Details</summary>
Motivation: Existing spatial transcriptomics methods lack effective integration of histological morphology with molecular profiles, using shallow fusion or omitting tissue images, which limits their ability to resolve ambiguous spatial domain boundaries.

Method: MultiST uses a unified multimodal framework with cross-attention-based fusion to jointly model spatial topology, gene expression, and tissue morphology. It employs graph-based gene encoders with adversarial alignment for robust spatial representations and integrates color-normalized histological features to capture molecular-morphological dependencies.

Result: Evaluated on 13 diverse ST datasets spanning human brain cortex and breast cancer tissue, MultiST yields spatial domains with clearer and more coherent boundaries than existing methods, leading to more stable pseudotime trajectories and more biologically interpretable cell-cell interaction patterns.

Conclusion: MultiST provides an effective multimodal framework that improves spatial domain boundary resolution by integrating histological morphology with molecular profiles, offering better biological interpretation of tissue organization and cell-cell interactions.

Abstract: Spatial transcriptomics (ST) enables transcriptome-wide profiling while preserving the spatial context of tissues, offering unprecedented opportunities to study tissue organization and cell-cell interactions in situ. Despite recent advances, existing methods often lack effective integration of histological morphology with molecular profiles, relying on shallow fusion strategies or omitting tissue images altogether, which limits their ability to resolve ambiguous spatial domain boundaries. To address this challenge, we propose MultiST, a unified multimodal framework that jointly models spatial topology, gene expression, and tissue morphology through cross-attention-based fusion. MultiST employs graph-based gene encoders with adversarial alignment to learn robust spatial representations, while integrating color-normalized histological features to capture molecular-morphological dependencies and refine domain boundaries. We evaluated the proposed method on 13 diverse ST datasets spanning two organs, including human brain cortex and breast cancer tissue. MultiST yields spatial domains with clearer and more coherent boundaries than existing methods, leading to more stable pseudotime trajectories and more biologically interpretable cell-cell interaction patterns. The MultiST framework and source code are available at https://github.com/LabJunBMI/MultiST.git.

</details>


### [183] [Real-Time 4D Radar Perception for Robust Human Detection in Harsh Enclosed Environments](https://arxiv.org/abs/2601.13364)
*Zhenan Liu,Yaodong Cui,Amir Khajepour,George Shaker*

Main category: cs.CV

TL;DR: Novel method for generating controlled dust concentrations in cluttered environments enables repeatable mm-wave propagation studies. Presents 4D mmWave radar dataset with camera/LiDAR augmentation, develops noise filtering framework, and achieves real-time pedestrian detection in dust-laden mining environments.


<details>
  <summary>Details</summary>
Motivation: Need to study mm-wave propagation in harsh, enclosed environments (mines, tunnels, collapsed buildings) with dust particles and reflective surfaces that impact sensing functionality. Current methods lack controlled, repeatable dust concentration generation and effective clutter mitigation.

Method: 1) Novel methodology for generating controlled multi-level dust concentrations in cluttered environments. 2) 4D mmWave radar dataset augmented with camera and LiDAR. 3) Threshold-based noise filtering framework using radar parameters (RCS, velocity, azimuth, elevation) to suppress ghost targets and multipath reflections. 4) Cluster-level, rule-based classification pipeline using radar semantics (velocity, RCS, volumetric spread) for pedestrian detection.

Result: Experimental results confirm significant enhancement in clutter mitigation, detection robustness, and overall system resilience in dust-laden mining environments. Achieves reliable, real-time pedestrian detection without extensive domain-specific training.

Conclusion: The integrated approach of controlled dust generation, multi-sensor dataset creation, noise filtering, and rule-based classification provides an effective solution for mm-wave sensing in harsh, dust-laden environments, enabling robust pedestrian detection in challenging conditions like underground mines.

Abstract: This paper introduces a novel methodology for generating controlled, multi-level dust concentrations in a highly cluttered environment representative of harsh, enclosed environments, such as underground mines, road tunnels, or collapsed buildings, enabling repeatable mm-wave propagation studies under severe electromagnetic constraints. We also present a new 4D mmWave radar dataset, augmented by camera and LiDAR, illustrating how dust particles and reflective surfaces jointly impact the sensing functionality. To address these challenges, we develop a threshold-based noise filtering framework leveraging key radar parameters (RCS, velocity, azimuth, elevation) to suppress ghost targets and mitigate strong multipath reflections at the raw data level. Building on the filtered point clouds, a cluster-level, rule-based classification pipeline exploits radar semantics-velocity, RCS, and volumetric spread-to achieve reliable, real-time pedestrian detection without extensive domainspecific training. Experimental results confirm that this integrated approach significantly enhances clutter mitigation, detection robustness, and overall system resilience in dust-laden mining environments.

</details>


### [184] [Spherical Geometry Diffusion: Generating High-quality 3D Face Geometry via Sphere-anchored Representations](https://arxiv.org/abs/2601.13371)
*Junyi Zhang,Yiming Wang,Yunhong Lu,Qichao Wang,Wenzhe Qian,Xiaoyin Xu,David Gu,Min Zhang*

Main category: cs.CV

TL;DR: Proposes Spherical Geometry Representation and Spherical Geometry Diffusion for high-quality text-to-3D face generation by constraining geometry to a topological sphere, enabling robust mesh connectivity and synergy with 2D generative models.


<details>
  <summary>Details</summary>
Motivation: Existing text-to-3D face generation methods struggle with poor geometry quality due to arbitrary vertex distributions that make it difficult to establish clean mesh connectivity, resulting in suboptimal 3D geometry.

Method: 1) Spherical Geometry Representation: anchors geometric signals to uniform spherical coordinates, ensuring regular point distribution for robust mesh reconstruction. 2) Spherical Geometry Diffusion: a conditional diffusion framework built on the 2D unwrapped sphere map that jointly models geometry and texture, with geometry explicitly conditioning texture synthesis.

Result: The method demonstrates success in text-to-3D generation, face reconstruction, and text-based 3D editing. Extensive experiments show substantial improvements over existing methods in geometric quality, textual fidelity, and inference efficiency.

Conclusion: By constraining 3D face geometry to a topological sphere and leveraging 2D generative models through spherical unwrapping, the approach achieves superior geometry quality and enables diverse, controllable 3D face generation from text.

Abstract: A fundamental challenge in text-to-3D face generation is achieving high-quality geometry. The core difficulty lies in the arbitrary and intricate distribution of vertices in 3D space, making it challenging for existing models to establish clean connectivity and resulting in suboptimal geometry. To address this, our core insight is to simplify the underlying geometric structure by constraining the distribution onto a simple and regular manifold, a topological sphere. Building on this, we first propose the Spherical Geometry Representation, a novel face representation that anchors geometric signals to uniform spherical coordinates. This guarantees a regular point distribution, from which the mesh connectivity can be robustly reconstructed. Critically, this canonical sphere can be seamlessly unwrapped into a 2D map, creating a perfect synergy with powerful 2D generative models. We then introduce Spherical Geometry Diffusion, a conditional diffusion framework built upon this 2D map. It enables diverse and controllable generation by jointly modeling geometry and texture, where the geometry explicitly conditions the texture synthesis process. Our method's effectiveness is demonstrated through its success in a wide range of tasks: text-to-3D generation, face reconstruction, and text-based 3D editing. Extensive experiments show that our approach substantially outperforms existing methods in geometric quality, textual fidelity, and inference efficiency.

</details>


### [185] [A Lightweight Model-Driven 4D Radar Framework for Pervasive Human Detection in Harsh Conditions](https://arxiv.org/abs/2601.13373)
*Zhenan Liu,Amir Khajepour,George Shaker*

Main category: cs.CV

TL;DR: A model-driven 4D mmWave radar framework for robust human detection in dust-filled industrial/underground environments where cameras/LiDAR fail.


<details>
  <summary>Details</summary>
Motivation: Industrial/underground environments have dust, smoke, confined spaces, and metallic structures that degrade optical/LiDAR perception. 4D mmWave radar is resilient but lacks understanding for processing its sparse point clouds for reliable human detection in visibility-degraded spaces.

Method: Fully model-driven 4D radar perception framework with domain-aware multi-threshold filtering, ego motion compensated temporal accumulation, KD tree Euclidean clustering with Doppler aware refinement, and rule-based 3D classifier. Runs in real-time on embedded edge hardware using radar as sole perception modality.

Result: Framework evaluated in dust-filled enclosed trailer and real underground mining tunnels. Radar detector maintained stable pedestrian identification while camera and LiDAR modalities failed under severe visibility degradation.

Conclusion: Model-driven approach provides robust, interpretable, and computationally efficient perception for safety-critical applications in harsh industrial/subterranean environments.

Abstract: Pervasive sensing in industrial and underground environments is severely constrained by airborne dust, smoke, confined geometry, and metallic structures, which rapidly degrade optical and LiDAR based perception. Elevation resolved 4D mmWave radar offers strong resilience to such conditions, yet there remains a limited understanding of how to process its sparse and anisotropic point clouds for reliable human detection in enclosed, visibility degraded spaces. This paper presents a fully model-driven 4D radar perception framework designed for real-time execution on embedded edge hardware. The system uses radar as its sole perception modality and integrates domain aware multi threshold filtering, ego motion compensated temporal accumulation, KD tree Euclidean clustering with Doppler aware refinement, and a rule based 3D classifier. The framework is evaluated in a dust filled enclosed trailer and in real underground mining tunnels, and in the tested scenarios the radar based detector maintains stable pedestrian identification as camera and LiDAR modalities fail under severe visibility degradation. These results suggest that the proposed model-driven approach provides robust, interpretable, and computationally efficient perception for safety-critical applications in harsh industrial and subterranean environments.

</details>


### [186] [Practical Insights into Semi-Supervised Object Detection Approaches](https://arxiv.org/abs/2601.13380)
*Chaoxin Wang,Bharaneeshwar Balasubramaniyam,Anurag Sangem,Nicolais Guevara,Doina Caragea*

Main category: cs.CV

TL;DR: Comprehensive comparison of three state-of-the-art SSOD methods (MixPL, Semi-DETR, Consistent-Teacher) across different labeled data regimes on standard and custom datasets.


<details>
  <summary>Details</summary>
Motivation: Address the challenge of learning in data-scarce settings by evaluating semi-supervised object detection approaches that leverage unlabeled data alongside limited labeled images (few-shot learning).

Method: Comparative evaluation of three SSOD methods (MixPL, Semi-DETR, Consistent-Teacher) using MS-COCO and Pascal VOC benchmarks plus a custom Beetle dataset, analyzing performance variation with different amounts of labeled data.

Result: Findings reveal trade-offs between accuracy, model size, and latency, providing insights into which methods perform best in low-data regimes across different dataset types.

Conclusion: The study offers practical guidance for selecting appropriate SSOD methods based on specific requirements (accuracy vs. efficiency) in data-scarce object detection scenarios.

Abstract: Learning in data-scarce settings has recently gained significant attention in the research community. Semi-supervised object detection(SSOD) aims to improve detection performance by leveraging a large number of unlabeled images alongside a limited number of labeled images(a.k.a.,few-shot learning). In this paper, we present a comprehensive comparison of three state-of-the-art SSOD approaches, including MixPL, Semi-DETR and Consistent-Teacher, with the goal of understanding how performance varies with the number of labeled images. We conduct experiments using the MS-COCO and Pascal VOC datasets, two popular object detection benchmarks which allow for standardized evaluation. In addition, we evaluate the SSOD approaches on a custom Beetle dataset which enables us to gain insights into their performance on specialized datasets with a smaller number of object categories. Our findings highlight the trade-offs between accuracy, model size, and latency, providing insights into which methods are best suited for low-data regimes.

</details>


### [187] [Organ-Aware Attention Improves CT Triage and Classification](https://arxiv.org/abs/2601.13385)
*Lavsen Dahal,Yubraj Bhandari,Geoffrey D. Rubin,Joseph Y. Lo*

Main category: cs.CV

TL;DR: ORACLE-CT: An organ-aware attention model for CT scan triage that outperforms vision-language models by using organ-masked attention and organ-scalar fusion for calibrated predictions with spatial evidence.


<details>
  <summary>Details</summary>
Motivation: There's an urgent need for automated triage and classification of high-volume CT scans to improve patient care and reduce radiologist burnout. Current vision-language models struggle with 3D anatomy, protocol variations, and noisy report supervision in medical imaging.

Method: ORACLE-CT uses an encoder-agnostic, organ-aware head with two key components: 1) Organ-Masked Attention (mask-restricted, per-organ pooling for spatial evidence), and 2) Organ-Scalar Fusion (lightweight fusion of normalized volume and mean-HU cues). Tested on chest CT datasets (CT-RATE, RADCHEST-CT) and abdomen CT (MERLIN).

Result: Achieved AUROC 0.86 on chest CT-RATE, and AUROC 0.85 on abdomen MERLIN (30 findings). The supervised baseline already exceeded reproduced zero-shot VLM baselines, and ORACLE-CT further improved performance, establishing new state-of-the-art supervised classification across both chest and abdomen CT.

Conclusion: ORACLE-CT delivers state-of-the-art supervised CT classification performance with calibrated predictions and localized evidence, addressing limitations of off-the-shelf VLMs for medical imaging triage applications.

Abstract: There is an urgent need for triage and classification of high-volume medical imaging modalities such as computed tomography (CT), which can improve patient care and mitigate radiologist burnout. Study-level CT triage requires calibrated predictions with localized evidence; however, off-the-shelf Vision Language Models (VLM) struggle with 3D anatomy, protocol shifts, and noisy report supervision. This study used the two largest publicly available chest CT datasets: CT-RATE and RADCHEST-CT (held-out external test set). Our carefully tuned supervised baseline (instantiated as a simple Global Average Pooling head) establishes a new supervised state of the art, surpassing all reported linear-probe VLMs. Building on this baseline, we present ORACLE-CT, an encoder-agnostic, organ-aware head that pairs Organ-Masked Attention (mask-restricted, per-organ pooling that yields spatial evidence) with Organ-Scalar Fusion (lightweight fusion of normalized volume and mean-HU cues). In the chest setting, ORACLE-CT masked attention model achieves AUROC 0.86 on CT-RATE; in the abdomen setting, on MERLIN (30 findings), our supervised baseline exceeds a reproduced zero-shot VLM baseline obtained by running publicly released weights through our pipeline, and adding masked attention plus scalar fusion further improves performance to AUROC 0.85. Together, these results deliver state-of-the-art supervised classification performance across both chest and abdomen CT under a unified evaluation protocol. The source code is available at https://github.com/lavsendahal/oracle-ct.

</details>


### [188] [Leveraging Transformer Decoder for Automotive Radar Object Detection](https://arxiv.org/abs/2601.13386)
*Changxu Zhang,Zhaoze Wang,Tai Fei,Christopher Grimm,Yi Jin,Claas Tebruegge,Ernst Warsitz,Markus Gardill*

Main category: cs.CV

TL;DR: Transformer-based 3D radar object detection using decoder for direct bounding box regression, with Pyramid Token Fusion for multi-scale feature integration, eliminating NMS.


<details>
  <summary>Details</summary>
Motivation: To improve 3D radar object detection by leveraging Transformer architecture to model spatial-temporal correlations and eliminate heuristic post-processing like NMS tuning.

Method: Transformer Decoder prediction head for direct regression of 3D boxes and class scores, with Pyramid Token Fusion module to convert multi-scale radar features into unified token sequence, formulated as set prediction with learnable queries.

Result: Achieves significant improvements over state-of-the-art radar-only baselines on RADDet dataset.

Conclusion: The proposed Transformer-based architecture with Pyramid Token Fusion effectively handles 3D radar object detection by modeling long-range correlations and eliminating dense proposal generation and NMS tuning.

Abstract: In this paper, we present a Transformer-based architecture for 3D radar object detection that uses a novel Transformer Decoder as the prediction head to directly regress 3D bounding boxes and class scores from radar feature representations. To bridge multi-scale radar features and the decoder, we propose Pyramid Token Fusion (PTF), a lightweight module that converts a feature pyramid into a unified, scale-aware token sequence. By formulating detection as a set prediction problem with learnable object queries and positional encodings, our design models long-range spatial-temporal correlations and cross-feature interactions. This approach eliminates dense proposal generation and heuristic post-processing such as extensive non-maximum suppression (NMS) tuning. We evaluate the proposed framework on the RADDet, where it achieves significant improvements over state-of-the-art radar-only baselines.

</details>


### [189] [Deep Image Prior with L0 Gradient Regularizer for Image Smoothing](https://arxiv.org/abs/2601.13400)
*Nhat Thanh Tran,Kevin Bui,Jack Xin*

Main category: cs.CV

TL;DR: DIP-ℓ₀ is a deep image prior framework that uses ℓ₀ gradient regularization for training-free image smoothing, outperforming existing methods in edge preservation and JPEG artifact removal.


<details>
  <summary>Details</summary>
Motivation: Traditional image smoothing methods rely on local statistics or optimization, while deep learning approaches require carefully curated training datasets. Constructing proper training datasets for image smoothing is challenging, motivating a training-free deep learning approach.

Method: Proposes DIP-ℓ₀, a deep image prior framework incorporating ℓ₀ gradient regularizer. Uses alternating direction method of multipliers algorithm with off-the-shelf ℓ₀ gradient minimization solver to handle the nonconvex, nonsmooth ℓ₀ "norm" optimization.

Result: Numerical experiments show DIP-ℓ₀ outperforms many image smoothing algorithms in edge-preserving image smoothing and JPEG artifact removal without requiring any training data.

Conclusion: DIP-ℓ₀ provides a training-free deep learning solution for high-quality image smoothing that effectively preserves edges while removing details and textures, addressing the challenge of dataset construction in this domain.

Abstract: Image smoothing is a fundamental image processing operation that preserves the underlying structure, such as strong edges and contours, and removes minor details and textures in an image. Many image smoothing algorithms rely on computing local window statistics or solving an optimization problem. Recent state-of-the-art methods leverage deep learning, but they require a carefully curated training dataset. Because constructing a proper training dataset for image smoothing is challenging, we propose DIP-$\ell_0$, a deep image prior framework that incorporates the $\ell_0$ gradient regularizer. This framework can perform high-quality image smoothing without any training data. To properly minimize the associated loss function that has the nonconvex, nonsmooth $\ell_0$ ``norm", we develop an alternating direction method of multipliers algorithm that utilizes an off-the-shelf $\ell_0$ gradient minimization solver. Numerical experiments demonstrate that the proposed DIP-$\ell_0$ outperforms many image smoothing algorithms in edge-preserving image smoothing and JPEG artifact removal.

</details>


### [190] [Reasoning with Pixel-level Precision: QVLM Architecture and SQuID Dataset for Quantitative Geospatial Analytics](https://arxiv.org/abs/2601.13401)
*Peter A. Massih,Eric Cosatto*

Main category: cs.CV

TL;DR: QVLM is a code-generation architecture that preserves pixel-level information for quantitative spatial reasoning in satellite images, achieving 42% accuracy vs 28% for standard VLMs on the new SQuID benchmark.


<details>
  <summary>Details</summary>
Motivation: Current Vision-Language Models fail at quantitative spatial reasoning because their patch-based vision encoders destroy pixel-level information needed for counting and measurements, especially in satellite imagery analysis.

Method: QVLM decouples language understanding from visual analysis by generating executable code that calls segmentation models to obtain pixel-level masks, then operates directly on these masks to preserve spatial indexing throughout reasoning.

Result: QVLM using GPT-5 as coder achieves 42.0% accuracy on the SQuID benchmark (2,000 satellite image QA pairs), significantly outperforming standard VLMs which only achieve 28.1% accuracy.

Conclusion: Architectural decoupling that maintains pixel precision enables better accuracy on quantitative spatial reasoning tasks, revealing a fundamental limitation in current VLM architectures for spatial analysis.

Abstract: Current Vision-Language Models (VLMs) fail at quantitative spatial reasoning because their architectures destroy pixel-level information required for counting and measurements. Vision encoders compress images through patch embeddings, reducing spatial indexing and losing the precise pixel-level tracking required for accurate counting. We present two contributions to address this fundamental limitation. First, we introduce SQuID (Satellite Quantitative Intelligence Dataset), a benchmark of 2,000 satellite image Question-Answer pairs with both numerical range and categorical answers, designed to evaluate quantitative spatial reasoning. The dataset spans three difficulty tiers with annotations automatically generated from human labels and their learned variability. Second, we propose QVLM (Quantitative Vision-Language Model), a code-generation architecture that maintains pixel precision by decoupling language understanding from visual analysis. Instead of encoding images into embeddings, QVLM generates executable code that first calls a segmentation model to obtain pixel-level masks, then operates directly on these masks, preserving spatial indexing throughout the reasoning process. Our experiments show that QVLM using GPT-5 as coder achieves 42.0% accuracy on SQuID compared to 28.1% for a VLM prompted with image-question pairs. Our work reveals that, for quantitative spatial reasoning, architectural decoupling enables better accuracy on quantitative tasks.

</details>


### [191] [Local-to-Global Logical Explanations for Deep Vision Models](https://arxiv.org/abs/2601.13404)
*Bhavan Vasu,Giuseppe Raffa,Prasad Tadepalli*

Main category: cs.CV

TL;DR: The paper introduces interpretable explanation methods for black-box neural networks that generate explanations using human-recognizable primitive concepts in logical formulas.


<details>
  <summary>Details</summary>
Motivation: Deep neural networks are highly effective but opaque and hard to interpret, creating a need for explanation methods that make model decisions understandable to humans.

Method: The authors propose local and global explanation methods that generate explanations as logical formulas in monotone disjunctive-normal-form (MDNF) using primitive concepts. They also present an algorithm for multi-class explanations in the form of monotone explanation lists.

Result: The explanations maintain high fidelity and coverage with respect to the black-box models they seek to explain, as demonstrated on challenging vision datasets.

Conclusion: The proposed methods provide interpretable explanations for black-box models while preserving model performance, bridging the gap between neural network effectiveness and human interpretability.

Abstract: While deep neural networks are extremely effective at classifying images, they remain opaque and hard to interpret. We introduce local and global explanation methods for black-box models that generate explanations in terms of human-recognizable primitive concepts. Both the local explanations for a single image and the global explanations for a set of images are cast as logical formulas in monotone disjunctive-normal-form (MDNF), whose satisfaction guarantees that the model yields a high score on a given class. We also present an algorithm for explaining the classification of examples into multiple classes in the form of a monotone explanation list over primitive concepts. Despite their simplicity and interpretability we show that the explanations maintain high fidelity and coverage with respect to the blackbox models they seek to explain in challenging vision datasets.

</details>


### [192] [Using deep learning for predicting cleansing quality of colon capsule endoscopy images](https://arxiv.org/abs/2601.13412)
*Puneet Sharma,Kristian Dalsbø Hindberg,Benedicte Schelde-Olesen,Ulrik Deding,Esmaeil S. Nadimi,Jan-Matthias Braun*

Main category: cs.CV

TL;DR: Deep learning with ResNet-18 predicts colon capsule endoscopy cleansing quality using Leighton-Rex scale, achieving 88% accuracy with 79% sparsity through structured pruning, while evaluating explainability methods and model calibration.


<details>
  <summary>Details</summary>
Motivation: To develop an efficient deep learning model for predicting cleansing quality in colon capsule endoscopy images, addressing the clinical need for automated assessment while maintaining explainability for clinical trust and application.

Method: Used ResNet-18 trained on 500 clinician-labeled images with stratified K-fold cross-validation, applied structured pruning for sparsity, evaluated explainability using Grad-CAM, Grad-CAM++, Eigen-CAM, Ablation-CAM, and Random-CAM with ROAD method, and employed adaptive temperature scaling for calibration.

Result: Achieved 88% cross-validation accuracy with 79% sparsity (improved from 84% accuracy without pruning), demonstrating effective pruning without performance compromise, while highlighting challenges in cleansing quality evaluation and ROAD method application.

Conclusion: Structured pruning enables efficient deep learning models for CCE cleansing quality prediction while maintaining accuracy, with explainability being crucial for clinical adoption, though challenges remain in evaluation methods and external dataset calibration.

Abstract: In this study, we explore the application of deep learning techniques for predicting cleansing quality in colon capsule endoscopy (CCE) images. Using a dataset of 500 images labeled by 14 clinicians on the Leighton-Rex scale (Poor, Fair, Good, and Excellent), a ResNet-18 model was trained for classification, leveraging stratified K-fold cross-validation to ensure robust performance. To optimize the model, structured pruning techniques were applied iteratively, achieving significant sparsity while maintaining high accuracy. Explainability of the pruned model was evaluated using Grad-CAM, Grad-CAM++, Eigen-CAM, Ablation-CAM, and Random-CAM, with the ROAD method employed for consistent evaluation. Our results indicate that for a pruned model, we can achieve a cross-validation accuracy of 88% with 79% sparsity, demonstrating the effectiveness of pruning in improving efficiency from 84% without compromising performance. We also highlight the challenges of evaluating cleansing quality of CCE images, emphasize the importance of explainability in clinical applications, and discuss the challenges associated with using the ROAD method for our task. Finally, we employ a variant of adaptive temperature scaling to calibrate the pruned models for an external dataset.

</details>


### [193] [Diffusion Representations for Fine-Grained Image Classification: A Marine Plankton Case Study](https://arxiv.org/abs/2601.13416)
*A. Nieto Juscafresa,Á. Mazcuñán Herreros,J. Sullivan*

Main category: cs.CV

TL;DR: Frozen diffusion models serve as effective feature encoders for fine-grained recognition, outperforming other self-supervised methods and matching supervised baselines in plankton monitoring tasks, with strong robustness to distribution shifts.


<details>
  <summary>Details</summary>
Motivation: Diffusion models are state-of-the-art for image generation but their potential as general-purpose feature encoders remains underexplored. The authors aim to demonstrate that diffusion models, trained for denoising without labels, can serve as effective self-supervised learners that capture both low- and high-level image structure for downstream recognition tasks.

Method: Use a frozen diffusion model backbone as a feature encoder. Probe intermediate denoising features across different layers and timesteps. For each layer-timestep pair, train a linear classifier on these features. Evaluate in a real-world plankton-monitoring setting with controlled training setups against established supervised and self-supervised baselines.

Result: Frozen diffusion features are competitive with supervised baselines and outperform other self-supervised methods in both balanced and naturally long-tailed settings. They maintain strong accuracy and Macro F1 under substantial distribution shift when evaluated on temporally and geographically shifted plankton datasets.

Conclusion: Diffusion models can serve as effective general-purpose feature encoders for fine-grained recognition tasks, demonstrating strong performance that matches supervised methods and surpasses other self-supervised approaches, with particular robustness to distribution shifts in real-world applications.

Abstract: Diffusion models have emerged as state-of-the-art generative methods for image synthesis, yet their potential as general-purpose feature encoders remains underexplored. Trained for denoising and generation without labels, they can be interpreted as self-supervised learners that capture both low- and high-level structure. We show that a frozen diffusion backbone enables strong fine-grained recognition by probing intermediate denoising features across layers and timesteps and training a linear classifier for each pair. We evaluate this in a real-world plankton-monitoring setting with practical impact, using controlled and comparable training setups against established supervised and self-supervised baselines. Frozen diffusion features are competitive with supervised baselines and outperform other self-supervised methods in both balanced and naturally long-tailed settings. Out-of-distribution evaluations on temporally and geographically shifted plankton datasets further show that frozen diffusion features maintain strong accuracy and Macro F1 under substantial distribution shift.

</details>


### [194] [SGW-GAN: Sliced Gromov-Wasserstein Guided GANs for Retinal Fundus Image Enhancement](https://arxiv.org/abs/2601.13417)
*Yujian Xiong,Xuanzhao Dong,Wenhui Zhu,Xin Li,Oana Dumitrascu,Yalin Wang*

Main category: cs.CV

TL;DR: SGW-GAN: A retinal image enhancement framework using Sliced Gromov Wasserstein distance to preserve intra-class geometry while improving image quality, outperforming GAN/diffusion methods on clinical tasks.


<details>
  <summary>Details</summary>
Motivation: Current GAN- and diffusion-based retinal image enhancement methods improve perceptual quality but distort intra-class geometry, harming downstream clinical tasks like disease grading and lesion detection. There's a need for enhancement that preserves clinical relationships between samples.

Method: Propose SGW-GAN framework incorporating Sliced Gromov Wasserstein (SGW) distance into retinal image enhancement. SGW approximates the computationally expensive Gromov Wasserstein discrepancy via random projections, preserving intra-class relational structure while being efficient.

Result: Experiments on public datasets show SGW-GAN produces visually compelling enhancements, achieves superior diabetic retinopathy grading performance, and reports the lowest GW discrepancy across disease labels compared to other methods.

Conclusion: SGW-GAN offers an efficient and clinically faithful solution for unpaired medical image enhancement by preserving intra-class geometry through sliced Gromov Wasserstein distance, balancing perceptual quality with clinical utility.

Abstract: Retinal fundus photography is indispensable for ophthalmic screening and diagnosis, yet image quality is often degraded by noise, artifacts, and uneven illumination. Recent GAN- and diffusion-based enhancement methods improve perceptual quality by aligning degraded images with high-quality distributions, but our analysis shows that this focus can distort intra-class geometry: clinically related samples become dispersed, disease-class boundaries blur, and downstream tasks such as grading or lesion detection are harmed. The Gromov Wasserstein (GW) discrepancy offers a principled solution by aligning distributions through internal pairwise distances, naturally preserving intra-class structure, but its high computational cost restricts practical use. To overcome this, we propose SGW-GAN, the first framework to incorporate Sliced GW (SGW) into retinal image enhancement. SGW approximates GW via random projections, retaining relational fidelity while greatly reducing cost. Experiments on public datasets show that SGW-GAN produces visually compelling enhancements, achieves superior diabetic retinopathy grading, and reports the lowest GW discrepancy across disease labels, demonstrating both efficiency and clinical fidelity for unpaired medical image enhancement.

</details>


### [195] [Analyzing VLM-Based Approaches for Anomaly Classification and Segmentation](https://arxiv.org/abs/2601.13440)
*Mohit Kakda,Mirudula Shri Muthukumaran,Uttapreksha Patel,Lawrence Swaminathan Xavier Prince*

Main category: cs.CV

TL;DR: This paper provides a comprehensive analysis of Vision-Language Model (VLM) approaches for anomaly detection, focusing on CLIP-based methods for zero-shot and few-shot defect identification without extensive labeled data.


<details>
  <summary>Details</summary>
Motivation: Traditional anomaly detection requires extensive labeled datasets and task-specific training. VLMs like CLIP offer a paradigm shift by enabling zero-shot and few-shot defect identification through aligned image-text representations, eliminating the need for defect examples or specialized training.

Method: Systematic investigation of VLM-based approaches including: 1) sliding window-based dense feature extraction (WinCLIP), 2) multi-stage feature alignment with learnable projections (AprilLab framework), and 3) compositional prompt ensemble strategies. Analysis covers feature extraction, text-visual alignment, prompt engineering, zero-shot vs few-shot trade-offs, computational efficiency, and cross-domain generalization.

Result: Rigorous experimentation on benchmarks (MVTec AD, VisA) comparing classification accuracy, segmentation precision, and inference efficiency. Provides foundational understanding of why VLMs succeed in anomaly detection and practical insights for method selection.

Conclusion: VLMs revolutionize anomaly detection by enabling zero-shot/few-shot approaches without extensive labeled data. The analysis synthesizes practical insights for industrial adoption and identifies current limitations to guide future research in quality control applications.

Abstract: Vision-Language Models (VLMs), particularly CLIP, have revolutionized anomaly detection by enabling zero-shot and few-shot defect identification without extensive labeled datasets. By learning aligned representations of images and text, VLMs facilitate anomaly classification and segmentation through natural language descriptions of normal and abnormal states, eliminating traditional requirements for task-specific training or defect examples. This project presents a comprehensive analysis of VLM-based approaches for anomaly classification (AC) and anomaly segmentation (AS). We systematically investigate key architectural paradigms including sliding window-based dense feature extraction (WinCLIP), multi-stage feature alignment with learnable projections (AprilLab framework), and compositional prompt ensemble strategies. Our analysis evaluates these methods across critical dimensions: feature extraction mechanisms, text-visual alignment strategies, prompt engineering techniques, zero-shot versus few-shot trade-offs, computational efficiency, and cross-domain generalization. Through rigorous experimentation on benchmarks such as MVTec AD and VisA, we compare classification accuracy, segmentation precision, and inference efficiency. The primary contribution is a foundational understanding of how and why VLMs succeed in anomaly detection, synthesizing practical insights for method selection and identifying current limitations. This work aims to facilitate informed adoption of VLM-based methods in industrial quality control and guide future research directions.

</details>


### [196] [Optical Linear Systems Framework for Event Sensing and Computational Neuromorphic Imaging](https://arxiv.org/abs/2601.13498)
*Nimrod Kruger,Nicholas Owen Ralph,Gregory Cohen,Paul Hurley*

Main category: cs.CV

TL;DR: Event cameras produce sparse, asynchronous data that doesn't fit traditional linear imaging models. This paper presents a physics-based pipeline to convert event streams into log-intensity estimates, enabling inverse filtering and deconvolution for dynamic optical systems.


<details>
  <summary>Details</summary>
Motivation: Event vision sensors output sparse, asynchronous data that doesn't integrate well with traditional linear forward models used in computational imaging and optical system design. There's a need to bridge event sensing with model-based computational imaging approaches.

Method: Developed a physics-grounded processing pipeline that maps event streams to per-pixel log-intensity and intensity derivative estimates. Embedded these measurements in a dynamic linear systems model with time-varying point spread function, enabling frequency-domain Wiener deconvolution with known dynamic transfer functions.

Result: Validated the approach in simulation for single and overlapping point sources under modulated defocus, and on real event data from a tunable-focus telescope imaging star fields. Demonstrated successful source localization and separability.

Conclusion: The framework provides a practical bridge between event sensing and model-based computational imaging for dynamic optical systems, enabling inverse filtering directly from event data using frequency-domain Wiener deconvolution.

Abstract: Event vision sensors (neuromorphic cameras) output sparse, asynchronous ON/OFF events triggered by log-intensity threshold crossings, enabling microsecond-scale sensing with high dynamic range and low data bandwidth. As a nonlinear system, this event representation does not readily integrate with the linear forward models that underpin most computational imaging and optical system design. We present a physics-grounded processing pipeline that maps event streams to estimates of per-pixel log-intensity and intensity derivatives, and embeds these measurements in a dynamic linear systems model with a time-varying point spread function. This enables inverse filtering directly from event data, using frequency-domain Wiener deconvolution with a known (or parameterised) dynamic transfer function. We validate the approach in simulation for single and overlapping point sources under modulated defocus, and on real event data from a tunable-focus telescope imaging a star field, demonstrating source localisation and separability. The proposed framework provides a practical bridge between event sensing and model-based computational imaging for dynamic optical systems.

</details>


### [197] [DIS2: Disentanglement Meets Distillation with Classwise Attention for Robust Remote Sensing Segmentation under Missing Modalities](https://arxiv.org/abs/2601.13502)
*Nhi Kieu,Kien Nguyen,Arnold Wiliem,Clinton Fookes,Sridha Sridharan*

Main category: cs.CV

TL;DR: DIS2: A novel multimodal learning framework for remote sensing that addresses missing modalities through guided compensation rather than shared feature dependence, outperforming SOTA methods.


<details>
  <summary>Details</summary>
Motivation: Multimodal learning in remote sensing suffers from missing modalities, exacerbated by heterogeneous data and scale variation. Conventional approaches (disentanglement learning, knowledge distillation) fail due to insufficient feature overlap and ill-posed mimicry tasks that don't address semantic gaps.

Method: DIS2 introduces DLKD (Disentanglement Learning & Knowledge Distillation synergy) for principled missing information compensation, Classwise Feature Learning Module (CFLM) for adaptive class-specific modality contribution, and hierarchical hybrid fusion (HF) for multi-resolution feature importance.

Result: Extensive experiments show the proposed approach significantly outperforms state-of-the-art methods across benchmarks, validating its effectiveness in handling missing modalities in remote sensing.

Conclusion: DIS2 represents a paradigm shift from modality-shared feature dependence to active, guided missing feature compensation, specifically designed to address the unique challenges of remote sensing data heterogeneity and missing modalities.

Abstract: The efficacy of multimodal learning in remote sensing (RS) is severely undermined by missing modalities. The challenge is exacerbated by the RS highly heterogeneous data and huge scale variation. Consequently, paradigms proven effective in other domains often fail when confronted with these unique data characteristics. Conventional disentanglement learning, which relies on significant feature overlap between modalities (modality-invariant), is insufficient for this heterogeneity. Similarly, knowledge distillation becomes an ill-posed mimicry task where a student fails to focus on the necessary compensatory knowledge, leaving the semantic gap unaddressed. Our work is therefore built upon three pillars uniquely designed for RS: (1) principled missing information compensation, (2) class-specific modality contribution, and (3) multi-resolution feature importance. We propose a novel method DIS2, a new paradigm shifting from modality-shared feature dependence and untargeted imitation to active, guided missing features compensation. Its core novelty lies in a reformulated synergy between disentanglement learning and knowledge distillation, termed DLKD. Compensatory features are explicitly captured which, when fused with the features of the available modality, approximate the ideal fused representation of the full-modality case. To address the class-specific challenge, our Classwise Feature Learning Module (CFLM) adaptively learn discriminative evidence for each target depending on signal availability. Both DLKD and CFLM are supported by a hierarchical hybrid fusion (HF) structure using features across resolutions to strengthen prediction. Extensive experiments validate that our proposed approach significantly outperforms state-of-the-art methods across benchmarks.

</details>


### [198] [GO-MLVTON: Garment Occlusion-Aware Multi-Layer Virtual Try-On with Diffusion Models](https://arxiv.org/abs/2601.13524)
*Yang Yu,Yunze Deng,Yige Zhang,Yanjie Xiao,Youkun Ou,Wenhao Hu,Mingchao Li,Bin Feng,Wenyu Liu,Dandan Zheng,Jingdong Chen*

Main category: cs.CV

TL;DR: GO-MLVTON is the first multi-layer virtual try-on method that handles multiple garment layers with realistic deformation and occlusion modeling.


<details>
  <summary>Details</summary>
Motivation: Existing VTON methods focus on single-layer or multi-garment try-on but neglect multi-layer scenarios where garments have realistic occlusion relationships and layering effects.

Method: Proposes Garment Occlusion Learning module to learn occlusion relationships between inner/outer garments, and StableDiffusion-based Garment Morphing & Fitting module for realistic garment deformation and fitting.

Result: State-of-the-art performance demonstrated through extensive experiments, with new MLG dataset and Layered Appearance Coherence Difference (LACD) metric for evaluation.

Conclusion: GO-MLVTON successfully addresses multi-layer VTON challenges, producing high-quality try-on results with realistic garment layering and occlusion relationships.

Abstract: Existing Image-based virtual try-on (VTON) methods primarily focus on single-layer or multi-garment VTON, neglecting multi-layer VTON (ML-VTON), which involves dressing multiple layers of garments onto the human body with realistic deformation and layering to generate visually plausible outcomes. The main challenge lies in accurately modeling occlusion relationships between inner and outer garments to reduce interference from redundant inner garment features. To address this, we propose GO-MLVTON, the first multi-layer VTON method, introducing the Garment Occlusion Learning module to learn occlusion relationships and the StableDiffusion-based Garment Morphing & Fitting module to deform and fit garments onto the human body, producing high-quality multi-layer try-on results. Additionally, we present the MLG dataset for this task and propose a new metric named Layered Appearance Coherence Difference (LACD) for evaluation. Extensive experiments demonstrate the state-of-the-art performance of GO-MLVTON. Project page: https://upyuyang.github.io/go-mlvton/.

</details>


### [199] [DiffFace-Edit: A Diffusion-Based Facial Dataset for Forgery-Semantic Driven Deepfake Detection Analysis](https://arxiv.org/abs/2601.13551)
*Feng Ding,Wenhui Yi,Xinan He,Mengyao Xiao,Jianfeng Xu,Jianqiang Du*

Main category: cs.CV

TL;DR: DiffFace-Edit is a new dataset with over 2 million AI-generated fake faces featuring fine-grained regional manipulations across 8 facial areas, addressing the lack of datasets focused on detector-evasive splice attacks between real and manipulated samples.


<details>
  <summary>Details</summary>
Motivation: Current AI-generated face datasets lack focus on fine-grained regional manipulations, and no research has studied the real impact of splice attacks (detector-evasive samples) where real and manipulated samples are combined, posing significant privacy risks.

Method: Created DiffFace-Edit dataset with over 2 million AI-generated fake images featuring edits across 8 facial regions (eyes, nose, etc.) with single-region and multi-region editing combinations. Conducted comprehensive dataset analysis and proposed cross-domain evaluation combining IMDL methods.

Result: Dataset contains extensive fine-grained manipulated faces with diverse editing combinations. The study specifically analyzes the impact of detector-evasive samples on detection models, providing insights into splice attack vulnerabilities.

Conclusion: DiffFace-Edit addresses critical gaps in AI-generated face datasets by focusing on fine-grained regional manipulations and detector-evasive samples, enabling better evaluation of detection models against sophisticated privacy threats. The dataset will be publicly available.

Abstract: Generative models now produce imperceptible, fine-grained manipulated faces, posing significant privacy risks. However, existing AI-generated face datasets generally lack focus on samples with fine-grained regional manipulations. Furthermore, no researchers have yet studied the real impact of splice attacks, which occur between real and manipulated samples, on detectors. We refer to these as detector-evasive samples. Based on this, we introduce the DiffFace-Edit dataset, which has the following advantages: 1) It contains over two million AI-generated fake images. 2) It features edits across eight facial regions (e.g., eyes, nose) and includes a richer variety of editing combinations, such as single-region and multi-region edits. Additionally, we specifically analyze the impact of detector-evasive samples on detection models. We conduct a comprehensive analysis of the dataset and propose a cross-domain evaluation that combines IMDL methods. Dataset will be available at https://github.com/ywh1093/DiffFace-Edit.

</details>


### [200] [Learning Fine-Grained Correspondence with Cross-Perspective Perception for Open-Vocabulary 6D Object Pose Estimation](https://arxiv.org/abs/2601.13565)
*Yu Qin,Shimeng Fan,Fan Yang,Zixuan Xue,Zijie Mai,Wenrui Chen,Kailun Yang,Zhiyong Li*

Main category: cs.CV

TL;DR: FiCoP improves open-vocabulary 6D object pose estimation by replacing global matching with patch-level correspondence using structural priors to filter background noise.


<details>
  <summary>Details</summary>
Motivation: Existing open-vocabulary 6D pose estimation methods suffer from ambiguity in global matching where target features get confused with background distractors in unconstrained open-world scenarios.

Method: Three-stage framework: 1) Object-centric disentanglement preprocessing to isolate semantic targets from noise; 2) Cross-Perspective Global Perception module to fuse dual-view features with explicit context reasoning; 3) Patch Correlation Predictor that generates block-wise association maps as spatial filters for fine-grained matching.

Result: Improves Average Recall by 8.0% on REAL275 and 6.1% on Toyota-Light datasets compared to state-of-the-art methods.

Conclusion: FiCoP enables robust and generalized perception for robotic agents in complex open-world environments by transitioning from noise-prone global matching to spatially-constrained patch-level correspondence.

Abstract: Open-vocabulary 6D object pose estimation empowers robots to manipulate arbitrary unseen objects guided solely by natural language. However, a critical limitation of existing approaches is their reliance on unconstrained global matching strategies. In open-world scenarios, trying to match anchor features against the entire query image space introduces excessive ambiguity, as target features are easily confused with background distractors. To resolve this, we propose Fine-grained Correspondence Pose Estimation (FiCoP), a framework that transitions from noise-prone global matching to spatially-constrained patch-level correspondence. Our core innovation lies in leveraging a patch-to-patch correlation matrix as a structural prior to narrowing the matching scope, effectively filtering out irrelevant clutter to prevent it from degrading pose estimation. Firstly, we introduce an object-centric disentanglement preprocessing to isolate the semantic target from environmental noise. Secondly, a Cross-Perspective Global Perception (CPGP) module is proposed to fuse dual-view features, establishing structural consensus through explicit context reasoning. Finally, we design a Patch Correlation Predictor (PCP) that generates a precise block-wise association map, acting as a spatial filter to enforce fine-grained, noise-resilient matching. Experiments on the REAL275 and Toyota-Light datasets demonstrate that FiCoP improves Average Recall by 8.0% and 6.1%, respectively, compared to the state-of-the-art method, highlighting its capability to deliver robust and generalized perception for robotic agents operating in complex, unconstrained open-world environments. The source code will be made publicly available at https://github.com/zjjqinyu/FiCoP.

</details>


### [201] [ChartVerse: Scaling Chart Reasoning via Reliable Programmatic Synthesis from Scratch](https://arxiv.org/abs/2601.13606)
*Zheng Liu,Honglin Lin,Chonghan Qin,Xiaoyang Wang,Xin Gao,Yu Li,Mengzhang Cai,Yun Zhu,Zhanping Zhong,Qizhi Pei,Zhuoshi Pan,Xiaoran Shang,Bin Cui,Conghui He,Wentao Zhang,Lijun Wu*

Main category: cs.CV

TL;DR: ChartVerse is a framework that synthesizes complex charts and high-quality reasoning data to train VLMs for chart understanding, achieving SOTA performance with an 8B model.


<details>
  <summary>Details</summary>
Motivation: Open-source VLMs for chart reasoning lack high-quality training data - existing datasets have simplistic synthetic charts and hallucinated QA pairs with insufficient reasoning depth.

Method: 1) Introduces Rollout Posterior Entropy (RPE) metric to quantify chart complexity, enabling complexity-aware synthesis of diverse charts via executable programs. 2) Uses truth-anchored inverse QA synthesis (answer-first approach) with deterministic answers from source code, strict consistency verification, and model fail-rate filtering with CoT distillation.

Result: ChartVerse-8B achieves state-of-the-art performance, surpassing its teacher Qwen3-VL-30B-A3B-Thinking and rivaling the stronger Qwen3-VL-32B-Thinking model.

Conclusion: ChartVerse successfully addresses the data quality bottleneck in chart reasoning by providing a scalable framework for synthesizing complex charts and rigorous reasoning data, enabling smaller models to outperform larger ones.

Abstract: Chart reasoning is a critical capability for Vision Language Models (VLMs). However, the development of open-source models is severely hindered by the lack of high-quality training data. Existing datasets suffer from a dual challenge: synthetic charts are often simplistic and repetitive, while the associated QA pairs are prone to hallucinations and lack the reasoning depth required for complex tasks. To bridge this gap, we propose ChartVerse, a scalable framework designed to synthesize complex charts and reliable reasoning data from scratch. (1) To address the bottleneck of simple patterns, we first introduce Rollout Posterior Entropy (RPE), a novel metric that quantifies chart complexity. Guided by RPE, we develop complexity-aware chart coder to autonomously synthesize diverse, high-complexity charts via executable programs. (2) To guarantee reasoning rigor, we develop truth-anchored inverse QA synthesis. Diverging from standard generation, we adopt an answer-first paradigm: we extract deterministic answers directly from the source code, generate questions conditional on these anchors, and enforce strict consistency verification. To further elevate difficulty and reasoning depth, we filter samples based on model fail-rate and distill high-quality Chain-of-Thought (CoT) reasoning. We curate ChartVerse-SFT-600K and ChartVerse-RL-40K using Qwen3-VL-30B-A3B-Thinking as the teacher. Experimental results demonstrate that ChartVerse-8B achieves state-of-the-art performance, notably surpassing its teacher and rivaling the stronger Qwen3-VL-32B-Thinking.

</details>


### [202] [CARPE: Context-Aware Image Representation Prioritization via Ensemble for Large Vision-Language Models](https://arxiv.org/abs/2601.13622)
*Donghee Lee,Rui Cai,Zhe Zhao*

Main category: cs.CV

TL;DR: CARPE is a model-agnostic framework that improves LVLMs' vision capabilities through vision-integration layers and context-aware ensemble strategy, enhancing performance on both image classification and vision-language tasks.


<details>
  <summary>Details</summary>
Motivation: LVLMs underperform on vision-centric tasks like image classification compared to their base CLIP-based vision encoders, showing a gap in effectively utilizing visual information.

Method: Proposes CARPE framework with vision-integration layers and context-aware ensemble strategy to adaptively prioritize image representations or rely on language model reasoning based on context.

Result: CARPE improves performance on image classification benchmarks and enhances results across various vision-language benchmarks, showing consistent generalization improvements.

Conclusion: CARPE is an effective, adaptable framework that can be integrated with most open-source LVLMs to enhance their vision capabilities while maintaining strong language reasoning.

Abstract: Recent advancements in Large Vision-Language Models (LVLMs) have pushed them closer to becoming general-purpose assistants. Despite their strong performance, LVLMs still struggle with vision-centric tasks such as image classification, underperforming compared to their base vision encoders, which are often CLIP-based models. To address this limitation, we propose Context-Aware Image Representation Prioritization via Ensemble (CARPE), a novel, model-agnostic framework which introduces vision-integration layers and a context-aware ensemble strategy to identify when to prioritize image representations or rely on the reasoning capabilities of the language model. This design enhances the model's ability to adaptively weight visual and textual modalities and enables the model to capture various aspects of image representations, leading to consistent improvements in generalization across classification and vision-language benchmarks. Extensive experiments demonstrate that CARPE not only improves performance on image classification benchmarks but also enhances results across various vision-language benchmarks. Finally, CARPE is designed to be effectively integrated with most open-source LVLMs that consist of a vision encoder and a language model, ensuring its adaptability across diverse architectures.

</details>


### [203] [Scaling Test-time Inference for Visual Grounding](https://arxiv.org/abs/2601.13633)
*Guanqi Zhan,Changye Li,Zhijian Liu,Yao Lu,Yi Wu,Song Han,Ligeng Zhu*

Main category: cs.CV

TL;DR: EGM improves small VLMs' visual grounding by scaling test-time computation (#generated tokens) rather than model size, achieving comparable performance to large models with much faster inference.


<details>
  <summary>Details</summary>
Motivation: Small VLMs lag behind large ones in visual grounding mainly due to language understanding limitations, not visual processing. Large models are slow and heavy for deployment, creating a need for efficient alternatives.

Method: EGM (Efficient visual Grounding language Models) scales test-time computation by generating more tokens during inference. This is deployment-friendly as small models have cheaper per-token costs, allowing increased computation without model size growth.

Result: EGM-Qwen3-VL-8B achieves 91.4 IoU on RefCOCO with 737ms latency (5.9x faster than Qwen3-VL-235B's 4,320ms for 90.5 IoU). The method also improves amodal grounding (predicting both visible and occluded object parts), consistently boosting small models to match or exceed large model performance.

Conclusion: Scaling test-time computation for small VLMs effectively bridges the grounding performance gap with large models while maintaining deployment efficiency and faster inference, offering a practical solution for real-world visual grounding applications.

Abstract: Visual grounding is an essential capability of Visual Language Models (VLMs) to understand the real physical world. Previous state-of-the-art grounding visual language models usually have large model sizes, making them heavy for deployment and slow for inference. However, we notice that the sizes of visual encoders are nearly the same for small and large VLMs and the major difference is the sizes of the language models. Small VLMs fall behind larger VLMs in grounding because of the difference in language understanding capability rather than visual information handling. To mitigate the gap, we introduce 'Efficient visual Grounding language Models' (EGM): a method to scale the test-time computation (#generated tokens). Scaling the test-time computation of a small model is deployment-friendly, and yields better end-to-end latency as the cost of each token is much cheaper compared to directly running a large model. On the RefCOCO benchmark, our EGM-Qwen3-VL-8B demonstrates 91.4 IoU with an average of 737ms (5.9x faster) latency while Qwen3-VL-235B demands 4,320ms to achieve 90.5 IoU. To validate our approach's generality, we further set up a new amodal grounding setting that requires the model to predict both the visible and occluded parts of the objects. Experiments show our method can consistently and significantly improve the vanilla grounding and amodal grounding capabilities of small models to be on par with or outperform the larger models, thereby improving the efficiency for visual grounding.

</details>


### [204] [Face-Voice Association with Inductive Bias for Maximum Class Separation](https://arxiv.org/abs/2601.13651)
*Marta Moscati,Oleksandr Kats,Mubashir Noman,Muhammad Zaigham Zaheer,Yufang Hou,Markus Schedl,Shah Nawaz*

Main category: cs.CV

TL;DR: First work to apply maximum class separation as inductive bias for face-voice association, achieving SOTA performance by combining it with inter-class orthogonality losses.


<details>
  <summary>Details</summary>
Motivation: Previous face-voice association methods use loss functions for similarity, but recent classification advances show maximum class separation strengthens discriminative ability. This technique hasn't been applied to multimodal face-voice association, creating a research gap.

Method: Develops face-voice association method that imposes maximum class separation among multimodal representations of different speakers as inductive bias. Combines this with losses for inter-class orthogonality.

Result: Achieves state-of-the-art performance on two face-voice association task formulations. Ablation study shows inductive bias is most effective when combined with inter-class orthogonality losses.

Conclusion: First work to apply and demonstrate effectiveness of maximum class separation as inductive bias in multimodal learning, establishing a new paradigm for face-voice association.

Abstract: Face-voice association is widely studied in multimodal learning and is approached representing faces and voices with embeddings that are close for a same person and well separated from those of others. Previous work achieved this with loss functions. Recent advancements in classification have shown that the discriminative ability of embeddings can be strengthened by imposing maximum class separation as inductive bias. This technique has never been used in the domain of face-voice association, and this work aims at filling this gap. More specifically, we develop a method for face-voice association that imposes maximum class separation among multimodal representations of different speakers as an inductive bias. Through quantitative experiments we demonstrate the effectiveness of our approach, showing that it achieves SOTA performance on two task formulation of face-voice association. Furthermore, we carry out an ablation study to show that imposing inductive bias is most effective when combined with losses for inter-class orthogonality. To the best of our knowledge, this work is the first that applies and demonstrates the effectiveness of maximum class separation as an inductive bias in multimodal learning; it hence paves the way to establish a new paradigm.

</details>


### [205] [VIAFormer: Voxel-Image Alignment Transformer for High-Fidelity Voxel Refinement](https://arxiv.org/abs/2601.13664)
*Tiancheng Fang,Bowen Pan,Lingxi Chen,Jiangjing Lyu,Chengfei Lyu,Chaoyue Niu,Fan Wu*

Main category: cs.CV

TL;DR: VIAFormer is a transformer model that refines incomplete/noisy 3D voxels using multi-view images as guidance, achieving state-of-the-art performance and practical utility in 3D creation pipelines.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the problem of repairing incomplete and noisy voxel representations using multi-view images as guidance, which is crucial for improving 3D shape quality obtained from vision foundation models and enabling practical 3D creation workflows.

Method: VIAFormer uses three key components: 1) Image Index for explicit 3D spatial grounding of 2D image tokens, 2) Correctional Flow objective that learns direct voxel-refinement trajectories, and 3) Hybrid Stream Transformer for robust cross-modal fusion between voxels and images.

Result: VIAFormer establishes new state-of-the-art performance in correcting both severe synthetic corruptions and realistic artifacts on voxel shapes from vision foundation models, and demonstrates practical utility as a bridge in real-world 3D creation pipelines.

Conclusion: The proposed VIAFormer effectively bridges voxel-based methods with large-model, big-data approaches, paving the way for voxel-based techniques to thrive in modern 3D creation workflows and demonstrating reliable performance in real-world applications.

Abstract: We propose VIAFormer, a Voxel-Image Alignment Transformer model designed for Multi-view Conditioned Voxel Refinement--the task of repairing incomplete noisy voxels using calibrated multi-view images as guidance. Its effectiveness stems from a synergistic design: an Image Index that provides explicit 3D spatial grounding for 2D image tokens, a Correctional Flow objective that learns a direct voxel-refinement trajectory, and a Hybrid Stream Transformer that enables robust cross-modal fusion. Experiments show that VIAFormer establishes a new state of the art in correcting both severe synthetic corruptions and realistic artifacts on the voxel shape obtained from powerful Vision Foundation Models. Beyond benchmarking, we demonstrate VIAFormer as a practical and reliable bridge in real-world 3D creation pipelines, paving the way for voxel-based methods to thrive in large-model, big-data wave.

</details>


### [206] [Transformer based Multi-task Fusion Network for Food Spoilage Detection and Shelf life Forecasting](https://arxiv.org/abs/2601.13665)
*Mounika Kanulla,Rajasree Dadigi,Sailaja Thota,Vivek Yelleti*

Main category: cs.CV

TL;DR: Proposed fusion architectures (CNN+CNN-LSTM and CNN+DeiT Transformer) for simultaneous vegetable classification, spoilage detection, and shelf life forecasting, achieving state-of-the-art performance on a custom dataset.


<details>
  <summary>Details</summary>
Motivation: Food wastage is a critical challenge in agricultural supply chains. Accurate spoilage detection and forecasting can reduce waste and improve supply chain longevity in agriculture.

Method: Developed fusion architectures combining CNN with LSTM and DeiT transformer for multi-task learning. Created custom dataset by capturing vegetable images from fresh to completely spoiled states. Compared against CNN, VGG16, ResNet50, Capsule Networks, and DeiT Transformers.

Result: CNN+DeiT Transformer achieved F1-score of 0.98 for classification, 0.61 for spoilage detection, and MSE/SMAPE of 3.58/41.66% for spoilage forecasting. Fusion models outperformed all baseline models and were validated on noisy images with LIME visualization.

Conclusion: The proposed fusion architectures effectively address multiple agricultural supply chain challenges simultaneously, providing accurate classification, spoilage detection, and shelf life forecasting to reduce food waste.

Abstract: Food wastage is one of the critical challenges in the agricultural supply chain, and accurate and effective spoilage detection can help to reduce it. Further, it is highly important to forecast the spoilage information. This aids the longevity of the supply chain management in the agriculture field. This motivated us to propose fusion based architectures by combining CNN with LSTM and DeiT transformer for the following multi-tasks simultaneously: (i) vegetable classification, (ii) food spoilage detection, and (iii) shelf life forecasting. We developed a dataset by capturing images of vegetables from their fresh state until they were completely spoiled. From the experimental analysis it is concluded that the proposed fusion architectures CNN+CNN-LSTM and CNN+DeiT Transformer outperformed several deep learning models such as CNN, VGG16, ResNet50, Capsule Networks, and DeiT Transformers. Overall, CNN + DeiT Transformer yielded F1-score of 0.98 and 0.61 in vegetable classification and spoilage detection respectively and mean squared error (MSE) and symmetric mean absolute percentage error (SMAPE) of 3.58, and 41.66% respectively in spoilage forecasting. Further, the reliability of the fusion models was validated on noisy images and integrated with LIME to visualize the model decisions.

</details>


### [207] [Finally Outshining the Random Baseline: A Simple and Effective Solution for Active Learning in 3D Biomedical Imaging](https://arxiv.org/abs/2601.13677)
*Carsten T. Lüth,Jeremias Traub,Kim-Celine Kahl,Till J. Bungert,Lukas Klein,Lars Krämer,Paul F. Jäger,Klaus Maier-Hein,Fabian Isensee*

Main category: cs.CV

TL;DR: ClaSP PE is a novel active learning method for 3D biomedical image segmentation that consistently outperforms random sampling baselines by addressing class imbalance and redundancy through class-stratified querying and scheduled power noising.


<details>
  <summary>Details</summary>
Motivation: Active learning could reduce expensive 3D biomedical image annotation costs, but existing methods fail to consistently outperform improved random sampling baselines adapted to 3D data, leaving the field without reliable solutions.

Method: Class-stratified Scheduled Power Predictive Entropy (ClaSP PE) combines class-stratified querying to ensure coverage of underrepresented structures with log-scale power noising and a decaying schedule to enforce query diversity early and encourage exploitation later.

Result: In 24 experimental settings across four 3D biomedical datasets, ClaSP PE was the only method that consistently outperformed improved random baselines with statistically significant gains in segmentation quality while remaining annotation-efficient. It also robustly generalized to four unseen datasets without manual adaptation.

Conclusion: ClaSP PE provides the first reliable active learning solution for 3D biomedical segmentation that consistently outperforms random baselines in realistic scenarios, with open-source implementation and clear deployment guidelines for practical application.

Abstract: Active learning (AL) has the potential to drastically reduce annotation costs in 3D biomedical image segmentation, where expert labeling of volumetric data is both time-consuming and expensive. Yet, existing AL methods are unable to consistently outperform improved random sampling baselines adapted to 3D data, leaving the field without a reliable solution. We introduce Class-stratified Scheduled Power Predictive Entropy (ClaSP PE), a simple and effective query strategy that addresses two key limitations of standard uncertainty-based AL methods: class imbalance and redundancy in early selections. ClaSP PE combines class-stratified querying to ensure coverage of underrepresented structures and log-scale power noising with a decaying schedule to enforce query diversity in early-stage AL and encourage exploitation later. In our evaluation on 24 experimental settings using four 3D biomedical datasets within the comprehensive nnActive benchmark, ClaSP PE is the only method that generally outperforms improved random baselines in terms of both segmentation quality with statistically significant gains, whilst remaining annotation efficient. Furthermore, we explicitly simulate the real-world application by testing our method on four previously unseen datasets without manual adaptation, where all experiment parameters are set according to predefined guidelines. The results confirm that ClaSP PE robustly generalizes to novel tasks without requiring dataset-specific tuning. Within the nnActive framework, we present compelling evidence that an AL method can consistently outperform random baselines adapted to 3D segmentation, in terms of both performance and annotation efficiency in a realistic, close-to-production scenario. Our open-source implementation and clear deployment guidelines make it readily applicable in practice. Code is at https://github.com/MIC-DKFZ/nnActive.

</details>


### [208] [Dynamic Differential Linear Attention: Enhancing Linear Diffusion Transformer for High-Quality Image Generation](https://arxiv.org/abs/2601.13683)
*Boyuan Cao,Xingbo Yao,Chenhui Wang,Jiaxin Ye,Yujie Wei,Hongming Shan*

Main category: cs.CV

TL;DR: DyDiLA introduces a novel linear attention formulation with dynamic projection, dynamic measure kernel, and token differential operator to enhance linear diffusion transformers (LiTs) by mitigating oversmoothing issues and improving generation quality.


<details>
  <summary>Details</summary>
Motivation: Diffusion transformers (DiTs) face scalability bottlenecks due to quadratic self-attention costs. While linear attention mechanisms reduce computational cost, they often sacrifice generative performance by producing over-smoothed attention weights that limit expressiveness.

Method: Proposes Dynamic Differential Linear Attention (DyDiLA) with three key designs: (1) dynamic projection module for token representation decoupling, (2) dynamic measure kernel for better similarity measurement with fine-grained semantic distinction, and (3) token differential operator for robust query-to-key retrieval. These are incorporated into DyDi-LiT, a refined linear diffusion transformer.

Result: DyDi-LiT consistently outperforms current state-of-the-art models across multiple metrics, demonstrating strong practical potential for high-fidelity image generation with improved efficiency.

Conclusion: DyDiLA successfully addresses the oversmoothing problem in linear attention mechanisms, enabling linear diffusion transformers to achieve better generative performance while maintaining computational efficiency, making it a promising approach for scalable high-quality image generation.

Abstract: Diffusion transformers (DiTs) have emerged as a powerful architecture for high-fidelity image generation, yet the quadratic cost of self-attention poses a major scalability bottleneck. To address this, linear attention mechanisms have been adopted to reduce computational cost; unfortunately, the resulting linear diffusion transformers (LiTs) models often come at the expense of generative performance, frequently producing over-smoothed attention weights that limit expressiveness. In this work, we introduce Dynamic Differential Linear Attention (DyDiLA), a novel linear attention formulation that enhances the effectiveness of LiTs by mitigating the oversmoothing issue and improving generation quality. Specifically, the novelty of DyDiLA lies in three key designs: (i) dynamic projection module, which facilitates the decoupling of token representations by learning with dynamically assigned knowledge; (ii) dynamic measure kernel, which provides a better similarity measurement to capture fine-grained semantic distinctions between tokens by dynamically assigning kernel functions for token processing; and (iii) token differential operator, which enables more robust query-to-key retrieval by calculating the differences between the tokens and their corresponding information redundancy produced by dynamic measure kernel. To capitalize on DyDiLA, we introduce a refined LiT, termed DyDi-LiT, that systematically incorporates our advancements. Extensive experiments show that DyDi-LiT consistently outperforms current state-of-the-art (SOTA) models across multiple metrics, underscoring its strong practical potential.

</details>


### [209] [Reasoning or Pattern Matching? Probing Large Vision-Language Models with Visual Puzzles](https://arxiv.org/abs/2601.13705)
*Maria Lymperaiou,Vasileios Karampinis,Giorgos Filandrianos,Angelos Vlachos,Chrysoula Zerva,Athanasios Voulodimos*

Main category: cs.CV

TL;DR: Survey paper analyzing visual puzzles as diagnostic tools for evaluating reasoning abilities in Large Vision-Language Models (LVLMs), organizing benchmarks by reasoning mechanisms and identifying current model limitations.


<details>
  <summary>Details</summary>
Motivation: Visual puzzles serve as compact probes of human cognition that can isolate reasoning abilities with minimal prior knowledge. The authors want to leverage these properties to create better diagnostic tools for evaluating LVLM reasoning capabilities, moving beyond open-ended multimodal benchmarks to more controlled, verifiable assessments.

Method: The survey provides a unified perspective by framing visual puzzles through a common abstraction and organizing existing benchmarks by the reasoning mechanisms they target: inductive, analogical, algorithmic, deductive, and geometric/spatial reasoning. This links puzzle design to specific cognitive operations required for solving.

Result: Synthesis of empirical evidence reveals consistent limitations in current LVLMs: brittle generalization, tight entanglement between perception and reasoning, and a persistent gap between fluent explanations and faithful execution. The survey positions visual puzzles as diagnostic instruments rather than just task formats.

Conclusion: Visual puzzles offer powerful diagnostic tools for evaluating LVLM reasoning. The survey outlines key directions for future benchmarks and reasoning-aware multimodal systems, emphasizing the need to treat puzzles as instruments for understanding model capabilities rather than just performance metrics.

Abstract: Puzzles have long served as compact and revealing probes of human cognition, isolating abstraction, rule discovery, and systematic reasoning with minimal reliance on prior knowledge. Leveraging these properties, visual puzzles have recently emerged as a powerful diagnostic tool for evaluating the reasoning abilities of Large Vision-Language Models (LVLMs), offering controlled, verifiable alternatives to open-ended multimodal benchmarks. This survey provides a unified perspective of visual puzzle reasoning in LVLMs. We frame visual puzzles through a common abstraction and organize existing benchmarks by the reasoning mechanisms they target (inductive, analogical, algorithmic, deductive, and geometric/spatial), thereby linking puzzle design to the cognitive operations required for solving. Synthesizing empirical evidence across these categories, we identify consistent limitations in current models, including brittle generalization, tight entanglement between perception and reasoning, and a persistent gap between fluent explanations and faithful execution. By framing visual puzzles as diagnostic instruments rather than task formats, this survey elaborates on the state of LVLM reasoning and outlines key directions for future benchmarks and reasoning-aware multimodal systems.

</details>


### [210] [ParkingTwin: Training-Free Streaming 3D Reconstruction for Parking-Lot Digital Twins](https://arxiv.org/abs/2601.13706)
*Xinhao Liu,Yu Wang,Xiansheng Guo,Gordon Owusu Boateng,Yu Cao,Haonan Si,Xingchen Guo,Nirwan Ansari*

Main category: cs.CV

TL;DR: ParkingTwin is a training-free, lightweight system for online streaming 3D reconstruction of parking lots that runs at 30+ FPS on entry-level GPUs, achieving high-fidelity digital twins without expensive optimization.


<details>
  <summary>Details</summary>
Motivation: Current robot-oriented reconstruction for parking lot digital twins faces a trilemma: sparse forward-facing views cause weak parallax and ill-posed geometry; dynamic occlusions and extreme lighting hinder stable texture fusion; and neural rendering typically requires expensive offline optimization, violating edge-side streaming constraints.

Method: Three key innovations: 1) OSM-prior-driven geometric construction using OpenStreetMap semantic topology to directly generate metric-consistent TSDF, 2) geometry-aware dynamic filtering with quad-modal constraint field (normal/height/depth consistency) to reject moving vehicles and transient occlusions, 3) illumination-robust fusion in CIELAB space with adaptive L-channel weighting and depth-gradient suppression.

Result: Achieves 30+ FPS on entry-level GTX 1660, SSIM 0.87 (+16.0% improvement), 15x end-to-end speedup, and 83.3% GPU memory reduction compared to state-of-the-art 3D Gaussian Splatting (which requires high-end RTX 4090D). Validated on 68,000 m² real-world dataset.

Conclusion: ParkingTwin provides an efficient, training-free solution for online streaming 3D reconstruction of parking lots, enabling real-time digital twin creation on edge devices with explicit triangle mesh outputs compatible with Unity/Unreal pipelines.

Abstract: High-fidelity parking-lot digital twins provide essential priors for path planning, collision checking, and perception validation in Automated Valet Parking (AVP). Yet robot-oriented reconstruction faces a trilemma: sparse forward-facing views cause weak parallax and ill-posed geometry; dynamic occlusions and extreme lighting hinder stable texture fusion; and neural rendering typically needs expensive offline optimization, violating edge-side streaming constraints. We propose ParkingTwin, a training-free, lightweight system for online streaming 3D reconstruction. First, OSM-prior-driven geometric construction uses OpenStreetMap semantic topology to directly generate a metric-consistent TSDF, replacing blind geometric search with deterministic mapping and avoiding costly optimization. Second, geometry-aware dynamic filtering employs a quad-modal constraint field (normal/height/depth consistency) to reject moving vehicles and transient occlusions in real time. Third, illumination-robust fusion in CIELAB decouples luminance and chromaticity via adaptive L-channel weighting and depth-gradient suppression, reducing seams under abrupt lighting changes. ParkingTwin runs at 30+ FPS on an entry-level GTX 1660. On a 68,000 m^2 real-world dataset, it achieves SSIM 0.87 (+16.0%), delivers about 15x end-to-end speedup, and reduces GPU memory by 83.3% compared with state-of-the-art 3D Gaussian Splatting (3DGS) that typically requires high-end GPUs (RTX 4090D). The system outputs explicit triangle meshes compatible with Unity/Unreal digital-twin pipelines. Project page: https://mihoutao-liu.github.io/ParkingTwin/

</details>


### [211] [Attention-space Contrastive Guidance for Efficient Hallucination Mitigation in LVLMs](https://arxiv.org/abs/2601.13707)
*Yujin Jo,Sangyoon Bae,Taesup Kim*

Main category: cs.CV

TL;DR: ACG is a single-pass contrastive guidance method that reduces hallucinations in vision-language models by steering attention away from language priors toward visual evidence, achieving SOTA results with 2x speedup.


<details>
  <summary>Details</summary>
Motivation: LVLMs often hallucinate when language priors dominate over visual evidence, causing object misidentification and visually inconsistent descriptions. There's a need for efficient hallucination mitigation that preserves visual grounding.

Method: Attention-space Contrastive Guidance (ACG) operates within self-attention layers to construct both vision-language and language-only attention paths in a single forward pass. It uses orthogonalized correction to remove language-only aligned components and amplify visual contributions.

Result: ACG achieves state-of-the-art faithfulness and caption quality on CHAIR and POPE benchmarks while reducing computational cost by up to 2x compared to prior methods requiring multiple forward passes.

Conclusion: ACG provides a principled, efficient alternative for hallucination mitigation in LVLMs by directly embedding contrastive guidance in attention mechanisms, balancing visual grounding with computational efficiency.

Abstract: Hallucinations in large vision-language models (LVLMs) often arise when language priors dominate over visual evidence, causing object misidentification and visually inconsistent descriptions. We address this issue by framing hallucination mitigation as contrastive guidance, steering generation toward visually grounded and semantically faithful text. This approach regulates the model's internal behavior by reducing over-dependence on language priors and contrasting visually grounded with language-only representations. We propose Attention-space Contrastive Guidance (ACG), a single-pass mechanism that operates within self-attention layers to construct both vision-language and language-only attention paths in a single forward computation. This integration enables computationally efficient guidance directly embedded in the model's representation contextualization. To correct approximation bias introduced by the single-pass formulation, we further apply an orthogonalized correction that removes components aligned with the language-only path, selectively amplifying visual contributions. Experiments on the CHAIR and POPE benchmarks show that ACG achieves state-of-the-art faithfulness and caption quality while significantly reducing computational cost. Our method establishes a principled and efficient alternative, reducing latency by up to 2x compared to prior contrastive decoding methods that require multiple forward passes.

</details>


### [212] [MVGD-Net: A Novel Motion-aware Video Glass Surface Detection Network](https://arxiv.org/abs/2601.13715)
*Yiwei Lu,Hao Huang,Tao Yan*

Main category: cs.CV

TL;DR: MVGD-Net detects glass surfaces in videos using motion inconsistency cues, outperforming state-of-the-art methods with novel temporal-spatial modules and a new large-scale dataset.


<details>
  <summary>Details</summary>
Motivation: Glass surfaces pose threats to vision-based systems like robot/drone navigation, and current Video Glass Surface Detection (VGSD) methods need improvement. The key observation is that reflected/transmitted objects on glass surfaces appear to move slower than objects in non-glass regions due to motion inconsistency, which can reveal glass presence.

Method: Proposes MVGD-Net with three novel modules: 1) Cross-scale Multimodal Fusion Module (CMFM) integrates spatial features and optical flow maps, 2) History Guided Attention Module (HGAM) and 3) Temporal Cross Attention Module (TCAM) enhance temporal features, plus a Temporal-Spatial Decoder (TSD) fuses spatial and temporal features for mask generation.

Result: Extensive experiments show MVGD-Net outperforms relevant state-of-the-art methods. The paper also introduces a large-scale dataset with 312 diverse glass scenarios and 19,268 frames for training and evaluation.

Conclusion: MVGD-Net effectively detects glass surfaces in videos by leveraging motion inconsistency cues through innovative temporal-spatial fusion modules, providing a robust solution for vision-based navigation systems threatened by glass surfaces.

Abstract: Glass surface ubiquitous in both daily life and professional environments presents a potential threat to vision-based systems, such as robot and drone navigation. To solve this challenge, most recent studies have shown significant interest in Video Glass Surface Detection (VGSD). We observe that objects in the reflection (or transmission) layer appear farther from the glass surfaces. Consequently, in video motion scenarios, the notable reflected (or transmitted) objects on the glass surface move slower than objects in non-glass regions within the same spatial plane, and this motion inconsistency can effectively reveal the presence of glass surfaces. Based on this observation, we propose a novel network, named MVGD-Net, for detecting glass surfaces in videos by leveraging motion inconsistency cues. Our MVGD-Net features three novel modules: the Cross-scale Multimodal Fusion Module (CMFM) that integrates extracted spatial features and estimated optical flow maps, the History Guided Attention Module (HGAM) and Temporal Cross Attention Module (TCAM), both of which further enhances temporal features. A Temporal-Spatial Decoder (TSD) is also introduced to fuse the spatial and temporal features for generating the glass region mask. Furthermore, for learning our network, we also propose a large-scale dataset, which comprises 312 diverse glass scenarios with a total of 19,268 frames. Extensive experiments demonstrate that our MVGD-Net outperforms relevant state-of-the-art methods.

</details>


### [213] [Hierarchical Long Video Understanding with Audiovisual Entity Cohesion and Agentic Search](https://arxiv.org/abs/2601.13719)
*Xinlei Yin,Xiulian Peng,Xiao Li,Zhiwei Xiong,Yan Lu*

Main category: cs.CV

TL;DR: HAVEN is a unified framework for long-video understanding that integrates audiovisual entity cohesion and hierarchical video indexing with agentic search to overcome information fragmentation and maintain global coherence.


<details>
  <summary>Details</summary>
Motivation: Existing solutions for long video understanding using naive chunking with retrieval-augmented generation suffer from information fragmentation and loss of global coherence, making comprehensive reasoning challenging.

Method: HAVEN integrates entity-level representations across visual and auditory streams, organizes content into a structured hierarchy (global summary, scene, segment, entity levels), and employs agentic search for dynamic retrieval and reasoning across these layers.

Result: Achieves state-of-the-art with 84.1% overall accuracy on LVBench, with outstanding 80.1% performance in challenging reasoning category, demonstrating good temporal coherence, entity consistency, and retrieval efficiency.

Conclusion: HAVEN's structured, multimodal reasoning approach enables comprehensive and context-consistent understanding of long-form videos, establishing a new standard for long-video understanding frameworks.

Abstract: Long video understanding presents significant challenges for vision-language models due to extremely long context windows. Existing solutions relying on naive chunking strategies with retrieval-augmented generation, typically suffer from information fragmentation and a loss of global coherence. We present HAVEN, a unified framework for long-video understanding that enables coherent and comprehensive reasoning by integrating audiovisual entity cohesion and hierarchical video indexing with agentic search. First, we preserve semantic consistency by integrating entity-level representations across visual and auditory streams, while organizing content into a structured hierarchy spanning global summary, scene, segment, and entity levels. Then we employ an agentic search mechanism to enable dynamic retrieval and reasoning across these layers, facilitating coherent narrative reconstruction and fine-grained entity tracking. Extensive experiments demonstrate that our method achieves good temporal coherence, entity consistency, and retrieval efficiency, establishing a new state-of-the-art with an overall accuracy of 84.1% on LVBench. Notably, it achieves outstanding performance in the challenging reasoning category, reaching 80.1%. These results highlight the effectiveness of structured, multimodal reasoning for comprehensive and context-consistent understanding of long-form videos.

</details>


### [214] [Facial Spatiotemporal Graphs: Leveraging the 3D Facial Surface for Remote Physiological Measurement](https://arxiv.org/abs/2601.13724)
*Sam Cantrill,David Ahmedt-Aristizabal,Lars Petersson,Hanna Suominen,Mohammad Ali Armin*

Main category: cs.CV

TL;DR: A novel facial rPPG method using 3D facial mesh sequences to create surface-aligned spatiotemporal graphs, achieving SOTA performance with improved interpretability and generalization.


<details>
  <summary>Details</summary>
Motivation: Existing facial rPPG methods fail to explicitly align their receptive fields with the 3D facial surface where the physiological signals actually occur, limiting their performance and interpretability.

Method: Proposes Facial Spatiotemporal Graph (STGraph) representation using 3D facial mesh sequences, and MeshPhys - a lightweight spatiotemporal graph convolutional network that operates on this graph to estimate physiological signals.

Result: Achieves state-of-the-art or competitive performance across four benchmark datasets in both intra- and cross-dataset settings. Ablation studies confirm the importance of surface-aligned receptive fields and 3D-aware node features.

Conclusion: The STGraph and MeshPhys provide a principled modeling paradigm for facial rPPG that enables robust, interpretable, and generalizable physiological signal estimation through explicit alignment with the 3D facial surface.

Abstract: Facial remote photoplethysmography (rPPG) methods estimate physiological signals by modeling subtle color changes on the 3D facial surface over time. However, existing methods fail to explicitly align their receptive fields with the 3D facial surface-the spatial support of the rPPG signal. To address this, we propose the Facial Spatiotemporal Graph (STGraph), a novel representation that encodes facial color and structure using 3D facial mesh sequences-enabling surface-aligned spatiotemporal processing. We introduce MeshPhys, a lightweight spatiotemporal graph convolutional network that operates on the STGraph to estimate physiological signals. Across four benchmark datasets, MeshPhys achieves state-of-the-art or competitive performance in both intra- and cross-dataset settings. Ablation studies show that constraining the model's receptive field to the facial surface acts as a strong structural prior, and that surface-aligned, 3D-aware node features are critical for robustly encoding facial surface color. Together, the STGraph and MeshPhys constitute a novel, principled modeling paradigm for facial rPPG, enabling robust, interpretable, and generalizable estimation. Code is available at https://samcantrill.github.io/facial-stgraph-rppg/ .

</details>


### [215] [HiT: History-Injection Transformers for Onboard Continuous Flood Change Detection](https://arxiv.org/abs/2601.13751)
*Daniel Kyselica,Jonáš Herec,Oliver Kutis,Rado Pitoňák*

Main category: cs.CV

TL;DR: HiT-Prithvi model enables real-time flood detection on small satellites by maintaining historical context with 99%+ data compression, achieving 43 FPS on onboard hardware while preserving accuracy.


<details>
  <summary>Details</summary>
Motivation: Natural disaster monitoring requires continuous satellite observation under strict operational constraints (memory, computation) of small satellites, with flood detection being critical for hazard management.

Method: Proposed History Injection mechanism for Transformer models (HiT) that maintains historical context from previous observations while reducing data storage by over 99% of original image size, implemented within Prithvi-tiny foundation model.

Result: HiT mechanism maintains detection accuracy compared to bitemporal baseline on STTORM-CD flood dataset, with HiT-Prithvi achieving 43 FPS on Jetson Orin Nano (representative nanosat hardware).

Conclusion: Establishes practical framework for satellite-based continuous monitoring of natural disasters, supporting real-time hazard assessment without dependency on ground-based processing infrastructure.

Abstract: Natural disaster monitoring through continuous satellite observation requires processing multi-temporal data under strict operational constraints. This paper addresses flood detection, a critical application for hazard management, by developing an onboard change detection system that operates within the memory and computational limits of small satellites. We propose History Injection mechanism for Transformer models (HiT), that maintains historical context from previous observations while reducing data storage by over 99\% of original image size. Moreover, testing on the STTORM-CD flood dataset confirms that the HiT mechanism within the Prithvi-tiny foundation model maintains detection accuracy compared to the bitemporal baseline. The proposed HiT-Prithvi model achieved 43 FPS on Jetson Orin Nano, a representative onboard hardware used in nanosats. This work establishes a practical framework for satellite-based continuous monitoring of natural disasters, supporting real-time hazard assessment without dependency on ground-based processing infrastructure. Architecture as well as model checkpoints is available at https://github.com/zaitra/HiT-change-detection

</details>


### [216] [PREGEN: Uncovering Latent Thoughts in Composed Video Retrieval](https://arxiv.org/abs/2601.13797)
*Gabriele Serussi,David Vainshtein,Jonathan Kouchly,Dotan Di Castro,Chaim Baskin*

Main category: cs.CV

TL;DR: PREGEN is an efficient Composed Video Retrieval framework that pairs a frozen VLM with a lightweight encoder, achieving state-of-the-art performance without fine-tuning.


<details>
  <summary>Details</summary>
Motivation: Current CoVR methods fail to fully leverage modern VLMs, either using outdated architectures or requiring computationally expensive fine-tuning and slow caption generation.

Method: Pairs a frozen pre-trained VLM with a lightweight encoding model, extracts hidden states from final tokens of each layer, and trains a simple encoder on pooled representations to create compact embeddings for retrieval.

Result: Significantly advances state-of-the-art with substantial gains in Recall@1 (+27.23 and +69.59), demonstrates robustness across VLM backbones, and shows strong zero-shot generalization to complex textual modifications.

Conclusion: PREGEN provides an efficient and powerful CoVR framework that overcomes limitations of previous methods, highlighting effectiveness and semantic capabilities without requiring VLM fine-tuning.

Abstract: Composed Video Retrieval (CoVR) aims to retrieve a video based on a query video and a modifying text. Current CoVR methods fail to fully exploit modern Vision-Language Models (VLMs), either using outdated architectures or requiring computationally expensive fine-tuning and slow caption generation. We introduce PREGEN (PRE GENeration extraction), an efficient and powerful CoVR framework that overcomes these limitations. Our approach uniquely pairs a frozen, pre-trained VLM with a lightweight encoding model, eliminating the need for any VLM fine-tuning. We feed the query video and modifying text into the VLM and extract the hidden state of the final token from each layer. A simple encoder is then trained on these pooled representations, creating a semantically rich and compact embedding for retrieval. PREGEN significantly advances the state of the art, surpassing all prior methods on standard CoVR benchmarks with substantial gains in Recall@1 of +27.23 and +69.59. Our method demonstrates robustness across different VLM backbones and exhibits strong zero-shot generalization to more complex textual modifications, highlighting its effectiveness and semantic capabilities.

</details>


### [217] [Insight: Interpretable Semantic Hierarchies in Vision-Language Encoders](https://arxiv.org/abs/2601.13798)
*Kai Wittenmayer,Sukrut Rao,Amin Parchami-Araghi,Bernt Schiele,Jonas Fischer*

Main category: cs.CV

TL;DR: Insight is a language-aligned concept foundation model that extracts fine-grained, human-interpretable concepts with spatial grounding in images, using hierarchical sparse autoencoders and concept relationships for improved explanations.


<details>
  <summary>Details</summary>
Motivation: Vision foundation models have strong performance but opaque representations that are hard to interpret. Existing concept decomposition methods lack spatial grounding and are limited to classification tasks.

Method: Uses hierarchical sparse autoencoder with foundation model to automatically extract concepts at various granularities. Examines local co-occurrence dependencies to define concept relationships, improving concept naming and explanations.

Result: Insight provides competitive performance on classification and segmentation benchmarks while offering fine-grained, high-quality concept-based explanations.

Conclusion: Insight bridges the gap between performance and interpretability by providing spatially-grounded, human-interpretable concepts that maintain competitive task performance while enabling transparent decision-making.

Abstract: Language-aligned vision foundation models perform strongly across diverse downstream tasks. Yet, their learned representations remain opaque, making interpreting their decision-making hard. Recent works decompose these representations into human-interpretable concepts, but provide poor spatial grounding and are limited to image classification tasks. In this work, we propose Insight, a language-aligned concept foundation model that provides fine-grained concepts, which are human-interpretable and spatially grounded in the input image. We leverage a hierarchical sparse autoencoder and a foundation model with strong semantic representations to automatically extract concepts at various granularities. Examining local co-occurrence dependencies of concepts allows us to define concept relationships. Through these relations we further improve concept naming and obtain richer explanations. On benchmark data, we show that Insight provides performance on classification and segmentation that is competitive with opaque foundation models while providing fine-grained, high quality concept-based explanations. Code is available at https://github.com/kawi19/Insight.

</details>


### [218] [Discriminant Learning-based Colorspace for Blade Segmentation](https://arxiv.org/abs/2601.13816)
*Raül Pérez-Gonzalo,Andreas Espersen,Antonio Agudo*

Main category: cs.CV

TL;DR: CSDA introduces a novel colorspace transformation algorithm that improves image segmentation by optimizing color representation through deep learning-based discriminant analysis.


<details>
  <summary>Details</summary>
Motivation: Current segmentation algorithms often neglect color preprocessing, leading to suboptimal performance due to poor color representation. Domain-specific segmentation requires tailored color preprocessing for better accuracy.

Method: Developed Colorspace Discriminant Analysis (CSDA) - a multidimensional nonlinear discriminant analysis algorithm that extends Linear Discriminant Analysis into deep learning. It customizes color representation by maximizing inter-class separability and minimizing intra-class variability using a generalized discriminative loss. Introduced three alternative losses for stable end-to-end training of both colorspace transformation and segmentation.

Result: Experiments on wind turbine blade data show significant accuracy improvements, demonstrating the effectiveness of tailored color preprocessing for domain-specific segmentation tasks.

Conclusion: CSDA successfully addresses the color representation problem in segmentation, proving that customized preprocessing is crucial for domain-specific applications and can substantially boost segmentation accuracy.

Abstract: Suboptimal color representation often hinders accurate image segmentation, yet many modern algorithms neglect this critical preprocessing step. This work presents a novel multidimensional nonlinear discriminant analysis algorithm, Colorspace Discriminant Analysis (CSDA), for improved segmentation. Extending Linear Discriminant Analysis into a deep learning context, CSDA customizes color representation by maximizing multidimensional signed inter-class separability while minimizing intra-class variability through a generalized discriminative loss. To ensure stable training, we introduce three alternative losses that enable end-to-end optimization of both the discriminative colorspace and segmentation process. Experiments on wind turbine blade data demonstrate significant accuracy gains, emphasizing the importance of tailored preprocessing in domain-specific segmentation.

</details>


### [219] [FastGHA: Generalized Few-Shot 3D Gaussian Head Avatars with Real-Time Animation](https://arxiv.org/abs/2601.13837)
*Xinya Ji,Sebastian Weiss,Manuel Kansy,Jacek Naruniec,Xun Cao,Barbara Solenthaler,Derek Bradley*

Main category: cs.CV

TL;DR: A feed-forward method called \OURS that generates high-quality 3D Gaussian head avatars from few input images with real-time animation, outperforming existing methods in quality and efficiency.


<details>
  <summary>Details</summary>
Motivation: Current 3D Gaussian-based head avatar methods require extensive multi-view setups or per-identity optimization during inference, limiting scalability and ease of use on unseen subjects.

Method: Learns per-pixel Gaussian representation from input images, aggregates multi-view information using transformer-based encoder with DINOv3 and Stable Diffusion VAE features, extends Gaussians with per-Gaussian features, uses lightweight MLP-based dynamic network for real-time animation, and employs point maps from pre-trained reconstruction model for geometry supervision.

Result: Significantly outperforms existing methods in both rendering quality and inference efficiency while supporting real-time dynamic avatar animation.

Conclusion: Proposes an efficient feed-forward approach for high-quality 3D Gaussian head avatars from few images with real-time animation capabilities, addressing scalability and ease-of-use limitations of current methods.

Abstract: Despite recent progress in 3D Gaussian-based head avatar modeling, efficiently generating high fidelity avatars remains a challenge. Current methods typically rely on extensive multi-view capture setups or monocular videos with per-identity optimization during inference, limiting their scalability and ease of use on unseen subjects. To overcome these efficiency drawbacks, we propose \OURS, a feed-forward method to generate high-quality Gaussian head avatars from only a few input images while supporting real-time animation. Our approach directly learns a per-pixel Gaussian representation from the input images, and aggregates multi-view information using a transformer-based encoder that fuses image features from both DINOv3 and Stable Diffusion VAE. For real-time animation, we extend the explicit Gaussian representations with per-Gaussian features and introduce a lightweight MLP-based dynamic network to predict 3D Gaussian deformations from expression codes. Furthermore, to enhance geometric smoothness of the 3D head, we employ point maps from a pre-trained large reconstruction model as geometry supervision. Experiments show that our approach significantly outperforms existing methods in both rendering quality and inference efficiency, while supporting real-time dynamic avatar animation.

</details>


### [220] [DisasterVQA: A Visual Question Answering Benchmark Dataset for Disaster Scenes](https://arxiv.org/abs/2601.13839)
*Aisha Al-Mohannadi,Ayisha Firoz,Yin Yang,Muhammad Imran,Ferda Ofli*

Main category: cs.CV

TL;DR: DisasterVQA is a benchmark dataset for evaluating vision-language models on disaster response tasks, featuring 1,395 real-world images and 4,405 expert-curated QA pairs across diverse disaster scenarios.


<details>
  <summary>Details</summary>
Motivation: Social media imagery provides low-latency situational information during disasters, but existing VQA systems designed for general domains may not handle the complex, safety-critical reasoning required for disaster response. There's a need to assess and improve vision-language models for practical humanitarian applications.

Method: Created DisasterVQA dataset with 1,395 real-world disaster images and 4,405 expert-curated question-answer pairs spanning floods, wildfires, earthquakes, etc. Questions are grounded in humanitarian frameworks (FEMA ESF and OCHA MIRA) and include binary, multiple-choice, and open-ended formats covering situational awareness and operational decision-making. Benchmarked seven state-of-the-art vision-language models on this dataset.

Result: Models show performance variability across question types, disaster categories, regions, and humanitarian tasks. While achieving high accuracy on binary questions, models struggle with fine-grained quantitative reasoning, object counting, and context-sensitive interpretation, particularly for underrepresented disaster scenarios.

Conclusion: DisasterVQA provides a challenging, practical benchmark to guide development of more robust and operationally meaningful vision-language models for disaster response. The dataset is publicly available to support research in this critical domain.

Abstract: Social media imagery provides a low-latency source of situational information during natural and human-induced disasters, enabling rapid damage assessment and response. While Visual Question Answering (VQA) has shown strong performance in general-purpose domains, its suitability for the complex and safety-critical reasoning required in disaster response remains unclear. We introduce DisasterVQA, a benchmark dataset designed for perception and reasoning in crisis contexts. DisasterVQA consists of 1,395 real-world images and 4,405 expert-curated question-answer pairs spanning diverse events such as floods, wildfires, and earthquakes. Grounded in humanitarian frameworks including FEMA ESF and OCHA MIRA, the dataset includes binary, multiple-choice, and open-ended questions covering situational awareness and operational decision-making tasks. We benchmark seven state-of-the-art vision-language models and find performance variability across question types, disaster categories, regions, and humanitarian tasks. Although models achieve high accuracy on binary questions, they struggle with fine-grained quantitative reasoning, object counting, and context-sensitive interpretation, particularly for underrepresented disaster scenarios. DisasterVQA provides a challenging and practical benchmark to guide the development of more robust and operationally meaningful vision-language models for disaster response. The dataset is publicly available at https://zenodo.org/records/18267770.

</details>


### [221] [Probabilistic Deep Discriminant Analysis for Wind Blade Segmentation](https://arxiv.org/abs/2601.13852)
*Raül Pérez-Gonzalo,Andreas Espersen,Antonio Agudo*

Main category: cs.CV

TL;DR: Deep Discriminant Analysis (DDA) optimizes Fisher criterion using deep networks for better class separability, with Probabilistic DDA (PDDA) adding probability loss to minimize class overlap - first applied to wind blade segmentation.


<details>
  <summary>Details</summary>
Motivation: Linear discriminant analysis has limitations with non-linearly separable data. The authors aim to overcome this by leveraging deep networks to directly optimize the Fisher criterion for improved class separability in complex data scenarios.

Method: Introduces Deep Discriminant Analysis (DDA) that directly optimizes Fisher criterion using deep networks. Uses signed between-class variance, sigmoid-bound outputs, and converts multiplicative to additive relationships for stability. Develops two stable DDA loss functions and adds probability loss to create Probabilistic DDA (PDDA).

Result: PDDA effectively minimizes class overlap in output distributions, produces highly confident predictions with reduced within-class variance. When applied to wind blade segmentation, shows notable advances in performance and consistency - first application of DDA to image segmentation.

Conclusion: PDDA successfully extends discriminant analysis to deep networks for complex data, demonstrating practical value in wind energy maintenance through improved wind blade segmentation performance and consistency.

Abstract: Linear discriminant analysis improves class separability but struggles with non-linearly separable data. To overcome this, we introduce Deep Discriminant Analysis (DDA), which directly optimizes the Fisher criterion utilizing deep networks. To ensure stable training and avoid computational instabilities, we incorporate signed between-class variance, bound outputs with a sigmoid function, and convert multiplicative relationships into additive ones. We present two stable DDA loss functions and augment them with a probability loss, resulting in Probabilistic DDA (PDDA). PDDA effectively minimizes class overlap in output distributions, producing highly confident predictions with reduced within-class variance. When applied to wind blade segmentation, PDDA showcases notable advances in performance and consistency, critical for wind energy maintenance. To our knowledge, this is the first application of DDA to image segmentation.

</details>


### [222] [OCCAM: Class-Agnostic, Training-Free, Prior-Free and Multi-Class Object Counting](https://arxiv.org/abs/2601.13871)
*Michail Spanakis,Iason Oikonomidis,Antonis Argyros*

Main category: cs.CV

TL;DR: OCCAM is a training-free, multi-class object counting method that uses SAM2 and custom FINCH clustering without needing exemplars or text prompts.


<details>
  <summary>Details</summary>
Motivation: Existing CAC methods require extensive training, assume single-class images, and rely on additional information like exemplars or text prompts. There's a need for training-free, multi-class approaches that work without supplementary data.

Method: Uses Segment Anything Model 2 (SAM2) foundation model with custom threshold-based variant of FINCH clustering algorithm. No training required and operates without exemplars or text prompts.

Result: Achieves competitive performance on FSC-147 and CARPK benchmarks. Introduces synthetic multi-class dataset and proposes F1 score as more suitable evaluation metric.

Conclusion: OCCAM is the first training-free CAC approach that handles multi-class counting without supplementary information, demonstrating competitive performance with foundation models and custom clustering.

Abstract: Class-Agnostic object Counting (CAC) involves counting instances of objects from arbitrary classes within an image. Due to its practical importance, CAC has received increasing attention in recent years. Most existing methods assume a single object class per image, rely on extensive training of large deep learning models and address the problem by incorporating additional information, such as visual exemplars or text prompts. In this paper, we present OCCAM, the first training-free approach to CAC that operates without the need of any supplementary information. Moreover, our approach addresses the multi-class variant of the problem, as it is capable of counting the object instances in each and every class among arbitrary object classes within an image. We leverage Segment Anything Model 2 (SAM2), a foundation model, and a custom threshold-based variant of the First Integer Neighbor Clustering Hierarchy (FINCH) algorithm to achieve competitive performance on widely used benchmark datasets, FSC-147 and CARPK. We propose a synthetic multi-class dataset and F1 score as a more suitable evaluation metric. The code for our method and the proposed synthetic dataset will be made publicly available at https://mikespanak.github.io/OCCAM_counter.

</details>


### [223] [Revisiting Multi-Task Visual Representation Learning](https://arxiv.org/abs/2601.13886)
*Shangzhe Di,Zhonghua Zhai,Weidi Xie*

Main category: cs.CV

TL;DR: MTV is a multi-task visual pretraining framework that combines vision-language contrastive learning, self-supervised learning, and dense spatial supervision to create a unified visual encoder with both global semantic understanding and fine-grained spatial precision.


<details>
  <summary>Details</summary>
Motivation: Current visual representation learning is bifurcated: vision-language models (like CLIP) excel at global semantic alignment but lack spatial precision, while self-supervised methods (like MAE, DINO) capture local structures but struggle with high-level semantics. These paradigms are complementary and should be integrated.

Method: MTV jointly optimizes a shared backbone across three objectives: vision-language contrastive learning, self-supervised learning, and dense spatial supervision. To avoid manual annotations, it leverages high-capacity "expert" models (Depth Anything V2, OWLv2) to synthesize dense pseudo-labels at scale.

Result: MTV achieves "best-of-both-worlds" performance, significantly enhancing fine-grained spatial reasoning without compromising global semantic understanding. The framework demonstrates scalable multi-task learning with high-quality pseudo-supervision.

Conclusion: Multi-task learning with high-quality pseudo-supervision is a scalable path toward more general visual encoders that combine the strengths of both vision-language and self-supervised paradigms.

Abstract: Current visual representation learning remains bifurcated: vision-language models (e.g., CLIP) excel at global semantic alignment but lack spatial precision, while self-supervised methods (e.g., MAE, DINO) capture intricate local structures yet struggle with high-level semantic context. We argue that these paradigms are fundamentally complementary and can be integrated into a principled multi-task framework, further enhanced by dense spatial supervision. We introduce MTV, a multi-task visual pretraining framework that jointly optimizes a shared backbone across vision-language contrastive, self-supervised, and dense spatial objectives. To mitigate the need for manual annotations, we leverage high-capacity "expert" models -- such as Depth Anything V2 and OWLv2 -- to synthesize dense, structured pseudo-labels at scale. Beyond the framework, we provide a systematic investigation into the mechanics of multi-task visual learning, analyzing: (i) the marginal gain of each objective, (ii) task synergies versus interference, and (iii) scaling behavior across varying data and model scales. Our results demonstrate that MTV achieves "best-of-both-worlds" performance, significantly enhancing fine-grained spatial reasoning without compromising global semantic understanding. Our findings suggest that multi-task learning, fueled by high-quality pseudo-supervision, is a scalable path toward more general visual encoders.

</details>


### [224] [OmniOVCD: Streamlining Open-Vocabulary Change Detection with SAM 3](https://arxiv.org/abs/2601.13895)
*Xu Zhang,Danyang Li,Yingjie Xia,Xiaohang Dong,Hualong Yu,Jianye Wang,Qicheng Li*

Main category: cs.CV

TL;DR: OmniOVCD is a standalone open-vocabulary change detection framework that leverages SAM 3's integrated segmentation and identification capabilities through a Synergistic Fusion to Instance Decoupling strategy, achieving state-of-the-art performance on multiple benchmarks.


<details>
  <summary>Details</summary>
Motivation: Existing training-free Open-Vocabulary Change Detection (OVCD) methods rely on combining different models like CLIP and DINO, which causes feature matching problems and system instability. The introduction of SAM 3, which integrates segmentation and identification in one promptable model, offers new possibilities for a more unified approach.

Method: Proposes OmniOVCD framework with Synergistic Fusion to Instance Decoupling (SFID) strategy. SFID leverages SAM 3's decoupled output heads to fuse semantic, instance, and presence outputs to construct land-cover masks, then decomposes them into individual instance masks for change comparison, maintaining instance-level consistency across images.

Result: Achieves state-of-the-art performance on four public benchmarks: LEVIR-CD (67.2 IoU), WHU-CD (66.5 IoU), S2Looking (24.5 IoU), and SECOND (27.1 IoU) in class-average scores, surpassing all previous methods.

Conclusion: OmniOVCD demonstrates that leveraging SAM 3's integrated capabilities through the SFID strategy enables accurate open-vocabulary change detection without relying on multiple disparate models, solving feature matching and stability issues while maintaining high category recognition accuracy and instance-level consistency.

Abstract: Change Detection (CD) is a fundamental task in remote sensing. It monitors the evolution of land cover over time. Based on this, Open-Vocabulary Change Detection (OVCD) introduces a new requirement. It aims to reduce the reliance on predefined categories. Existing training-free OVCD methods mostly use CLIP to identify categories. These methods also need extra models like DINO to extract features. However, combining different models often causes problems in matching features and makes the system unstable. Recently, the Segment Anything Model 3 (SAM 3) is introduced. It integrates segmentation and identification capabilities within one promptable model, which offers new possibilities for the OVCD task. In this paper, we propose OmniOVCD, a standalone framework designed for OVCD. By leveraging the decoupled output heads of SAM 3, we propose a Synergistic Fusion to Instance Decoupling (SFID) strategy. SFID first fuses the semantic, instance, and presence outputs of SAM 3 to construct land-cover masks, and then decomposes them into individual instance masks for change comparison. This design preserves high accuracy in category recognition and maintains instance-level consistency across images. As a result, the model can generate accurate change masks. Experiments on four public benchmarks (LEVIR-CD, WHU-CD, S2Looking, and SECOND) demonstrate SOTA performance, achieving IoU scores of 67.2, 66.5, 24.5, and 27.1 (class-average), respectively, surpassing all previous methods.

</details>


### [225] [Towards Visually Explaining Statistical Tests with Applications in Biomedical Imaging](https://arxiv.org/abs/2601.13899)
*Masoumeh Javanbakhat,Piotr Komorowski,Dilyara Bareeva,Wei-Chang Lai,Wojciech Samek,Christoph Lippert*

Main category: cs.CV

TL;DR: Proposes an explainable deep statistical testing framework that adds sample-level and feature-level explanations to deep two-sample tests, enabling interpretable, label-free population analysis in medical imaging.


<details>
  <summary>Details</summary>
Motivation: Deep neural two-sample tests have shown strong power for detecting distributional differences but their black-box nature limits interpretability and practical adoption in biomedical analysis. Existing explainability methods rely on class labels, making them unsuitable for label-free statistical testing settings.

Method: An explainable deep statistical testing framework that augments deep two-sample tests with sample-level and feature-level explanations. The method reveals which individual samples and which input features drive statistically significant group differences, providing spatial and instance-wise insight into the test's decision.

Result: The framework identifies influential samples and highlights anatomically meaningful regions associated with disease-related variation in biomedical imaging data. It provides both spatial (image regions) and instance-wise (individual samples) insight into detected group differences.

Conclusion: This work bridges statistical inference and explainable AI, enabling interpretable, label-free population analysis in medical imaging by making deep two-sample tests explainable at both sample and feature levels.

Abstract: Deep neural two-sample tests have recently shown strong power for detecting distributional differences between groups, yet their black-box nature limits interpretability and practical adoption in biomedical analysis. Moreover, most existing post-hoc explainability methods rely on class labels, making them unsuitable for label-free statistical testing settings. We propose an explainable deep statistical testing framework that augments deep two-sample tests with sample-level and feature-level explanations, revealing which individual samples and which input features drive statistically significant group differences. Our method highlights which image regions and which individual samples contribute most to the detected group difference, providing spatial and instance-wise insight into the test's decision. Applied to biomedical imaging data, the proposed framework identifies influential samples and highlights anatomically meaningful regions associated with disease-related variation. This work bridges statistical inference and explainable AI, enabling interpretable, label-free population analysis in medical imaging.

</details>


### [226] [On the Role of Rotation Equivariance in Monocular 3D Human Pose Estimation](https://arxiv.org/abs/2601.13913)
*Pavlo Melnyk,Cuong Le,Urs Waldmann,Per-Erik Forssén,Bastian Wandt*

Main category: cs.CV

TL;DR: Monocular 3D human pose estimation using 2D rotation equivariance learned through data augmentation outperforms equivariant-by-design methods.


<details>
  <summary>Details</summary>
Motivation: Existing 2D-to-3D lifting models fail when encountering rotated inputs. Learning pose with in-plane rotations is easier and more geometrically grounded than direct point-to-point mapping.

Method: Proposes learning 2D rotation equivariance through data augmentation rather than explicit architectural constraints. Uses standard two-step approach: 2D joint detection followed by 2D-to-3D lifting, but enhances it with rotation-aware training.

Result: Rotation equivariance improves model performance on human poses with image plane rotations. The augmentation-based approach outperforms state-of-the-art equivariant-by-design methods on common HPE benchmarks.

Conclusion: Learning rotation equivariance through data augmentation is more effective and straightforward than building equivariance into model architecture, leading to better performance in monocular 3D human pose estimation.

Abstract: Estimating 3D from 2D is one of the central tasks in computer vision. In this work, we consider the monocular setting, i.e. single-view input, for 3D human pose estimation (HPE). Here, the task is to predict a 3D point set of human skeletal joints from a single 2D input image. While by definition this is an ill-posed problem, recent work has presented methods that solve it with up to several-centimetre error. Typically, these methods employ a two-step approach, where the first step is to detect the 2D skeletal joints in the input image, followed by the step of 2D-to-3D lifting. We find that common lifting models fail when encountering a rotated input. We argue that learning a single human pose along with its in-plane rotations is considerably easier and more geometrically grounded than directly learning a point-to-point mapping. Furthermore, our intuition is that endowing the model with the notion of rotation equivariance without explicitly constraining its parameter space should lead to a more straightforward learning process than one with equivariance by design. Utilising the common HPE benchmarks, we confirm that the 2D rotation equivariance per se improves the model performance on human poses akin to rotations in the image plane, and can be efficiently and straightforwardly learned by augmentation, outperforming state-of-the-art equivariant-by-design methods.

</details>


### [227] [TrackletGPT: A Language-like GPT Framework for White Matter Tract Segmentation](https://arxiv.org/abs/2601.13935)
*Anoushkrit Goel,Simroop Singh,Ankita Joshi,Ranjeet Ranjan Jha,Chirag Ahuja,Aditya Nigam,Arnav Bhavsar*

Main category: cs.CV

TL;DR: TrackletGPT is a novel GPT-based framework for white matter tract segmentation that uses tracklets (granular sub-streamline segments) to encode sequential information, achieving state-of-the-art performance across multiple datasets.


<details>
  <summary>Details</summary>
Motivation: White matter tract segmentation is crucial for studying brain connectivity, neurological disorders, and neurosurgery, but remains challenging due to tract variability across subjects/conditions while maintaining similar 3D structure across hemispheres and subjects.

Method: Proposes TrackletGPT, a language-like GPT framework that reintroduces sequential information in tokens using tracklets (granular sub-streamline segments). The method is fully automatic, generalizes across datasets, and scales/refines GPT models for tractography segmentation.

Result: TrackletGPT outperforms state-of-the-art methods on average DICE, Overlap and Overreach scores on both TractoInferno and HCP datasets, including in inter-dataset experiments.

Conclusion: TrackletGPT successfully addresses tract segmentation challenges by leveraging GPT architecture with tracklets, demonstrating superior performance and generalization capabilities across different brain imaging datasets.

Abstract: White Matter Tract Segmentation is imperative for studying brain structural connectivity, neurological disorders and neurosurgery. This task remains complex, as tracts differ among themselves, across subjects and conditions, yet have similar 3D structure across hemispheres and subjects. To address these challenges, we propose TrackletGPT, a language-like GPT framework which reintroduces sequential information in tokens using tracklets. TrackletGPT generalises seamlessly across datasets, is fully automatic, and encodes granular sub-streamline segments, Tracklets, scaling and refining GPT models in Tractography Segmentation. Based on our experiments, TrackletGPT outperforms state-of-the-art methods on average DICE, Overlap and Overreach scores on TractoInferno and HCP datasets, even on inter-dataset experiments.

</details>


### [228] [Glance-or-Gaze: Incentivizing LMMs to Adaptively Focus Search via Reinforcement Learning](https://arxiv.org/abs/2601.13942)
*Hongbo Bai,Yujin Zhou,Yile Wu,Chi-Min Chan,Pengcheng Wen,Kunhao Pan,Sirui Han,Yike Guo*

Main category: cs.CV

TL;DR: GoG is an autonomous visual planning framework that uses selective gaze mechanisms and complexity-adaptive reinforcement learning to improve LMM performance on knowledge-intensive visual queries by reducing visual redundancy and enabling iterative reasoning.


<details>
  <summary>Details</summary>
Motivation: Large Multimodal Models struggle with knowledge-intensive queries involving long-tail entities or evolving information due to static parametric knowledge. Existing search-augmented approaches suffer from indiscriminate whole-image retrieval (introducing visual redundancy/noise) and lack deep iterative reflection for complex visual queries.

Method: Proposes Glance-or-Gaze (GoG) framework with: 1) Selective Gaze mechanism that dynamically chooses between glancing at global context or gazing into high-value regions, filtering irrelevant information before retrieval; 2) Dual-stage training: Reflective GoG Behavior Alignment via supervised fine-tuning, and Complexity-Adaptive Reinforcement Learning for iterative reasoning on complex queries.

Result: State-of-the-art performance across six benchmarks. Ablation studies confirm both Selective Gaze and complexity-adaptive RL are essential for effective visual search.

Conclusion: GoG successfully shifts from passive perception to active visual planning, addressing limitations of existing methods by reducing visual redundancy and enabling deep iterative reflection for complex visual queries.

Abstract: Large Multimodal Models (LMMs) have achieved remarkable success in visual understanding, yet they struggle with knowledge-intensive queries involving long-tail entities or evolving information due to static parametric knowledge. Recent search-augmented approaches attempt to address this limitation, but existing methods rely on indiscriminate whole-image retrieval that introduces substantial visual redundancy and noise, and lack deep iterative reflection, limiting their effectiveness on complex visual queries. To overcome these challenges, we propose Glance-or-Gaze (GoG), a fully autonomous framework that shifts from passive perception to active visual planning. GoG introduces a Selective Gaze mechanism that dynamically chooses whether to glance at global context or gaze into high-value regions, filtering irrelevant information before retrieval. We design a dual-stage training strategy: Reflective GoG Behavior Alignment via supervised fine-tuning instills the fundamental GoG paradigm, while Complexity-Adaptive Reinforcement Learning further enhances the model's capability to handle complex queries through iterative reasoning. Experiments across six benchmarks demonstrate state-of-the-art performance. Ablation studies confirm that both Selective Gaze and complexity-adaptive RL are essential for effective visual search. We will release our data and models for further exploration soon.

</details>


### [229] [VTONGuard: Automatic Detection and Authentication of AI-Generated Virtual Try-On Content](https://arxiv.org/abs/2601.13951)
*Shengyi Wu,Yan Hong,Shengyao Chen,Zheng Wang,Xianbing Sun,Jiahui Zhan,Jun Lan,Jianfu Zhang*

Main category: cs.CV

TL;DR: VTONGuard: A large-scale benchmark dataset of 775K+ real and synthetic virtual try-on images for evaluating AI-generated content detection methods, with a proposed multi-task framework achieving best performance.


<details>
  <summary>Details</summary>
Motivation: Address growing concerns about authenticity and responsible use of increasingly realistic AI-generated virtual try-on content in e-commerce and digital entertainment.

Method: Create VTONGuard dataset with 775K+ images covering diverse conditions; systematically evaluate detection paradigms; propose multi-task framework integrating auxiliary segmentation for boundary-aware feature learning.

Result: Dataset enables fair comparisons; reveals strengths/weaknesses of existing methods; highlights cross-paradigm generalization challenge; proposed framework achieves best overall performance.

Conclusion: VTONGuard benchmark facilitates development of robust detection models and promotes safe, responsible deployment of virtual try-on technologies.

Abstract: With the rapid advancement of generative AI, virtual try-on (VTON) systems are becoming increasingly common in e-commerce and digital entertainment. However, the growing realism of AI-generated try-on content raises pressing concerns about authenticity and responsible use. To address this, we present VTONGuard, a large-scale benchmark dataset containing over 775,000 real and synthetic try-on images. The dataset covers diverse real-world conditions, including variations in pose, background, and garment styles, and provides both authentic and manipulated examples. Based on this benchmark, we conduct a systematic evaluation of multiple detection paradigms under unified training and testing protocols. Our results reveal each method's strengths and weaknesses and highlight the persistent challenge of cross-paradigm generalization. To further advance detection, we design a multi-task framework that integrates auxiliary segmentation to enhance boundary-aware feature learning, achieving the best overall performance on VTONGuard. We expect this benchmark to enable fair comparisons, facilitate the development of more robust detection models, and promote the safe and responsible deployment of VTON technologies in practice.

</details>


### [230] [DExTeR: Weakly Semi-Supervised Object Detection with Class and Instance Experts for Medical Imaging](https://arxiv.org/abs/2601.13954)
*Adrien Meyer,Didier Mutter,Nicolas Padoy*

Main category: cs.CV

TL;DR: DExTeR is a transformer-based Point-to-Box regressor for medical imaging that converts single-point annotations into pseudo-box labels using class-guided deformable attention and CLICK-MoE architecture to handle overlapping anatomy and variable object sizes.


<details>
  <summary>Details</summary>
Motivation: Medical imaging requires anatomical landmark detection for diagnosis, but traditional object detection needs costly bounding box annotations. Weakly semi-supervised approaches with point annotations reduce annotation time, but struggle with medical imaging challenges like overlapping anatomy, variable sizes, and elusive structures.

Method: DExTeR builds on Point-DETR, encoding single-point annotations as object queries. It uses class-guided deformable attention to guide sampling with point coordinates and class labels, CLICK-MoE to decouple class and instance representations, and multi-point training for robustness to annotation variability.

Result: Achieves state-of-the-art performance across three medical imaging datasets (endoscopy, chest X-rays, endoscopic ultrasound), demonstrating effectiveness in reducing annotation costs while maintaining high detection accuracy.

Conclusion: DExTeR effectively addresses medical imaging challenges in weakly semi-supervised object detection, offering a practical solution to reduce annotation burden while achieving accurate landmark detection across diverse medical domains.

Abstract: Detecting anatomical landmarks in medical imaging is essential for diagnosis and intervention guidance. However, object detection models rely on costly bounding box annotations, limiting scalability. Weakly Semi-Supervised Object Detection (WSSOD) with point annotations proposes annotating each instance with a single point, minimizing annotation time while preserving localization signals. A Point-to-Box teacher model, trained on a small box-labeled subset, converts these point annotations into pseudo-box labels to train a student detector. Yet, medical imagery presents unique challenges, including overlapping anatomy, variable object sizes, and elusive structures, which hinder accurate bounding box inference. To overcome these challenges, we introduce DExTeR (DETR with Experts), a transformer-based Point-to-Box regressor tailored for medical imaging. Built upon Point-DETR, DExTeR encodes single-point annotations as object queries, refining feature extraction with the proposed class-guided deformable attention, which guides attention sampling using point coordinates and class labels to capture class-specific characteristics. To improve discrimination in complex structures, it introduces CLICK-MoE (CLass, Instance, and Common Knowledge Mixture of Experts), decoupling class and instance representations to reduce confusion among adjacent or overlapping instances. Finally, we implement a multi-point training strategy which promotes prediction consistency across different point placements, improving robustness to annotation variability. DExTeR achieves state-of-the-art performance across three datasets spanning different medical domains (endoscopy, chest X-rays, and endoscopic ultrasound) highlighting its potential to reduce annotation costs while maintaining high detection accuracy.

</details>


### [231] [STEC: A Reference-Free Spatio-Temporal Entropy Coverage Metric for Evaluating Sampled Video Frames](https://arxiv.org/abs/2601.13974)
*Shih-Yao Lin*

Main category: cs.CV

TL;DR: STEC is a new metric for evaluating video frame sampling quality by measuring spatial information, temporal coverage, and redundancy, without requiring reference frames or task-specific evaluation.


<details>
  <summary>Details</summary>
Motivation: Existing metrics for frame sampling focus on perceptual quality or reconstruction fidelity, but don't assess whether sampled frames adequately capture informative and representative video content. There's a need for a task-agnostic way to evaluate sampling effectiveness.

Method: STEC builds on Spatio-Temporal Frame Entropy (STFE) which measures per-frame spatial information via entropy-based structural complexity. STEC evaluates sampled frames based on their temporal coverage and redundancy, jointly modeling spatial information strength, temporal dispersion, and non-redundancy.

Result: On MSR-VTT test-1k benchmark, STEC clearly differentiates common sampling strategies (random, uniform, content-aware methods). It reveals robustness patterns across individual videos not captured by average performance alone, showing practical value as a general-purpose evaluation tool.

Conclusion: STEC provides a principled, lightweight, task-agnostic diagnostic signal for analyzing frame sampling behavior under constrained budgets, though it's not designed to predict downstream task accuracy directly.

Abstract: Frame sampling is a fundamental component in video understanding and video--language model pipelines, yet evaluating the quality of sampled frames remains challenging. Existing evaluation metrics primarily focus on perceptual quality or reconstruction fidelity, and are not designed to assess whether a set of sampled frames adequately captures informative and representative video content.
  We propose Spatio-Temporal Entropy Coverage (STEC), a simple and non-reference metric for evaluating the effectiveness of video frame sampling. STEC builds upon Spatio-Temporal Frame Entropy (STFE), which measures per-frame spatial information via entropy-based structural complexity, and evaluates sampled frames based on their temporal coverage and redundancy. By jointly modeling spatial information strength, temporal dispersion, and non-redundancy, STEC provides a principled and lightweight measure of sampling quality.
  Experiments on the MSR-VTT test-1k benchmark demonstrate that STEC clearly differentiates common sampling strategies, including random, uniform, and content-aware methods. We further show that STEC reveals robustness patterns across individual videos that are not captured by average performance alone, highlighting its practical value as a general-purpose evaluation tool for efficient video understanding.
  We emphasize that STEC is not designed to predict downstream task accuracy, but to provide a task-agnostic diagnostic signal for analyzing frame sampling behavior under constrained budgets.

</details>


### [232] [Harmonizing the Deep: A Unified Information Pipeline for Robust Marine Biodiversity Assessment Across Heterogeneous Domains](https://arxiv.org/abs/2601.13975)
*Marco Piccolo,Qiwei Han,Astrid van Toor,Joachim Vanneste*

Main category: cs.CV

TL;DR: The paper develops a unified detection pipeline for marine biodiversity monitoring that addresses cross-domain performance degradation, finding structural factors (scene composition, object density) matter more than visual degradation for reliable invasive species detection.


<details>
  <summary>Details</summary>
Motivation: Existing marine biodiversity detection solutions suffer from deployment gaps where performance degrades sharply when transferred to new sites, creating challenges for conservation and invasive-species management in complex underwater environments.

Method: Developed a Unified Information Pipeline that standardizes heterogeneous datasets into comparable information flow, evaluated a fixed deployment-relevant detector under controlled cross-domain protocols, and benchmarked inference on low-cost edge hardware.

Result: Structural factors (scene composition, object density, contextual redundancy) explain cross-domain performance loss more strongly than visual degradation like turbidity; sparse scenes cause "Context Collapse" failure; runtime optimization enables practical sampling rates on edge hardware.

Conclusion: The work shifts emphasis from image enhancement toward structure-aware reliability, providing a democratized tool for consistent marine ecosystem assessment that addresses the deployment gap in marine biodiversity monitoring.

Abstract: Marine biodiversity monitoring requires scalability and reliability across complex underwater environments to support conservation and invasive-species management. Yet existing detection solutions often exhibit a pronounced deployment gap, with performance degrading sharply when transferred to new sites. This work establishes the foundational detection layer for a multi-year invasive species monitoring initiative targeting Arctic and Atlantic marine ecosystems. We address this challenge by developing a Unified Information Pipeline that standardises heterogeneous datasets into a comparable information flow and evaluates a fixed, deployment-relevant detector under controlled cross-domain protocols. Across multiple domains, we find that structural factors, such as scene composition, object density, and contextual redundancy, explain cross-domain performance loss more strongly than visual degradation such as turbidity, with sparse scenes inducing a characteristic "Context Collapse" failure mode. We further validate operational feasibility by benchmarking inference on low-cost edge hardware, showing that runtime optimisation enables practical sampling rates for remote monitoring. The results shift emphasis from image enhancement toward structure-aware reliability, providing a democratised tool for consistent marine ecosystem assessment.

</details>


### [233] [FantasyVLN: Unified Multimodal Chain-of-Thought Reasoning for Vision-Language Navigation](https://arxiv.org/abs/2601.13976)
*Jing Zuo,Lingzhou Mu,Fan Jiang,Chengcheng Ma,Mu Xu,Yonggang Qi*

Main category: cs.CV

TL;DR: FantasyVLN is a novel implicit reasoning framework for Vision-and-Language Navigation that achieves CoT reasoning benefits without explicit token overhead by encoding imagined visual tokens into a compact latent space.


<details>
  <summary>Details</summary>
Motivation: Existing CoT approaches for VLN have critical drawbacks: textual CoTs lack spatial grounding and overfit to sparse annotations, while multimodal CoTs suffer from severe token inflation from generating imagined visual observations, making real-time navigation impractical.

Method: Proposes FantasyVLN with a pretrained Visual AutoRegressor (VAR) to encode imagined visual tokens into a compact latent space during CoT training, and uses a unified multi-CoT strategy where the model jointly learns from textual, visual, and multimodal CoT modes.

Result: Extensive experiments on LH-VLN show reasoning-aware yet real-time navigation, improving success rates and efficiency while reducing inference latency by an order of magnitude compared to explicit CoT methods.

Conclusion: FantasyVLN provides a unified implicit reasoning framework that preserves CoT reasoning benefits without explicit token overhead, enabling practical real-time navigation while maintaining reasoning capabilities.

Abstract: Achieving human-level performance in Vision-and-Language Navigation (VLN) requires an embodied agent to jointly understand multimodal instructions and visual-spatial context while reasoning over long action sequences. Recent works, such as NavCoT and NavGPT-2, demonstrate the potential of Chain-of-Thought (CoT) reasoning for improving interpretability and long-horizon planning. Moreover, multimodal extensions like OctoNav-R1 and CoT-VLA further validate CoT as a promising pathway toward human-like navigation reasoning. However, existing approaches face critical drawbacks: purely textual CoTs lack spatial grounding and easily overfit to sparse annotated reasoning steps, while multimodal CoTs incur severe token inflation by generating imagined visual observations, making real-time navigation impractical. In this work, we propose FantasyVLN, a unified implicit reasoning framework that preserves the benefits of CoT reasoning without explicit token overhead. Specifically, imagined visual tokens are encoded into a compact latent space using a pretrained Visual AutoRegressor (VAR) during CoT reasoning training, and the model jointly learns from textual, visual, and multimodal CoT modes under a unified multi-CoT strategy. At inference, our model performs direct instruction-to-action mapping while still enjoying reasoning-aware representations. Extensive experiments on LH-VLN show that our approach achieves reasoning-aware yet real-time navigation, improving success rates and efficiency while reducing inference latency by an order of magnitude compared to explicit CoT methods.

</details>


### [234] [Equivariant Learning for Unsupervised Image Dehazing](https://arxiv.org/abs/2601.13986)
*Zhang Wen,Jiangwei Xie,Dongdong Chen*

Main category: cs.CV

TL;DR: EID is an unsupervised image dehazing framework that uses equivariant learning and adversarial training to remove haze without needing clean ground truth or handcrafted priors, showing strong performance on both scientific and natural images.


<details>
  <summary>Details</summary>
Motivation: Current image dehazing methods require expensive clean ground truth or carefully designed priors, which are impractical for scientific imaging where such data is scarce or unavailable.

Method: EID uses equivariant learning to exploit image symmetry, enforcing haze consistency and systematic equivariance. It also employs adversarial learning to model unknown haze physics, enabling unsupervised haze removal from raw images.

Result: EID significantly outperforms state-of-the-art methods on scientific image benchmarks (cell microscopy, medical endoscopy) and natural image dehazing tasks.

Conclusion: By combining equivariant learning with haze physics modeling, EID enables versatile and effective haze removal in scientific imaging without requiring clean ground truth or handcrafted priors.

Abstract: Image Dehazing (ID) aims to produce a clear image from an observation contaminated by haze. Current ID methods typically rely on carefully crafted priors or extensive haze-free ground truth, both of which are expensive or impractical to acquire, particularly in the context of scientific imaging. We propose a new unsupervised learning framework called Equivariant Image Dehazing (EID) that exploits the symmetry of image signals to restore clarity to hazy observations. By enforcing haze consistency and systematic equivariance, EID can recover clear patterns directly from raw, hazy images. Additionally, we propose an adversarial learning strategy to model unknown haze physics and facilitate EID learning. Experiments on two scientific image dehazing benchmarks (including cell microscopy and medical endoscopy) and on natural image dehazing have demonstrated that EID significantly outperforms state-of-the-art approaches. By unifying equivariant learning with modelling haze physics, we hope that EID will enable more versatile and effective haze removal in scientific imaging. Code and datasets will be published.

</details>


### [235] [Likelihood-Separable Diffusion Inference for Multi-Image MRI Super-Resolution](https://arxiv.org/abs/2601.14030)
*Samuel W. Remedios,Zhangxing Bian,Shuwen Wei,Aaron Carass,Jerry L. Prince,Blake E. Dewey*

Main category: cs.CV

TL;DR: Generalizes diffusion-based inverse problem solvers for multi-image super-resolution MRI, enabling reconstruction of near-isotropic anatomy from routine 2D multi-slice acquisitions without modifying diffusion models or increasing computational cost.


<details>
  <summary>Details</summary>
Motivation: Current diffusion-based inverse problem solvers focus on single-image problems, but MRI often involves multiple complementary low-resolution measurements along different axes. There's a need to extend these methods to handle multi-image super-resolution for better MRI reconstruction.

Method: Generalizes common diffusion-based inverse solvers (DPS, DMAP, DPPS, PnP/ADMM) for multi-image super-resolution by showing DPS likelihood correction allows exactly-separable gradient decomposition across independently acquired measurements. This enables MISR without constructing joint operators, modifying diffusion models, or increasing network evaluations.

Result: Demonstrates substantial gains over single-image super-resolution across 4×/8×/16× anisotropic degradations. Achieves state-of-the-art super-resolution of anisotropic MRI volumes and enables reconstruction of near-isotropic anatomy from routine 2D multi-slice acquisitions.

Conclusion: The proposed approach successfully extends diffusion-based inverse problem solvers to multi-image MRI super-resolution, providing a practical solution for reconstructing high-quality 3D volumes from standard clinical 2D acquisitions without additional computational overhead.

Abstract: Diffusion models are the current state-of-the-art for solving inverse problems in imaging. Their impressive generative capability allows them to approximate sampling from a prior distribution, which alongside a known likelihood function permits posterior sampling without retraining the model. While recent methods have made strides in advancing the accuracy of posterior sampling, the majority focuses on single-image inverse problems. However, for modalities such as magnetic resonance imaging (MRI), it is common to acquire multiple complementary measurements, each low-resolution along a different axis. In this work, we generalize common diffusion-based inverse single-image problem solvers for multi-image super-resolution (MISR) MRI. We show that the DPS likelihood correction allows an exactly-separable gradient decomposition across independently acquired measurements, enabling MISR without constructing a joint operator, modifying the diffusion model, or increasing network function evaluations. We derive MISR versions of DPS, DMAP, DPPS, and diffusion-based PnP/ADMM, and demonstrate substantial gains over SISR across $4\times/8\times/16\times$ anisotropic degradations. Our results achieve state-of-the-art super-resolution of anisotropic MRI volumes and, critically, enable reconstruction of near-isotropic anatomy from routine 2D multi-slice acquisitions, which are otherwise highly degraded in orthogonal views.

</details>


### [236] [Human detectors are surprisingly powerful reward models](https://arxiv.org/abs/2601.14037)
*Kumar Ashutosh,XuDong Wang,Xi Yin,Kristen Grauman,Adam Polyak,Ishan Misra,Rohit Girdhar*

Main category: cs.CV

TL;DR: HuDA is a simple reward model that improves human motion quality in video generation by combining human detection confidence and temporal prompt alignment, outperforming specialized models without additional training.


<details>
  <summary>Details</summary>
Motivation: Current video generation models struggle with complex non-rigid motions, especially for humans performing dynamic actions like sports and dance, often producing distorted poses, missing limbs, or physically implausible actions.

Method: HuDA integrates two components: human detection confidence for appearance quality and temporal prompt alignment score for motion realism. It uses off-the-shelf models without additional training and is applied via Group Reward Policy Optimization (GRPO) post-training.

Result: HuDA outperforms specialized models finetuned with manually annotated data, achieves 73% win-rate against state-of-the-art models like Wan 2.1, and improves generation quality beyond humans to include animals and human-object interactions.

Conclusion: A simple reward model leveraging existing components can effectively quantify and improve human motion quality in video generation, demonstrating broad applicability beyond just human motion enhancement.

Abstract: Video generation models have recently achieved impressive visual fidelity and temporal coherence. Yet, they continue to struggle with complex, non-rigid motions, especially when synthesizing humans performing dynamic actions such as sports, dance, etc. Generated videos often exhibit missing or extra limbs, distorted poses, or physically implausible actions. In this work, we propose a remarkably simple reward model, HuDA, to quantify and improve the human motion in generated videos. HuDA integrates human detection confidence for appearance quality, and a temporal prompt alignment score to capture motion realism. We show this simple reward function that leverages off-the-shelf models without any additional training, outperforms specialized models finetuned with manually annotated data. Using HuDA for Group Reward Policy Optimization (GRPO) post-training of video models, we significantly enhance video generation, especially when generating complex human motions, outperforming state-of-the-art models like Wan 2.1, with win-rate of 73%. Finally, we demonstrate that HuDA improves generation quality beyond just humans, for instance, significantly improving generation of animal videos and human-object interactions.

</details>


### [237] [Correcting and Quantifying Systematic Errors in 3D Box Annotations for Autonomous Driving](https://arxiv.org/abs/2601.14038)
*Alexandre Justo Miro,Ludvig af Klinteberg,Bogdan Timus,Aron Asefaw,Ajinkya Khoche,Thomas Gustafsson,Sina Sharif Mansouri,Masoud Daneshtalab*

Main category: cs.CV

TL;DR: The paper identifies systematic 3D box annotation errors in autonomous vehicle datasets caused by dynamic object motion and sensor timing, proposes a correction method to achieve physically feasible trajectories, and shows these errors significantly impact benchmarking results.


<details>
  <summary>Details</summary>
Motivation: Ground truth annotations are critical for supervised learning and evaluating autonomous vehicle systems, but 3D box annotations in dynamic scenarios suffer from systematic errors due to objects being observed at different timestamps and positions by active sensors like LiDAR.

Method: A novel offline estimation method that corrects annotations to follow physically feasible trajectories and achieve spatial and temporal consistency with sensor data, with defined metrics for evaluation.

Result: The method increases annotation quality by more than 17% across Argoverse 2, MAN TruckScenes, and proprietary datasets, correcting annotation errors of up to 2.5m (most severe for highly dynamic objects). The errors impact benchmarking more than typical state-of-the-art improvements.

Conclusion: Accurate annotations are essential for correct interpretation of autonomous vehicle performance, and the discovered annotation errors in widely used datasets are larger than typical improvements between state-of-the-art methods, highlighting the importance of proper annotation correction.

Abstract: Accurate ground truth annotations are critical to supervised learning and evaluating the performance of autonomous vehicle systems. These vehicles are typically equipped with active sensors, such as LiDAR, which scan the environment in predefined patterns. 3D box annotation based on data from such sensors is challenging in dynamic scenarios, where objects are observed at different timestamps, hence different positions. Without proper handling of this phenomenon, systematic errors are prone to being introduced in the box annotations. Our work is the first to discover such annotation errors in widely used, publicly available datasets. Through our novel offline estimation method, we correct the annotations so that they follow physically feasible trajectories and achieve spatial and temporal consistency with the sensor data. For the first time, we define metrics for this problem; and we evaluate our method on the Argoverse 2, MAN TruckScenes, and our proprietary datasets. Our approach increases the quality of box annotations by more than 17% in these datasets. Furthermore, we quantify the annotation errors in them and find that the original annotations are misplaced by up to 2.5 m, with highly dynamic objects being the most affected. Finally, we test the impact of the errors in benchmarking and find that the impact is larger than the improvements that state-of-the-art methods typically achieve with respect to the previous state-of-the-art methods; showing that accurate annotations are essential for correct interpretation of performance. Our code is available at https://github.com/alexandre-justo-miro/annotation-correction-3D-boxes.

</details>


### [238] [Generalizing Abstention for Noise-Robust Learning in Medical Image Segmentation](https://arxiv.org/abs/2601.14039)
*Wesam Moustafa,Hossam Elsafty,Helen Schneider,Lorenz Sparrenberg,Rafet Sifa*

Main category: cs.CV

TL;DR: A universal abstention framework for medical image segmentation that enhances noise-robustness by allowing models to selectively ignore corrupted samples, outperforming baselines especially under high label noise.


<details>
  <summary>Details</summary>
Motivation: Label noise is a critical problem in medical image segmentation due to difficulty of manual annotation, causing models to overfit and degrade generalization. While abstention mechanisms work well for classification tasks, their potential in segmentation remains unverified.

Method: Introduces a universal modular abstention framework with two key components: 1) informed regularization term to guide abstention behavior, and 2) flexible power-law-based auto-tuning algorithm for abstention penalty. Framework integrates with various loss functions to create noise-robust variants (GAC, SAC, ADS).

Result: Experiments on CaDIS and DSAD medical datasets show methods consistently and significantly outperform non-abstaining baselines, especially under high noise levels. Framework demonstrates versatility across different loss functions.

Conclusion: Enabling models to selectively ignore corrupted samples is a powerful and generalizable strategy for building more reliable segmentation models. The abstention framework effectively addresses label noise in medical image segmentation.

Abstract: Label noise is a critical problem in medical image segmentation, often arising from the inherent difficulty of manual annotation. Models trained on noisy data are prone to overfitting, which degrades their generalization performance. While a number of methods and strategies have been proposed to mitigate noisy labels in the segmentation domain, this area remains largely under-explored. The abstention mechanism has proven effective in classification tasks by enhancing the capabilities of Cross Entropy, yet its potential in segmentation remains unverified. In this paper, we address this gap by introducing a universal and modular abstention framework capable of enhancing the noise-robustness of a diverse range of loss functions. Our framework improves upon prior work with two key components: an informed regularization term to guide abstention behaviour, and a more flexible power-law-based auto-tuning algorithm for the abstention penalty. We demonstrate the framework's versatility by systematically integrating it with three distinct loss functions to create three novel, noise-robust variants: GAC, SAC, and ADS. Experiments on the CaDIS and DSAD medical datasets show our methods consistently and significantly outperform their non-abstaining baselines, especially under high noise levels. This work establishes that enabling models to selectively ignore corrupted samples is a powerful and generalizable strategy for building more reliable segmentation models. Our code is publicly available at https://github.com/wemous/abstention-for-segmentation.

</details>


### [239] [Federated Balanced Learning](https://arxiv.org/abs/2601.14042)
*Jiaze Li,Haoran Xu,Wanyi Wu,Changwei Wang,Shuaiguang Li,Jianzhong Ju,Zhenbo Luo,Jian Luan,Youyang Qu,Longxiang Gao,Xudong Yang,Lumin Xing*

Main category: cs.CV

TL;DR: FBL addresses client drift in non-iid federated learning by achieving sample balance on client side through knowledge filling/sampling with edge-side generation models.


<details>
  <summary>Details</summary>
Motivation: In non-iid federated learning, client drift seriously affects final model performance. Previous methods correct the already-deviated global model based on loss/gradient, overlooking client sample impact.

Method: Federated Balanced Learning (FBL) achieves sample balance on client side through knowledge filling and knowledge sampling using edge-side generation models, with fixed data sample constraints. Includes Knowledge Alignment Strategy (bridges synthetic-real data gap) and Knowledge Drop Strategy (regularization). Scales to complex scenarios with heterogeneous client methods.

Result: Numerous experiments show FBL outperforms state-of-the-art baselines.

Conclusion: FBL effectively prevents client drift from the beginning through client-side sample balance, offering a novel approach to non-iid federated learning challenges.

Abstract: Federated learning is a paradigm of joint learning in which clients collaborate by sharing model parameters instead of data. However, in the non-iid setting, the global model experiences client drift, which can seriously affect the final performance of the model. Previous methods tend to correct the global model that has already deviated based on the loss function or gradient, overlooking the impact of the client samples. In this paper, we rethink the role of the client side and propose Federated Balanced Learning, i.e., FBL, to prevent this issue from the beginning through sample balance on the client side. Technically, FBL allows unbalanced data on the client side to achieve sample balance through knowledge filling and knowledge sampling using edge-side generation models, under the limitation of a fixed number of data samples on clients. Furthermore, we design a Knowledge Alignment Strategy to bridge the gap between synthetic and real data, and a Knowledge Drop Strategy to regularize our method. Meanwhile, we scale our method to real and complex scenarios, allowing different clients to adopt various methods, and extend our framework to further improve performance. Numerous experiments show that our method outperforms state-of-the-art baselines. The code is released upon acceptance.

</details>


### [240] [Weather-R1: Logically Consistent Reinforcement Fine-Tuning for Multimodal Reasoning in Meteorology](https://arxiv.org/abs/2601.14044)
*Kaiyu Wu,Pucheng Han,Hualong Zhang,Naigeng Wu,Keze Wang*

Main category: cs.CV

TL;DR: Weather-R1: First logically faithful reasoning VLM for meteorology using LoCo-RFT to fix self-contradictory reasoning, improving WeatherQA benchmark performance by 9.8%.


<details>
  <summary>Details</summary>
Motivation: VLMs have domain gap in meteorology and reasoning faithfulness gap where reinforcement fine-tuning causes self-contradictory reasoning (contradiction between reasoning steps and final answer), which is unacceptable in high-stakes meteorological applications.

Method: 1) Construct WeatherQA multimodal reasoning benchmark for meteorology; 2) Propose LoCo-RFT (Logically Consistent Reinforcement Fine-Tuning) with logical consistency reward to resolve self-contradictory reasoning; 3) Develop Weather-R1 as first reasoning VLM with logical faithfulness in meteorology.

Result: Weather-R1 improves WeatherQA performance by 9.8 percentage points over baseline, outperforms Supervised Fine-Tuning and standard RFT, and even surpasses original Qwen2.5-VL-32B model.

Conclusion: LoCo-RFT effectively resolves self-contradictory reasoning in VLMs for meteorology, Weather-R1 demonstrates superiority as first logically faithful reasoning VLM in this domain, addressing critical gaps for high-stakes applications.

Abstract: While Vision Language Models (VLMs) show advancing reasoning capabilities, their application in meteorology is constrained by a domain gap and a reasoning faithfulness gap. Specifically, mainstream Reinforcement Fine-Tuning (RFT) can induce Self-Contradictory Reasoning (Self-Contra), where the model's reasoning contradicts its final answer, which is unacceptable in such a high-stakes domain. To address these challenges, we construct WeatherQA, a novel multimodal reasoning benchmark in meteorology. We also propose Logically Consistent Reinforcement Fine-Tuning (LoCo-RFT), which resolves Self-Contra by introducing a logical consistency reward. Furthermore, we introduce Weather-R1, the first reasoning VLM with logical faithfulness in meteorology, to the best of our knowledge. Experiments demonstrate that Weather-R1 improves performance on WeatherQA by 9.8 percentage points over the baseline, outperforming Supervised Fine-Tuning and RFT, and even surpassing the original Qwen2.5-VL-32B. These results highlight the effectiveness of our LoCo-RFT and the superiority of Weather-R1. Our benchmark and code are available at https://github.com/Marcowky/Weather-R1.

</details>


### [241] [Vision Also You Need: Navigating Out-of-Distribution Detection with Multimodal Large Language Model](https://arxiv.org/abs/2601.14052)
*Haoran Xu,Yanlin Liu,Zizhao Tong,Jiaze Li,Kexue Fu,Yuyang Zhang,Longxiang Gao,Shuaiguang Li,Xingyu Li,Yanran Xu,Changwei Wang*

Main category: cs.CV

TL;DR: MM-OOD: A novel multimodal OOD detection pipeline using MLLMs' reasoning and conversation capabilities for improved near and far OOD detection.


<details>
  <summary>Details</summary>
Motivation: Current zero-shot OOD detection methods over-rely on text-space knowledge from LLMs, neglecting the inherent challenges of detecting OOD samples in image space. There's a need for better multimodal approaches that leverage both visual and textual information.

Method: Proposes MM-OOD pipeline using MLLMs' multimodal reasoning and multi-round conversation capabilities. For near OOD: directly feed ID images and text prompts to MLLMs. For far OOD: introduce sketch-generate-elaborate framework - sketch outlier exposure with text prompts, generate visual OOD samples, then elaborate using multimodal prompts.

Result: Achieves significant improvements on widely used multimodal datasets like Food-101, and validates scalability on ImageNet-1K.

Conclusion: MM-OOD effectively leverages MLLMs' multimodal capabilities to address limitations of text-only approaches, improving both near and far OOD detection performance across diverse datasets.

Abstract: Out-of-Distribution (OOD) detection is a critical task that has garnered significant attention. The emergence of CLIP has spurred extensive research into zero-shot OOD detection, often employing a training-free approach. Current methods leverage expert knowledge from large language models (LLMs) to identify potential outliers. However, these approaches tend to over-rely on knowledge in the text space, neglecting the inherent challenges involved in detecting out-of-distribution samples in the image space. In this paper, we propose a novel pipeline, MM-OOD, which leverages the multimodal reasoning capabilities of MLLMs and their ability to conduct multi-round conversations for enhanced outlier detection. Our method is designed to improve performance in both near OOD and far OOD tasks. Specifically, (1) for near OOD tasks, we directly feed ID images and corresponding text prompts into MLLMs to identify potential outliers; and (2) for far OOD tasks, we introduce the sketch-generate-elaborate framework: first, we sketch outlier exposure using text prompts, then generate corresponding visual OOD samples, and finally elaborate by using multimodal prompts. Experiments demonstrate that our method achieves significant improvements on widely used multimodal datasets such as Food-101, while also validating its scalability on ImageNet-1K.

</details>


### [242] [Decoder-Free Supervoxel GNN for Accurate Brain-Tumor Localization in Multi-Modal MRI](https://arxiv.org/abs/2601.14055)
*Andrea Protani,Marc Molina Van Den Bosch,Lorenzo Giusti,Heloisa Barbosa Da Silva,Paolo Cacace,Albert Sund Aillet,Miguel Angel Gonzalez Ballester,Friedhelm Hummel,Luigi Serio*

Main category: cs.CV

TL;DR: SVGFormer is a decoder-free 3D medical imaging pipeline that partitions volumes into semantic graphs of supervoxels, using a hierarchical encoder with patch-level Transformer and supervoxel-level Graph Attention Network for feature learning.


<details>
  <summary>Details</summary>
Motivation: Traditional 3D medical vision backbones use parameter-heavy encoder-decoder structures that allocate significant parameters to spatial reconstruction rather than feature learning. The authors aim to create a more efficient and interpretable alternative.

Method: Introduces SVGFormer with content-aware grouping that partitions 3D volumes into semantic graphs of supervoxels. Uses hierarchical encoder combining patch-level Transformer for intra-region features and supervoxel-level Graph Attention Network for inter-regional dependencies. This decoder-free design concentrates all learnable capacity on feature encoding.

Result: Trained two specialized models on BraTS dataset: node-level classification achieved F1-score of 0.875, and tumor proportion regression achieved MAE of 0.028. Both models showed strong performance, confirming the encoder's ability to learn discriminative and localized features.

Conclusion: A graph-based, encoder-only paradigm offers an accurate and inherently interpretable alternative for 3D medical image representation, providing dual-scale explainability from patch to region level while concentrating computational resources on feature learning rather than spatial reconstruction.

Abstract: Modern vision backbones for 3D medical imaging typically process dense voxel grids through parameter-heavy encoder-decoder structures, a design that allocates a significant portion of its parameters to spatial reconstruction rather than feature learning. Our approach introduces SVGFormer, a decoder-free pipeline built upon a content-aware grouping stage that partitions the volume into a semantic graph of supervoxels. Its hierarchical encoder learns rich node representations by combining a patch-level Transformer with a supervoxel-level Graph Attention Network, jointly modeling fine-grained intra-region features and broader inter-regional dependencies. This design concentrates all learnable capacity on feature encoding and provides inherent, dual-scale explainability from the patch to the region level. To validate the framework's flexibility, we trained two specialized models on the BraTS dataset: one for node-level classification and one for tumor proportion regression. Both models achieved strong performance, with the classification model achieving a F1-score of 0.875 and the regression model a MAE of 0.028, confirming the encoder's ability to learn discriminative and localized features. Our results establish that a graph-based, encoder-only paradigm offers an accurate and inherently interpretable alternative for 3D medical image representation.

</details>


### [243] [POCI-Diff: Position Objects Consistently and Interactively with 3D-Layout Guided Diffusion](https://arxiv.org/abs/2601.14056)
*Andrea Rigo,Luca Stornaiuolo,Weijie Wang,Mauro Martino,Bruno Lepri,Nicu Sebe*

Main category: cs.CV

TL;DR: POCI-Diff is a diffusion-based text-to-image generation framework that provides consistent 3D layout control and interactive editing without geometric distortions, using semantic binding to 3D bounding boxes and warping-free editing.


<details>
  <summary>Details</summary>
Motivation: Prior methods for spatial control in text-to-image generation often distort object geometry and fail to preserve consistency across edits, using 2D cues or iterative copy-warp-paste strategies that cause artifacts.

Method: Introduces POCI-Diff framework with: 1) Joint enforcement of 3D geometric constraints and instance-level semantic binding via Blended Latent Diffusion, 2) Warping-free generative editing pipeline for object operations via regeneration instead of pixel deformation, 3) IP-Adapter conditioning for object identity preservation across edits using reference images.

Result: Outperforms state-of-the-art methods in visual fidelity and layout adherence, eliminates warping-induced geometric artifacts, produces high-quality images consistent with specified 3D layouts and edits, and enables coherent object appearance throughout interactive 3D editing.

Conclusion: POCI-Diff successfully addresses limitations of prior methods by providing consistent 3D layout control and interactive editing without geometric distortions, enabling complex multi-object scene synthesis with semantic control and coherent editing operations.

Abstract: We propose a diffusion-based approach for Text-to-Image (T2I) generation with consistent and interactive 3D layout control and editing. While prior methods improve spatial adherence using 2D cues or iterative copy-warp-paste strategies, they often distort object geometry and fail to preserve consistency across edits. To address these limitations, we introduce a framework for Positioning Objects Consistently and Interactively (POCI-Diff), a novel formulation for jointly enforcing 3D geometric constraints and instance-level semantic binding within a unified diffusion process. Our method enables explicit per-object semantic control by binding individual text descriptions to specific 3D bounding boxes through Blended Latent Diffusion, allowing one-shot synthesis of complex multi-object scenes. We further propose a warping-free generative editing pipeline that supports object insertion, removal, and transformation via regeneration rather than pixel deformation. To preserve object identity and consistency across edits, we condition the diffusion process on reference images using IP-Adapter, enabling coherent object appearance throughout interactive 3D editing while maintaining global scene coherence. Experimental results demonstrate that POCI-Diff produces high-quality images consistent with the specified 3D layouts and edits, outperforming state-of-the-art methods in both visual fidelity and layout adherence while eliminating warping-induced geometric artifacts.

</details>


### [244] [Fine-Grained Zero-Shot Composed Image Retrieval with Complementary Visual-Semantic Integration](https://arxiv.org/abs/2601.14060)
*Yongcong Ye,Kai Zhang,Yanghai Zhang,Enhong Chen,Longfei Li,Jun Zhou*

Main category: cs.CV

TL;DR: CVSI is a novel zero-shot composed image retrieval method that integrates complementary visual and semantic information through three key components to capture fine-grained modifications, outperforming existing methods on multiple datasets.


<details>
  <summary>Details</summary>
Motivation: Existing ZS-CIR methods struggle with fine-grained changes and effective visual-semantic integration, often relying on approaches that fail to capture complementary visual information and complete semantic context.

Method: CVSI uses three components: (1) Visual Information Extraction with global features and pseudo token generation, (2) Semantic Information Extraction with caption generation and LLM-based modification, and (3) Complementary Information Retrieval that integrates query and database information.

Result: Extensive experiments on CIRR, CIRCO, and FashionIQ datasets demonstrate that CVSI significantly outperforms existing state-of-the-art methods.

Conclusion: CVSI effectively addresses limitations in ZS-CIR by integrating complementary visual and semantic information, enabling fine-grained retrieval across various scenarios.

Abstract: Zero-shot composed image retrieval (ZS-CIR) is a rapidly growing area with significant practical applications, allowing users to retrieve a target image by providing a reference image and a relative caption describing the desired modifications. Existing ZS-CIR methods often struggle to capture fine-grained changes and integrate visual and semantic information effectively. They primarily rely on either transforming the multimodal query into a single text using image-to-text models or employing large language models for target image description generation, approaches that often fail to capture complementary visual information and complete semantic context. To address these limitations, we propose a novel Fine-Grained Zero-Shot Composed Image Retrieval method with Complementary Visual-Semantic Integration (CVSI). Specifically, CVSI leverages three key components: (1) Visual Information Extraction, which not only extracts global image features but also uses a pre-trained mapping network to convert the image into a pseudo token, combining it with the modification text and the objects most likely to be added. (2) Semantic Information Extraction, which involves using a pre-trained captioning model to generate multiple captions for the reference image, followed by leveraging an LLM to generate the modified captions and the objects most likely to be added. (3) Complementary Information Retrieval, which integrates information extracted from both the query and database images to retrieve the target image, enabling the system to efficiently handle retrieval queries in a variety of situations. Extensive experiments on three public datasets (e.g., CIRR, CIRCO, and FashionIQ) demonstrate that CVSI significantly outperforms existing state-of-the-art methods. Our code is available at https://github.com/yyc6631/CVSI.

</details>


### [245] [VERIDAH: Solving Enumeration Anomaly Aware Vertebra Labeling across Imaging Sequences](https://arxiv.org/abs/2601.14066)
*Hendrik Möller,Hanna Schoen,Robert Graf,Matan Atad,Nathan Molinier,Anjany Sekuboyina,Bettina K. Budai,Fabian Bamberg,Steffen Ringhof,Christopher Schlett,Tobias Pischon,Thoralf Niendorf,Josua A. Decker,Marc-André Weber,Bjoern Menze,Daniel Rueckert,Jan S. Kirschke*

Main category: cs.CV

TL;DR: VERIDAH is a novel deep learning algorithm that accurately labels vertebrae in medical images while automatically detecting and handling enumeration anomalies (unusual numbers of thoracic/lumbar vertebrae), outperforming existing methods on both MRI and CT scans.


<details>
  <summary>Details</summary>
Motivation: Current clinical practice often misses vertebral enumeration anomalies (11/13 thoracic or 4/6 lumbar vertebrae), which have clinical implications for back pain and surgery planning. Existing deep learning labeling algorithms lack the ability to automatically detect and handle these anomalies.

Method: VERIDAH uses multiple classification heads combined with a weighted vertebra sequence prediction algorithm to identify vertebrae and detect enumeration anomalies in arbitrary field-of-view medical images.

Result: Significantly outperforms existing models: 98.30% vs 94.24% correct labeling on T2w MRI (p<0.001) and 99.18% vs 77.26% on CT (p<0.001). Detects thoracic anomalies in 87.80% (MRI) and 96.30% (CT), and lumbar anomalies in 94.48% (MRI) and 97.22% (CT).

Conclusion: VERIDAH successfully addresses the gap in automated vertebra labeling by handling enumeration anomalies, providing accurate labeling that could improve clinical assessment of the thoracolumbar junction and support better treatment planning.

Abstract: The human spine commonly consists of seven cervical, twelve thoracic, and five lumbar vertebrae. However, enumeration anomalies may result in individuals having eleven or thirteen thoracic vertebrae and four or six lumbar vertebrae. Although the identification of enumeration anomalies has potential clinical implications for chronic back pain and operation planning, the thoracolumbar junction is often poorly assessed and rarely described in clinical reports. Additionally, even though multiple deep-learning-based vertebra labeling algorithms exist, there is a lack of methods to automatically label enumeration anomalies. Our work closes that gap by introducing "Vertebra Identification with Anomaly Handling" (VERIDAH), a novel vertebra labeling algorithm based on multiple classification heads combined with a weighted vertebra sequence prediction algorithm. We show that our approach surpasses existing models on T2w TSE sagittal (98.30% vs. 94.24% of subjects with all vertebrae correctly labeled, p < 0.001) and CT imaging (99.18% vs. 77.26% of subjects with all vertebrae correctly labeled, p < 0.001) and works in arbitrary field-of-view images. VERIDAH correctly labeled the presence 2 Möller et al. of thoracic enumeration anomalies in 87.80% and 96.30% of T2w and CT images, respectively, and lumbar enumeration anomalies in 94.48% and 97.22% for T2w and CT, respectively. Our code and models are available at: https://github.com/Hendrik-code/spineps.

</details>


### [246] [Unsupervised Video Class-Incremental Learning via Deep Embedded Clustering Management](https://arxiv.org/abs/2601.14069)
*Nattapong Kurpukdee,Adrian G. Bors*

Main category: cs.CV

TL;DR: Proposes an unsupervised video class incremental learning (uVCIL) approach using deep feature extraction and progressive clustering without requiring labels or task boundaries.


<details>
  <summary>Details</summary>
Motivation: Existing supervised class-incremental learning approaches require costly human annotation and task boundary knowledge, which is unrealistic for many real-world video applications. There's a need for unsupervised methods that can learn video information incrementally without forgetting.

Method: 1) Use deep feature extractor network to obtain representative video features without class/task information. 2) Progressively build series of deep clusters from extracted features. 3) Transfer knowledge by using model from previous task as initial state for current learning task.

Result: Significantly outperforms other baselines on three standard video action recognition datasets (UCF101, HMDB51, Something-to-Something V2) when ignoring labels from supervised setting.

Conclusion: Proposes a simple yet effective unsupervised approach for video class incremental learning that eliminates the need for labels and task boundaries while achieving strong performance on benchmark datasets.

Abstract: Unsupervised video class incremental learning (uVCIL) represents an important learning paradigm for learning video information without forgetting, and without considering any data labels. Prior approaches have focused on supervised class-incremental learning, relying on using the knowledge of labels and task boundaries, which is costly, requires human annotation, or is simply not a realistic option. In this paper, we propose a simple yet effective approach to address the uVCIL. We first consider a deep feature extractor network, providing a set of representative video features during each task without assuming any class or task information. We then progressively build a series of deep clusters from the extracted features. During the successive task learning, the model updated from the previous task is used as an initial state in order to transfer knowledge to the current learning task. We perform in-depth evaluations on three standard video action recognition datasets, including UCF101, HMDB51, and Something-to-Something V2, by ignoring the labels from the supervised setting. Our approach significantly outperforms other baselines on all datasets.

</details>


### [247] [VENI: Variational Encoder for Natural Illumination](https://arxiv.org/abs/2601.14079)
*Paul Walker,James A. D. Gardner,Andreea Ardelean,William A. P. Smith,Bernhard Egger*

Main category: cs.CV

TL;DR: Proposes a rotation-equivariant VAE for modeling natural illumination on spheres using Vector Neuron Vision Transformer and equivariant neural fields, enabling better latent space interpolation.


<details>
  <summary>Details</summary>
Motivation: Existing inverse rendering methods either ignore the spherical/rotation-equivariant nature of illumination environments or fail to provide well-behaved latent spaces, making the ill-posed problem harder to solve.

Method: Develops a rotation-equivariant variational autoencoder with: 1) Novel Vector Neuron Vision Transformer (VN-ViT) encoder, 2) Rotation-equivariant conditional neural field decoder, 3) Novel SO(2)-equivariant fully connected layer that reduces equivariance from SO(3) to SO(2) while preserving spherical properties.

Result: The SO(2)-equivariant fully connected layer outperforms standard Vector Neurons in their model. The VAE enables smoother interpolation in latent space and provides a more well-behaved latent space compared to previous methods.

Conclusion: The proposed rotation-equivariant VAE effectively models natural illumination on spheres without 2D projections, addressing both the spherical nature of illumination and the need for well-behaved latent spaces in inverse rendering.

Abstract: Inverse rendering is an ill-posed problem, but priors like illumination priors, can simplify it. Existing work either disregards the spherical and rotation-equivariant nature of illumination environments or does not provide a well-behaved latent space. We propose a rotation-equivariant variational autoencoder that models natural illumination on the sphere without relying on 2D projections. To preserve the SO(2)-equivariance of environment maps, we use a novel Vector Neuron Vision Transformer (VN-ViT) as encoder and a rotation-equivariant conditional neural field as decoder. In the encoder, we reduce the equivariance from SO(3) to SO(2) using a novel SO(2)-equivariant fully connected layer, an extension of Vector Neurons. We show that our SO(2)-equivariant fully connected layer outperforms standard Vector Neurons when used in our SO(2)-equivariant model. Compared to previous methods, our variational autoencoder enables smoother interpolation in latent space and offers a more well-behaved latent space.

</details>


### [248] [DermaBench: A Clinician-Annotated Benchmark Dataset for Dermatology Visual Question Answering and Reasoning](https://arxiv.org/abs/2601.14084)
*Abdurrahim Yilmaz,Ozan Erdem,Ece Gokyayla,Ayda Acar,Burc Bugra Dagtas,Dilara Ilhan Erdil,Gulsum Gencoglan,Burak Temelkuran*

Main category: cs.CV

TL;DR: DermaBench is a clinician-annotated dermatology visual question answering (VQA) benchmark built on the Diverse Dermatology Images dataset, addressing limitations of current dermatology evaluation datasets that focus only on image classification.


<details>
  <summary>Details</summary>
Motivation: Current vision-language model evaluation in dermatology is limited by datasets focusing primarily on image-level classification tasks like lesion recognition, which cannot assess full visual understanding, language grounding, and clinical reasoning capabilities of multimodal models.

Method: Built DermaBench on the Diverse Dermatology Images (DDI) dataset with 656 clinical images from 570 unique patients spanning Fitzpatrick skin types I-VI. Used hierarchical annotation schema with 22 main questions (single-choice, multi-choice, open-ended) where expert dermatologists annotated each image for diagnosis, anatomic site, lesion morphology, distribution, surface features, color, image quality, plus open-ended narrative descriptions and summaries.

Result: Created benchmark with approximately 14,474 VQA-style annotations. Released as metadata-only dataset to respect upstream licensing, publicly available at Harvard Dataverse.

Conclusion: DermaBench provides a comprehensive VQA benchmark for evaluating dermatology vision-language models' capabilities in interpreting images, reasoning over fine-grained morphology, and generating clinically meaningful descriptions beyond simple classification tasks.

Abstract: Vision-language models (VLMs) are increasingly important in medical applications; however, their evaluation in dermatology remains limited by datasets that focus primarily on image-level classification tasks such as lesion recognition. While valuable for recognition, such datasets cannot assess the full visual understanding, language grounding, and clinical reasoning capabilities of multimodal models. Visual question answering (VQA) benchmarks are required to evaluate how models interpret dermatological images, reason over fine-grained morphology, and generate clinically meaningful descriptions. We introduce DermaBench, a clinician-annotated dermatology VQA benchmark built on the Diverse Dermatology Images (DDI) dataset. DermaBench comprises 656 clinical images from 570 unique patients spanning Fitzpatrick skin types I-VI. Using a hierarchical annotation schema with 22 main questions (single-choice, multi-choice, and open-ended), expert dermatologists annotated each image for diagnosis, anatomic site, lesion morphology, distribution, surface features, color, and image quality, together with open-ended narrative descriptions and summaries, yielding approximately 14.474 VQA-style annotations. DermaBench is released as a metadata-only dataset to respect upstream licensing and is publicly available at Harvard Dataverse.

</details>


### [249] [Two-Stream temporal transformer for video action classification](https://arxiv.org/abs/2601.14086)
*Nattapong Kurpukdee,Adrian G. Bors*

Main category: cs.CV

TL;DR: A two-stream transformer video classifier that combines content frames and optical flow for improved action recognition using self-attention mechanisms across spatio-temporal domains.


<details>
  <summary>Details</summary>
Motivation: Motion representation is crucial for video understanding applications like action recognition and autonomous systems. While transformers have shown strong performance in various domains through self-attention, there's a need to effectively combine spatial content information with temporal motion information (optical flow) for better video classification.

Method: Proposes a two-stream transformer architecture that processes both content frames and optical flow streams. The model extracts spatio-temporal information from content and optical flow (representing movement), identifies self-attention features across the joint optical flow and temporal frame domain, and represents their relationships within transformer encoder mechanisms.

Result: The proposed methodology provides excellent classification results on three well-known video datasets of human activities, demonstrating the effectiveness of combining content and optical flow information through transformer self-attention mechanisms.

Conclusion: The two-stream transformer approach successfully leverages both spatial content and temporal motion information through self-attention mechanisms, achieving strong performance on human activity recognition tasks and showing promise for video understanding applications.

Abstract: Motion representation plays an important role in video understanding and has many applications including action recognition, robot and autonomous guidance or others. Lately, transformer networks, through their self-attention mechanism capabilities, have proved their efficiency in many applications. In this study, we introduce a new two-stream transformer video classifier, which extracts spatio-temporal information from content and optical flow representing movement information. The proposed model identifies self-attention features across the joint optical flow and temporal frame domain and represents their relationships within the transformer encoder mechanism. The experimental results show that our proposed methodology provides excellent classification results on three well-known video datasets of human activities.

</details>


### [250] [Curriculum-Based Strategies for Efficient Cross-Domain Action Recognition](https://arxiv.org/abs/2601.14101)
*Emily Kim,Allen Wu,Jessica Hodgins*

Main category: cs.CV

TL;DR: Curriculum learning strategies improve cross-view action recognition efficiency by combining synthetic aerial and real ground data, reducing training iterations by 30-37% while maintaining comparable accuracy.


<details>
  <summary>Details</summary>
Motivation: Human action recognition models trained on ground-level datasets struggle to generalize to aerial views due to domain shift. There's a need for methods that can adapt to different viewpoints without requiring real aerial data during training.

Method: Two curriculum learning approaches using synthetic aerial-view data and real ground-view data: 1) Two-stage curriculum with direct fine-tuning, and 2) Progressive curriculum with multi-stage dataset expansion before fine-tuning. Evaluated on REMAG dataset using SlowFast (CNN) and MViTv2 (Transformer) architectures.

Result: Combining both out-of-domain datasets outperforms single-domain training. Both curriculum strategies match top-1 accuracy of simple dataset combination while offering efficiency gains: two-step method reduces iterations by 37% (SlowFast) and 30% (MViTv2); progressive approach further reduces by 9% (SlowFast) and 30% (MViTv2). Performance remains within 3% accuracy range.

Conclusion: Curriculum-based training enables efficient cross-view action recognition without real aerial data, maintaining comparable performance while significantly reducing training iterations through strategic combination of synthetic aerial and real ground data.

Abstract: Despite significant progress in human action recognition, generalizing to diverse viewpoints remains a challenge. Most existing datasets are captured from ground-level perspectives, and models trained on them often struggle to transfer to drastically different domains such as aerial views. This paper examines how curriculum-based training strategies can improve generalization to unseen real aerial-view data without using any real aerial data during training.
  We explore curriculum learning for cross-view action recognition using two out-of-domain sources: synthetic aerial-view data and real ground-view data. Our results on the evaluation on order of training (fine-tuning on synthetic aerial data vs. real ground data) shows that fine-tuning on real ground data but differ in how they transition from synthetic to real. The first uses a two-stage curriculum with direct fine-tuning, while the second applies a progressive curriculum that expands the dataset in multiple stages before fine-tuning. We evaluate both methods on the REMAG dataset using SlowFast (CNN-based) and MViTv2 (Transformer-based) architectures.
  Results show that combining the two out-of-domain datasets clearly outperforms training on a single domain, whether real ground-view or synthetic aerial-view. Both curriculum strategies match the top-1 accuracy of simple dataset combination while offering efficiency gains. With the two-step fine-tuning method, SlowFast achieves up to a 37% reduction in iterations and MViTv2 up to a 30% reduction compared to simple combination. The multi-step progressive approach further reduces iterations, by up to 9% for SlowFast and 30% for MViTv2, relative to the two-step method. These findings demonstrate that curriculum-based training can maintain comparable performance (top-1 accuracy within 3% range) while improving training efficiency in cross-view action recognition.

</details>


### [251] [Interp3D: Correspondence-aware Interpolation for Generative Textured 3D Morphing](https://arxiv.org/abs/2601.14103)
*Xiaolu Liu,Yicong Li,Qiyuan He,Jiayin Zhu,Wei Ji,Angela Yao,Jianke Zhu*

Main category: cs.CV

TL;DR: Interp3D is a training-free framework for textured 3D morphing that generates smooth transitions between 3D assets while preserving both geometric structure and texture coherence, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing 3D morphing approaches either handle only geometry (ignoring textures) or extend 2D interpolation strategies to 3D, causing semantic ambiguity, structural misalignment, and texture blurring. There's a need for joint preservation of geometric consistency and texture alignment throughout transitions.

Method: Interp3D uses generative priors with progressive alignment: 1) semantically aligned interpolation in condition space, 2) SLAT-guided structure interpolation for structural consistency, and 3) fine-grained texture fusion for appearance detail transfer. It's training-free and leverages structured latent guidance.

Result: The method outperforms previous approaches on the Interp3DData dataset (with graded difficulty levels) across fidelity, transition smoothness, and plausibility metrics. Both quantitative evaluation and human studies show significant advantages.

Conclusion: Interp3D successfully addresses textured 3D morphing challenges by jointly preserving geometric consistency and texture alignment through progressive alignment principles, enabling smooth and plausible transitions between 3D assets for animation, editing, and content creation applications.

Abstract: Textured 3D morphing seeks to generate smooth and plausible transitions between two 3D assets, preserving both structural coherence and fine-grained appearance. This ability is crucial not only for advancing 3D generation research but also for practical applications in animation, editing, and digital content creation. Existing approaches either operate directly on geometry, limiting them to shape-only morphing while neglecting textures, or extend 2D interpolation strategies into 3D, which often causes semantic ambiguity, structural misalignment, and texture blurring. These challenges underscore the necessity to jointly preserve geometric consistency, texture alignment, and robustness throughout the transition process. To address this, we propose Interp3D, a novel training-free framework for textured 3D morphing. It harnesses generative priors and adopts a progressive alignment principle to ensure both geometric fidelity and texture coherence. Starting from semantically aligned interpolation in condition space, Interp3D enforces structural consistency via SLAT (Structured Latent)-guided structure interpolation, and finally transfers appearance details through fine-grained texture fusion. For comprehensive evaluations, we construct a dedicated dataset, Interp3DData, with graded difficulty levels and assess generation results from fidelity, transition smoothness, and plausibility. Both quantitative metrics and human studies demonstrate the significant advantages of our proposed approach over previous methods. Source code is available at https://github.com/xiaolul2/Interp3D.

</details>


### [252] [PMCE: Probabilistic Multi-Granularity Semantics with Caption-Guided Enhancement for Few-Shot Learning](https://arxiv.org/abs/2601.14111)
*Jiaying Wu,Can Gao,Jinglu Hu,Hui Li,Xiaofeng Cao,Jingcai Guo*

Main category: cs.CV

TL;DR: PMCE is a probabilistic few-shot learning framework that uses multi-granularity semantics and caption-guided enhancement to improve prototype estimation for novel categories with limited labeled samples.


<details>
  <summary>Details</summary>
Motivation: Few-shot learning prototypes estimated from scarce data are often biased and generalize poorly. Existing semantic methods mostly apply class-level information only on the support side, leaving query representations unchanged, limiting their effectiveness.

Method: PMCE constructs a nonparametric knowledge bank storing visual statistics and CLIP-encoded class name embeddings of base classes. At meta-test time, it retrieves relevant base classes using class name similarity, aggregates statistics into category-specific priors, and fuses them with support prototypes via MAP update. Simultaneously, a frozen BLIP captioner provides instance-level image descriptions, and a lightweight enhancer optimizes both support prototypes and query features with consistency regularization to stabilize noisy captions.

Result: Experiments on four benchmarks show PMCE consistently improves over strong baselines, achieving up to 7.71% absolute gain over the strongest semantic competitor on MiniImageNet in the 1-shot setting.

Conclusion: PMCE effectively leverages multi-granularity semantics (class-level and instance-level) with caption-guided enhancement to address prototype bias in few-shot learning, demonstrating significant performance improvements across multiple benchmarks.

Abstract: Few-shot learning aims to identify novel categories from only a handful of labeled samples, where prototypes estimated from scarce data are often biased and generalize poorly. Semantic-based methods alleviate this by introducing coarse class-level information, but they are mostly applied on the support side, leaving query representations unchanged. In this paper, we present PMCE, a Probabilistic few-shot framework that leverages Multi-granularity semantics with Caption-guided Enhancement. PMCE constructs a nonparametric knowledge bank that stores visual statistics for each category as well as CLIP-encoded class name embeddings of the base classes. At meta-test time, the most relevant base classes are retrieved based on the similarities of class name embeddings for each novel category. These statistics are then aggregated into category-specific prior information and fused with the support set prototypes via a simple MAP update. Simultaneously, a frozen BLIP captioner provides label-free instance-level image descriptions, and a lightweight enhancer trained on base classes optimizes both support prototypes and query features under an inductive protocol with a consistency regularization to stabilize noisy captions. Experiments on four benchmarks show that PMCE consistently improves over strong baselines, achieving up to 7.71% absolute gain over the strongest semantic competitor on MiniImageNet in the 1-shot setting. Our code is available at https://anonymous.4open.science/r/PMCE-275D

</details>


### [253] [The Side Effects of Being Smart: Safety Risks in MLLMs' Multi-Image Reasoning](https://arxiv.org/abs/2601.14127)
*Renmiao Chen,Yida Lu,Shiyao Cui,Xuan Ouyang,Victor Shea-Jay Huang,Shumin Zhang,Chengwei Pan,Han Qiu,Minlie Huang*

Main category: cs.CV

TL;DR: MIR-SafetyBench is the first benchmark for evaluating safety risks in multimodal LLMs during multi-image reasoning, revealing that models with stronger reasoning capabilities are paradoxically more vulnerable to safety issues.


<details>
  <summary>Details</summary>
Motivation: As Multimodal Large Language Models develop stronger reasoning abilities to handle complex multi-image instructions, these advanced capabilities may introduce new safety risks that need systematic evaluation.

Method: Created MIR-SafetyBench with 2,676 instances across 9 multi-image relation categories, then evaluated 19 MLLMs on this benchmark while analyzing attack success rates, superficial safe responses, and attention entropy patterns.

Result: Models with more advanced multi-image reasoning show greater vulnerability on safety benchmarks; many "safe" responses are superficial due to misunderstanding or evasion; unsafe generations exhibit lower attention entropy than safe ones.

Conclusion: Advanced reasoning capabilities in MLLMs can paradoxically increase safety risks, with models potentially over-focusing on task-solving while neglecting safety constraints, highlighting the need for specialized safety evaluation in multi-image reasoning contexts.

Abstract: As Multimodal Large Language Models (MLLMs) acquire stronger reasoning capabilities to handle complex, multi-image instructions, this advancement may pose new safety risks. We study this problem by introducing MIR-SafetyBench, the first benchmark focused on multi-image reasoning safety, which consists of 2,676 instances across a taxonomy of 9 multi-image relations. Our extensive evaluations on 19 MLLMs reveal a troubling trend: models with more advanced multi-image reasoning can be more vulnerable on MIR-SafetyBench. Beyond attack success rates, we find that many responses labeled as safe are superficial, often driven by misunderstanding or evasive, non-committal replies. We further observe that unsafe generations exhibit lower attention entropy than safe ones on average. This internal signature suggests a possible risk that models may over-focus on task solving while neglecting safety constraints. Our code and data are available at https://github.com/thu-coai/MIR-SafetyBench.

</details>


### [254] [GIC-DLC: Differentiable Logic Circuits for Hardware-Friendly Grayscale Image Compression](https://arxiv.org/abs/2601.14130)
*Till Aczel,David F. Jenny,Simon Bührer,Andreas Plesner,Antonio Di Maio,Roger Wattenhofer*

Main category: cs.CV

TL;DR: GIC-DLC is a hardware-friendly neural image codec that uses lookup tables and Boolean operations to achieve better compression than traditional methods while reducing energy consumption for edge devices.


<details>
  <summary>Details</summary>
Motivation: Neural image codecs achieve better compression than traditional methods like PNG/JPEG-XL but have high computational overhead, making them unsuitable for energy-constrained edge devices like smartphones, cameras, and drones.

Method: Proposes GIC-DLC (Grayscale Image Compression with Differentiable Logic Circuits), a hardware-aware codec that trains lookup tables to combine neural network flexibility with Boolean operation efficiency.

Result: Outperforms traditional codecs in compression efficiency on grayscale benchmark datasets while enabling substantial reductions in energy consumption and latency.

Conclusion: Learned compression can be hardware-friendly, offering a promising direction for low-power image compression on edge devices.

Abstract: Neural image codecs achieve higher compression ratios than traditional hand-crafted methods such as PNG or JPEG-XL, but often incur substantial computational overhead, limiting their deployment on energy-constrained devices such as smartphones, cameras, and drones. We propose Grayscale Image Compression with Differentiable Logic Circuits (GIC-DLC), a hardware-aware codec where we train lookup tables to combine the flexibility of neural networks with the efficiency of Boolean operations. Experiments on grayscale benchmark datasets show that GIC-DLC outperforms traditional codecs in compression efficiency while allowing substantial reductions in energy consumption and latency. These results demonstrate that learned compression can be hardware-friendly, offering a promising direction for low-power image compression on edge devices.

</details>


### [255] [LLM Augmented Intervenable Multimodal Adaptor for Post-operative Complication Prediction in Lung Cancer Surgery](https://arxiv.org/abs/2601.14154)
*Shubham Pandey,Bhavin Jawade,Srirangaraj Setlur,Venu Govindaraju,Kenneth Seastedt*

Main category: cs.CV

TL;DR: MIRACLE is a deep learning system that predicts postoperative complication risks in lung cancer surgery by fusing clinical and radiological data, offering interpretable and actionable insights for clinicians.


<details>
  <summary>Details</summary>
Motivation: Postoperative complications significantly impact patient outcomes and increase healthcare costs, creating a need for better risk prediction tools that can integrate diverse data sources and provide clinically useful insights.

Method: MIRACLE uses hyperspherical embedding space fusion to integrate heterogeneous preoperative clinical and radiological data, combined with an interventional deep learning module for interpretable predictions that clinicians can adjust based on expertise.

Result: MIRACLE outperforms traditional machine learning models and contemporary LLM variants on the POC-L dataset of 3,094 lung cancer surgery patients, demonstrating superior performance for personalized and explainable risk management.

Conclusion: The MIRACLE architecture provides an effective solution for postoperative complication prediction by combining multimodal data fusion with interpretability features, enabling clinically actionable insights for personalized risk management in lung cancer surgery.

Abstract: Postoperative complications remain a critical concern in clinical practice, adversely affecting patient outcomes and contributing to rising healthcare costs. We present MIRACLE, a deep learning architecture for prediction of risk of postoperative complications in lung cancer surgery by integrating preoperative clinical and radiological data. MIRACLE employs a hyperspherical embedding space fusion of heterogeneous inputs, enabling the extraction of robust, discriminative features from both structured clinical records and high-dimensional radiological images. To enhance transparency of prediction and clinical utility, we incorporate an interventional deep learning module in MIRACLE, that not only refines predictions but also provides interpretable and actionable insights, allowing domain experts to interactively adjust recommendations based on clinical expertise. We validate our approach on POC-L, a real-world dataset comprising 3,094 lung cancer patients who underwent surgery at Roswell Park Comprehensive Cancer Center. Our results demonstrate that MIRACLE outperforms various traditional machine learning models and contemporary large language models (LLM) variants alone, for personalized and explainable postoperative risk management.

</details>


### [256] [One-Shot Refiner: Boosting Feed-forward Novel View Synthesis via One-Step Diffusion](https://arxiv.org/abs/2601.14161)
*Yitong Dong,Qi Zhang,Minchao Jiang,Zhiqiang Wu,Qingnan Fan,Ying Feng,Huaqi Zhang,Hujun Bao,Guofeng Zhang*

Main category: cs.CV

TL;DR: A novel framework for high-fidelity novel view synthesis from sparse images using 3D Gaussian Splatting with Vision Transformer backbones, enhanced by dual-domain detail perception and feature-guided diffusion for consistent high-resolution results.


<details>
  <summary>Details</summary>
Motivation: Current ViT-based 3DGS methods are limited by low-resolution inputs due to computational costs, and existing generative enhancement methods are 3D-agnostic, leading to inconsistent structures across views, especially in unseen regions.

Method: 1) Dual-Domain Detail Perception Module to handle high-resolution images without ViT backbone limitations and store high-frequency details in Gaussians; 2) Feature-guided diffusion network to preserve high-frequency details during restoration; 3) Unified training strategy for joint optimization of ViT-based geometric backbone and diffusion-based refinement module.

Result: Experiments demonstrate superior generation quality across multiple datasets, maintaining high-fidelity novel view synthesis from sparse images with consistent structures across views.

Conclusion: The proposed framework successfully addresses limitations of existing ViT-based 3DGS methods by enabling high-resolution processing and 3D-aware refinement, achieving state-of-the-art novel view synthesis performance.

Abstract: We present a novel framework for high-fidelity novel view synthesis (NVS) from sparse images, addressing key limitations in recent feed-forward 3D Gaussian Splatting (3DGS) methods built on Vision Transformer (ViT) backbones. While ViT-based pipelines offer strong geometric priors, they are often constrained by low-resolution inputs due to computational costs. Moreover, existing generative enhancement methods tend to be 3D-agnostic, resulting in inconsistent structures across views, especially in unseen regions. To overcome these challenges, we design a Dual-Domain Detail Perception Module, which enables handling high-resolution images without being limited by the ViT backbone, and endows Gaussians with additional features to store high-frequency details. We develop a feature-guided diffusion network, which can preserve high-frequency details during the restoration process. We introduce a unified training strategy that enables joint optimization of the ViT-based geometric backbone and the diffusion-based refinement module. Experiments demonstrate that our method can maintain superior generation quality across multiple datasets.

</details>


### [257] [ASBA: A-line State Space Model and B-line Attention for Sparse Optical Doppler Tomography Reconstruction](https://arxiv.org/abs/2601.14165)
*Zhenghong Li,Wensheng Cheng,Congwu Du,Yingtian Pan,Zhaozheng Yin,Haibin Ling*

Main category: cs.CV

TL;DR: ASBA network reconstructs Optical Doppler Tomography images from highly sparse A-scan sampling using A-line ROI state space modeling and B-line phase attention with flow-aware loss.


<details>
  <summary>Details</summary>
Motivation: Current ODT requires dense sampling which prolongs scanning time, increases storage demands, and limits capture of rapid blood flow dynamics. Sparse sampling approaches have been limited by conservative sampling rates and uniform modeling of flow/background signals.

Method: Proposes ASBA network with: 1) A-line ROI state space model to extract sparsely distributed flow features along depth axis, 2) B-line phase attention to capture long-range flow signals along lateral axis based on phase difference, and 3) flow-aware weighted loss function to prioritize accurate reconstruction of flow signals.

Result: Extensive experiments on real animal data demonstrate that the proposed approach clearly outperforms existing state-of-the-art reconstruction methods for sparse ODT imaging.

Conclusion: ASBA enables high-fidelity ODT image reconstruction from highly sparsely sampled raw A-scans, addressing limitations of current dense sampling practices while maintaining accurate blood flow signal reconstruction.

Abstract: Optical Doppler Tomography (ODT) is an emerging blood flow analysis technique. A 2D ODT image (B-scan) is generated by sequentially acquiring 1D depth-resolved raw A-scans (A-line) along the lateral axis (B-line), followed by Doppler phase-subtraction analysis. To ensure high-fidelity B-scan images, current practices rely on dense sampling, which prolongs scanning time, increases storage demands, and limits the capture of rapid blood flow dynamics. Recent studies have explored sparse sampling of raw A-scans to alleviate these limitations, but their effectiveness is hindered by the conservative sampling rates and the uniform modeling of flow and background signals. In this study, we introduce a novel blood flow-aware network, named ASBA (A-line ROI State space model and B-line phase Attention), to reconstruct ODT images from highly sparsely sampled raw A-scans. Specifically, we propose an A-line ROI state space model to extract sparsely distributed flow features along the A-line, and a B-line phase attention to capture long-range flow signals along each B-line based on phase difference. Moreover, we introduce a flow-aware weighted loss function that encourages the network to prioritize the accurate reconstruction of flow signals. Extensive experiments on real animal data demonstrate that the proposed approach clearly outperforms existing state-of-the-art reconstruction methods.

</details>


### [258] [Progressive self-supervised blind-spot denoising method for LDCT denoising](https://arxiv.org/abs/2601.14180)
*Yichao Liu,Yueyang Teng,Junwen Guo*

Main category: cs.CV

TL;DR: Novel self-supervised method for LDCT denoising using step-wise blind-spot mechanism and Gaussian noise regularization, achieving performance comparable to supervised methods.


<details>
  <summary>Details</summary>
Motivation: Self-supervised learning is needed for LDCT denoising because paired normal-dose CT data is difficult to acquire in clinical practice, reducing dependence on scarce labeled data.

Method: Proposes a self-supervised training strategy using only LDCT images, featuring: 1) step-wise blind-spot denoising mechanism that enforces conditional independence progressively, and 2) adding Gaussian noise to LDCT images as regularization to prevent overfitting.

Result: Extensive experiments on Mayo LDCT dataset show the method consistently outperforms existing self-supervised approaches and achieves performance comparable to or better than several representative supervised denoising methods.

Conclusion: The proposed self-supervised method effectively addresses the data scarcity problem in LDCT denoising while achieving competitive performance with supervised approaches, making it clinically practical.

Abstract: Self-supervised learning is increasingly investigated for low-dose computed tomography (LDCT) image denoising, as it alleviates the dependence on paired normal-dose CT (NDCT) data, which are often difficult to acquire in clinical practice. In this paper, we propose a novel self-supervised training strategy that relies exclusively on LDCT images. We introduce a step-wise blind-spot denoising mechanism that enforces conditional independence in a progressive manner, enabling more fine-grained denoising learning. In addition, we add Gaussian noise to LDCT images, which acts as a regularization and mitigates overfitting. Extensive experiments on the Mayo LDCT dataset demonstrate that the proposed method consistently outperforms existing self-supervised approaches and achieves performance comparable to, or better than, several representative supervised denoising methods.

</details>


### [259] [IIR-VLM: In-Context Instance-level Recognition for Large Vision-Language Models](https://arxiv.org/abs/2601.14188)
*Liang Shi,Wei Li,Kevin M Beussman,Lin Chen,Yun Fu*

Main category: cs.CV

TL;DR: IIR-VLM enhances VLMs for instance-level recognition by integrating pre-trained ILR expert models as auxiliary visual encoders, enabling one-shot in-context learning of new instances and instance-aware visual understanding.


<details>
  <summary>Details</summary>
Motivation: Current VLMs underperform on instance-level recognition (ILR) compared to domain-specific models, limiting practical applications where recognizing familiar people and objects is crucial. Existing solutions require costly instance-specific training and struggle with fine-grained discrimination.

Method: Proposes IIR-VLM that integrates pre-trained ILR expert models as auxiliary visual encoders to provide specialized features. This enables VLMs to learn new instances in-context in a one-shot manner and leverage this knowledge for instance-aware visual understanding.

Result: Validated on existing instance personalization benchmarks and demonstrated superior ILR performance on a new challenging benchmark assessing ILR capabilities across varying difficulty and diverse categories (person, face, pet, general objects).

Conclusion: IIR-VLM effectively enhances VLMs for instance-level recognition by leveraging pre-trained ILR experts, enabling efficient one-shot learning and improving practical applications requiring instance-aware visual understanding.

Abstract: Instance-level recognition (ILR) concerns distinguishing individual instances from one another, with person re-identification as a prominent example. Despite the impressive visual perception capabilities of modern VLMs, we find their performance on ILR unsatisfactory, often dramatically underperforming domain-specific ILR models. This limitation hinders many practical application of VLMs, e.g. where recognizing familiar people and objects is crucial for effective visual understanding. Existing solutions typically learn to recognize instances one at a time using instance-specific datasets, which not only incur substantial data collection and training costs but also struggle with fine-grained discrimination. In this work, we propose IIR-VLM, a VLM enhanced for In-context Instance-level Recognition. We integrate pre-trained ILR expert models as auxiliary visual encoders to provide specialized features for learning diverse instances, which enables VLMs to learn new instances in-context in a one-shot manner. Further, IIR-VLM leverages this knowledge for instance-aware visual understanding. We validate IIR-VLM's efficacy on existing instance personalization benchmarks. Finally, we demonstrate its superior ILR performance on a challenging new benchmark, which assesses ILR capabilities across varying difficulty and diverse categories, with person, face, pet and general objects as the instances at task.

</details>


### [260] [Rig-Aware 3D Reconstruction of Vehicle Undercarriages using Gaussian Splatting](https://arxiv.org/abs/2601.14208)
*Nitin Kulkarni,Akhil Devarashetti,Charlie Cluss,Livio Forte,Dan Buckmaster,Philip Schneider,Chunming Qiao,Alina Vereshchaka*

Main category: cs.CV

TL;DR: An end-to-end pipeline for creating interactive 3D models of vehicle undercarriages using a three-camera rig, overcoming challenges with wide-angle distortion and low-parallax scenes through rig-aware SfM and Gaussian splatting.


<details>
  <summary>Details</summary>
Motivation: Inspecting vehicle undercarriages is labor-intensive and requires inspectors to crouch/crawl underneath vehicles. Online buyers rarely see undercarriage photos, creating safety concerns for inspectors and confidence issues for buyers.

Method: Uses a three-camera rig to capture synchronized videos as vehicles drive over it. Implements a rig-aware Structure-from-Motion pipeline with precise calibration, constrained matching strategy using DISK feature extractor and LightGlue matcher, and Gaussian splatting for real-time rendering.

Result: Produces interactive 3D models that allow rotation, zooming, and slicing to detect rust, leaks, or damage in seconds. Achieves state-of-the-art quality through ablation studies showing design choices are essential for overcoming wide-angle distortion and low-parallax challenges.

Conclusion: The pipeline improves workplace safety for inspectors and buyer confidence by providing detailed, interactive 3D undercarriage models that overcome traditional SfM limitations in challenging automotive inspection scenarios.

Abstract: Inspecting the undercarriage of used vehicles is a labor-intensive task that requires inspectors to crouch or crawl underneath each vehicle to thoroughly examine it. Additionally, online buyers rarely see undercarriage photos. We present an end-to-end pipeline that utilizes a three-camera rig to capture videos of the undercarriage as the vehicle drives over it, and produces an interactive 3D model of the undercarriage. The 3D model enables inspectors and customers to rotate, zoom, and slice through the undercarriage, allowing them to detect rust, leaks, or impact damage in seconds, thereby improving both workplace safety and buyer confidence. Our primary contribution is a rig-aware Structure-from-Motion (SfM) pipeline specifically designed to overcome the challenges of wide-angle lens distortion and low-parallax scenes. Our method overcomes the challenges of wide-angle lens distortion and low-parallax scenes by integrating precise camera calibration, synchronized video streams, and strong geometric priors from the camera rig. We use a constrained matching strategy with learned components, the DISK feature extractor, and the attention-based LightGlue matcher to generate high-quality sparse point clouds that are often unattainable with standard SfM pipelines. These point clouds seed the Gaussian splatting process to generate photorealistic undercarriage models that render in real-time. Our experiments and ablation studies demonstrate that our design choices are essential to achieve state-of-the-art quality.

</details>


### [261] [Soft Tail-dropping for Adaptive Visual Tokenization](https://arxiv.org/abs/2601.14246)
*Zeyuan Chen,Kai Zhang,Zhuowen Tu,Yuanjun Xiong*

Main category: cs.CV

TL;DR: STAT is a 1D discrete visual tokenizer that adaptively chooses token count based on image complexity, enabling better compatibility with causal autoregressive visual generation models.


<details>
  <summary>Details</summary>
Motivation: Current visual tokenizers produce fixed-length sequences regardless of image complexity, which is suboptimal for causal autoregressive models that need to handle varying levels of image detail efficiently.

Method: STAT encodes images into discrete codes with per-token keep probabilities, regularized to be monotonically decreasing and aligned with image-level complexity measures, producing length-adaptive 1D visual tokens.

Result: On ImageNet-1k, STAT-equipped causal AR models achieve competitive/superior generation quality compared to other probabilistic models, with favorable scaling behavior previously elusive in vanilla AR visual generation.

Conclusion: STAT enables effective length-adaptive visual tokenization that naturally integrates with causal AR models, overcoming previous limitations and showing promising scaling properties for visual generation tasks.

Abstract: We present Soft Tail-dropping Adaptive Tokenizer (STAT), a 1D discrete visual tokenizer that adaptively chooses the number of output tokens per image according to its structural complexity and level of detail. STAT encodes an image into a sequence of discrete codes together with per-token keep probabilities. Beyond standard autoencoder objectives, we regularize these keep probabilities to be monotonically decreasing along the sequence and explicitly align their distribution with an image-level complexity measure. As a result, STAT produces length-adaptive 1D visual tokens that are naturally compatible with causal 1D autoregressive (AR) visual generative models. On ImageNet-1k, equipping vanilla causal AR models with STAT yields competitive or superior visual generation quality compared to other probabilistic model families, while also exhibiting favorable scaling behavior that has been elusive in prior vanilla AR visual generation attempts.

</details>


### [262] [OmniTransfer: All-in-one Framework for Spatio-temporal Video Transfer](https://arxiv.org/abs/2601.14250)
*Pengze Zhang,Yanze Wu,Mengtian Li,Xu Bai,Songtao Zhao,Fulong Ye,Chong Mou,Xinghui Li,Zhuowei Chen,Qian He,Mingyuan Gao*

Main category: cs.CV

TL;DR: OmniTransfer is a unified framework for spatio-temporal video transfer that outperforms existing methods in appearance and temporal transfer tasks without requiring pose information.


<details>
  <summary>Details</summary>
Motivation: Most existing video customization methods rely on reference images or task-specific temporal priors, failing to fully exploit rich spatio-temporal information in videos, which limits flexibility and generalization in video generation.

Method: OmniTransfer uses multi-view information across frames for appearance consistency and temporal cues for fine-grained control. It incorporates three key designs: Task-aware Positional Bias for adaptive reference video usage, Reference-decoupled Causal Learning to separate reference and target branches, and Task-adaptive Multimodal Alignment for dynamic task handling.

Result: Extensive experiments show OmniTransfer outperforms existing methods in appearance transfer (ID and style) and temporal transfer (camera movement and video effects), while matching pose-guided methods in motion transfer without using pose information.

Conclusion: OmniTransfer establishes a new paradigm for flexible, high-fidelity video generation by unifying various video transfer tasks and fully exploiting spatio-temporal information.

Abstract: Videos convey richer information than images or text, capturing both spatial and temporal dynamics. However, most existing video customization methods rely on reference images or task-specific temporal priors, failing to fully exploit the rich spatio-temporal information inherent in videos, thereby limiting flexibility and generalization in video generation. To address these limitations, we propose OmniTransfer, a unified framework for spatio-temporal video transfer. It leverages multi-view information across frames to enhance appearance consistency and exploits temporal cues to enable fine-grained temporal control. To unify various video transfer tasks, OmniTransfer incorporates three key designs: Task-aware Positional Bias that adaptively leverages reference video information to improve temporal alignment or appearance consistency; Reference-decoupled Causal Learning separating reference and target branches to enable precise reference transfer while improving efficiency; and Task-adaptive Multimodal Alignment using multimodal semantic guidance to dynamically distinguish and tackle different tasks. Extensive experiments show that OmniTransfer outperforms existing methods in appearance (ID and style) and temporal transfer (camera movement and video effects), while matching pose-guided methods in motion transfer without using pose, establishing a new paradigm for flexible, high-fidelity video generation.

</details>


### [263] [LightOnOCR: A 1B End-to-End Multilingual Vision-Language Model for State-of-the-Art OCR](https://arxiv.org/abs/2601.14251)
*Said Taghadouini,Adrien Cavaillès,Baptiste Aubertin*

Main category: cs.CV

TL;DR: LightOnOCR-2-1B is a compact 1B-parameter multilingual vision-language model that converts document images to clean text without traditional OCR pipelines, achieving SOTA results while being 9x smaller and faster than previous models.


<details>
  <summary>Details</summary>
Motivation: To create an efficient end-to-end document image-to-text model that avoids brittle OCR pipelines, handles multilingual content (especially French and scientific PDFs), and provides additional capabilities like image localization.

Method: Trained on large-scale high-quality distillation data covering scans, French documents, and scientific PDFs. Uses a resume strategy for bounding box prediction during pretraining, refined with RLVR using IoU-based rewards. Employs checkpoint averaging and task-arithmetic merging for robustness.

Result: Achieves state-of-the-art results on OlmOCR-Bench while being 9x smaller and substantially faster than prior best models. Also predicts normalized bounding boxes for embedded images.

Conclusion: LightOnOCR-2-1B demonstrates that efficient, high-performance document understanding models can be built without traditional OCR pipelines, with additional capabilities for image localization, and is released as open source with evaluation benchmarks.

Abstract: We present \textbf{LightOnOCR-2-1B}, a 1B-parameter end-to-end multilingual vision--language model that converts document images (e.g., PDFs) into clean, naturally ordered text without brittle OCR pipelines. Trained on a large-scale, high-quality distillation mix with strong coverage of scans, French documents, and scientific PDFs, LightOnOCR-2 achieves state-of-the-art results on OlmOCR-Bench while being 9$\times$ smaller and substantially faster than prior best-performing models. We further extend the output format to predict normalized bounding boxes for embedded images, introducing localization during pretraining via a resume strategy and refining it with RLVR using IoU-based rewards. Finally, we improve robustness with checkpoint averaging and task-arithmetic merging. We release model checkpoints under Apache 2.0, and publicly release the dataset and \textbf{LightOnOCR-bbox-bench} evaluation under their respective licenses.

</details>


### [264] [Motion 3-to-4: 3D Motion Reconstruction for 4D Synthesis](https://arxiv.org/abs/2601.14253)
*Hongyuan Chen,Xingyu Chen,Youjia Zhang,Zexiang Xu,Anpei Chen*

Main category: cs.CV

TL;DR: Motion 3-to-4 is a feed-forward framework for generating high-quality 4D dynamic objects from single monocular videos with optional 3D reference meshes, addressing 4D synthesis challenges through decomposition into static 3D shape generation and motion reconstruction.


<details>
  <summary>Details</summary>
Motivation: 4D synthesis remains challenging due to limited training data and inherent ambiguities in recovering geometry and motion from monocular viewpoints, creating a need for robust methods that can generate temporally coherent 4D content from limited inputs.

Method: Decomposes 4D synthesis into static 3D shape generation and motion reconstruction. Uses canonical reference mesh to learn compact motion latent representation and predicts per-frame vertex trajectories. Employs scalable frame-wise transformer for robustness to varying sequence lengths.

Result: Superior fidelity and spatial consistency compared to prior work on both standard benchmarks and a new dataset with accurate ground-truth geometry, demonstrating effective 4D dynamic object synthesis from monocular inputs.

Conclusion: Motion 3-to-4 successfully addresses 4D synthesis challenges through decomposition approach and motion latent representation, enabling high-quality dynamic object generation from single monocular videos with optional 3D references.

Abstract: We present Motion 3-to-4, a feed-forward framework for synthesising high-quality 4D dynamic objects from a single monocular video and an optional 3D reference mesh. While recent advances have significantly improved 2D, video, and 3D content generation, 4D synthesis remains difficult due to limited training data and the inherent ambiguity of recovering geometry and motion from a monocular viewpoint. Motion 3-to-4 addresses these challenges by decomposing 4D synthesis into static 3D shape generation and motion reconstruction. Using a canonical reference mesh, our model learns a compact motion latent representation and predicts per-frame vertex trajectories to recover complete, temporally coherent geometry. A scalable frame-wise transformer further enables robustness to varying sequence lengths. Evaluations on both standard benchmarks and a new dataset with accurate ground-truth geometry show that Motion 3-to-4 delivers superior fidelity and spatial consistency compared to prior work. Project page is available at https://motion3-to-4.github.io/.

</details>


### [265] [VideoMaMa: Mask-Guided Video Matting via Generative Prior](https://arxiv.org/abs/2601.14255)
*Sangbeom Lim,Seoung Wug Oh,Jiahui Huang,Heeji Yoon,Seungryong Kim,Joon-Young Lee*

Main category: cs.CV

TL;DR: VideoMaMa converts segmentation masks to alpha mattes using pretrained video diffusion models, enabling zero-shot generalization to real videos. The authors create MA-V dataset with 50K+ video annotations via pseudo-labeling, and fine-tune SAM2 to SAM2-Matte which outperforms existing models.


<details>
  <summary>Details</summary>
Motivation: Video matting models struggle with real-world generalization due to limited labeled data. There's a need for scalable approaches that can handle diverse scenes and motions in real videos without extensive manual annotation.

Method: 1) VideoMaMa: Uses pretrained video diffusion models to convert coarse segmentation masks into accurate alpha mattes. 2) Pseudo-labeling pipeline: Creates MA-V dataset with 50K+ real-world video annotations. 3) SAM2-Matte: Fine-tunes SAM2 model on the MA-V dataset for improved video matting.

Result: VideoMaMa shows strong zero-shot generalization to real videos despite synthetic-only training. MA-V provides high-quality annotations for diverse videos. SAM2-Matte outperforms models trained on existing datasets, demonstrating better robustness on in-the-wild videos.

Conclusion: Large-scale pseudo-labeled video matting data is crucial for advancing video matting research. Generative priors and accessible segmentation cues enable scalable progress, with VideoMaMa and MA-V dataset providing effective solutions for real-world video matting challenges.

Abstract: Generalizing video matting models to real-world videos remains a significant challenge due to the scarcity of labeled data. To address this, we present Video Mask-to-Matte Model (VideoMaMa) that converts coarse segmentation masks into pixel accurate alpha mattes, by leveraging pretrained video diffusion models. VideoMaMa demonstrates strong zero-shot generalization to real-world footage, even though it is trained solely on synthetic data. Building on this capability, we develop a scalable pseudo-labeling pipeline for large-scale video matting and construct the Matting Anything in Video (MA-V) dataset, which offers high-quality matting annotations for more than 50K real-world videos spanning diverse scenes and motions. To validate the effectiveness of this dataset, we fine-tune the SAM2 model on MA-V to obtain SAM2-Matte, which outperforms the same model trained on existing matting datasets in terms of robustness on in-the-wild videos. These findings emphasize the importance of large-scale pseudo-labeled video matting and showcase how generative priors and accessible segmentation cues can drive scalable progress in video matting research.

</details>


### [266] [Implicit Neural Representation Facilitates Unified Universal Vision Encoding](https://arxiv.org/abs/2601.14256)
*Matthew Gwilliam,Xiao Wang,Xuefeng Hu,Zhenheng Yang*

Main category: cs.CV

TL;DR: A unified model that learns image representations useful for both recognition and generation tasks, using hyper-networks for implicit neural representations combined with knowledge distillation.


<details>
  <summary>Details</summary>
Motivation: Current image representation learning models are typically specialized for either recognition (via contrastive learning) or generation (via reconstruction losses). The authors seek to unify these two directions to create a single model that can perform both tasks effectively.

Method: Train a hyper-network for implicit neural representation (INR) that maps images to model weights for fast, accurate reconstruction. Integrate this with knowledge distillation to improve generalization and performance. The model learns a compressed embedding space that serves dual purposes.

Result: The model achieves state-of-the-art results for image representation learning while also enabling generative capabilities through high-quality tiny embeddings. It learns an unprecedented compressed embedding space with outstanding performance for various visual tasks.

Conclusion: The proposed unified model successfully bridges the gap between recognition and generation in image representation learning, offering both competitive performance on visual tasks and generative capabilities through a novel compressed embedding space.

Abstract: Models for image representation learning are typically designed for either recognition or generation. Various forms of contrastive learning help models learn to convert images to embeddings that are useful for classification, detection, and segmentation. On the other hand, models can be trained to reconstruct images with pixel-wise, perceptual, and adversarial losses in order to learn a latent space that is useful for image generation. We seek to unify these two directions with a first-of-its-kind model that learns representations which are simultaneously useful for recognition and generation. We train our model as a hyper-network for implicit neural representation, which learns to map images to model weights for fast, accurate reconstruction. We further integrate our INR hyper-network with knowledge distillation to improve its generalization and performance. Beyond the novel training design, the model also learns an unprecedented compressed embedding space with outstanding performance for various visual tasks. The complete model competes with state-of-the-art results for image representation learning, while also enabling generative capabilities with its high-quality tiny embeddings. The code is available at https://github.com/tiktok/huvr.

</details>
