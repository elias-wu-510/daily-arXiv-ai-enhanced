<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 161]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Scientific Image Synthesis: Benchmarking, Methodologies, and Downstream Utility](https://arxiv.org/abs/2601.17027)
*Honglin Lin,Chonghan Qin,Zheng Liu,Qizhi Pei,Yu Li,Zhanping Zhong,Xin Gao,Yanfeng Wang,Conghui He,Lijun Wu*

Main category: cs.CV

TL;DR: ImgCoder framework improves scientific image synthesis via logic-driven "understand-plan-code" workflow, with SciGenBench for evaluation, showing synthetic scientific images can boost multimodal reasoning when rigorously verified.


<details>
  <summary>Details</summary>
Motivation: Multimodal reasoning is limited by the difficulty of synthesizing scientifically rigorous images. Existing T2I models produce visually plausible but scientifically incorrect outputs, creating visual-logic divergence that hinders downstream reasoning.

Method: Propose ImgCoder, a logic-driven framework with explicit "understand - plan - code" workflow for improved structural precision. Analyze both direct pixel-based generation and programmatic synthesis. Introduce SciGenBench for evaluating scientific correctness based on information utility and logical validity.

Result: Evaluation reveals systematic failure modes in pixel-based models and highlights fundamental expressiveness-precision trade-off. Fine-tuning LMMs on rigorously verified synthetic scientific images yields consistent reasoning gains with potential scaling trends analogous to text domain.

Conclusion: High-fidelity scientific synthesis is a viable path to unlocking massive multimodal reasoning capabilities, with synthetic scientific images showing promise for improving multimodal reasoning when properly verified.

Abstract: While synthetic data has proven effective for improving scientific reasoning in the text domain, multimodal reasoning remains constrained by the difficulty of synthesizing scientifically rigorous images. Existing Text-to-Image (T2I) models often produce outputs that are visually plausible yet scientifically incorrect, resulting in a persistent visual-logic divergence that limits their value for downstream reasoning. Motivated by recent advances in next-generation T2I models, we conduct a systematic study of scientific image synthesis across generation paradigms, evaluation, and downstream use. We analyze both direct pixel-based generation and programmatic synthesis, and propose ImgCoder, a logic-driven framework that follows an explicit "understand - plan - code" workflow to improve structural precision. To rigorously assess scientific correctness, we introduce SciGenBench, which evaluates generated images based on information utility and logical validity. Our evaluation reveals systematic failure modes in pixel-based models and highlights a fundamental expressiveness-precision trade-off. Finally, we show that fine-tuning Large Multimodal Models (LMMs) on rigorously verified synthetic scientific images yields consistent reasoning gains, with potential scaling trends analogous to the text domain, validating high-fidelity scientific synthesis as a viable path to unlocking massive multimodal reasoning capabilities.

</details>


### [2] [Data-Efficient Meningioma Segmentation via Implicit Spatiotemporal Mixing and Sim2Real Semantic Injection](https://arxiv.org/abs/2601.17031)
*Yunhao Xu,Fuquan Zong,Yexuan Xing,Chulong Zhang,Guang Yang,Shilong Yang,Xiaokun Liang,Juan Yu*

Main category: cs.CV

TL;DR: A dual-augmentation framework combining spatial manifold expansion via Implicit Neural Representations and semantic lesion injection to maximize data efficiency for medical image segmentation with limited annotations.


<details>
  <summary>Details</summary>
Motivation: Medical image segmentation performance depends more on efficient data utilization than raw data volume. Complex pathologies like meningiomas require models to fully exploit limited high-quality annotations, necessitating better ways to maximize value from existing datasets.

Method: Proposes a novel dual-augmentation framework: 1) Spatial manifold expansion using Implicit Neural Representations (INR) to model continuous velocity fields, performing linear mixing on integrated deformation fields to generate anatomically plausible variations; 2) Sim2Real lesion injection module that transplants lesion textures into healthy anatomical backgrounds to bridge synthetic-real gap.

Result: Comprehensive experiments on hybrid datasets show the framework significantly enhances data efficiency and robustness of state-of-the-art models (nnU-Net and U-Mamba), offering high-performance medical image analysis with limited annotation budgets.

Conclusion: The proposed dual-augmentation framework effectively maximizes data utilization for medical image segmentation, particularly valuable for complex pathologies where high-quality annotations are scarce, enabling robust performance with limited annotation resources.

Abstract: The performance of medical image segmentation is increasingly defined by the efficiency of data utilization rather than merely the volume of raw data. Accurate segmentation, particularly for complex pathologies like meningiomas, demands that models fully exploit the latent information within limited high-quality annotations. To maximize the value of existing datasets, we propose a novel dual-augmentation framework that synergistically integrates spatial manifold expansion and semantic object injection. Specifically, we leverage Implicit Neural Representations (INR) to model continuous velocity fields. Unlike previous methods, we perform linear mixing on the integrated deformation fields, enabling the efficient generation of anatomically plausible variations by interpolating within the deformation space. This approach allows for the extensive exploration of structural diversity from a small set of anchors. Furthermore, we introduce a Sim2Real lesion injection module. This module constructs a high-fidelity simulation domain by transplanting lesion textures into healthy anatomical backgrounds, effectively bridging the gap between synthetic augmentation and real-world pathology. Comprehensive experiments on a hybrid dataset demonstrate that our framework significantly enhances the data efficiency and robustness of state-of-the-art models, including nnU-Net and U-Mamba, offering a potent strategy for high-performance medical image analysis with limited annotation budgets.

</details>


### [3] [Diagnosis Support of Sickle Cell Anemia by Classifying Red Blood Cell Shape in Peripheral Blood Images](https://arxiv.org/abs/2601.17032)
*Wilkie Delgado-Font,Miriela Escobedo-Nicot,Manuel González-Hidalgo,Silena Herold-Garcia,Antoni Jaume-i-Capó,Arnau Mir*

Main category: cs.CV

TL;DR: Automated method for classifying RBCs in blood smear images using Chan-Vese segmentation and shape analysis to diagnose sickle cell anemia.


<details>
  <summary>Details</summary>
Motivation: Manual microscopic examination of RBCs for sickle cell anemia diagnosis is time-consuming, requires specialists, and has high error rates due to subjective observation.

Method: Uses Chan-Vese active contour model for segmentation, then classifies RBCs as normal or deformed using circular shape factor (CSF) and elliptical shape factor (ESF). Includes elliptical adjustment for partially occluded cells.

Result: Achieved F-measure of 0.97 for normal cells and 0.95 for elongated cells, outperforming state-of-the-art methods. Suitable for clinical treatment and diagnostic support.

Conclusion: The proposed automated method provides superior performance for sickle cell anemia diagnosis compared to existing methods, offering reliable clinical support.

Abstract: Red blood cell (RBC) deformation is the consequence of several diseases, including sickle cell anemia, which causes recurring episodes of pain and severe pronounced anemia. Monitoring patients with these diseases involves the observation of peripheral blood samples under a microscope, a time-consuming procedure. Moreover, a specialist is required to perform this technique, and owing to the subjective nature of the observation of isolated RBCs, the error rate is high. In this paper, we propose an automated method for differentially enumerating RBCs that uses peripheral blood smear image analysis. In this method, the objects of interest in the image are segmented using a Chan-Vese active contour model. An analysis is then performed to classify the RBCs, also called erythrocytes, as normal or elongated or having other deformations, using the basic shape analysis descriptors: circular shape factor (CSF) and elliptical shape factor (ESF). To analyze cells that become partially occluded in a cluster during sample preparation, an elliptical adjustment is performed to allow the analysis of erythrocytes with discoidal and elongated shapes. The images of patient blood samples used in the study were acquired by a clinical laboratory specialist in the Special Hematology Department of the ``Dr. Juan Bruno Zayas'' General Hospital in Santiago de Cuba. A comparison of the results obtained by the proposed method in our experiments with those obtained by some state-of-the-art methods showed that the proposed method is superior for the diagnosis of sickle cell anemia. This superiority is achieved for evidenced by the obtained F-measure value (0.97 for normal cells and 0.95 for elongated ones) and several overall multiclass performance measures. The results achieved by the proposed method are suitable for the purpose of clinical treatment and diagnostic support of sickle cell anemia.

</details>


### [4] [AMVICC: A Novel Benchmark for Cross-Modal Failure Mode Profiling for VLMs and IGMs](https://arxiv.org/abs/2601.17037)
*Aahana Basappa,Pranay Goel,Anusri Karra,Anish Karra,Asa Gilmore,Kevin Zhu*

Main category: cs.CV

TL;DR: Created AMVICC benchmark to systematically compare visual reasoning failures across MLLMs and IGMs, revealing shared and modality-specific limitations in basic visual concepts.


<details>
  <summary>Details</summary>
Motivation: Despite rapid growth in vision-language models, they still fail to understand/generate basic visual concepts like object orientation, quantity, and spatial relationships, highlighting gaps in elementary visual reasoning that need systematic evaluation.

Method: Adapted MMVP benchmark questions into explicit and implicit prompts to create AMVICC benchmark, then tested 11 MLLMs and 3 IGMs across nine categories of visual reasoning to profile failure modes across modalities.

Result: Failure modes are often shared between models and modalities, but certain failures are model-specific and modality-specific. IGMs particularly struggled with manipulating specific visual components in response to prompts, especially with explicit prompts, showing poor control over fine-grained visual attributes.

Conclusion: The work provides a framework for cross-modal evaluation of visual understanding, laying foundation for future alignment studies to determine if generation and interpretation failures stem from shared limitations, guiding improvements in unified vision-language modeling.

Abstract: We investigated visual reasoning limitations of both multimodal large language models (MLLMs) and image generation models (IGMs) by creating a novel benchmark to systematically compare failure modes across image-to-text and text-to-image tasks, enabling cross-modal evaluation of visual understanding. Despite rapid growth in machine learning, vision language models (VLMs) still fail to understand or generate basic visual concepts such as object orientation, quantity, or spatial relationships, which highlighted gaps in elementary visual reasoning. By adapting MMVP benchmark questions into explicit and implicit prompts, we create \textit{AMVICC}, a novel benchmark for profiling failure modes across various modalities. After testing 11 MLLMs and 3 IGMs in nine categories of visual reasoning, our results show that failure modes are often shared between models and modalities, but certain failures are model-specific and modality-specific, and this can potentially be attributed to various factors. IGMs consistently struggled to manipulate specific visual components in response to prompts, especially in explicit prompts, suggesting poor control over fine-grained visual attributes. Our findings apply most directly to the evaluation of existing state-of-the-art models on structured visual reasoning tasks. This work lays the foundation for future cross-modal alignment studies, offering a framework to probe whether generation and interpretation failures stem from shared limitations to guide future improvements in unified vision-language modeling.

</details>


### [5] [Hybrid Deep Feature Extraction and ML for Construction and Demolition Debris Classification](https://arxiv.org/abs/2601.17038)
*Obai Alashram,Nejad Alagha,Mahmoud AlKakuri,Zeeshan Swaveel,Abigail Copiaco*

Main category: cs.CV

TL;DR: Hybrid vision-based pipeline combining Xception deep features with classical ML classifiers achieves 99.5% accuracy for automated C&D debris classification using a novel UAE dataset.


<details>
  <summary>Details</summary>
Motivation: Construction industry generates large debris volumes requiring effective sorting for sustainable waste management and resource recovery. Current methods need improvement for automated classification.

Method: Collected novel dataset of 1,800 balanced images (Ceramic/Tile, Concrete, Trash/Waste, Wood) from UAE construction sites. Used pre-trained Xception network for deep feature extraction, then evaluated multiple ML classifiers (SVM, kNN, Bagged Trees, LDA, Logistic Regression).

Result: Hybrid pipelines with Xception features and simple classifiers (Linear SVM, kNN, Bagged Trees) achieved state-of-the-art performance up to 99.5% accuracy and macro-F1 scores, outperforming complex end-to-end deep learning approaches.

Conclusion: The hybrid approach offers robust, field-deployable debris identification with operational benefits for construction waste management. Provides pathways for future integration with robotics and onsite automation systems.

Abstract: The construction industry produces significant volumes of debris, making effective sorting and classification critical for sustainable waste management and resource recovery. This study presents a hybrid vision-based pipeline that integrates deep feature extraction with classical machine learning (ML) classifiers for automated construction and demolition (C\&D) debris classification. A novel dataset comprising 1,800 balanced, high-quality images representing four material categories, Ceramic/Tile, Concrete, Trash/Waste, and Wood was collected from real construction sites in the UAE, capturing diverse real-world conditions. Deep features were extracted using a pre-trained Xception network, and multiple ML classifiers, including SVM, kNN, Bagged Trees, LDA, and Logistic Regression, were systematically evaluated. The results demonstrate that hybrid pipelines using Xception features with simple classifiers such as Linear SVM, kNN, and Bagged Trees achieve state-of-the-art performance, with up to 99.5\% accuracy and macro-F1 scores, surpassing more complex or end-to-end deep learning approaches. The analysis highlights the operational benefits of this approach for robust, field-deployable debris identification and provides pathways for future integration with robotics and onsite automation systems.

</details>


### [6] [MANGO: A Global Single-Date Paired Dataset for Mangrove Segmentation](https://arxiv.org/abs/2601.17039)
*Junhyuk Heo,Beomkyu Choi,Hyunjin Shin,Darongsae Kwon*

Main category: cs.CV

TL;DR: MANGO is a large-scale global dataset of 42,703 labeled image-mask pairs for mangrove detection across 124 countries, addressing limitations of existing datasets for reliable mangrove monitoring.


<details>
  <summary>Details</summary>
Motivation: Existing mangrove datasets have limitations: they often provide only annual map products without curated single-date image-mask pairs, are limited to specific regions rather than global coverage, or remain inaccessible to the public. These limitations hinder progress in deep learning-based mangrove detection for climate-change mitigation.

Method: The authors construct MANGO by retrieving all available Sentinel-2 imagery within 2020 for mangrove regions and selecting the best single-date observations that align with mangrove annual masks. They use a target detection-driven approach that leverages pixel-wise coordinate references to ensure adaptive and representative image-mask pairings.

Result: The resulting MANGO dataset contains 42,703 labeled image-mask pairs across 124 countries, providing comprehensive global coverage. The authors also establish a benchmark across diverse semantic segmentation architectures using a country-disjoint split.

Conclusion: MANGO addresses critical gaps in existing mangrove datasets and establishes a foundation for scalable and reliable global mangrove monitoring, supporting climate-change mitigation efforts through improved deep learning applications.

Abstract: Mangroves are critical for climate-change mitigation, requiring reliable monitoring for effective conservation. While deep learning has emerged as a powerful tool for mangrove detection, its progress is hindered by the limitations of existing datasets. In particular, many resources provide only annual map products without curated single-date image-mask pairs, limited to specific regions rather than global coverage, or remain inaccessible to the public. To address these challenges, we introduce MANGO, a large-scale global dataset comprising 42,703 labeled image-mask pairs across 124 countries. To construct this dataset, we retrieve all available Sentinel-2 imagery within the year 2020 for mangrove regions and select the best single-date observations that align with the mangrove annual mask. This selection is performed using a target detection-driven approach that leverages pixel-wise coordinate references to ensure adaptive and representative image-mask pairings. We also provide a benchmark across diverse semantic segmentation architectures under a country-disjoint split, establishing a foundation for scalable and reliable global mangrove monitoring.

</details>


### [7] [FP-THD: Full page transcription of historical documents](https://arxiv.org/abs/2601.17040)
*H Neji,J Nogueras-Iso,J Lacasta,MÁ Latre,FJ García-Marco*

Main category: cs.CV

TL;DR: A pipeline for transcribing historical Latin documents (15th-16th centuries) that preserves special characters and symbols using layout analysis and OCR models.


<details>
  <summary>Details</summary>
Motivation: Historical Latin documents from the 15th-16th centuries contain special characters and symbols with distinct meanings that must be preserved during transcription to maintain the original style and significance of these texts.

Method: Extends existing text line recognition with layout analysis: 1) Layout analysis model extracts text lines from historical document images, 2) OCR model processes extracted lines to generate fully digitized pages, 3) Uses masked autoencoder approach.

Result: The pipeline facilitates page processing and produces efficient results. Evaluation on multiple datasets shows the masked autoencoder effectively handles different text types including handwritten, printed, and multi-language documents.

Conclusion: The proposed pipeline successfully addresses the challenges of transcribing historical Latin documents by preserving special features through combined layout analysis and OCR processing.

Abstract: The transcription of historical documents written in Latin in XV and XVI centuries has special challenges as it must maintain the characters and special symbols that have distinct meanings to ensure that historical texts retain their original style and significance. This work proposes a pipeline for the transcription of historical documents preserving these special features. We propose to extend an existing text line recognition method with a layout analysis model. We analyze historical text images using a layout analysis model to extract text lines, which are then processed by an OCR model to generate a fully digitized page. We showed that our pipeline facilitates the processing of the page and produces an efficient result. We evaluated our approach on multiple datasets and demonstrate that the masked autoencoder effectively processes different types of text, including handwritten, printed and multi-language.

</details>


### [8] [Arabic Sign Language Recognition using Multimodal Approach](https://arxiv.org/abs/2601.17041)
*Ghadeer Alanazi,Abir Benabid*

Main category: cs.CV

TL;DR: Multimodal fusion of Leap Motion and RGB camera data improves Arabic Sign Language recognition accuracy to 78% on 18 words, overcoming limitations of single-sensor approaches.


<details>
  <summary>Details</summary>
Motivation: Existing ArSL recognition systems rely on single sensors (Leap Motion or RGB cameras) which struggle with complex hand orientations and 3D movement tracking. A multimodal approach is needed to overcome these limitations and improve recognition accuracy.

Method: Two parallel subnetworks: 1) Custom dense neural network with dropout and L2 regularization for Leap Motion data, 2) Fine-tuned VGG16 model with data augmentation for RGB camera data. Features from both modalities are concatenated in a fusion model, processed through fully connected layers, and classified via SoftMax activation.

Result: System achieved 78% accuracy on a custom dataset of 18 ArSL words, correctly recognizing 13 out of 18 words. This demonstrates improved performance over single-sensor approaches.

Conclusion: Multimodal fusion shows promising viability for ArSL recognition, though further optimization and dataset expansion are needed. The approach addresses limitations of single-sensor systems by combining spatial and temporal features from different modalities.

Abstract: Arabic Sign Language (ArSL) is an essential communication method for individuals in the Deaf and Hard-of-Hearing community. However, existing recognition systems face significant challenges due to their reliance on single sensor approaches like Leap Motion or RGB cameras. These systems struggle with limitations such as inadequate tracking of complex hand orientations and imprecise recognition of 3D hand movements. This research paper aims to investigate the potential of a multimodal approach that combines Leap Motion and RGB camera data to explore the feasibility of recognition of ArSL. The system architecture includes two parallel subnetworks: a custom dense neural network for Leap Motion data, incorporating dropout and L2 regularization, and an image subnetwork based on a fine-tuned VGG16 model enhanced with data augmentation techniques. Feature representations from both modalities are concatenated in a fusion model and passed through fully connected layers, with final classification performed via SoftMax activation to analyze spatial and temporal features of hand gestures. The system was evaluated on a custom dataset comprising 18 ArSL words, of which 13 were correctly recognized, yielding an overall accuracy of 78%. These results offer preliminary insights into the viability of multimodal fusion for sign language recognition and highlight areas for further optimization and dataset expansion.

</details>


### [9] [Interpretable and Sparse Linear Attention with Decoupled Membership-Subspace Modeling via MCR2 Objective](https://arxiv.org/abs/2601.17042)
*Tianyuan Liu,Libin Hou,Linyuan Wang,Bin Yan*

Main category: cs.CV

TL;DR: The paper proposes Decoupled Membership-Subspace Attention (DMSA), a novel sparse linear attention operator derived from optimizing the MCR2 objective, which improves both efficiency and interpretability in visual transformers.


<details>
  <summary>Details</summary>
Motivation: Existing MCR2-driven transformers suffer from redundant coding due to tight coupling between membership matrix and subspace matrix, leading to inefficiencies when token projection is incorrect. There's a need to decouple these components for better interpretability and computational efficiency.

Method: Decouple the functional relationship between membership matrix and subspaces in MCR2 objective. Directly learn membership matrix from inputs, then derive sparse subspaces from fullspace S. Use gradient unrolling of optimized MCR2 objective to obtain interpretable sparse linear attention operator (DMSA). Replace attention module in ToST with DMSA to create DMST.

Result: DMST achieves 1.08%-1.45% higher top-1 accuracy than ToST on ImageNet-1K, with faster coding reduction rate. Compared to vanilla Transformers, DMST exhibits significantly higher computational efficiency and interpretability.

Conclusion: The proposed DMSA attention mechanism successfully addresses coupling issues in MCR2-driven transformers, providing a more interpretable and efficient white-box solution for visual modeling that outperforms existing approaches.

Abstract: Maximal Coding Rate Reduction (MCR2)-driven white-box transformer, grounded in structured representation learning, unifies interpretability and efficiency, providing a reliable white-box solution for visual modeling. However, in existing designs, tight coupling between "membership matrix" and "subspace matrix U" in MCR2 causes redundant coding under incorrect token projection. To this end, we decouple the functional relationship between the "membership matrix" and "subspaces U" in the MCR2 objective and derive an interpretable sparse linear attention operator from unrolled gradient descent of the optimized objective. Specifically, we propose to directly learn the membership matrix from inputs and subsequently derive sparse subspaces from the fullspace S. Consequently, gradient unrolling of the optimized MCR2 objective yields an interpretable sparse linear attention operator: Decoupled Membership-Subspace Attention (DMSA). Experimental results on visual tasks show that simply replacing the attention module in Token Statistics Transformer (ToST) with DMSA (we refer to as DMST) not only achieves a faster coding reduction rate but also outperforms ToST by 1.08%-1.45% in top-1 accuracy on the ImageNet-1K dataset. Compared with vanilla Transformer architectures, DMST exhibits significantly higher computational efficiency and interpretability.

</details>


### [10] [Atomic Depth Estimation From Noisy Electron Microscopy Data Via Deep Learning](https://arxiv.org/abs/2601.17046)
*Matan Leibovich,Mai Tan,Adria Marcos-Morales,Sreyas Mohan,Peter A. Crozier,Carlos Fernandez-Granda*

Main category: cs.CV

TL;DR: A deep learning approach for 3D atomic depth estimation from noisy TEM images using semantic segmentation.


<details>
  <summary>Details</summary>
Motivation: Transmission electron microscopy (TEM) images often contain significant noise that makes extracting 3D atomic-level information challenging, necessitating robust methods for accurate depth estimation.

Method: Formulates depth estimation as a semantic segmentation problem, trains a deep convolutional neural network on simulated data with synthetic noise to generate pixel-wise depth segmentation maps.

Result: The method successfully estimated atomic column depths in CeO2 nanoparticles from both simulated and real TEM data, producing accurate, calibrated depth estimates robust to noise.

Conclusion: The semantic segmentation approach provides an effective solution for extracting reliable 3D atomic information from noisy TEM images, with potential applications in materials science.

Abstract: We present a novel approach for extracting 3D atomic-level information from transmission electron microscopy (TEM) images affected by significant noise. The approach is based on formulating depth estimation as a semantic segmentation problem. We address the resulting segmentation problem by training a deep convolutional neural network to generate pixel-wise depth segmentation maps using simulated data corrupted by synthetic noise. The proposed method was applied to estimate the depth of atomic columns in CeO2 nanoparticles from simulated images and real-world TEM data. Our experiments show that the resulting depth estimates are accurate, calibrated and robust to noise.

</details>


### [11] [A Contrastive Pre-trained Foundation Model for Deciphering Imaging Noisomics across Modalities](https://arxiv.org/abs/2601.17047)
*Yuanjie Gu,Yiqun Wang,Chaohui Yu,Ang Xuan,Fan Wang,Zhi Lu,Biqin Dong*

Main category: cs.CV

TL;DR: Noisomics is a framework that decodes imaging noise as information rather than suppressing it, using a Contrastive Pre-trained Foundation Model that achieves superior performance with 100x less data than traditional methods.


<details>
  <summary>Details</summary>
Motivation: Current approaches to imaging noise characterization are data-intensive, device-dependent, and treat noise as interference rather than an information resource. There's a need for methods that can disentangle physical signals from algorithmic artifacts without massive supervised datasets.

Method: The Contrastive Pre-trained (CoP) Foundation Model leverages the manifold hypothesis and synthetic noise genome. It uses contrastive learning to disentangle semantic signals from stochastic perturbations, breaking traditional deep learning scaling laws.

Result: CoP achieves superior performance with only 100 training samples, outperforming supervised baselines trained on 100,000 samples (1000x reduction). It shows 63.8% reduction in estimation error and 85.1% improvement in coefficient of determination across 12 diverse out-of-domain datasets with robust zero-shot generalization.

Conclusion: Noisomics redefines stochastic degradation as a vital information resource, enabling precise imaging diagnostics without prior device calibration. It demonstrates utility across scales from consumer photography to deep-tissue microscopy by decoding noise as a multi-parametric footprint.

Abstract: Characterizing imaging noise is notoriously data-intensive and device-dependent, as modern sensors entangle physical signals with complex algorithmic artifacts. Current paradigms struggle to disentangle these factors without massive supervised datasets, often reducing noise to mere interference rather than an information resource. Here, we introduce "Noisomics", a framework shifting the focus from suppression to systematic noise decoding via the Contrastive Pre-trained (CoP) Foundation Model. By leveraging the manifold hypothesis and synthetic noise genome, CoP employs contrastive learning to disentangle semantic signals from stochastic perturbations. Crucially, CoP breaks traditional deep learning scaling laws, achieving superior performance with only 100 training samples, outperforming supervised baselines trained on 100,000 samples, thereby reducing data and computational dependency by three orders of magnitude. Extensive benchmarking across 12 diverse out-of-domain datasets confirms its robust zero-shot generalization, demonstrating a 63.8% reduction in estimation error and an 85.1% improvement in the coefficient of determination compared to the conventional training strategy. We demonstrate CoP's utility across scales: from deciphering non-linear hardware-noise interplay in consumer photography to optimizing photon-efficient protocols for deep-tissue microscopy. By decoding noise as a multi-parametric footprint, our work redefines stochastic degradation as a vital information resource, empowering precise imaging diagnostics without prior device calibration.

</details>


### [12] [SiMiC: Context-Aware Silicon Microstructure Characterization Using Attention-Based Convolutional Neural Networks for Field-Emission Tip Analysis](https://arxiv.org/abs/2601.17048)
*Jing Jie Tan,Rupert Schreiner,Matthias Hausladen,Ali Asgharzade,Simon Edler,Julian Bartsch,Michael Bachmann,Andreas Schels,Ban-Hoe Kwan,Danny Wee-Kiat Ng,Yan-Chai Hum*

Main category: cs.CV

TL;DR: SiMiC uses attention-based CNNs to automatically analyze silicon microstructure features from SEM images, reducing manual labor and improving consistency for field-emission tip characterization.


<details>
  <summary>Details</summary>
Motivation: Traditional SEM analysis of silicon microstructures requires labor-intensive manual evaluation, limiting throughput and reproducibility. There's a need for automated, accurate characterization methods to advance microscale fabrication and device performance.

Method: Developed a specialized dataset of silicon field-emitter tips and created a customized CNN architecture with attention mechanisms for multi-class microstructure classification and dimensional prediction from SEM images.

Result: SiMiC achieves high accuracy in extracting morphological features (size, shape, apex curvature) while maintaining interpretability, outperforming classical image processing techniques.

Conclusion: The framework establishes data-driven microstructure analysis linked to field-emission performance, enabling correlation between emitter geometry and emission behavior for optimized cold-cathode and SEM electron source design.

Abstract: Accurate characterization of silicon microstructures is essential for advancing microscale fabrication, quality control, and device performance. Traditional analysis using Scanning Electron Microscopy (SEM) often requires labor-intensive, manual evaluation of feature geometry, limiting throughput and reproducibility. In this study, we propose SiMiC: Context-Aware Silicon Microstructure Characterization Using Attention-Based Convolutional Neural Networks for Field-Emission Tip Analysis. By leveraging deep learning, our approach efficiently extracts morphological features-such as size, shape, and apex curvature-from SEM images, significantly reducing human intervention while improving measurement consistency. A specialized dataset of silicon-based field-emitter tips was developed, and a customized CNN architecture incorporating attention mechanisms was trained for multi-class microstructure classification and dimensional prediction. Comparative analysis with classical image processing techniques demonstrates that SiMiC achieves high accuracy while maintaining interpretability. The proposed framework establishes a foundation for data-driven microstructure analysis directly linked to field-emission performance, opening avenues for correlating emitter geometry with emission behavior and guiding the design of optimized cold-cathode and SEM electron sources. The related dataset and algorithm repository that could serve as a baseline in this area can be found at https://research.jingjietan.com/?q=SIMIC

</details>


### [13] [Summary of the Unusual Activity Recognition Challenge for Developmental Disability Support](https://arxiv.org/abs/2601.17049)
*Christina Garcia,Nhat Tan Le,Taihei Fujioka,Umang Dobhal,Milyun Ni'ma Shoumi,Thanh Nha Nguyen,Sozo Inoue*

Main category: cs.CV

TL;DR: A challenge overview paper about unusual behavior recognition from pose data, focusing on developmental disability facilities, with 40 teams participating and evaluated using macro-averaged F1 scores.


<details>
  <summary>Details</summary>
Motivation: Address the critical need for automated recognition of unusual behaviors in facilities for individuals with developmental disabilities using non-invasive pose estimation data.

Method: Challenge-based approach where teams developed models to distinguish normal vs unusual activities using skeleton keypoints from video recordings. Evaluation used Leave-One-Subject-Out (LOSO) strategy to ensure subject-agnostic generalization.

Result: Broad participation from 40 teams using diverse approaches (classical ML to deep learning). Results highlight difficulty of modeling rare, abrupt actions in noisy, low-dimensional data, emphasizing importance of capturing temporal and contextual nuances.

Conclusion: The challenge provides insights for future developments in socially responsible AI applications for healthcare and behavior monitoring, addressing real-world challenges in behavior recognition.

Abstract: This paper presents an overview of the Recognize the Unseen: Unusual Behavior Recognition from Pose Data Challenge, hosted at ISAS 2025. The challenge aims to address the critical need for automated recognition of unusual behaviors in facilities for individuals with developmental disabilities using non-invasive pose estimation data. Participating teams were tasked with distinguishing between normal and unusual activities based on skeleton keypoints extracted from video recordings of simulated scenarios. The dataset reflects real-world imbalance and temporal irregularities in behavior, and the evaluation adopted a Leave-One-Subject-Out (LOSO) strategy to ensure subject-agnostic generalization. The challenge attracted broad participation from 40 teams applying diverse approaches ranging from classical machine learning to deep learning architectures. Submissions were assessed primarily using macro-averaged F1 scores to account for class imbalance. The results highlight the difficulty of modeling rare, abrupt actions in noisy, low-dimensional data, and emphasize the importance of capturing both temporal and contextual nuances in behavior modeling. Insights from this challenge may contribute to future developments in socially responsible AI applications for healthcare and behavior monitoring.

</details>


### [14] [Single-Pixel Vision-Language Model for Intrinsic Privacy-Preserving Behavioral Intelligence](https://arxiv.org/abs/2601.17050)
*Hongjun An,Yiliang Song,Jiawei Shao,Zhe Sun,Xuelong Li*

Main category: cs.CV

TL;DR: SP-VLM is a privacy-by-design framework that uses single-pixel sensing and vision-language models to monitor human behavior in sensitive spaces while protecting personal identity.


<details>
  <summary>Details</summary>
Motivation: Need for safety monitoring in privacy-sensitive environments (restrooms, changing rooms) where conventional surveillance is prohibited due to privacy regulations and ethical concerns, while still addressing threats like bullying and harassment.

Method: Single-Pixel Vision-Language Model (SP-VLM) framework that captures human dynamics through low-dimensional single-pixel modalities and infers behavioral patterns via vision-language integration.

Result: Single-pixel sensing intrinsically suppresses identity recoverability (makes face recognition ineffective below critical sampling rate), while still enabling robust anomaly detection, people counting, and activity understanding from degraded observations.

Conclusion: Identifies a practical sampling-rate regime where behavioral intelligence emerges while personal identity remains protected, offering a human-rights-aligned pathway for safety monitoring without intrusive surveillance.

Abstract: Adverse social interactions, such as bullying, harassment, and other illicit activities, pose significant threats to individual well-being and public safety, leaving profound impacts on physical and mental health. However, these critical events frequently occur in privacy-sensitive environments like restrooms, and changing rooms, where conventional surveillance is prohibited or severely restricted by stringent privacy regulations and ethical concerns. Here, we propose the Single-Pixel Vision-Language Model (SP-VLM), a novel framework that reimagines secure environmental monitoring. It achieves intrinsic privacy-by-design by capturing human dynamics through inherently low-dimensional single-pixel modalities and inferring complex behavioral patterns via seamless vision-language integration. Building on this framework, we demonstrate that single-pixel sensing intrinsically suppresses identity recoverability, rendering state-of-the-art face recognition systems ineffective below a critical sampling rate. We further show that SP-VLM can nonetheless extract meaningful behavioral semantics, enabling robust anomaly detection, people counting, and activity understanding from severely degraded single-pixel observations. Combining these findings, we identify a practical sampling-rate regime in which behavioral intelligence emerges while personal identity remains strongly protected. Together, these results point to a human-rights-aligned pathway for safety monitoring that can support timely intervention without normalizing intrusive surveillance in privacy-sensitive spaces.

</details>


### [15] [Synthetic Data Guided Feature Selection for Robust Activity Recognition in Older Adults](https://arxiv.org/abs/2601.17053)
*Shuhao Que,Dieuwke van Dartel,Ilse Heeringa,Han Hegeman,Miriam Vollenbroek-Hutten,Ying Wang*

Main category: cs.CV

TL;DR: Researchers developed a robust human activity recognition system using synthetic data to improve physical activity monitoring in older adults undergoing hip fracture rehabilitation, achieving high accuracy for daily activities including often-overlooked postural transfers.


<details>
  <summary>Details</summary>
Motivation: Physical activity monitoring during hip fracture rehabilitation is crucial for preventing functional decline in older adults, but existing wearable systems developed for middle-aged adults perform poorly in this population due to slower, more variable gait patterns.

Method: Study included 24 healthy adults over 80 performing daily activities (walking, standing, sitting, lying, postural transfers) under simulated free-living conditions for 75 minutes while wearing accelerometers on lower back and upper thigh. Used leave-one-subject-out cross-validation and developed a feature intervention model (FIM) aided by synthetic data guidance.

Result: The FIM achieved mean F1-scores: 0.896 (walking), 0.927 (standing), 0.997 (sitting), 0.937 (lying down), and 0.816 (postural transfers). Synthetic data improved generalization and significantly enhanced detection of clinically important postural transfers compared to control model without synthetic data.

Conclusion: Preliminary results demonstrate feasibility of robust activity recognition in older adults using synthetic data. Further validation in actual hip fracture patients is needed to assess clinical utility of the proposed monitoring system.

Abstract: Physical activity during hip fracture rehabilitation is essential for mitigating long-term functional decline in geriatric patients. However, it is rarely quantified in clinical practice. Existing continuous monitoring systems with commercially available wearable activity trackers are typically developed in middle-aged adults and therefore perform unreliably in older adults with slower and more variable gait patterns. This study aimed to develop a robust human activity recognition (HAR) system to improve continuous physical activity recognition in the context of hip fracture rehabilitation. 24 healthy older adults aged over 80 years were included to perform activities of daily living (walking, standing, sitting, lying down, and postural transfers) under simulated free-living conditions for 75 minutes while wearing two accelerometers positioned on the lower back and anterior upper thigh. Model robustness was evaluated using leave-one-subject-out cross-validation. The synthetic data demonstrated potential to improve generalization across participants. The resulting feature intervention model (FIM), aided by synthetic data guidance, achieved reliable activity recognition with mean F1-scores of 0.896 for walking, 0.927 for standing, 0.997 for sitting, 0.937 for lying down, and 0.816 for postural transfers. Compared with a control condition model without synthetic data, the FIM significantly improved the postural transfer detection, i.e., an activity class of high clinical relevance that is often overlooked in existing HAR literature. In conclusion, these preliminary results demonstrate the feasibility of robust activity recognition in older adults. Further validation in hip fracture patient populations is required to assess the clinical utility of the proposed monitoring system.

</details>


### [16] [Ego4OOD: Rethinking Egocentric Video Domain Generalization via Covariate Shift Scoring](https://arxiv.org/abs/2601.17056)
*Zahra Vaseqi,James Clark*

Main category: cs.CV

TL;DR: Ego4OOD: A new domain generalization benchmark for egocentric video action recognition that focuses on covariate shift rather than concept shift, with a lightweight binary classification approach achieving competitive performance.


<details>
  <summary>Details</summary>
Motivation: Existing egocentric domain generalization benchmarks conflate covariate shifts with concept shifts, making it difficult to reliably evaluate models' ability to generalize across input distributions. Egocentric video faces challenges like large intra-class spatio-temporal variability, long-tailed feature distributions, and strong action-environment correlations.

Method: 1) Introduce Ego4OOD benchmark derived from Ego4D with eight geographically distinct domains, emphasizing measurable covariate diversity while reducing concept shift through semantically coherent, moment-level action categories. 2) Develop clustering-based covariate shift metric to quantify domain difficulty. 3) Propose one-vs-all binary training objective that decomposes multi-class action recognition into independent binary classification tasks, reducing interference between visually similar classes under feature distribution shift.

Result: A lightweight two-layer fully connected network achieves performance competitive with state-of-the-art egocentric domain generalization methods on both Argo1M and Ego4OOD benchmarks, despite using fewer parameters and no additional modalities. Empirical analysis shows clear relationship between measured covariate shift and recognition performance.

Conclusion: The work highlights the importance of controlled benchmarks and quantitative domain characterization for studying out-of-distribution generalization in egocentric video. The proposed binary classification formulation is particularly effective for covariate shift scenarios by reducing interference between visually similar classes.

Abstract: Egocentric video action recognition under domain shifts remains challenging due to large intra-class spatio-temporal variability, long-tailed feature distributions, and strong correlations between actions and environments. Existing benchmarks for egocentric domain generalization often conflate covariate shifts with concept shifts, making it difficult to reliably evaluate a model's ability to generalize across input distributions. To address this limitation, we introduce Ego4OOD, a domain generalization benchmark derived from Ego4D that emphasizes measurable covariate diversity while reducing concept shift through semantically coherent, moment-level action categories. Ego4OOD spans eight geographically distinct domains and is accompanied by a clustering-based covariate shift metric that provides a quantitative proxy for domain difficulty. We further leverage a one-vs-all binary training objective that decomposes multi-class action recognition into independent binary classification tasks. This formulation is particularly well-suited for covariate shift by reducing interference between visually similar classes under feature distribution shift. Using this formulation, we show that a lightweight two-layer fully connected network achieves performance competitive with state-of-the-art egocentric domain generalization methods on both Argo1M and Ego4OOD, despite using fewer parameters and no additional modalities. Our empirical analysis demonstrates a clear relationship between measured covariate shift and recognition performance, highlighting the importance of controlled benchmarks and quantitative domain characterization for studying out-of-distribution generalization in egocentric video.

</details>


### [17] [A Computer Vision Pipeline for Iterative Bullet Hole Tracking in Rifle Zeroing](https://arxiv.org/abs/2601.17062)
*Robert M. Belcher,Brendan C. Degryse,Leonard R. Kosta,Christopher J. Lowrance*

Main category: cs.CV

TL;DR: Automated computer vision system for bullet hole detection and iteration tracking using YOLOv8 with IoU analysis, novel data augmentation, and perspective correction, achieving 97% mAP detection and 88.8% iteration assignment accuracy.


<details>
  <summary>Details</summary>
Motivation: Traditional rifle sight zeroing requires physical inspection of bullet holes, causing delays due to range safety protocols and increasing human error risk. An automated system is needed to streamline this process.

Method: Combines YOLOv8 for small-object detection with IoU analysis to differentiate bullet holes across sequential images. Uses novel data augmentation that removes objects to simulate firing sequences, and ORB-based perspective correction for target orientation standardization.

Result: Achieves 97.0% mean average precision on bullet hole detection and 88.8% accuracy in assigning bullet holes to the correct firing iteration.

Conclusion: The system successfully automates bullet hole detection and iteration tracking for rifle zeroing, with potential broader applications in domains requiring temporal differentiation of visually similar objects.

Abstract: Adjusting rifle sights, a process commonly called "zeroing," requires shooters to identify and differentiate bullet holes from multiple firing iterations. Traditionally, this process demands physical inspection, introducing delays due to range safety protocols and increasing the risk of human error. We present an end-to-end computer vision system for automated bullet hole detection and iteration-based tracking directly from images taken at the firing line. Our approach combines YOLOv8 for accurate small-object detection with Intersection over Union (IoU) analysis to differentiate bullet holes across sequential images. To address the scarcity of labeled sequential data, we propose a novel data augmentation technique that removes rather than adds objects to simulate realistic firing sequences. Additionally, we introduce a preprocessing pipeline that standardizes target orientation using ORB-based perspective correction, improving model accuracy. Our system achieves 97.0% mean average precision on bullet hole detection and 88.8% accuracy in assigning bullet holes to the correct firing iteration. While designed for rifle zeroing, this framework offers broader applicability in domains requiring the temporal differentiation of visually similar objects.

</details>


### [18] [A Mechanistic View on Video Generation as World Models: State and Dynamics](https://arxiv.org/abs/2601.17067)
*Luozhou Wang,Zhifei Chen,Yihua Du,Dongyu Yan,Wenhang Ge,Guibao Shen,Xinli Xu,Leyi Wu,Man Chen,Tianshuo Xu,Peiran Ren,Xin Tao,Pengfei Wan,Ying-Cong Chen*

Main category: cs.CV

TL;DR: Proposes taxonomy for video generation models as world models, focusing on state construction and dynamics modeling, advocating shift from visual fidelity to functional benchmarks for physical reasoning.


<details>
  <summary>Details</summary>
Motivation: Bridge the gap between contemporary "stateless" video generation architectures and classic state-centric world model theories, positioning video generation models as potential world simulators.

Method: Introduces taxonomy with two pillars: State Construction (implicit/context vs explicit/latent compression) and Dynamics Modeling (knowledge integration & architectural reformulation). Advocates for functional benchmarks testing physical persistence and causal reasoning.

Result: Proposes framework to categorize existing approaches and identifies critical frontiers: enhancing persistence via data-driven memory/compressed fidelity, and advancing causality through latent factor decoupling/reasoning-prior integration.

Conclusion: Field should evolve from generating visually plausible videos to building robust, general-purpose world simulators by addressing persistence and causality challenges through the proposed taxonomy and evaluation shift.

Abstract: Large-scale video generation models have demonstrated emergent physical coherence, positioning them as potential world models. However, a gap remains between contemporary "stateless" video architectures and classic state-centric world model theories. This work bridges this gap by proposing a novel taxonomy centered on two pillars: State Construction and Dynamics Modeling. We categorize state construction into implicit paradigms (context management) and explicit paradigms (latent compression), while dynamics modeling is analyzed through knowledge integration and architectural reformulation. Furthermore, we advocate for a transition in evaluation from visual fidelity to functional benchmarks, testing physical persistence and causal reasoning. We conclude by identifying two critical frontiers: enhancing persistence via data-driven memory and compressed fidelity, and advancing causality through latent factor decoupling and reasoning-prior integration. By addressing these challenges, the field can evolve from generating visually plausible videos to building robust, general-purpose world simulators.

</details>


### [19] [Superpixel-Based Image Segmentation Using Squared 2-Wasserstein Distances](https://arxiv.org/abs/2601.17071)
*Jisui Huang,Andreas Alpers,Ke Chen,Na Lei*

Main category: cs.CV

TL;DR: Efficient image segmentation method using two-level clustering: superpixel grouping via optimal transport and greedy merging using Wasserstein distance.


<details>
  <summary>Details</summary>
Motivation: To address image segmentation challenges in the presence of strong inhomogeneities, where conventional methods based on mean-color distances may be inadequate.

Method: Two-level clustering: 1) Pixels grouped into superpixels via linear least-squares assignment (special case of discrete optimal transport), 2) Superpixels greedily merged using squared 2-Wasserstein distance between empirical distributions.

Result: Improved segmentation accuracy on challenging images while maintaining high computational efficiency compared to conventional methods.

Conclusion: The optimal transport framework provides a mathematically unified formulation for both clustering levels, leading to better performance in inhomogeneous image segmentation.

Abstract: We present an efficient method for image segmentation in the presence of strong inhomogeneities. The approach can be interpreted as a two-level clustering procedure: pixels are first grouped into superpixels via a linear least-squares assignment problem, which can be viewed as a special case of a discrete optimal transport (OT) problem, and these superpixels are subsequently greedily merged into object-level segments using the squared 2-Wasserstein distance between their empirical distributions. In contrast to conventional superpixel merging strategies based on mean-color distances, our framework employs a distributional OT distance, yielding a mathematically unified formulation across both clustering levels. Numerical experiments demonstrate that this perspective leads to improved segmentation accuracy on challenging images while retaining high computational efficiency.

</details>


### [20] [GlassesGB: Controllable 2D GAN-Based Eyewear Personalization for 3D Gaussian Blendshapes Head Avatars](https://arxiv.org/abs/2601.17088)
*Rui-Yang Ju,Jen-Shiun Chiang*

Main category: cs.CV

TL;DR: GlassesGB is a framework that bridges 2D generative customization with 3D head avatar rendering for personalized eyewear design in VR applications.


<details>
  <summary>Details</summary>
Motivation: Existing virtual try-on systems lack fine-grained user-driven customization and are limited to predefined templates or 2D image generation, creating a gap for personalized 3D eyewear design in VR scenarios.

Method: Integrates 3D Gaussian Blendshapes for head reconstruction with 2D generative customization techniques to create a framework that supports customizable eyewear generation for 3D head avatars.

Result: GlassesGB effectively bridges 2D generative customization with 3D head avatar rendering, enabling personalized eyewear design for VR applications.

Conclusion: The proposed framework addresses the challenge of achieving personalized eyewear design for VR applications by combining 2D generative methods with 3D avatar rendering techniques.

Abstract: Virtual try-on systems allow users to interactively try different products within VR scenarios. However, most existing VTON methods operate only on predefined eyewear templates and lack support for fine-grained, user-driven customization. While GlassesGAN enables personalized 2D eyewear design, its capability remains limited to 2D image generation. Motivated by the success of 3D Gaussian Blendshapes in head reconstruction, we integrate these two techniques and propose GlassesGB, a framework that supports customizable eyewear generation for 3D head avatars. GlassesGB effectively bridges 2D generative customization with 3D head avatar rendering, addressing the challenge in achieving personalized eyewear design for VR applications. The implementation code is available at https://ruiyangju.github.io/GlassesGB.

</details>


### [21] [GRASP: Guided Region-Aware Sparse Prompting for Adapting MLLMs to Remote Sensing](https://arxiv.org/abs/2601.17089)
*Qigan Sun,Chaoning Zhang,Jianwei Zhang,Xudong Wang,Jiehui Xie,Pengcheng Zheng,Haoyu Wang,Sungyoung Lee,Chi-lok Andy Tai,Yang Yang,Heng Tao Shen*

Main category: cs.CV

TL;DR: GRASP is a parameter-efficient fine-tuning method for MLLMs on remote sensing images that uses guided region-aware sparse prompting to focus on relevant regions while filtering background noise.


<details>
  <summary>Details</summary>
Motivation: Existing fine-tuning methods for MLLMs perform poorly on remote sensing images due to large-scale variations, sparse target distributions, and complex regional semantics, causing overfitting on background noise or neglect of target details.

Method: GRASP introduces spatially structured soft prompts associated with spatial blocks from a frozen visual token grid, using a question-guided sparse fusion mechanism to dynamically aggregate task-specific context into a compact global prompt.

Result: Extensive experiments on multiple RSVQA benchmarks show GRASP achieves competitive performance compared to existing fine-tuning and prompt-based methods while maintaining high parameter efficiency.

Conclusion: GRASP effectively addresses the challenges of applying MLLMs to remote sensing images by enabling focused attention on relevant regions while filtering background noise through parameter-efficient fine-tuning.

Abstract: In recent years, Multimodal Large Language Models (MLLMs) have made significant progress in visual question answering tasks. However, directly applying existing fine-tuning methods to remote sensing (RS) images often leads to issues such as overfitting on background noise or neglecting target details. This is primarily due to the large-scale variations, sparse target distributions, and complex regional semantic features inherent in RS images. These challenges limit the effectiveness of MLLMs in RS tasks. To address these challenges, we propose a parameter-efficient fine-tuning (PEFT) strategy called Guided Region-Aware Sparse Prompting (GRASP). GRASP introduces spatially structured soft prompts associated with spatial blocks extracted from a frozen visual token grid. Through a question-guided sparse fusion mechanism, GRASP dynamically aggregates task-specific context into a compact global prompt, enabling the model to focus on relevant regions while filtering out background noise. Extensive experiments on multiple RSVQA benchmarks show that GRASP achieves competitive performance compared to existing fine-tuning and prompt-based methods while maintaining high parameter efficiency.

</details>


### [22] [LoD Sketch Extraction from Architectural Models Using Generative AI: Dataset Construction for Multi-Level Architectural Design Generation](https://arxiv.org/abs/2601.17095)
*Xusheng Du,Athiwat Kongkaeo,Ye Zhang,Haoran Xie*

Main category: cs.CV

TL;DR: Proposes an automatic LoD sketch extraction framework using generative AI to create geometrically consistent multi-level architectural representations from high-detail models, addressing the lack of paired training data for AI-driven architectural design.


<details>
  <summary>Details</summary>
Motivation: Traditional LoD modeling is manual, time-consuming, and prone to inconsistencies. While generative AI offers potential for multi-level architectural generation, it's limited by the lack of high-quality paired LoD training data.

Method: An automatic LoD sketch extraction framework that progressively simplifies high-detail architectural models using generative AI and computer vision techniques, creating geometrically consistent and hierarchically coherent multi-LoD representations.

Result: Achieves SSIM values of 0.7319 (LoD3→LoD2) and 0.7532 (LoD2→LoD1) with normalized Hausdorff distances of 25.1% and 61.0% of image diagonal, demonstrating strong geometric consistency and controlled deviation during abstraction.

Conclusion: The framework effectively preserves global structure while achieving progressive semantic simplification across LoD levels, providing reliable data and technical support for AI-driven multi-level architectural generation and hierarchical modeling.

Abstract: For architectural design, representation across multiple Levels of Details (LoD) is essential for achieving a smooth transition from conceptual massing to detailed modeling. However, traditional LoD modeling processes rely on manual operations that are time-consuming, labor-intensive, and prone to geometric inconsistencies. While the rapid advancement of generative artificial intelligence (AI) has opened new possibilities for generating multi-level architectural models from sketch inputs, its application remains limited by the lack of high-quality paired LoD training data. To address this issue, we propose an automatic LoD sketch extraction framework using generative AI models, which progressively simplifies high-detail architectural models to automatically generate geometrically consistent and hierarchically coherent multi-LoD representations. The proposed framework integrates computer vision techniques with generative AI methods to establish a progressive extraction pipeline that transitions from detailed representations to volumetric abstractions. Experimental results demonstrate that the method maintains strong geometric consistency across LoD levels, achieving SSIM values of 0.7319 and 0.7532 for the transitions from LoD3 to LoD2 and from LoD2 to LoD1, respectively, with corresponding normalized Hausdorff distances of 25.1% and 61.0% of the image diagonal, reflecting controlled geometric deviation during abstraction. These results verify that the proposed framework effectively preserves global structure while achieving progressive semantic simplification across different LoD levels, providing reliable data and technical support for AI-driven multi-level architectural generation and hierarchical modeling.

</details>


### [23] [Performance uncertainty in medical image analysis: a large-scale investigation of confidence intervals](https://arxiv.org/abs/2601.17103)
*Pascaline André,Charles Heitz,Evangelia Christodoulou,Annika Reinke,Carole H. Sudre,Michela Antonelli,Patrick Godau,M. Jorge Cardoso,Antoine Gilson,Sophie Tezenas du Montcel,Gaël Varoquaux,Lena Maier-Hein,Olivier Colliot*

Main category: cs.CV

TL;DR: Large-scale empirical analysis of confidence interval methods for medical imaging AI performance uncertainty quantification, revealing key dependencies on study parameters like sample size, metrics, aggregation strategies, and problem type.


<details>
  <summary>Details</summary>
Motivation: The medical imaging AI community lacks awareness of diverse confidence interval methods and their behavior in specific settings, despite CI's central role in performance uncertainty quantification for reliable validation and clinical translation.

Method: Conducted large-scale empirical analysis across 24 segmentation and classification tasks, using 19 trained models per task group, multiple performance metrics, aggregation strategies, and widely adopted CI methods to evaluate reliability (coverage) and precision (width).

Result: Five principal findings: 1) required sample size varies from dozens to thousands depending on parameters; 2) CI behavior strongly affected by performance metric choice; 3) aggregation strategy substantially influences reliability; 4) problem type (segmentation vs classification) modulates effects; 5) different CI methods vary in reliability and precision by use case.

Conclusion: Results provide key components for developing future guidelines on reporting performance uncertainty in medical imaging AI, addressing the community's gap in understanding CI method behavior.

Abstract: Performance uncertainty quantification is essential for reliable validation and eventual clinical translation of medical imaging artificial intelligence (AI). Confidence intervals (CIs) play a central role in this process by indicating how precise a reported performance estimate is. Yet, due to the limited amount of work examining CI behavior in medical imaging, the community remains largely unaware of how many diverse CI methods exist and how they behave in specific settings. The purpose of this study is to close this gap. To this end, we conducted a large-scale empirical analysis across a total of 24 segmentation and classification tasks, using 19 trained models per task group, a broad spectrum of commonly used performance metrics, multiple aggregation strategies, and several widely adopted CI methods. Reliability (coverage) and precision (width) of each CI method were estimated across all settings to characterize their dependence on study characteristics. Our analysis revealed five principal findings: 1) the sample size required for reliable CIs varies from a few dozens to several thousands of cases depending on study parameters; 2) CI behavior is strongly affected by the choice of performance metric; 3) aggregation strategy substantially influences the reliability of CIs, e.g. they require more observations for macro than for micro; 4) the machine learning problem (segmentation versus classification) modulates these effects; 5) different CI methods are not equally reliable and precise depending on the use case. These results form key components for the development of future guidelines on reporting performance uncertainty in medical imaging AI.

</details>


### [24] [StealthMark: Harmless and Stealthy Ownership Verification for Medical Segmentation via Uncertainty-Guided Backdoors](https://arxiv.org/abs/2601.17107)
*Qinkai Yu,Chong Zhang,Gaojie Jin,Tianjin Huang,Wei Zhou,Wenhui Li,Xiaobo Jin,Bo Huang,Yitian Zhao,Guang Yang,Gregory Y. H. Lip,Yalin Zheng,Aline Villavicencio,Yanda Meng*

Main category: cs.CV

TL;DR: StealthMark is a novel black-box watermarking method for protecting medical segmentation models by subtly modulating model uncertainty without affecting segmentation performance, enabling ownership verification through explanation methods like LIME.


<details>
  <summary>Details</summary>
Motivation: Medical segmentation models are valuable IP but lack protection methods; existing techniques focus on classification/generative tasks, leaving segmentation models vulnerable. Privacy concerns and limited expert annotations make model protection crucial.

Method: StealthMark subtly modulates model uncertainty without changing final segmentation outputs. Uses model-agnostic explanation methods (e.g., LIME) to extract feature attributions that reveal a QR code watermark under triggering conditions.

Result: Achieved ASR above 95% across four medical imaging datasets and five segmentation models (including SAM), with less than 1% drop in Dice and AUC scores, significantly outperforming backdoor-based methods.

Conclusion: StealthMark provides effective, stealthy, and harmless ownership verification for medical segmentation models, demonstrating strong potential for practical deployment while preserving model performance.

Abstract: Annotating medical data for training AI models is often costly and limited due to the shortage of specialists with relevant clinical expertise. This challenge is further compounded by privacy and ethical concerns associated with sensitive patient information. As a result, well-trained medical segmentation models on private datasets constitute valuable intellectual property requiring robust protection mechanisms. Existing model protection techniques primarily focus on classification and generative tasks, while segmentation models-crucial to medical image analysis-remain largely underexplored. In this paper, we propose a novel, stealthy, and harmless method, StealthMark, for verifying the ownership of medical segmentation models under black-box conditions. Our approach subtly modulates model uncertainty without altering the final segmentation outputs, thereby preserving the model's performance. To enable ownership verification, we incorporate model-agnostic explanation methods, e.g. LIME, to extract feature attributions from the model outputs. Under specific triggering conditions, these explanations reveal a distinct and verifiable watermark. We further design the watermark as a QR code to facilitate robust and recognizable ownership claims. We conducted extensive experiments across four medical imaging datasets and five mainstream segmentation models. The results demonstrate the effectiveness, stealthiness, and harmlessness of our method on the original model's segmentation performance. For example, when applied to the SAM model, StealthMark consistently achieved ASR above 95% across various datasets while maintaining less than a 1% drop in Dice and AUC scores, significantly outperforming backdoor-based watermarking methods and highlighting its strong potential for practical deployment. Our implementation code is made available at: https://github.com/Qinkaiyu/StealthMark.

</details>


### [25] [iFSQ: Improving FSQ for Image Generation with 1 Line of Code](https://arxiv.org/abs/2601.17124)
*Bin Lin,Zongjian Li,Yuwei Niu,Kaixiong Gong,Yunyang Ge,Yunlong Lin,Mingzhe Zheng,JianWei Zhang,Miles Yang,Zhao Zhong,Liefeng Bo,Li Yuan*

Main category: cs.CV

TL;DR: iFSQ improves FSQ quantization by replacing activation function with distribution-matching mapping, enabling optimal bin utilization and reconstruction. Analysis reveals 4 bits/dim as optimal discrete-continuous equilibrium, and shows AR models converge faster but diffusion models achieve higher ceilings.


<details>
  <summary>Details</summary>
Motivation: The field is split between autoregressive models (discrete tokens) and diffusion models (continuous latents), hindering unified modeling and fair benchmarking. FSQ bridges this gap theoretically but suffers from activation collapse due to equal-interval quantization, forcing trade-offs between reconstruction fidelity and information efficiency.

Method: iFSQ replaces the activation function in original FSQ with a distribution-matching mapping to enforce a uniform prior. This simple one-line code change mathematically guarantees both optimal bin utilization and reconstruction precision. Using iFSQ as a controlled benchmark, the authors analyze discrete vs. continuous representations and adapt Representation Alignment (REPA) to AR models.

Result: Two key insights: (1) Optimal equilibrium between discrete and continuous representations is approximately 4 bits per dimension. (2) Under identical reconstruction constraints, AR models converge faster initially, but diffusion models achieve superior performance ceilings, suggesting sequential ordering may limit generation quality upper bounds. LlamaGen-REPA is developed by adapting REPA to AR models.

Conclusion: iFSQ resolves the FSQ quantization dilemma with minimal changes, providing a unified benchmark for comparing discrete and continuous image generation approaches. The analysis reveals fundamental trade-offs between AR and diffusion models, with 4 bits/dim as the sweet spot. The work enables fair comparison and suggests directions for improving AR models.

Abstract: The field of image generation is currently bifurcated into autoregressive (AR) models operating on discrete tokens and diffusion models utilizing continuous latents. This divide, rooted in the distinction between VQ-VAEs and VAEs, hinders unified modeling and fair benchmarking. Finite Scalar Quantization (FSQ) offers a theoretical bridge, yet vanilla FSQ suffers from a critical flaw: its equal-interval quantization can cause activation collapse. This mismatch forces a trade-off between reconstruction fidelity and information efficiency. In this work, we resolve this dilemma by simply replacing the activation function in original FSQ with a distribution-matching mapping to enforce a uniform prior. Termed iFSQ, this simple strategy requires just one line of code yet mathematically guarantees both optimal bin utilization and reconstruction precision. Leveraging iFSQ as a controlled benchmark, we uncover two key insights: (1) The optimal equilibrium between discrete and continuous representations lies at approximately 4 bits per dimension. (2) Under identical reconstruction constraints, AR models exhibit rapid initial convergence, whereas diffusion models achieve a superior performance ceiling, suggesting that strict sequential ordering may limit the upper bounds of generation quality. Finally, we extend our analysis by adapting Representation Alignment (REPA) to AR models, yielding LlamaGen-REPA. Codes is available at https://github.com/Tencent-Hunyuan/iFSQ

</details>


### [26] [Scaling medical imaging report generation with multimodal reinforcement learning](https://arxiv.org/abs/2601.17151)
*Qianchu Liu,Sheng Zhang,Guanghui Qin,Yu Gu,Ying Jin,Sam Preston,Yanbo Xu,Sid Kiblawi,Wen-wai Yim,Tim Ossowski,Tristan Naumann,Mu Wei,Hoifung Poon*

Main category: cs.CV

TL;DR: UniRG is a reinforcement learning framework for medical imaging report generation that outperforms supervised fine-tuning and achieves state-of-the-art results on chest X-ray report generation benchmarks.


<details>
  <summary>Details</summary>
Motivation: Frontier models have competency gaps in multimodal understanding, especially in biomedical domains like medical imaging report generation. Supervised fine-tuning tends to overfit to superficial patterns rather than learning meaningful medical reasoning.

Method: Universal Report Generation (UniRG) uses reinforcement learning as a unifying mechanism to directly optimize for evaluation metrics designed for end applications, enabling durable generalization across diverse institutions and clinical practices.

Result: UniRG-CXR, trained on publicly available chest X-ray data, sets new overall state-of-the-art on the authoritative ReXrank benchmark, outperforming prior methods by a wide margin in rigorous evaluation scenarios.

Conclusion: Reinforcement learning provides a superior approach to supervised fine-tuning for medical imaging report generation, enabling models to learn meaningful medical reasoning rather than superficial patterns, with strong generalization capabilities across different healthcare settings.

Abstract: Frontier models have demonstrated remarkable capabilities in understanding and reasoning with natural-language text, but they still exhibit major competency gaps in multimodal understanding and reasoning especially in high-value verticals such as biomedicine. Medical imaging report generation is a prominent example. Supervised fine-tuning can substantially improve performance, but they are prone to overfitting to superficial boilerplate patterns. In this paper, we introduce Universal Report Generation (UniRG) as a general framework for medical imaging report generation. By leveraging reinforcement learning as a unifying mechanism to directly optimize for evaluation metrics designed for end applications, UniRG can significantly improve upon supervised fine-tuning and attain durable generalization across diverse institutions and clinical practices. We trained UniRG-CXR on publicly available chest X-ray (CXR) data and conducted a thorough evaluation in CXR report generation with rigorous evaluation scenarios. On the authoritative ReXrank benchmark, UniRG-CXR sets new overall SOTA, outperforming prior state of the art by a wide margin.

</details>


### [27] [LGDWT-GS: Local and Global Discrete Wavelet-Regularized 3D Gaussian Splatting for Sparse-View Scene Reconstruction](https://arxiv.org/abs/2601.17185)
*Shima Salehi,Atharva Agashe,Andrew J. McFarland,Joshua Peeples*

Main category: cs.CV

TL;DR: New few-shot 3D reconstruction method with global/local frequency regularization for stable geometry and fine details, plus multispectral greenhouse dataset and benchmarking package.


<details>
  <summary>Details</summary>
Motivation: Address limitations of existing 3D Gaussian Splatting models in sparse-view conditions, and provide standardized evaluation protocols for few-shot 3D reconstruction.

Method: Integrates global and local frequency regularization to stabilize geometry and preserve fine details. Also introduces multispectral greenhouse dataset with four spectral bands and open-source benchmarking package with standardized protocols.

Result: Achieves sharper, more stable, and spectrally consistent reconstructions than existing baselines on both the multispectral dataset and standard benchmarks.

Conclusion: Proposed method effectively addresses sparse-view reconstruction challenges, and the released dataset and benchmarking package provide valuable resources for the 3DGS community.

Abstract: We propose a new method for few-shot 3D reconstruction that integrates global and local frequency regularization to stabilize geometry and preserve fine details under sparse-view conditions, addressing a key limitation of existing 3D Gaussian Splatting (3DGS) models. We also introduce a new multispectral greenhouse dataset containing four spectral bands captured from diverse plant species under controlled conditions. Alongside the dataset, we release an open-source benchmarking package that defines standardized few-shot reconstruction protocols for evaluating 3DGS-based methods. Experiments on our multispectral dataset, as well as standard benchmarks, demonstrate that the proposed method achieves sharper, more stable, and spectrally consistent reconstructions than existing baselines. The dataset and code for this work are publicly available

</details>


### [28] [Decoding Psychological States Through Movement: Inferring Human Kinesic Functions with Application to Built Environments](https://arxiv.org/abs/2601.17194)
*Cheyu Lin,Katherine A. Flanigan,Sirajum Munir*

Main category: cs.CV

TL;DR: DUET dataset enables privacy-preserving measurement of socially meaningful interactions in built environments using kinesics taxonomy, addressing field-level gaps in social capital research.


<details>
  <summary>Details</summary>
Motivation: Current research lacks consistent, privacy-preserving methods to measure socially meaningful interactions in built environments, limiting evaluation of design interventions' impact on social capital-relevant behaviors.

Method: Developed DUET dataset capturing 12 dyadic interactions across five kinesic functions (emblems, illustrators, affect displays, adaptors, regulators) using four sensing modalities in three built-environment contexts, with a recognition framework using transfer learning on skeletal motion.

Result: Benchmarking shows difficulty of communicative-function recognition; framework reveals structured clustering of kinesic functions and strong association between representation quality and classification performance while generalizing across subjects/contexts.

Conclusion: DUET provides a privacy-preserving, standardized approach to measure socially grounded interactions aligned with social capital theory, enabling better evaluation of built environment designs.

Abstract: Social infrastructure and other built environments are increasingly expected to support well-being and community resilience by enabling social interaction. Yet in civil and built-environment research, there is no consistent and privacy-preserving way to represent and measure socially meaningful interaction in these spaces, leaving studies to operationalize "interaction" differently across contexts and limiting practitioners' ability to evaluate whether design interventions are changing the forms of interaction that social capital theory predicts should matter. To address this field-level and methodological gap, we introduce the Dyadic User Engagement DataseT (DUET) dataset and an embedded kinesics recognition framework that operationalize Ekman and Friesen's kinesics taxonomy as a function-level interaction vocabulary aligned with social capital-relevant behaviors (e.g., reciprocity and attention coordination). DUET captures 12 dyadic interactions spanning all five kinesic functions-emblems, illustrators, affect displays, adaptors, and regulators-across four sensing modalities and three built-environment contexts, enabling privacy-preserving analysis of communicative intent through movement. Benchmarking six open-source, state-of-the-art human activity recognition models quantifies the difficulty of communicative-function recognition on DUET and highlights the limitations of ubiquitous monadic, action-level recognition when extended to dyadic, socially grounded interaction measurement. Building on DUET, our recognition framework infers communicative function directly from privacy-preserving skeletal motion without handcrafted action-to-function dictionaries; using a transfer-learning architecture, it reveals structured clustering of kinesic functions and a strong association between representation quality and classification performance while generalizing across subjects and contexts.

</details>


### [29] [Structural Complexity of Brain MRI reveals age-associated patterns](https://arxiv.org/abs/2601.17211)
*Anzhe Cheng,Italo Ivo Lima Dias Pinto,Paul Bogdan*

Main category: cs.CV

TL;DR: Adapts structural complexity analysis to 3D brain MRI using multiscale coarse-graining, introduces sliding-window method for stability at coarse resolutions, finds age-related complexity decreases strongest at coarser scales.


<details>
  <summary>Details</summary>
Motivation: To develop a reliable signal processing framework for analyzing the multiscale organization of 3D volumetric data, particularly brain MRI, which can capture age-related changes in brain structure.

Method: Adapts structural complexity analysis to 3D signals by coarse-graining MRI data at progressively larger spatial scales and quantifying information loss between resolutions. Introduces sliding-window coarse-graining scheme to overcome instability of traditional block-based approach at coarse resolutions due to limited sampling.

Result: Analysis of large structural MRI datasets spanning mid- to late adulthood shows systematic decrease in structural complexity with age, with strongest effects emerging at coarser spatial scales.

Conclusion: Structural complexity is a reliable tool for multiscale analysis of 3D imaging data and demonstrates utility in predicting biological age from brain MRI, with the refined sliding-window method providing improved robustness for large-scale analysis.

Abstract: We adapt structural complexity analysis to three-dimensional signals, with an emphasis on brain magnetic resonance imaging (MRI). This framework captures the multiscale organization of volumetric data by coarse-graining the signal at progressively larger spatial scales and quantifying the information lost between successive resolutions. While the traditional block-based approach can become unstable at coarse resolutions due to limited sampling, we introduce a sliding-window coarse-graining scheme that provides smoother estimates and improved robustness at large scales. Using this refined method, we analyze large structural MRI datasets spanning mid- to late adulthood and find that structural complexity decreases systematically with age, with the strongest effects emerging at coarser scales. These findings highlight structural complexity as a reliable signal processing tool for multiscale analysis of 3D imaging data, while also demonstrating its utility in predicting biological age from brain MRI.

</details>


### [30] [Spatiotemporal Semantic V2X Framework for Cooperative Collision Prediction](https://arxiv.org/abs/2601.17216)
*Murat Arda Onsu,Poonam Lohan,Burak Kantarci,Aisha Syed,Matthew Andrews,Sean Kennedy*

Main category: cs.CV

TL;DR: Semantic V2X framework uses V-JEPA to generate future frame embeddings from RSU cameras, transmitted to vehicles for lightweight collision prediction, reducing bandwidth by 10,000x while improving F1-score by 10%.


<details>
  <summary>Details</summary>
Motivation: ITS needs real-time collision prediction, but conventional approaches transmitting raw video/data are impractical due to bandwidth and latency constraints in vehicular communications.

Method: RSU-mounted cameras use Video Joint Embedding Predictive Architecture (V-JEPA) to generate spatiotemporal semantic embeddings of future frames. These embeddings are transmitted via V2X links to vehicles, where a lightweight attentive probe and classifier decode them for collision prediction.

Result: The framework achieves 10% F1-score improvement for collision prediction while reducing transmission requirements by four orders of magnitude (10,000x reduction) compared to raw video transmission.

Conclusion: Semantic V2X communication enables cooperative, real-time collision prediction in ITS by significantly reducing communication overhead while maintaining predictive accuracy, validating its practical potential.

Abstract: Intelligent Transportation Systems (ITS) demand real-time collision prediction to ensure road safety and reduce accident severity. Conventional approaches rely on transmitting raw video or high-dimensional sensory data from roadside units (RSUs) to vehicles, which is impractical under vehicular communication bandwidth and latency constraints. In this work, we propose a semantic V2X framework in which RSU-mounted cameras generate spatiotemporal semantic embeddings of future frames using the Video Joint Embedding Predictive Architecture (V-JEPA). To evaluate the system, we construct a digital twin of an urban traffic environment enabling the generation of d verse traffic scenarios with both safe and collision events. These embeddings of the future frame, extracted from V-JEPA, capture task-relevant traffic dynamics and are transmitted via V2X links to vehicles, where a lightweight attentive probe and classifier decode them to predict imminent collisions. By transmitting only semantic embeddings instead of raw frames, the proposed system significantly reduces communication overhead while maintaining predictive accuracy. Experimental results demonstrate that the framework with an appropriate processing method achieves a 10% F1-score improvement for collision prediction while reducing transmission requirements by four orders of magnitude compared to raw video. This validates the potential of semantic V2X communication to enable cooperative, real-time collision prediction in ITS.

</details>


### [31] [Semi-Supervised Domain Adaptation with Latent Diffusion for Pathology Image Classification](https://arxiv.org/abs/2601.17228)
*Tengyue Zhang,Ruiwen Ding,Luoting Zhuang,Yuxiao Wu,Erika F. Rodriguez,William Hsu*

Main category: cs.CV

TL;DR: Semi-supervised domain adaptation using latent diffusion models to generate target-aware synthetic images for improving generalization in computational pathology.


<details>
  <summary>Details</summary>
Motivation: Deep learning models in computational pathology fail to generalize across cohorts/institutions due to domain shift. Existing approaches either don't leverage unlabeled target data or use image translation that distorts tissue structures.

Method: Proposed SSDA framework uses latent diffusion model trained on unlabeled source/target data to generate morphology-preserving, target-aware synthetic images. Conditions diffusion on foundation model features, cohort identity, and tissue preparation method. Synthetic images + real labeled source data train downstream classifier.

Result: Substantially better performance on target cohort test set without degrading source performance. Improved weighted F1 from 0.611 to 0.706 and macro F1 from 0.641 to 0.716 for lung adenocarcinoma prognostication.

Conclusion: Target-aware diffusion-based synthetic data augmentation provides promising approach for improving domain generalization in computational pathology.

Abstract: Deep learning models in computational pathology often fail to generalize across cohorts and institutions due to domain shift. Existing approaches either fail to leverage unlabeled data from the target domain or rely on image-to-image translation, which can distort tissue structures and compromise model accuracy. In this work, we propose a semi-supervised domain adaptation (SSDA) framework that utilizes a latent diffusion model trained on unlabeled data from both the source and target domains to generate morphology-preserving and target-aware synthetic images. By conditioning the diffusion model on foundation model features, cohort identity, and tissue preparation method, we preserve tissue structure in the source domain while introducing target-domain appearance characteristics. The target-aware synthetic images, combined with real, labeled images from the source cohort, are subsequently used to train a downstream classifier, which is then tested on the target cohort. The effectiveness of the proposed SSDA framework is demonstrated on the task of lung adenocarcinoma prognostication. The proposed augmentation yielded substantially better performance on the held-out test set from the target cohort, without degrading source-cohort performance. The approach improved the weighted F1 score on the target-cohort held-out test set from 0.611 to 0.706 and the macro F1 score from 0.641 to 0.716. Our results demonstrate that target-aware diffusion-based synthetic data augmentation provides a promising and effective approach for improving domain generalization in computational pathology.

</details>


### [32] [C-RADIOv4 (Tech Report)](https://arxiv.org/abs/2601.17237)
*Mike Ranzinger,Greg Heinrich,Collin McCarthy,Jan Kautz,Andrew Tao,Bryan Catanzaro,Pavlo Molchanov*

Main category: cs.CV

TL;DR: C-RADIOv4 is an improved vision backbone model family using multi-teacher distillation to combine capabilities from SigLIP2, DINOv3, and SAM3 teachers, offering better performance at same computational cost with enhanced resolution support and licensing.


<details>
  <summary>Details</summary>
Motivation: To create a unified student model that retains and improves the distinct capabilities of multiple specialized teacher models through multi-teacher distillation, providing enhanced performance on downstream tasks without increasing computational complexity.

Method: Uses multi-teacher distillation approach building upon AM-RADIO/RADIOv2.5 design. Trains with updated teacher set: SigLIP2, DINOv3, and SAM3. Implements agglomerative vision backbones that unify capabilities from multiple teachers into a single student model.

Result: Released two model variants: -SO400M (412M parameters) and -H (631M parameters). Achieves strong improvements on key downstream tasks at same computational complexity. Gains new capabilities from imitating SAM3, improves any-resolution support, adds ViTDet option for enhanced high-resolution efficiency, and uses permissive license.

Conclusion: C-RADIOv4 successfully advances the vision backbone family by effectively combining multiple teacher capabilities through distillation, offering improved performance, enhanced resolution flexibility, and practical deployment advantages with permissive licensing.

Abstract: By leveraging multi-teacher distillation, agglomerative vision backbones provide a unified student model that retains and improves the distinct capabilities of multiple teachers. In this tech report, we describe the most recent release of the C-RADIO family of models, C-RADIOv4, which builds upon AM-RADIO/RADIOv2.5 in design, offering strong improvements on key downstream tasks at the same computational complexity. We release -SO400M (412M params), and -H (631M) model variants, both trained with an updated set of teachers: SigLIP2, DINOv3, and SAM3. In addition to improvements on core metrics and new capabilities from imitating SAM3, the C-RADIOv4 model family further improves any-resolution support, brings back the ViTDet option for drastically enhanced efficiency at high-resolution, and comes with a permissive license.

</details>


### [33] [Multi-stage Bridge Inspection System: Integrating Foundation Models with Location Anonymization](https://arxiv.org/abs/2601.17254)
*Takato Yasuno*

Main category: cs.CV

TL;DR: Open-source bridge damage detection system with regional privacy protection using SAM3 for corrosion detection, DBSCAN for completion, and Gaussian blur for construction sign protection.


<details>
  <summary>Details</summary>
Motivation: Japan mandates visual infrastructure inspection every 5 years, but field images contain both damage features (cracks, rebar exposure) and regional information from construction signs. Need to protect regional privacy while accurately extracting damage features for repair decisions.

Method: Uses Segment Anything Model (SAM) 3 for rebar corrosion detection, DBSCAN for automatic completion of missed regions, Gaussian blur for construction sign protection, and four preprocessing methods to improve OCR accuracy. GPU optimization enables fast processing.

Result: System achieves 1.7-second processing per image with GPU optimization. Technology stack includes SAM3, PyTorch, OpenCV, pytesseract, and scikit-learn for efficient bridge inspection with regional information protection.

Conclusion: Proposed open-source system enables safe infrastructure monitoring while protecting regional privacy, balancing accurate damage feature extraction with information protection for public safety.

Abstract: In Japan, civil infrastructure condition monitoring is mandated through visual inspection every five years. Field-captured damage images frequently contain concrete cracks and rebar exposure, often accompanied by construction signs revealing regional information. To enable safe infrastructure use without causing public anxiety, it is essential to protect regional information while accurately extracting damage features and visualizing key indicators for repair decision-making. This paper presents an open-source bridge damage detection system with regional privacy protection capabilities. We employ Segment Anything Model (SAM) 3 for rebar corrosion detection and utilize DBSCAN for automatic completion of missed regions. Construction sign regions are detected and protected through Gaussian blur. Four preprocessing methods improve OCR accuracy, and GPU optimization enables 1.7-second processing per image. The technology stack includes SAM3, PyTorch, OpenCV, pytesseract, and scikit-learn, achieving efficient bridge inspection with regional information protection.

</details>


### [34] [FineVAU: A Novel Human-Aligned Benchmark for Fine-Grained Video Anomaly Understanding](https://arxiv.org/abs/2601.17258)
*João Pereira,Vasco Lopes,João Neves,David Semedo*

Main category: cs.CV

TL;DR: FineVAU is a new benchmark for Video Anomaly Understanding that introduces a human-aligned evaluation metric (FVScore) and a comprehensive dataset (FineW3) to address limitations in current VAU evaluation methods.


<details>
  <summary>Details</summary>
Motivation: Current Video Anomaly Understanding (VAU) evaluation methods are inadequate - n-gram metrics fail to capture rich LVLM responses, while LLM-based evaluation focuses on language quality over factual relevance, leading to subjective judgments misaligned with human perception.

Method: Proposes FineVAU benchmark with: 1) FVScore metric that assesses presence of critical visual elements (What, Who, Where) in LVLM answers, 2) FineW3 dataset curated through structured automatic procedure that augments existing annotations with fine-grained visual information.

Result: Human evaluation shows FVScore has superior alignment with human perception compared to current approaches. Experiments reveal LVLM limitations in perceiving anomalies requiring spatial and fine-grained temporal understanding, despite strong performance on coarse/static information.

Conclusion: FineVAU addresses critical gaps in VAU evaluation by shifting focus to fine-grained, domain-specific understanding of anomalies, providing interpretable feedback and revealing important limitations in current LVLM capabilities for video anomaly understanding.

Abstract: Video Anomaly Understanding (VAU) is a novel task focused on describing unusual occurrences in videos. Despite growing interest, the evaluation of VAU remains an open challenge. Existing benchmarks rely on n-gram-based metrics (e.g., BLEU, ROUGE-L) or LLM-based evaluation. The first fails to capture the rich, free-form, and visually grounded nature of LVLM responses, while the latter focuses on assessing language quality over factual relevance, often resulting in subjective judgments that are misaligned with human perception. In this work, we address this issue by proposing FineVAU, a new benchmark for VAU that shifts the focus towards rich, fine-grained and domain-specific understanding of anomalous videos. We formulate VAU as a three-fold problem, with the goal of comprehensively understanding key descriptive elements of anomalies in video: events (What), participating entities (Who) and location (Where). Our benchmark introduces a) FVScore, a novel, human-aligned evaluation metric that assesses the presence of critical visual elements in LVLM answers, providing interpretable, fine-grained feedback; and b) FineW3, a novel, comprehensive dataset curated through a structured and fully automatic procedure that augments existing human annotations with high quality, fine-grained visual information. Human evaluation reveals that our proposed metric has a superior alignment with human perception of anomalies in comparison to current approaches. Detailed experiments on FineVAU unveil critical limitations in LVLM's ability to perceive anomalous events that require spatial and fine-grained temporal understanding, despite strong performance on coarse grain, static information, and events with strong visual cues.

</details>


### [35] [Inference-Time Loss-Guided Colour Preservation in Diffusion Sampling](https://arxiv.org/abs/2601.17259)
*Angad Singh Ahuja,Aarush Ram Anandh*

Main category: cs.CV

TL;DR: A training-free method for precise color control in text-to-image diffusion models using region-constrained color preservation with distribution-aware objectives.


<details>
  <summary>Details</summary>
Motivation: Text-to-image diffusion systems struggle with precise color control, especially in design workflows requiring explicit color targets. Current methods can satisfy average color constraints but produce perceptually salient local failures.

Method: Inference-time method combining: (1) ROI-based inpainting for spatial selectivity, (2) background-latent re-imposition to prevent color drift, (3) latent nudging via gradient guidance using composite loss in CIE Lab and linear RGB with CVaR-style and soft-maximum penalties, plus late-start gate and time-dependent scheduling.

Result: The method provides practical, training-free color adherence that can be integrated into standard Stable Diffusion inpainting pipelines, addressing both mean color constraints and distribution of pixelwise errors.

Conclusion: Distribution-aware color control is necessary beyond mean-only approaches, and the proposed inference-time method effectively enables precise color targeting without additional training.

Abstract: Precise color control remains a persistent failure mode in text-to-image diffusion systems, particularly in design-oriented workflows where outputs must satisfy explicit, user-specified color targets. We present an inference-time, region-constrained color preservation method that steers a pretrained diffusion model without any additional training. Our approach combines (i) ROI-based inpainting for spatial selectivity, (ii) background-latent re-imposition to prevent color drift outside the ROI, and (iii) latent nudging via gradient guidance using a composite loss defined in CIE Lab and linear RGB. The loss is constructed to control not only the mean ROI color but also the tail of the pixelwise error distribution through CVaR-style and soft-maximum penalties, with a late-start gate and a time-dependent schedule to stabilize guidance across denoising steps. We show that mean-only baselines can satisfy average color constraints while producing perceptually salient local failures, motivating our distribution-aware objective. The resulting method provides a practical, training-free mechanism for targeted color adherence that can be integrated into standard Stable Diffusion inpainting pipelines.

</details>


### [36] [Cross360: 360° Monocular Depth Estimation via Cross Projections Across Scales](https://arxiv.org/abs/2601.17271)
*Kun Huang,Fang-Lue Zhang,Neil Dodgson*

Main category: cs.CV

TL;DR: Cross360 is a novel cross-attention-based architecture for 360° depth estimation that integrates local tangent patch features with global equirectangular features to achieve accurate and globally consistent depth estimation.


<details>
  <summary>Details</summary>
Motivation: Existing 360° depth estimation methods struggle with balancing global and local consistency. They have limited global perception in local patch features, and their combined global representations fail to address feature extraction discrepancies at patch boundaries.

Method: Proposes Cross360 with two key modules: 1) Cross Projection Feature Alignment module using cross-attention to align local tangent projection features with equirectangular projection's 360° field of view, and 2) Progressive Feature Aggregation with Attention module that refines multi-scaled features progressively.

Result: Cross360 significantly outperforms existing methods across most benchmark datasets, especially when the entire 360° image is available, demonstrating superior accuracy and global consistency in depth estimation.

Conclusion: The proposed cross-attention-based architecture effectively integrates local and global information for 360° depth estimation, addressing the limitations of existing methods and achieving state-of-the-art performance.

Abstract: 360° depth estimation is a challenging research problem due to the difficulty of finding a representation that both preserves global continuity and avoids distortion in spherical images. Existing methods attempt to leverage complementary information from multiple projections, but struggle with balancing global and local consistency. Their local patch features have limited global perception, and the combined global representation does not address discrepancies in feature extraction at the boundaries between patches. To address these issues, we propose Cross360, a novel cross-attention-based architecture integrating local and global information using less-distorted tangent patches along with equirectangular features. Our Cross Projection Feature Alignment module employs cross-attention to align local tangent projection features with the equirectangular projection's 360° field of view, ensuring each tangent projection patch is aware of the global context. Additionally, our Progressive Feature Aggregation with Attention module refines multi-scaled features progressively, enhancing depth estimation accuracy. Cross360 significantly outperforms existing methods across most benchmark datasets, especially those in which the entire 360° image is available, demonstrating its effectiveness in accurate and globally consistent depth estimation. The code and model are available at https://github.com/huangkun101230/Cross360.

</details>


### [37] [Fluxamba: Topology-Aware Anisotropic State Space Models for Geological Lineament Segmentation in Multi-Source Remote Sensing](https://arxiv.org/abs/2601.17288)
*Jin Bai,Huiyao Zhang,Qi Wen,Shengyang Li,Xiaolin Tian,Atta ur Rahman*

Main category: cs.CV

TL;DR: Fluxamba is a lightweight neural architecture for precise segmentation of geological linear features that addresses topological mismatches in SSMs through anisotropic information flux and noise reduction mechanisms.


<details>
  <summary>Details</summary>
Motivation: Existing State Space Models (SSMs) have near-linear complexity but use rigid, axis-aligned scanning trajectories that create topological mismatches with curvilinear geological features, causing fragmented context and feature erosion.

Method: Fluxamba introduces a topology-aware feature rectification framework with Structural Flux Block (SFB) that integrates Anisotropic Structural Gate (ASG) and Prior-Modulated Flow (PMF) to decouple feature orientation from spatial location. It also includes Hierarchical Spatial Regulator (HSR) for multi-scale alignment and High-Fidelity Focus Unit (HFFU) for signal-to-noise ratio maximization.

Result: Achieves SOTA on geological benchmarks: 89.22% F1-score and 89.87% mIoU on LROC-Lineament dataset. Runs at 24 FPS with only 3.4M parameters and 6.3G FLOPs, reducing computational costs by up to 100x compared to heavy-weight baselines.

Conclusion: Fluxamba establishes a new Pareto frontier between segmentation fidelity and onboard deployment feasibility for geological linear feature segmentation, enabling real-time processing with minimal computational resources.

Abstract: The precise segmentation of geological linear features, spanning from planetary lineaments to terrestrial fractures, demands capturing long-range dependencies across complex anisotropic topologies. Although State Space Models (SSMs) offer near-linear computational complexity, their dependence on rigid, axis-aligned scanning trajectories induces a fundamental topological mismatch with curvilinear targets, resulting in fragmented context and feature erosion. To bridge this gap, we propose Fluxamba, a lightweight architecture that introduces a topology-aware feature rectification framework. Central to our design is the Structural Flux Block (SFB), which orchestrates an anisotropic information flux by integrating an Anisotropic Structural Gate (ASG) with a Prior-Modulated Flow (PMF). This mechanism decouples feature orientation from spatial location, dynamically gating context aggregation along the target's intrinsic geometry rather than rigid paths. Furthermore, to mitigate serialization-induced noise in low-contrast environments, we incorporate a Hierarchical Spatial Regulator (HSR) for multi-scale semantic alignment and a High-Fidelity Focus Unit (HFFU) to explicitly maximize the signal-to-noise ratio of faint features. Extensive experiments on diverse geological benchmarks (LROC-Lineament, LineaMapper, and GeoCrack) demonstrate that Fluxamba establishes a new state-of-the-art. Notably, on the challenging LROC-Lineament dataset, it achieves an F1-score of 89.22% and mIoU of 89.87%. Achieving a real-time inference speed of over 24 FPS with only 3.4M parameters and 6.3G FLOPs, Fluxamba reduces computational costs by up to two orders of magnitude compared to heavy-weight baselines, thereby establishing a new Pareto frontier between segmentation fidelity and onboard deployment feasibility.

</details>


### [38] [Dynamic Meta-Ensemble Framework for Efficient and Accurate Deep Learning in Plant Leaf Disease Detection on Resource-Constrained Edge Devices](https://arxiv.org/abs/2601.17290)
*Weloday Fikadu Moges,Jianmei Su,Amin Waqas*

Main category: cs.CV

TL;DR: DMEF is a dynamic meta-ensemble framework that adaptively combines lightweight CNNs for plant disease detection on edge devices, achieving high accuracy with low computational cost.


<details>
  <summary>Details</summary>
Motivation: Edge devices for plant disease detection (IoT sensors, smartphones, embedded systems) have severe computational and energy constraints, creating a gap between high-accuracy AI models and practical field applications.

Method: Dynamic Meta-Ensemble Framework (DMEF) uses adaptive weighting to combine predictions from three lightweight CNNs (MobileNetV2, NASNetMobile, InceptionV3). It optimizes trade-off between accuracy improvements (DeltaAcc) and computational efficiency (model size) through iterative weight updates during training.

Result: Achieved state-of-the-art classification accuracies: 99.53% for potato diseases and 96.61% for maize diseases, surpassing standalone models and static ensembles by 2.1% and 6.3%. Inference latency <75ms with <1 million parameters.

Conclusion: DMEF bridges the gap between high-accuracy AI and practical field applications, showing strong potential for edge-based agricultural monitoring and scalable crop disease management.

Abstract: Deploying deep learning models for plant disease detection on edge devices such as IoT sensors, smartphones, and embedded systems is severely constrained by limited computational resources and energy budgets. To address this challenge, we introduce a novel Dynamic Meta-Ensemble Framework (DMEF) for high-accuracy plant disease diagnosis under resource constraints. DMEF employs an adaptive weighting mechanism that dynamically combines the predictions of three lightweight convolutional neural networks (MobileNetV2, NASNetMobile, and InceptionV3) by optimizing a trade-off between accuracy improvements (DeltaAcc) and computational efficiency (model size). During training, the ensemble weights are updated iteratively, favoring models exhibiting high performance and low complexity. Extensive experiments on benchmark datasets for potato and maize diseases demonstrate state-of-the-art classification accuracies of 99.53% and 96.61%, respectively, surpassing standalone models and static ensembles by 2.1% and 6.3%. With computationally efficient inference latency (<75ms) and a compact footprint (<1 million parameters), DMEF shows strong potential for edge-based agricultural monitoring, suggesting viability for scalable crop disease management. This bridges the gap between high-accuracy AI and practical field applications.

</details>


### [39] [ClinNet: Evidential Ordinal Regression with Bilateral Asymmetry and Prototype Memory for Knee Osteoarthritis Grading](https://arxiv.org/abs/2601.17315)
*Xiaoyang Li,Runni Zhou*

Main category: cs.CV

TL;DR: ClinNet: A trustworthy evidential ordinal regression framework for knee osteoarthritis grading that models bilateral asymmetry, uses diagnostic memory banks, and estimates uncertainty via Normal-Inverse-Gamma distribution.


<details>
  <summary>Details</summary>
Motivation: Knee osteoarthritis grading from radiographic images is challenging due to subtle inter-grade differences, annotation uncertainty, and the ordinal nature of disease progression. Conventional deep learning approaches treat it as deterministic multi-class classification, ignoring both continuous progression and annotation uncertainty.

Method: ClinNet integrates three components: (1) Bilateral Asymmetry Encoder to model medial-lateral structural discrepancies, (2) Diagnostic Memory Bank with class-wise prototypes to stabilize feature representations, and (3) Evidential Ordinal Head based on Normal-Inverse-Gamma distribution to jointly estimate continuous KL grades and epistemic uncertainty.

Result: Achieves Quadratic Weighted Kappa of 0.892 and Accuracy of 0.768, statistically outperforming state-of-the-art baselines (p < 0.001). The model's uncertainty estimates successfully flag out-of-distribution samples and potential misdiagnoses.

Conclusion: ClinNet provides a trustworthy framework for KOA grading that addresses both the ordinal nature of disease progression and annotation uncertainty, enabling safe clinical deployment through reliable uncertainty estimation.

Abstract: Knee osteoarthritis (KOA) grading based on radiographic images is a critical yet challenging task due to subtle inter-grade differences, annotation uncertainty, and the inherently ordinal nature of disease progression. Conventional deep learning approaches typically formulate this problem as deterministic multi-class classification, ignoring both the continuous progression of degeneration and the uncertainty in expert annotations. In this work, we propose ClinNet, a novel trustworthy framework that addresses KOA grading as an evidential ordinal regression problem. The proposed method integrates three key components: (1) a Bilateral Asymmetry Encoder (BAE) that explicitly models medial-lateral structural discrepancies; (2) a Diagnostic Memory Bank that maintains class-wise prototypes to stabilize feature representations; and (3) an Evidential Ordinal Head based on the Normal-Inverse-Gamma (NIG) distribution to jointly estimate continuous KL grades and epistemic uncertainty. Extensive experiments demonstrate that ClinNet achieves a Quadratic Weighted Kappa of 0.892 and Accuracy of 0.768, statistically outperforming state-of-the-art baselines (p < 0.001). Crucially, we demonstrate that the model's uncertainty estimates successfully flag out-of-distribution samples and potential misdiagnoses, paving the way for safe clinical deployment.

</details>


### [40] [SkyReels-V3 Technique Report](https://arxiv.org/abs/2601.17323)
*Debang Li,Zhengcong Fei,Tuanhui Li,Yikun Dou,Zheng Chen,Jiangping Yang,Mingyuan Fan,Jingtao Xu,Jiahua Wang,Baoxuan Gu,Mingshan Chang,Yuqiang Xie,Binjie Mao,Youqiang Zhang,Nuo Pang,Hao Zhang,Yuzhe Jin,Zhiheng Xu,Dixuan Lin,Guibin Chen,Yahui Zhou*

Main category: cs.CV

TL;DR: SkyReels-V3 is a unified multimodal video generation model using diffusion Transformers that supports three paradigms: reference image-to-video synthesis, video extension, and audio-guided video generation with state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: Video generation is crucial for building world models, and multimodal contextual inference represents a key capability test. The authors aim to create a unified framework that can handle multiple video generation tasks within a single architecture.

Method: Built on diffusion Transformers with unified multimodal in-context learning. Uses three approaches: (1) reference image-to-video with cross-frame pairing, image editing, semantic rewriting, and hybrid training; (2) video extension with spatio-temporal consistency modeling; (3) talking avatar with audio-conditioned generation using frame insertion patterns.

Result: Achieves state-of-the-art or near state-of-the-art performance on key metrics including visual quality, instruction following, and specific aspect metrics, approaching leading closed-source systems.

Conclusion: SkyReels-V3 demonstrates a unified approach to multiple video generation tasks with strong performance across different paradigms, showing promise for building comprehensive world models through multimodal contextual inference.

Abstract: Video generation serves as a cornerstone for building world models, where multimodal contextual inference stands as the defining test of capability. In this end, we present SkyReels-V3, a conditional video generation model, built upon a unified multimodal in-context learning framework with diffusion Transformers. SkyReels-V3 model supports three core generative paradigms within a single architecture: reference images-to-video synthesis, video-to-video extension and audio-guided video generation. (i) reference images-to-video model is designed to produce high-fidelity videos with strong subject identity preservation, temporal coherence, and narrative consistency. To enhance reference adherence and compositional stability, we design a comprehensive data processing pipeline that leverages cross frame pairing, image editing, and semantic rewriting, effectively mitigating copy paste artifacts. During training, an image video hybrid strategy combined with multi-resolution joint optimization is employed to improve generalization and robustness across diverse scenarios. (ii) video extension model integrates spatio-temporal consistency modeling with large-scale video understanding, enabling both seamless single-shot continuation and intelligent multi-shot switching with professional cinematographic patterns. (iii) Talking avatar model supports minute-level audio-conditioned video generation by training first-and-last frame insertion patterns and reconstructing key-frame inference paradigms. On the basis of ensuring visual quality, synchronization of audio and videos has been optimized.
  Extensive evaluations demonstrate that SkyReels-V3 achieves state-of-the-art or near state-of-the-art performance on key metrics including visual quality, instruction following, and specific aspect metrics, approaching leading closed-source systems. Github: https://github.com/SkyworkAI/SkyReels-V3.

</details>


### [41] [SymbolSight: Minimizing Inter-Symbol Interference for Reading with Prosthetic Vision](https://arxiv.org/abs/2601.17326)
*Jasmine Lesner,Michael Beyeler*

Main category: cs.CV

TL;DR: SymbolSight framework optimizes visual symbols for retinal prostheses to reduce temporal interference in letter recognition, achieving 22x confusion reduction.


<details>
  <summary>Details</summary>
Motivation: Retinal prostheses have low spatial resolution and temporal persistence, causing afterimages to interfere with sequential letter recognition. Current typography is poorly matched to prosthetic vision limitations.

Method: Computational framework using simulated prosthetic vision and neural proxy observer to estimate pairwise symbol confusability, then optimizing symbol-to-letter mappings using language-specific bigram statistics to minimize confusion among frequently adjacent letters.

Result: Across Arabic, Bulgarian, and English simulations, heterogeneous symbol sets reduced predicted confusion by median factor of 22 relative to native alphabets.

Conclusion: Standard typography is poorly suited for serial, low-bandwidth prosthetic vision; computational modeling can efficiently generate optimized visual encodings for future clinical evaluation.

Abstract: Retinal prostheses restore limited visual perception, but low spatial resolution and temporal persistence make reading difficult. In sequential letter presentation, the afterimage of one symbol can interfere with perception of the next, leading to systematic recognition errors. Rather than relying on future hardware improvements, we investigate whether optimizing the visual symbols themselves can mitigate this temporal interference. We present SymbolSight, a computational framework that selects symbol-to-letter mappings to minimize confusion among frequently adjacent letters. Using simulated prosthetic vision (SPV) and a neural proxy observer, we estimate pairwise symbol confusability and optimize assignments using language-specific bigram statistics. Across simulations in Arabic, Bulgarian, and English, the resulting heterogeneous symbol sets reduced predicted confusion by a median factor of 22 relative to native alphabets. These results suggest that standard typography is poorly matched to serial, low-bandwidth prosthetic vision and demonstrate how computational modeling can efficiently narrow the design space of visual encodings to generate high-potential candidates for future psychophysical and clinical evaluation.

</details>


### [42] [Learning with Geometric Priors in U-Net Variants for Polyp Segmentation](https://arxiv.org/abs/2601.17331)
*Fabian Vazquez,Jose A. Nuñez,Diego Adame,Alissen Moreno,Augustin Zhan,Huimin Li,Jinghao Yang,Haoteng Tang,Bin Fu,Pengfei Gu*

Main category: cs.CV

TL;DR: Proposes a Geometric Prior-guided Module (GPM) that injects explicit geometric priors into U-Net-based architectures for improved polyp segmentation in colonoscopy images.


<details>
  <summary>Details</summary>
Motivation: Existing CNN-, Transformer-, and Mamba-based U-Net variants struggle to capture geometric and structural cues in low-contrast or cluttered colonoscopy scenes, which is crucial for accurate polyp segmentation in early colorectal cancer detection.

Method: Fine-tunes Visual Geometry Grounded Transformer (VGGT) on simulated ColonDepth dataset to estimate depth maps, then processes these through GPM to encode geometric priors into encoder feature maps with spatial and channel attention mechanisms for refinement.

Result: Extensive experiments on five public polyp segmentation datasets show consistent gains over three strong baselines, demonstrating the effectiveness of the plug-and-play GPM module.

Conclusion: The proposed GPM successfully addresses geometric cue limitations in polyp segmentation by injecting explicit geometric priors, improving performance across diverse U-Net variants for more robust computer-aided diagnosis.

Abstract: Accurate and robust polyp segmentation is essential for early colorectal cancer detection and for computer-aided diagnosis. While convolutional neural network-, Transformer-, and Mamba-based U-Net variants have achieved strong performance, they still struggle to capture geometric and structural cues, especially in low-contrast or cluttered colonoscopy scenes. To address this challenge, we propose a novel Geometric Prior-guided Module (GPM) that injects explicit geometric priors into U-Net-based architectures for polyp segmentation. Specifically, we fine-tune the Visual Geometry Grounded Transformer (VGGT) on a simulated ColonDepth dataset to estimate depth maps of polyp images tailored to the endoscopic domain. These depth maps are then processed by GPM to encode geometric priors into the encoder's feature maps, where they are further refined using spatial and channel attention mechanisms that emphasize both local spatial and global channel information. GPM is plug-and-play and can be seamlessly integrated into diverse U-Net variants. Extensive experiments on five public polyp segmentation datasets demonstrate consistent gains over three strong baselines. Code and the generated depth maps are available at: https://github.com/fvazqu/GPM-PolypSeg

</details>


### [43] [AGE-Net: Spectral--Spatial Fusion and Anatomical Graph Reasoning with Evidential Ordinal Regression for Knee Osteoarthritis Grading](https://arxiv.org/abs/2601.17336)
*Xiaoyang Li,Runni Zhou*

Main category: cs.CV

TL;DR: AGE-Net: A ConvNeXt-based framework for automated KL grading from knee radiographs using spectral-spatial fusion, anatomical graph reasoning, and differential refinement with evidential regression for uncertainty estimation.


<details>
  <summary>Details</summary>
Motivation: Automated KL grading is challenging due to subtle structural changes, long-range anatomical dependencies, and ambiguity near grade boundaries in knee radiographs.

Method: AGE-Net integrates Spectral-Spatial Fusion (SSF), Anatomical Graph Reasoning (AGR), and Differential Refinement (DFR) with a ConvNeXt backbone. It uses Normal-Inverse-Gamma evidential regression for uncertainty estimation and pairwise ordinal ranking constraints to preserve label ordinality.

Result: Achieves quadratic weighted kappa (QWK) of 0.9017 ± 0.0045 and mean squared error (MSE) of 0.2349 ± 0.0028 on knee KL dataset, outperforming strong CNN baselines with consistent gains in ablation studies.

Conclusion: AGE-Net effectively addresses challenges in automated KL grading through multi-modal feature integration, anatomical reasoning, and uncertainty-aware ordinal regression, demonstrating superior performance and robustness.

Abstract: Automated Kellgren--Lawrence (KL) grading from knee radiographs is challenging due to subtle structural changes, long-range anatomical dependencies, and ambiguity near grade boundaries. We propose AGE-Net, a ConvNeXt-based framework that integrates Spectral--Spatial Fusion (SSF), Anatomical Graph Reasoning (AGR), and Differential Refinement (DFR). To capture predictive uncertainty and preserve label ordinality, AGE-Net employs a Normal-Inverse-Gamma (NIG) evidential regression head and a pairwise ordinal ranking constraint. On a knee KL dataset, AGE-Net achieves a quadratic weighted kappa (QWK) of 0.9017 +/- 0.0045 and a mean squared error (MSE) of 0.2349 +/- 0.0028 over three random seeds, outperforming strong CNN baselines and showing consistent gains in ablation studies. We further outline evaluations of uncertainty quality, robustness, and explainability, with additional experimental figures to be included in the full manuscript.

</details>


### [44] [TEXTS-Diff: TEXTS-Aware Diffusion Model for Real-World Text Image Super-Resolution](https://arxiv.org/abs/2601.17340)
*Haodong He,Xin Zhan,Yancheng Bai,Rui Lan,Lei Sun,Xiangxiang Chu*

Main category: cs.CV

TL;DR: Real-Texts dataset and TEXTS-Diff model for text image super-resolution, addressing poor text region performance in existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing text image super-resolution methods suffer from poor performance on text regions due to scarcity of text image data in datasets, and datasets with isolated text samples limit background reconstruction quality.

Method: Construct Real-Texts dataset from real-world images with diverse scenarios and natural text instances in Chinese/English. Propose TEXTS-Diff model using abstract concepts for textual understanding and concrete text regions for detail enhancement.

Result: Achieves state-of-the-art performance across multiple evaluation metrics with superior generalization ability and text restoration accuracy in complex scenarios.

Conclusion: The proposed dataset and diffusion model effectively address text region distortions while preserving visual scene fidelity, with code, model, and dataset to be released.

Abstract: Real-world text image super-resolution aims to restore overall visual quality and text legibility in images suffering from diverse degradations and text distortions. However, the scarcity of text image data in existing datasets results in poor performance on text regions. In addition, datasets consisting of isolated text samples limit the quality of background reconstruction. To address these limitations, we construct Real-Texts, a large-scale, high-quality dataset collected from real-world images, which covers diverse scenarios and contains natural text instances in both Chinese and English. Additionally, we propose the TEXTS-Aware Diffusion Model (TEXTS-Diff) to achieve high-quality generation in both background and textual regions. This approach leverages abstract concepts to improve the understanding of textual elements within visual scenes and concrete text regions to enhance textual details. It mitigates distortions and hallucination artifacts commonly observed in text regions, while preserving high-quality visual scene fidelity. Extensive experiments demonstrate that our method achieves state-of-the-art performance across multiple evaluation metrics, exhibiting superior generalization ability and text restoration accuracy in complex scenarios. All the code, model, and dataset will be released.

</details>


### [45] [STARS: Shared-specific Translation and Alignment for missing-modality Remote Sensing Semantic Segmentation](https://arxiv.org/abs/2601.17342)
*Tong Wang,Xiaodong Zhang,Guanzhou Chen,Jiaqi Wang,Chenxi Liu,Xiaoliang Tan,Wenchao Guo,Xuyang Li,Xuanrui Wang,Zifan Wang*

Main category: cs.CV

TL;DR: STARS is a robust semantic segmentation framework for incomplete multimodal remote sensing data that addresses missing modalities through asymmetric alignment and pixel-level semantic sampling.


<details>
  <summary>Details</summary>
Motivation: Missing modality data (optical, SAR, DSM) is common in practical remote sensing applications, causing performance decline in traditional multimodal fusion models. Existing methods suffer from feature collapse and overly generalized recovered features.

Method: STARS framework with two key designs: 1) Asymmetric alignment mechanism with bidirectional translation and stop-gradient to prevent feature collapse and reduce hyperparameter sensitivity; 2) Pixel-level Semantic sampling Alignment (PSA) combining class-balanced pixel sampling with cross-modality semantic alignment loss to address severe class imbalance.

Result: The proposed STARS framework effectively handles incomplete multimodal inputs, prevents feature collapse, reduces sensitivity to hyperparameters, and improves minority-class recognition despite severe class imbalance.

Conclusion: STARS provides a robust solution for semantic segmentation with missing multimodal remote sensing data, addressing key limitations of existing methods through innovative alignment and sampling strategies.

Abstract: Multimodal remote sensing technology significantly enhances the understanding of surface semantics by integrating heterogeneous data such as optical images, Synthetic Aperture Radar (SAR), and Digital Surface Models (DSM). However, in practical applications, the missing of modality data (e.g., optical or DSM) is a common and severe challenge, which leads to performance decline in traditional multimodal fusion models. Existing methods for addressing missing modalities still face limitations, including feature collapse and overly generalized recovered features. To address these issues, we propose \textbf{STARS} (\textbf{S}hared-specific \textbf{T}ranslation and \textbf{A}lignment for missing-modality \textbf{R}emote \textbf{S}ensing), a robust semantic segmentation framework for incomplete multimodal inputs. STARS is built on two key designs. First, we introduce an asymmetric alignment mechanism with bidirectional translation and stop-gradient, which effectively prevents feature collapse and reduces sensitivity to hyperparameters. Second, we propose a Pixel-level Semantic sampling Alignment (PSA) strategy that combines class-balanced pixel sampling with cross-modality semantic alignment loss, to mitigate alignment failures caused by severe class imbalance and improve minority-class recognition.

</details>


### [46] [Revisiting Lightweight Low-Light Image Enhancement: From a YUV Color Space Perspective](https://arxiv.org/abs/2601.17349)
*Hailong Yan,Shice Liu,Xiangtao Zhang,Lujian Yao,Fengxiang Yang,Jinwei Chen,Bo Li*

Main category: cs.CV

TL;DR: A novel YUV-based lightweight low-light image enhancement method using frequency-domain analysis and specialized attention modules for different channels achieves state-of-the-art performance with fewer parameters.


<details>
  <summary>Details</summary>
Motivation: Mobile devices need lightweight low-light image enhancement (L3IE) but face trade-offs between visual quality and model compactness. Existing methods using disentangling strategies like Retinex theory and YUV transformations overlook channel-specific degradation patterns and cross-channel interactions.

Method: Frequency-domain analysis reveals Y channel loses low-frequency content while UV channels have high-frequency noise. Proposes YUV-based paradigm with: 1) Dual-Stream Global-Local Attention for Y channel, 2) Y-guided Local-Aware Frequency Attention for UV channels, and 3) Guided Interaction module for feature fusion.

Result: Extensive experiments show the model establishes new state-of-the-art on multiple benchmarks, delivering superior visual quality with significantly lower parameter count.

Conclusion: The proposed YUV-based approach effectively addresses channel-specific degradation patterns in low-light images, achieving better performance with more compact models suitable for mobile applications.

Abstract: In the current era of mobile internet, Lightweight Low-Light Image Enhancement (L3IE) is critical for mobile devices, which faces a persistent trade-off between visual quality and model compactness. While recent methods employ disentangling strategies to simplify lightweight architectural design, such as Retinex theory and YUV color space transformations, their performance is fundamentally limited by overlooking channel-specific degradation patterns and cross-channel interactions. To address this gap, we perform a frequency-domain analysis that confirms the superiority of the YUV color space for L3IE. We identify a key insight: the Y channel primarily loses low-frequency content, while the UV channels are corrupted by high-frequency noise. Leveraging this finding, we propose a novel YUV-based paradigm that strategically restores channels using a Dual-Stream Global-Local Attention module for the Y channel, a Y-guided Local-Aware Frequency Attention module for the UV channels, and a Guided Interaction module for final feature fusion. Extensive experiments validate that our model establishes a new state-of-the-art on multiple benchmarks, delivering superior visual quality with a significantly lower parameter count.

</details>


### [47] [NeRF-MIR: Towards High-Quality Restoration of Masked Images with Neural Radiance Fields](https://arxiv.org/abs/2601.17350)
*Xianliang Huang,Zhizhou Zhong,Shuhang Chen,Yi Xu,Juhong Guan,Shuigeng Zhou*

Main category: cs.CV

TL;DR: NeRF-MIR is a novel neural rendering approach for restoring masked/corrupted images in 3D scene reconstruction using Neural Radiance Fields, featuring patch-based entropy ray emitting and progressive iterative restoration mechanisms.


<details>
  <summary>Details</summary>
Motivation: NeRF has shown great performance in novel view synthesis, but struggles with corrupted/masked images common in natural scene captures. Existing methods don't effectively handle image restoration within the NeRF framework, and current datasets don't support this task.

Method: Proposes three key components: 1) PERE (Patch-based Entropy for Ray Emitting) strategy to properly distribute emitted rays for learning intricate textures, 2) PIRE (Progressively Iterative Restoration) mechanism for self-training restoration of masked regions, and 3) dynamically-weighted loss function that automatically recalibrates loss weights for masked areas. Also constructs three masked datasets for evaluation.

Result: Extensive experiments on real data and constructed datasets demonstrate NeRF-MIR's superiority over existing methods in masked image restoration tasks.

Conclusion: NeRF-MIR successfully extends NeRF's capabilities to handle corrupted/masked images through novel ray emitting, progressive restoration, and adaptive loss weighting, showing strong potential for practical applications where image corruption is common.

Abstract: Neural Radiance Fields (NeRF) have demonstrated remarkable performance in novel view synthesis. However, there is much improvement room on restoring 3D scenes based on NeRF from corrupted images, which are common in natural scene captures and can significantly impact the effectiveness of NeRF. This paper introduces NeRF-MIR, a novel neural rendering approach specifically proposed for the restoration of masked images, demonstrating the potential of NeRF in this domain. Recognizing that randomly emitting rays to pixels in NeRF may not effectively learn intricate image textures, we propose a \textbf{P}atch-based \textbf{E}ntropy for \textbf{R}ay \textbf{E}mitting (\textbf{PERE}) strategy to distribute emitted rays properly. This enables NeRF-MIR to fuse comprehensive information from images of different views. Additionally, we introduce a \textbf{P}rogressively \textbf{I}terative \textbf{RE}storation (\textbf{PIRE}) mechanism to restore the masked regions in a self-training process. Furthermore, we design a dynamically-weighted loss function that automatically recalibrates the loss weights for masked regions. As existing datasets do not support NeRF-based masked image restoration, we construct three masked datasets to simulate corrupted scenarios. Extensive experiments on real data and constructed datasets demonstrate the superiority of NeRF-MIR over its counterparts in masked image restoration.

</details>


### [48] [HyDeMiC: A Deep Learning-based Mineral Classifier using Hyperspectral Data](https://arxiv.org/abs/2601.17352)
*M. L. Mamud,Piyoosh Jaysaval,Frederick D Day-Lewis,M. K. Mudunuru*

Main category: cs.CV

TL;DR: HyDeMiC is a CNN-based mineral classifier for hyperspectral imaging that maintains high accuracy even under noisy conditions, achieving near-perfect classification on clean data and strong performance with moderate noise.


<details>
  <summary>Details</summary>
Motivation: Traditional mineral classification methods struggle with environmental noise, sensor limitations, and computational complexity of high-dimensional hyperspectral data, creating a need for more robust solutions for real-world applications.

Method: Developed HyDeMiC, a CNN model trained on 115 USGS mineral spectra convolved with HSI sensor response functions, then evaluated on synthetic 2D datasets with varying noise levels (1%, 2%, 5%, 10%) to simulate field conditions.

Result: Achieved near-perfect classification (MCC = 1.00) on clean and low-noise datasets, maintained strong performance under moderate noise conditions, demonstrating robustness for real-world applications.

Conclusion: HyDeMiC shows significant potential for practical hyperspectral mineral classification by effectively handling noise challenges that typically hinder traditional methods in field applications.

Abstract: Hyperspectral imaging (HSI) has emerged as a powerful remote sensing tool for mineral exploration, capitalizing on unique spectral signatures of minerals. However, traditional classification methods such as discriminant analysis, logistic regression, and support vector machines often struggle with environmental noise in data, sensor limitations, and the computational complexity of analyzing high-dimensional HSI data. This study presents HyDeMiC (Hyperspectral Deep Learning-based Mineral Classifier), a convolutional neural network (CNN) model designed for robust mineral classification under noisy data. To train HyDeMiC, laboratory-measured hyperspectral data for 115 minerals spanning various mineral groups were used from the United States Geological Survey (USGS) library. The training dataset was generated by convolving reference mineral spectra with an HSI sensor response function. These datasets contained three copper-bearing minerals, Cuprite, Malachite, and Chalcopyrite, used as case studies for performance demonstration. The trained CNN model was evaluated on several synthetic 2D hyperspectral datasets with noise levels of 1%, 2%, 5%, and 10%. Our noisy data analysis aims to replicate realistic field conditions. The HyDeMiC's performance was assessed using the Matthews Correlation Coefficient (MCC), providing a comprehensive measure across different noise regimes. Results demonstrate that HyDeMiC achieved near-perfect classification accuracy (MCC = 1.00) on clean and low-noise datasets and maintained strong performance under moderate noise conditions. These findings emphasize HyDeMiC's robustness in the presence of moderate noise, highlighting its potential for real-world applications in hyperspectral imaging, where noise is often a significant challenge.

</details>


### [49] [PocketGS: On-Device Training of 3D Gaussian Splatting for High Perceptual Modeling](https://arxiv.org/abs/2601.17354)
*Wenzhi Guo,Guangchi Fang,Shu Yang,Bing Wang*

Main category: cs.CV

TL;DR: PocketGS enables efficient 3D Gaussian Splatting training on mobile devices by addressing memory and computational constraints while maintaining high fidelity.


<details>
  <summary>Details</summary>
Motivation: Current 3DGS methods require resource-unconstrained training that fails on mobile devices due to minute-scale training budgets and limited peak memory, preventing practical on-device scene modeling workflows.

Method: Three co-designed operators: G builds geometry-faithful point-cloud priors; I injects local surface statistics to seed anisotropic Gaussians; T unrolls alpha compositing with cached intermediates and index-mapped gradient scattering for stable mobile backpropagation.

Result: PocketGS outperforms powerful workstation 3DGS baselines, delivering high-quality reconstructions while enabling fully on-device capture-to-rendering workflows under mobile constraints.

Conclusion: PocketGS successfully resolves the fundamental contradictions of standard 3DGS for mobile deployment, satisfying competing requirements of training efficiency, memory compactness, and modeling fidelity for practical on-device 3D scene modeling.

Abstract: Efficient and high-fidelity 3D scene modeling is a long-standing pursuit in computer graphics. While recent 3D Gaussian Splatting (3DGS) methods achieve impressive real-time modeling performance, they rely on resource-unconstrained training assumptions that fail on mobile devices, which are limited by minute-scale training budgets and hardware-available peak-memory. We present PocketGS, a mobile scene modeling paradigm that enables on-device 3DGS training under these tightly coupled constraints while preserving high perceptual fidelity. Our method resolves the fundamental contradictions of standard 3DGS through three co-designed operators: G builds geometry-faithful point-cloud priors; I injects local surface statistics to seed anisotropic Gaussians, thereby reducing early conditioning gaps; and T unrolls alpha compositing with cached intermediates and index-mapped gradient scattering for stable mobile backpropagation. Collectively, these operators satisfy the competing requirements of training efficiency, memory compactness, and modeling fidelity. Extensive experiments demonstrate that PocketGS is able to outperform the powerful mainstream workstation 3DGS baseline to deliver high-quality reconstructions, enabling a fully on-device, practical capture-to-rendering workflow.

</details>


### [50] [UCAD: Uncertainty-guided Contour-aware Displacement for semi-supervised medical image segmentation](https://arxiv.org/abs/2601.17366)
*Chengbo Ding,Fenghe Tang,Shaohua Kevin Zhou*

Main category: cs.CV

TL;DR: UCAD is a semi-supervised medical image segmentation framework that uses uncertainty-guided contour-aware displacement to preserve anatomical structures and improve consistency learning.


<details>
  <summary>Details</summary>
Motivation: Existing displacement strategies in semi-supervised segmentation operate on rectangular regions, ignoring anatomical structures and causing boundary distortions and semantic inconsistency, which is problematic for medical image segmentation where anatomical coherence is crucial.

Method: UCAD uses superpixels to generate anatomically coherent regions aligned with anatomy boundaries, an uncertainty-guided selection mechanism to selectively displace challenging regions, and a dynamic uncertainty-weighted consistency loss to adaptively stabilize training and regularize the model on unlabeled regions.

Result: Extensive experiments show UCAD consistently outperforms state-of-the-art semi-supervised segmentation methods, achieving superior segmentation accuracy under limited annotation settings.

Conclusion: UCAD effectively addresses boundary distortions and semantic inconsistency in semi-supervised medical segmentation by preserving contour-aware semantics and enhancing consistency learning through anatomically coherent displacement strategies.

Abstract: Existing displacement strategies in semi-supervised segmentation only operate on rectangular regions, ignoring anatomical structures and resulting in boundary distortions and semantic inconsistency. To address these issues, we propose UCAD, an Uncertainty-Guided Contour-Aware Displacement framework for semi-supervised medical image segmentation that preserves contour-aware semantics while enhancing consistency learning. Our UCAD leverages superpixels to generate anatomically coherent regions aligned with anatomy boundaries, and an uncertainty-guided selection mechanism to selectively displace challenging regions for better consistency learning. We further propose a dynamic uncertainty-weighted consistency loss, which adaptively stabilizes training and effectively regularizes the model on unlabeled regions. Extensive experiments demonstrate that UCAD consistently outperforms state-of-the-art semi-supervised segmentation methods, achieving superior segmentation accuracy under limited annotation. The code is available at:https://github.com/dcb937/UCAD.

</details>


### [51] [Physical Prompt Injection Attacks on Large Vision-Language Models](https://arxiv.org/abs/2601.17383)
*Chen Ling,Kai Hu,Hangcheng Liu,Xingshuo Han,Tianwei Zhang,Changhai Ou*

Main category: cs.CV

TL;DR: PPIA is a black-box physical prompt injection attack that embeds malicious typographic instructions into physical objects to manipulate LVLMs without requiring model access or knowledge of user queries.


<details>
  <summary>Details</summary>
Motivation: Existing prompt injection attacks on LVLMs require unrealistic assumptions like access to input channels or knowledge of user queries, which don't hold in real-world deployments where LVLMs are used for perception and reasoning in open physical environments.

Method: Combines offline selection of highly recognizable visual prompts with strategic environment-aware placement guided by spatiotemporal attention. The attack is black-box, query-agnostic, and operates solely through visual observation without needing model access.

Result: Achieves attack success rates up to 98% across 10 state-of-the-art LVLMs in simulated and real-world settings on tasks including visual question answering, planning, and navigation. Shows strong robustness under varying physical conditions like distance, viewpoint, and illumination.

Conclusion: PPIA demonstrates a practical and effective physical prompt injection attack that exposes significant security vulnerabilities in LVLMs deployed in real-world environments, highlighting the need for robust defenses against such visual manipulation attacks.

Abstract: Large Vision-Language Models (LVLMs) are increasingly deployed in real-world intelligent systems for perception and reasoning in open physical environments. While LVLMs are known to be vulnerable to prompt injection attacks, existing methods either require access to input channels or depend on knowledge of user queries, assumptions that rarely hold in practical deployments. We propose the first Physical Prompt Injection Attack (PPIA), a black-box, query-agnostic attack that embeds malicious typographic instructions into physical objects perceivable by the LVLM. PPIA requires no access to the model, its inputs, or internal pipeline, and operates solely through visual observation. It combines offline selection of highly recognizable and semantically effective visual prompts with strategic environment-aware placement guided by spatiotemporal attention, ensuring that the injected prompts are both perceivable and influential on model behavior. We evaluate PPIA across 10 state-of-the-art LVLMs in both simulated and real-world settings on tasks including visual question answering, planning, and navigation, PPIA achieves attack success rates up to 98%, with strong robustness under varying physical conditions such as distance, viewpoint, and illumination. Our code is publicly available at https://github.com/2023cghacker/Physical-Prompt-Injection-Attack.

</details>


### [52] [ONRW: Optimizing inversion noise for high-quality and robust watermark](https://arxiv.org/abs/2601.17388)
*Xuan Ding,Xiu Yan,Chuanlong Xie,Yao Zhu*

Main category: cs.CV

TL;DR: A diffusion model-based watermarking framework that uses null-text optimization and iterative denoising to create robust watermarks resistant to image corruptions while maintaining visual quality.


<details>
  <summary>Details</summary>
Motivation: Existing deep learning watermarking systems lack robustness against image corruptions during transmission, undermining their practical value despite good visual quality.

Method: Uses diffusion model with null-text optimization to convert clean images to inversion noise, optimizes noise in latent space, then applies iterative denoising. Includes self-attention constraints and pseudo-mask strategies to preserve original image semantics.

Result: Outperforms stable signature method by average 10% across 12 different image transformations on COCO datasets, demonstrating superior robustness against various corruptions.

Conclusion: The proposed diffusion-based framework achieves both high visual quality and enhanced robustness for watermarking, addressing key limitations of existing methods.

Abstract: Watermarking methods have always been effective means of protecting intellectual property, yet they face significant challenges. Although existing deep learning-based watermarking systems can hide watermarks in images with minimal impact on image quality, they often lack robustness when encountering image corruptions during transmission, which undermines their practical application value. To this end, we propose a high-quality and robust watermark framework based on the diffusion model. Our method first converts the clean image into inversion noise through a null-text optimization process, and after optimizing the inversion noise in the latent space, it produces a high-quality watermarked image through an iterative denoising process of the diffusion model. The iterative denoising process serves as a powerful purification mechanism, ensuring both the visual quality of the watermarked image and enhancing the robustness of the watermark against various corruptions. To prevent the optimizing of inversion noise from distorting the original semantics of the image, we specifically introduced self-attention constraints and pseudo-mask strategies. Extensive experimental results demonstrate the superior performance of our method against various image corruptions. In particular, our method outperforms the stable signature method by an average of 10\% across 12 different image transformations on COCO datasets. Our codes are available at https://github.com/920927/ONRW.

</details>


### [53] [SMV-EAR: Bring Spatiotemporal Multi-View Representation Learning into Efficient Event-Based Action Recognition](https://arxiv.org/abs/2601.17391)
*Rui Fan,Weidong Hao*

Main category: cs.CV

TL;DR: The paper proposes a new spatiotemporal multi-view representation learning framework for event camera action recognition that improves accuracy while reducing parameters and computations.


<details>
  <summary>Details</summary>
Motivation: Existing spatiotemporal multi-view representation learning methods for event-based object recognition have limitations: translation-variant spatial binning representation and naive early concatenation fusion architecture, which hinder performance in event camera action recognition.

Method: Three key innovations: (1) translation-invariant dense conversion of sparse events for principled spatiotemporal multi-view representation, (2) dual-branch dynamic fusion architecture modeling sample-wise complementarity between motion features from different views, and (3) bio-inspired temporal warping augmentation mimicking real-world human action speed variability.

Result: Achieves significant accuracy gains: +7.0% on HARDVS, +10.7% on DailyDVS-200, and +10.2% on THU-EACT-50-CHL datasets, while reducing parameters by 30.1% and computations by 35.7% compared to existing methods.

Conclusion: The proposed framework establishes a novel and powerful paradigm for event camera action recognition, offering improved accuracy with reduced computational complexity.

Abstract: Event cameras action recognition (EAR) offers compelling privacy-protecting and efficiency advantages, where temporal motion dynamics is of great importance. Existing spatiotemporal multi-view representation learning (SMVRL) methods for event-based object recognition (EOR) offer promising solutions by projecting H-W-T events along spatial axis H and W, yet are limited by its translation-variant spatial binning representation and naive early concatenation fusion architecture. This paper reexamines the key SMVRL design stages for EAR and propose: (i) a principled spatiotemporal multi-view representation through translation-invariant dense conversion of sparse events, (ii) a dual-branch, dynamic fusion architecture that models sample-wise complementarity between motion features from different views, and (iii) a bio-inspired temporal warping augmentation that mimics speed variability of real-world human actions. On three challenging EAR datasets of HARDVS, DailyDVS-200 and THU-EACT-50-CHL, we show +7.0%, +10.7%, and +10.2% Top-1 accuracy gains over existing SMVRL EOR method with surprising 30.1% reduced parameters and 35.7% lower computations, establishing our framework as a novel and powerful EAR paradigm.

</details>


### [54] [ReLE: A Scalable System and Structured Benchmark for Diagnosing Capability Anisotropy in Chinese LLMs](https://arxiv.org/abs/2601.17399)
*Rui Fang,Jian Li,Wei Chen,Bin Hu,Ying-Cong Chen,Xin Tang,Liang Diao*

Main category: cs.CV

TL;DR: ReLE is a scalable evaluation system that diagnoses capability anisotropy in LLMs using hybrid scoring and dynamic scheduling, revealing models are specialized rather than generally superior.


<details>
  <summary>Details</summary>
Motivation: Current LLM evaluation faces challenges with benchmark saturation, high computational costs, and static rankings that mask structural trade-offs between capabilities. Traditional leaderboards provide snapshot views but don't reveal the non-uniform performance across domains (capability anisotropy).

Method: ReLE system with two key innovations: (1) Symbolic-Grounded Hybrid Scoring Mechanism to eliminate embedding-based false positives in reasoning tasks, and (2) Dynamic Variance-Aware Scheduler using Neyman allocation with noise correction to reduce compute costs by 70%. Evaluated 304 models across a Domain × Capability matrix with 207,843 samples.

Result: The system achieved 70% compute cost reduction while maintaining ranking correlation of ρ=0.96 compared to full evaluations. Analysis revealed aggregate rankings are highly sensitive to weighting schemes, with models showing Rank Stability Amplitude (RSA) of 11.4 in ReLE vs ~5.0 in traditional benchmarks, confirming models are specialized rather than generally superior.

Conclusion: ReLE serves as a high-frequency diagnostic monitor for the evolving LLM landscape, revealing capability anisotropy that static benchmarks miss. It positions itself not as a replacement for comprehensive static benchmarks, but as a complementary tool for understanding model specialization and performance trade-offs.

Abstract: Large Language Models (LLMs) have achieved rapid progress in Chinese language understanding, yet accurately evaluating their capabilities remains challenged by benchmark saturation and prohibitive computational costs. While static leaderboards provide snapshot rankings, they often mask the structural trade-offs between capabilities. In this work, we present ReLE (Robust Efficient Live Evaluation), a scalable system designed to diagnose Capability Anisotropy, the non-uniformity of model performance across domains. Using ReLE, we evaluate 304 models (189 commercial, 115 open-source) across a Domain $\times$ Capability orthogonal matrix comprising 207,843 samples. We introduce two methodological contributions to address current evaluation pitfalls: (1) A Symbolic-Grounded Hybrid Scoring Mechanism that eliminates embedding-based false positives in reasoning tasks; (2) A Dynamic Variance-Aware Scheduler based on Neyman allocation with noise correction, which reduces compute costs by 70\% compared to full-pass evaluations while maintaining a ranking correlation of $ρ=0.96$. Our analysis reveals that aggregate rankings are highly sensitive to weighting schemes: models exhibit a Rank Stability Amplitude (RSA) of 11.4 in ReLE versus $\sim$5.0 in traditional benchmarks, confirming that modern models are highly specialized rather than generally superior. We position ReLE not as a replacement for comprehensive static benchmarks, but as a high-frequency diagnostic monitor for the evolving model landscape.

</details>


### [55] [HAAF: Hierarchical Adaptation and Alignment of Foundation Models for Few-Shot Pathology Anomaly Detection](https://arxiv.org/abs/2601.17405)
*Chunze Yang,Wenjie Zhao,Yue Tang,Junbo Lu,Jiusong Ge,Qidong Liu,Zeyu Gao,Chen Li*

Main category: cs.CV

TL;DR: HAAF framework bridges granularity mismatch in pathology by cross-level alignment of visual and text features, enabling precise detection of fine-grained abnormalities in ROI regions.


<details>
  <summary>Details</summary>
Motivation: Precision pathology requires detecting subtle morphological abnormalities in specific ROIs, but current V-L models suffer from granularity mismatch where generic representations fail to capture these fine-grained cues. Existing methods treat modalities independently, missing the opportunity to ground semantic prompts in ROI-specific visual contexts.

Method: Proposes Hierarchical Adaptation and Alignment Framework (HAAF) with Cross-Level Scaled Alignment (CLSA) mechanism that enforces sequential calibration: visual features first inject context into text prompts to generate content-adaptive descriptors, which then spatially guide visual encoder to spotlight anomalies. Also includes dual-branch inference strategy integrating semantic scores with geometric prototypes for stability in few-shot settings.

Result: Experiments on four benchmarks show HAAF significantly outperforms state-of-the-art methods and effectively scales with domain-specific backbones (e.g., CONCH) in low-resource scenarios.

Conclusion: HAAF successfully bridges the granularity mismatch in pathology by enabling precise cross-modal alignment, making vision-language models more effective for fine-grained abnormality detection in medical imaging with limited data.

Abstract: Precision pathology relies on detecting fine-grained morphological abnormalities within specific Regions of Interest (ROIs), as these local, texture-rich cues - rather than global slide contexts - drive expert diagnostic reasoning. While Vision-Language (V-L) models promise data efficiency by leveraging semantic priors, adapting them faces a critical Granularity Mismatch, where generic representations fail to resolve such subtle defects. Current adaptation methods often treat modalities as independent streams, failing to ground semantic prompts in ROI-specific visual contexts. To bridge this gap, we propose the Hierarchical Adaptation and Alignment Framework (HAAF). At its core is a novel Cross-Level Scaled Alignment (CLSA) mechanism that enforces a sequential calibration order: visual features first inject context into text prompts to generate content-adaptive descriptors, which then spatially guide the visual encoder to spotlight anomalies. Additionally, a dual-branch inference strategy integrates semantic scores with geometric prototypes to ensure stability in few-shot settings. Experiments on four benchmarks show HAAF significantly outperforms state-of-the-art methods and effectively scales with domain-specific backbones (e.g., CONCH) in low-resource scenarios.

</details>


### [56] [Source-Free Domain Adaptation by Optimizing Batch-Wise Cosine Similarity](https://arxiv.org/abs/2601.17408)
*Harsharaj Pathak,Vineeth N Balasubramanian*

Main category: cs.CV

TL;DR: Proposes a novel SFDA method using neighborhood signatures to learn better clusters and reduce noisy neighbor effects with a single loss term.


<details>
  <summary>Details</summary>
Motivation: Current SFDA methods rely on neighborhood consistency but suffer from misleading neighborhood information and noisy neighbors, limiting adaptation performance.

Method: Introduces neighborhood signatures to capture more informative clusters, uses a single loss term optimizing similarity/dissimilarity of target domain predictions.

Result: Outperforms existing methods on challenging VisDA dataset and achieves competitive results on other benchmark datasets.

Conclusion: Neighborhood signatures effectively mitigate noisy neighbor issues in SFDA, enabling successful adaptation with simpler optimization.

Abstract: Source-Free Domain Adaptation (SFDA) is an emerging area of research that aims to adapt a model trained on a labeled source domain to an unlabeled target domain without accessing the source data. Most of the successful methods in this area rely on the concept of neighborhood consistency but are prone to errors due to misleading neighborhood information. In this paper, we explore this approach from the point of view of learning more informative clusters and mitigating the effect of noisy neighbors using a concept called neighborhood signature, and demonstrate that adaptation can be achieved using just a single loss term tailored to optimize the similarity and dissimilarity of predictions of samples in the target domain. In particular, our proposed method outperforms existing methods in the challenging VisDA dataset while also yielding competitive results on other benchmark datasets.

</details>


### [57] [Cloud-Enabled IoT System for Real-Time Environmental Monitoring and Remote Device Control Using Firebase](https://arxiv.org/abs/2601.17414)
*Abdul Hasib,A. S. M. Ahsanul Sarkar Akib*

Main category: cs.CV

TL;DR: A cloud-enabled IoT system using ESP32, sensors, and Firebase for real-time environmental monitoring and device control with 99.2% data transmission success.


<details>
  <summary>Details</summary>
Motivation: Traditional IoT monitoring systems lack real-time data accessibility, remote controllability, and cloud integration capabilities needed for modern applications.

Method: Uses ESP32 microcontroller with DHT22 temperature/humidity sensor and HC-SR04 ultrasonic distance sensor, integrated with Google Firebase Realtime Database for synchronized data and remote LED control.

Result: Achieved 99.2% data transmission success rate, real-time control latency under 1.5 seconds, persistent data storage, with total implementation cost of $32.50.

Conclusion: The system provides scalable, cost-effective IoT framework accessible to resource-limited developers, suitable for smart home and industrial applications.

Abstract: The proliferation of Internet of Things (IoT) devices has created unprecedented opportunities for remote monitoring and control applications across various domains. Traditional monitoring systems often suffer from limitations in real-time data accessibility, remote controllability, and cloud integration. This paper presents a cloud-enabled IoT system that leverages Google's Firebase Realtime Database for synchronized environmental monitoring and device control. The system utilizes an ESP32 microcontroller to interface with a DHT22 temperature/humidity sensor and an HC-SR04 ultrasonic distance sensor, while enabling remote control of two LED indicators through a cloud-based interface. Real-time sensor data is transmitted to Firebase, providing a synchronized platform accessible from multiple devices simultaneously. Experimental results demonstrate reliable data transmission with 99.2\% success rate, real-time control latency under 1.5 seconds, and persistent data storage for historical analysis. The system architecture offers a scalable framework for various IoT applications, from smart home automation to industrial monitoring, with a total implementation cost of \$32.50. The integration of Firebase provides robust cloud capabilities without requiring complex server infrastructure, making advanced IoT applications accessible to developers and researchers with limited resources.

</details>


### [58] [CoT-Seg: Rethinking Segmentation with Chain-of-Thought Reasoning and Self-Correction](https://arxiv.org/abs/2601.17420)
*Shiu-hong Kao,Chak Ho Huang,Huaiqian Liu,Yu-Wing Tai,Chi-Keung Tang*

Main category: cs.CV

TL;DR: CoT-Seg is a training-free framework that combines chain-of-thought reasoning with self-correction for complex reasoning segmentation tasks, using pre-trained MLLMs to decompose queries, extract semantics, and iteratively refine segmentation masks.


<details>
  <summary>Details</summary>
Motivation: Existing reasoning segmentation methods struggle with complex queries and out-of-domain images. Inspired by human problem-solving where harder problems require longer thinking steps, the authors aim to create a system that can think step-by-step, look up information if needed, generate results, self-evaluate, and refine results.

Method: CoT-Seg leverages pre-trained MLLMs (GPT-4o) without fine-tuning to: 1) decompose queries into meta-instructions, 2) extract fine-grained semantics from images, 3) identify target objects under implicit/complex prompts, and 4) incorporate self-correction where the model evaluates its segmentation against the original query, identifies mismatches, and iteratively refines the mask. The framework also allows retrieval-augmented reasoning for external knowledge access.

Result: The authors introduce ReasonSeg-Hard dataset to showcase CoT-Seg's ability to handle challenging cases. Results demonstrate that combining chain-of-thought reasoning with self-correction significantly improves reliability and robustness, especially in ambiguous or error-prone scenarios.

Conclusion: The tight integration of reasoning and correction offers a powerful paradigm for vision-language integration driven segmentation, providing a robust approach for handling complex reasoning segmentation tasks without requiring model fine-tuning.

Abstract: Existing works of reasoning segmentation often fall short in complex cases, particularly when addressing complicated queries and out-of-domain images. Inspired by the chain-of-thought reasoning, where harder problems require longer thinking steps/time, this paper aims to explore a system that can think step-by-step, look up information if needed, generate results, self-evaluate its own results, and refine the results, in the same way humans approach harder questions. We introduce CoT-Seg, a training-free framework that rethinks reasoning segmentation by combining chain-of-thought reasoning with self-correction. Instead of fine-tuning, CoT-Seg leverages the inherent reasoning ability of pre-trained MLLMs (GPT-4o) to decompose queries into meta-instructions, extract fine-grained semantics from images, and identify target objects even under implicit or complex prompts. Moreover, CoT-Seg incorporates a self-correction stage: the model evaluates its own segmentation against the original query and reasoning trace, identifies mismatches, and iteratively refines the mask. This tight integration of reasoning and correction significantly improves reliability and robustness, especially in ambiguous or error-prone cases. Furthermore, our CoT-Seg framework allows easy incorporation of retrieval-augmented reasoning, enabling the system to access external knowledge when the input lacks sufficient information. To showcase CoT-Seg's ability to handle very challenging cases ,we introduce a new dataset ReasonSeg-Hard. Our results highlight that combining chain-of-thought reasoning, self-correction, offers a powerful paradigm for vision-language integration driven segmentation.

</details>


### [59] [Coronary Artery Segmentation and Vessel-Type Classification in X-Ray Angiography](https://arxiv.org/abs/2601.17429)
*Mehdi Yousefzadeh,Siavash Shirzadeh Barough,Ashkan Fakharifar,Yashar Tayyarazad,Narges Eghbali,Mohaddeseh Mozaffari,Hoda Taeb,Negar Sadat Rafiee Tabatabaee,Parsa Esfahanian,Ghazaleh Sadeghi Gohar,Amineh Safavirad,Saeideh Mazloomzadeh,Ehsan khalilipur,Armin Elahifar,Majid Maleki*

Main category: cs.CV

TL;DR: This paper presents methods for robust coronary artery segmentation and vessel-type labeling in X-ray coronary angiography, addressing domain shift challenges through classical vesselness filters with per-image tuning and deep learning models with merged coronary+catheter supervision.


<details>
  <summary>Details</summary>
Motivation: X-ray coronary angiography (XCA) is the clinical gold standard for coronary artery disease assessment, but quantitative analysis is limited by poor vessel segmentation due to low contrast, motion, foreshortening, overlap, and catheter artifacts. These issues cause domain shift across centers and degrade segmentation quality needed for vessel-specific analytics and anatomical localization.

Method: The approach involves: 1) Selecting best frames near peak opacification using low-intensity histogram criteria with joint super-resolution and enhancement; 2) Benchmarking classical vesselness filters (Meijering, Frangi, Sato) with per-image oracle tuning, global mean settings, and SVR-based parameter prediction; 3) Training neural baselines (U-Net, FPN, Swin Transformer) with coronary-only and merged coronary+catheter supervision; 4) Adding a second stage for vessel-type labeling (LAD, LCX, RCA). External evaluation uses the public DCA1 cohort.

Result: SVR per-image tuning improved Dice scores over global means for all classical filters (e.g., Frangi: 0.759 vs. 0.741). Among deep models, FPN achieved 0.914±0.007 Dice with coronary-only supervision, improving to 0.931±0.006 with merged coronary+catheter labels. On external DCA1 test, Dice dropped to 0.798 (coronary-only) and 0.814 (merged), but light in-domain fine-tuning recovered to 0.881±0.014 and 0.882±0.015. Vessel-type labeling achieved 98.5% accuracy (Dice 0.844) for RCA, 95.4% (0.786) for LAD, and 96.2% (0.794) for LCX.

Conclusion: Learned per-image parameter tuning strengthens classical vessel segmentation pipelines, while high-resolution FPN models with merged-label supervision improve stability and external transfer capability with only modest adaptation needed. The approach enables reliable vessel segmentation and anatomical labeling for coronary analytics despite challenging domain shift issues.

Abstract: X-ray coronary angiography (XCA) is the clinical reference standard for assessing coronary artery disease, yet quantitative analysis is limited by the difficulty of robust vessel segmentation in routine data. Low contrast, motion, foreshortening, overlap, and catheter confounding degrade segmentation and contribute to domain shift across centers. Reliable segmentation, together with vessel-type labeling, enables vessel-specific coronary analytics and downstream measurements that depend on anatomical localization. From 670 cine sequences (407 subjects), we select a best frame near peak opacification using a low-intensity histogram criterion and apply joint super-resolution and enhancement. We benchmark classical Meijering, Frangi, and Sato vesselness filters under per-image oracle tuning, a single global mean setting, and per-image parameter prediction via Support Vector Regression (SVR). Neural baselines include U-Net, FPN, and a Swin Transformer, trained with coronary-only and merged coronary+catheter supervision. A second stage assigns vessel identity (LAD, LCX, RCA). External evaluation uses the public DCA1 cohort. SVR per-image tuning improves Dice over global means for all classical filters (e.g., Frangi: 0.759 vs. 0.741). Among deep models, FPN attains 0.914+/-0.007 Dice (coronary-only), and merged coronary+catheter labels further improve to 0.931+/-0.006. On DCA1 as a strict external test, Dice drops to 0.798 (coronary-only) and 0.814 (merged), while light in-domain fine-tuning recovers to 0.881+/-0.014 and 0.882+/-0.015. Vessel-type labeling achieves 98.5% accuracy (Dice 0.844) for RCA, 95.4% (0.786) for LAD, and 96.2% (0.794) for LCX. Learned per-image tuning strengthens classical pipelines, while high-resolution FPN models and merged-label supervision improve stability and external transfer with modest adaptation.

</details>


### [60] [ReflexSplit: Single Image Reflection Separation via Layer Fusion-Separation](https://arxiv.org/abs/2601.17468)
*Chia-Ming Lee,Yu-Fan Lin,Jing-Hui Jung,Yu-Jou Hsiao,Chih-Chung Hsu,Yu-Lun Liu*

Main category: cs.CV

TL;DR: ReflexSplit is a dual-stream framework for single image reflection separation that addresses transmission-reflection confusion through cross-scale gated fusion, layer fusion-separation blocks, and curriculum training.


<details>
  <summary>Details</summary>
Motivation: Existing SIRS methods suffer from transmission-reflection confusion under nonlinear mixing, especially in deep decoder layers, due to implicit fusion mechanisms and inadequate multi-scale coordination.

Method: Three key innovations: (1) Cross-scale Gated Fusion adaptively aggregates semantic priors, texture details, and decoder context across hierarchical depths; (2) Layer Fusion-Separation Blocks alternate between fusion for shared structure extraction and differential separation for layer-specific disentanglement; (3) Curriculum training with depth-dependent initialization and epoch-wise warmup to progressively strengthen differential separation.

Result: State-of-the-art performance on synthetic and real-world benchmarks with superior perceptual quality and robust generalization.

Conclusion: ReflexSplit effectively addresses transmission-reflection confusion in SIRS through its dual-stream architecture with cross-scale coordination and differential separation mechanisms.

Abstract: Single Image Reflection Separation (SIRS) disentangles mixed images into transmission and reflection layers. Existing methods suffer from transmission-reflection confusion under nonlinear mixing, particularly in deep decoder layers, due to implicit fusion mechanisms and inadequate multi-scale coordination. We propose ReflexSplit, a dual-stream framework with three key innovations. (1) Cross-scale Gated Fusion (CrGF) adaptively aggregates semantic priors, texture details, and decoder context across hierarchical depths, stabilizing gradient flow and maintaining feature consistency. (2) Layer Fusion-Separation Blocks (LFSB) alternate between fusion for shared structure extraction and differential separation for layer-specific disentanglement. Inspired by Differential Transformer, we extend attention cancellation to dual-stream separation via cross-stream subtraction. (3) Curriculum training progressively strengthens differential separation through depth-dependent initialization and epoch-wise warmup. Extensive experiments on synthetic and real-world benchmarks demonstrate state-of-the-art performance with superior perceptual quality and robust generalization. Our code is available at https://github.com/wuw2135/ReflexSplit.

</details>


### [61] [PhaSR: Generalized Image Shadow Removal with Physically Aligned Priors](https://arxiv.org/abs/2601.17470)
*Chia-Ming Lee,Yu-Fan Lin,Yu-Jou Hsiao,Jing-Hui Jung,Yu-Lun Liu,Chih-Chung Hsu*

Main category: cs.CV

TL;DR: PhaSR introduces a physically aligned shadow removal method that handles diverse lighting conditions through dual-level prior alignment, enabling robust performance from single-light shadows to multi-source ambient lighting.


<details>
  <summary>Details</summary>
Motivation: Shadow removal under diverse lighting conditions is challenging due to the need to disentangle illumination from intrinsic reflectance, especially when physical priors are not properly aligned. Traditional methods fail under multi-source illumination scenarios.

Method: Two-stage approach: 1) Physically Aligned Normalization (PAN) performs closed-form illumination correction via Gray-world normalization, log-domain Retinex decomposition, and dynamic range recombination to suppress chromatic bias. 2) Geometric-Semantic Rectification Attention (GSRA) extends differential attention to cross-modal alignment, harmonizing depth-derived geometry with DINO-v2 semantic embeddings to resolve modal conflicts under varying illumination.

Result: Experiments show competitive performance in shadow removal with lower complexity and generalization to ambient lighting where traditional methods fail under multi-source illumination.

Conclusion: PhaSR successfully addresses shadow removal under diverse lighting conditions through physically aligned prior alignment, enabling robust performance across different illumination scenarios from single-light shadows to complex multi-source ambient lighting.

Abstract: Shadow removal under diverse lighting conditions requires disentangling illumination from intrinsic reflectance, a challenge compounded when physical priors are not properly aligned. We propose PhaSR (Physically Aligned Shadow Removal), addressing this through dual-level prior alignment to enable robust performance from single-light shadows to multi-source ambient lighting. First, Physically Aligned Normalization (PAN) performs closed-form illumination correction via Gray-world normalization, log-domain Retinex decomposition, and dynamic range recombination, suppressing chromatic bias. Second, Geometric-Semantic Rectification Attention (GSRA) extends differential attention to cross-modal alignment, harmonizing depth-derived geometry with DINO-v2 semantic embeddings to resolve modal conflicts under varying illumination. Experiments show competitive performance in shadow removal with lower complexity and generalization to ambient lighting where traditional methods fail under multi-source illumination. Our source code is available at https://github.com/ming053l/PhaSR.

</details>


### [62] [BMDS-Net: A Bayesian Multi-Modal Deep Supervision Network for Robust Brain Tumor Segmentation](https://arxiv.org/abs/2601.17504)
*Yan Zhou,Zhen Huang,Yingqiu Li,Yue Ouyang,Suncheng Xiang,Zehua Wang*

Main category: cs.CV

TL;DR: BMDS-Net is a robust brain tumor segmentation framework that addresses clinical challenges of missing MRI modalities and uncertainty quantification, prioritizing clinical safety over just Dice score optimization.


<details>
  <summary>Details</summary>
Motivation: Current Transformer-based models like Swin UNETR have good benchmark performance but fail in clinical practice due to sensitivity to missing modalities (common in real clinical settings) and lack of confidence calibration. Simply maximizing Dice scores on idealized data doesn't meet safety requirements for real-world medical deployment.

Method: Three key contributions: 1) Robust deterministic backbone with Zero-Init Multimodal Contextual Fusion (MMCF) module and Residual-Gated Deep Decoder Supervision (DDS) for stable feature learning and boundary delineation. 2) Memory-efficient Bayesian fine-tuning strategy that transforms network into probabilistic predictor for voxel-wise uncertainty maps. 3) Comprehensive evaluation on BraTS 2021 dataset.

Result: BMDS-Net maintains competitive accuracy while showing superior stability in missing-modality scenarios where baseline models fail. It provides uncertainty maps to highlight potential errors for clinicians and reduces Hausdorff Distance even under modality corruption.

Conclusion: BMDS-Net prioritizes clinical robustness and trustworthiness over simple metric maximization, addressing critical safety requirements for real-world medical deployment of brain tumor segmentation models.

Abstract: Accurate brain tumor segmentation from multi-modal magnetic resonance imaging (MRI) is a prerequisite for precise radiotherapy planning and surgical navigation. While recent Transformer-based models such as Swin UNETR have achieved impressive benchmark performance, their clinical utility is often compromised by two critical issues: sensitivity to missing modalities (common in clinical practice) and a lack of confidence calibration. Merely chasing higher Dice scores on idealized data fails to meet the safety requirements of real-world medical deployment. In this work, we propose BMDS-Net, a unified framework that prioritizes clinical robustness and trustworthiness over simple metric maximization. Our contribution is three-fold. First, we construct a robust deterministic backbone by integrating a Zero-Init Multimodal Contextual Fusion (MMCF) module and a Residual-Gated Deep Decoder Supervision (DDS) mechanism, enabling stable feature learning and precise boundary delineation with significantly reduced Hausdorff Distance, even under modality corruption. Second, and most importantly, we introduce a memory-efficient Bayesian fine-tuning strategy that transforms the network into a probabilistic predictor, providing voxel-wise uncertainty maps to highlight potential errors for clinicians. Third, comprehensive experiments on the BraTS 2021 dataset demonstrate that BMDS-Net not only maintains competitive accuracy but, more importantly, exhibits superior stability in missing-modality scenarios where baseline models fail. The source code is publicly available at https://github.com/RyanZhou168/BMDS-Net.

</details>


### [63] [FMIR, a foundation model-based Image Registration Framework for Robust Image Registration](https://arxiv.org/abs/2601.17529)
*Fengting Zhang,Yue He,Qinghao Liu,Yaonan Wang,Xiang Chen,Hang Zhang*

Main category: cs.CV

TL;DR: FMIR is a foundation model-based medical image registration framework that achieves SOTA in-domain performance and robust out-of-domain generalization using a single dataset with channel regularization.


<details>
  <summary>Details</summary>
Motivation: Deep learning has revolutionized medical image registration with unprecedented speed, but clinical application is hindered by poor generalization beyond training domains, especially problematic given the typically small scale of medical datasets.

Method: FMIR combines a foundation model-based feature encoder for extracting anatomical structures with a general registration head, trained with a channel regularization strategy on just a single dataset.

Result: FMIR achieves state-of-the-art in-domain performance while maintaining robust registration on out-of-domain images, demonstrating a viable path toward building generalizable medical imaging foundation models with limited resources.

Conclusion: The approach shows promise for creating generalizable medical imaging foundation models even with limited training data, addressing a critical weakness in current deep learning registration methods.

Abstract: Deep learning has revolutionized medical image registration by achieving unprecedented speeds, yet its clinical application is hindered by a limited ability to generalize beyond the training domain, a critical weakness given the typically small scale of medical datasets. In this paper, we introduce FMIR, a foundation model-based registration framework that overcomes this limitation.Combining a foundation model-based feature encoder for extracting anatomical structures with a general registration head, and trained with a channel regularization strategy on just a single dataset, FMIR achieves state-of-the-art(SOTA) in-domain performance while maintaining robust registration on out-of-domain images.Our approach demonstrates a viable path toward building generalizable medical imaging foundation models with limited resources. The code is available at https://github.com/Monday0328/FMIR.git.

</details>


### [64] [Will It Zero-Shot?: Will It Zero-Shot?: Predicting Zero-Shot Classification Performance For Arbitrary Queries](https://arxiv.org/abs/2601.17535)
*Kevin Robbins,Xiaotong Liu,Yu Wu,Le Sun,Grady McPeak,Abby Stylianou,Robert Pless*

Main category: cs.CV

TL;DR: Using synthetic images alongside text improves zero-shot accuracy prediction for VLMs, helping users assess model suitability without labeled data.


<details>
  <summary>Details</summary>
Motivation: Users need to assess whether a Vision-Language Model (VLM) like CLIP will work for their specific domain without requiring labeled examples or expertise.

Method: Extends text-only evaluation by generating synthetic images relevant to the task, combining text and image-based approaches to predict zero-shot accuracy.

Result: Image-based approach substantially improves zero-shot accuracy predictions over text-only baselines and provides visual feedback on assessment basis.

Conclusion: Generated imagery enhances VLM suitability assessment, enabling users to predict model effectiveness for their applications without labeled data.

Abstract: Vision-Language Models like CLIP create aligned embedding spaces for text and images, making it possible for anyone to build a visual classifier by simply naming the classes they want to distinguish. However, a model that works well in one domain may fail in another, and non-expert users have no straightforward way to assess whether their chosen VLM will work on their problem. We build on prior work using text-only comparisons to evaluate how well a model works for a given natural language task, and explore approaches that also generate synthetic images relevant to that task to evaluate and refine the prediction of zero-shot accuracy. We show that generated imagery to the baseline text-only scores substantially improves the quality of these predictions. Additionally, it gives a user feedback on the kinds of images that were used to make the assessment. Experiments on standard CLIP benchmark datasets demonstrate that the image-based approach helps users predict, without any labeled examples, whether a VLM will be effective for their application.

</details>


### [65] [OTI: A Model-free and Visually Interpretable Measure of Image Attackability](https://arxiv.org/abs/2601.17536)
*Jiaming Liang,Haowei Liu,Chi-Man Pun*

Main category: cs.CV

TL;DR: The paper proposes OTI (Object Texture Intensity), a model-free and visually interpretable measure of image attackability that quantifies how easily images can be corrupted by adversarial perturbations based on the texture intensity of semantic objects.


<details>
  <summary>Details</summary>
Motivation: Images differ in their susceptibility to adversarial attacks, with some being easily corrupted while others are more resistant. Existing attackability measures have limitations: they require access to model proxies (gradients or minimal perturbations) and lack visual interpretability, making them impractical when task-specific models are inaccessible and obscuring the direct relationship with images.

Method: Proposes Object Texture Intensity (OTI), a model-free measure that quantifies image attackability based on the texture intensity of the image's semantic object. The method is theoretically grounded in decision boundaries and the mid- and high-frequency characteristics of adversarial perturbations.

Result: Comprehensive experiments demonstrate that OTI is both effective and computationally efficient. The measure successfully evaluates image attackability without requiring access to model proxies and provides visual interpretability for understanding attackability.

Conclusion: OTI addresses key limitations of existing attackability measures by being model-free and visually interpretable, providing the adversarial machine learning community with a practical tool for evaluating image attackability with applications in active learning, adversarial training, and attack enhancement.

Abstract: Despite the tremendous success of neural networks, benign images can be corrupted by adversarial perturbations to deceive these models. Intriguingly, images differ in their attackability. Specifically, given an attack configuration, some images are easily corrupted, whereas others are more resistant. Evaluating image attackability has important applications in active learning, adversarial training, and attack enhancement. This prompts a growing interest in developing attackability measures. However, existing methods are scarce and suffer from two major limitations: (1) They rely on a model proxy to provide prior knowledge (e.g., gradients or minimal perturbation) to extract model-dependent image features. Unfortunately, in practice, many task-specific models are not readily accessible. (2) Extracted features characterizing image attackability lack visual interpretability, obscuring their direct relationship with the images. To address these, we propose a novel Object Texture Intensity (OTI), a model-free and visually interpretable measure of image attackability, which measures image attackability as the texture intensity of the image's semantic object. Theoretically, we describe the principles of OTI from the perspectives of decision boundaries as well as the mid- and high-frequency characteristics of adversarial perturbations. Comprehensive experiments demonstrate that OTI is effective and computationally efficient. In addition, our OTI provides the adversarial machine learning community with a visual understanding of attackability.

</details>


### [66] [Saliency Driven Imagery Preprocessing for Efficient Compression -- Industrial Paper](https://arxiv.org/abs/2601.17555)
*Justin Downes,Sam Saltwick,Anthony Chen*

Main category: cs.CV

TL;DR: Variable rate satellite image compression using saliency maps and smoothing kernels to optimize storage for task-specific regions of interest.


<details>
  <summary>Details</summary>
Motivation: Satellite imagery generates massive data volumes (hundreds of terabytes daily) with high storage/bandwidth costs. Many downstream tasks only need small regions of interest, but current compression treats entire images equally without task-specific optimization.

Method: Use saliency maps to identify important regions, then apply variable-sized smoothing kernels mapped to different quantized saliency levels to preprocess imagery. This creates variable rate compression within a single image that works with traditional lossy compression standards.

Result: The paper demonstrates how saliency-driven preprocessing enables task-aware variable rate compression for satellite imagery, optimizing compression efficiency for specific regions of interest.

Conclusion: Saliency-guided preprocessing combined with traditional compression standards provides an effective approach for variable rate satellite image compression, reducing storage/bandwidth costs while preserving important regions for downstream tasks.

Abstract: The compression of satellite imagery remains an important research area as hundreds of terabytes of images are collected every day, which drives up storage and bandwidth costs. Although progress has been made in increasing the resolution of these satellite images, many downstream tasks are only interested in small regions of any given image. These areas of interest vary by task but, once known, can be used to optimize how information within the image is encoded. Whereas standard image encoding methods, even those optimized for remote sensing, work on the whole image equally, there are emerging methods that can be guided by saliency maps to focus on important areas. In this work we show how imagery preprocessing techniques driven by saliency maps can be used with traditional lossy compression coding standards to create variable rate image compression within a single large satellite image. Specifically, we use variable sized smoothing kernels that map to different quantized saliency levels to process imagery pixels in order to optimize downstream compression and encoding schemes.

</details>


### [67] [Sponge Tool Attack: Stealthy Denial-of-Efficiency against Tool-Augmented Agentic Reasoning](https://arxiv.org/abs/2601.17566)
*Qi Li,Xinchao Wang*

Main category: cs.CV

TL;DR: The paper introduces Sponge Tool Attack (STA), a method to disrupt LLM agentic reasoning by rewriting input prompts to create verbose, inefficient reasoning paths while preserving task semantics, under query-only access constraints.


<details>
  <summary>Details</summary>
Motivation: While LLMs augmented with external tools achieve high utility and efficiency for complex reasoning, their vulnerabilities to malicious manipulation of the tool-calling process remain unexplored. The paper aims to identify and exploit these vulnerabilities.

Method: STA is designed as an iterative, multi-agent collaborative framework with explicit rewritten policy control. It rewrites original prompts to create unnecessarily verbose and convoluted reasoning trajectories while maintaining semantic fidelity and benign appearance, requiring only query-only access without modifying models or tools.

Result: Extensive experiments across 6 models (open-source and closed-source APIs), 12 tools, 4 agentic frameworks, and 13 datasets spanning 5 domains validate STA's effectiveness in creating substantial computational overhead while remaining stealthy.

Conclusion: The work reveals critical vulnerabilities in LLM agentic reasoning systems, demonstrating that even with strict query-only access, attackers can significantly degrade efficiency through prompt manipulation, highlighting the need for robust defenses in tool-augmented LLM systems.

Abstract: Enabling large language models (LLMs) to solve complex reasoning tasks is a key step toward artificial general intelligence. Recent work augments LLMs with external tools to enable agentic reasoning, achieving high utility and efficiency in a plug-and-play manner. However, the inherent vulnerabilities of such methods to malicious manipulation of the tool-calling process remain largely unexplored. In this work, we identify a tool-specific attack surface and propose Sponge Tool Attack (STA), which disrupts agentic reasoning solely by rewriting the input prompt under a strict query-only access assumption. Without any modification on the underlying model or the external tools, STA converts originally concise and efficient reasoning trajectories into unnecessarily verbose and convoluted ones before arriving at the final answer. This results in substantial computational overhead while remaining stealthy by preserving the original task semantics and user intent. To achieve this, we design STA as an iterative, multi-agent collaborative framework with explicit rewritten policy control, and generates benign-looking prompt rewrites from the original one with high semantic fidelity. Extensive experiments across 6 models (including both open-source models and closed-source APIs), 12 tools, 4 agentic frameworks, and 13 datasets spanning 5 domains validate the effectiveness of STA.

</details>


### [68] [Stylizing ViT: Anatomy-Preserving Instance Style Transfer for Domain Generalization](https://arxiv.org/abs/2601.17586)
*Sebastian Doerrich,Francesco Di Salvo,Jonas Alle,Christian Ledig*

Main category: cs.CV

TL;DR: Stylizing ViT: A Vision Transformer encoder using weight-shared attention blocks for both self- and cross-attention to perform style transfer while maintaining anatomical consistency, improving domain generalization in medical image analysis.


<details>
  <summary>Details</summary>
Motivation: Deep learning models in medical image analysis struggle with generalizability across domains and demographic groups due to data heterogeneity and scarcity. Traditional augmentation fails under substantial domain shifts, and recent stylistic augmentation methods lack style diversity or introduce artifacts.

Method: Proposes Stylizing ViT, a Vision Transformer encoder with weight-shared attention blocks that handle both self-attention (for anatomical consistency) and cross-attention (for style transfer). This design enables style transfer while preserving anatomical structures.

Result: The method achieves up to +13% accuracy improvement over state-of-the-art methods for domain generalization on histopathology and dermatology image classification tasks. It generates perceptually convincing images without artifacts and shows 17% performance improvement during inference when used for test-time augmentation.

Conclusion: Stylizing ViT effectively addresses limitations of existing stylistic augmentation methods by providing diverse style transfer while maintaining anatomical consistency, significantly improving domain generalization in medical image analysis both during training and inference.

Abstract: Deep learning models in medical image analysis often struggle with generalizability across domains and demographic groups due to data heterogeneity and scarcity. Traditional augmentation improves robustness, but fails under substantial domain shifts. Recent advances in stylistic augmentation enhance domain generalization by varying image styles but fall short in terms of style diversity or by introducing artifacts into the generated images. To address these limitations, we propose Stylizing ViT, a novel Vision Transformer encoder that utilizes weight-shared attention blocks for both self- and cross-attention. This design allows the same attention block to maintain anatomical consistency through self-attention while performing style transfer via cross-attention. We assess the effectiveness of our method for domain generalization by employing it for data augmentation on three distinct image classification tasks in the context of histopathology and dermatology. Results demonstrate an improved robustness (up to +13% accuracy) over the state of the art while generating perceptually convincing images without artifacts. Additionally, we show that Stylizing ViT is effective beyond training, achieving a 17% performance improvement during inference when used for test-time augmentation. The source code is available at https://github.com/sdoerrich97/stylizing-vit .

</details>


### [69] [SPACE-CLIP: Spatial Perception via Adaptive CLIP Embeddings for Monocular Depth Estimation](https://arxiv.org/abs/2601.17657)
*Taewan Cho,Taeryang Kim,Andrew Jaeyong Choi*

Main category: cs.CV

TL;DR: SPACE-CLIP extracts geometric knowledge directly from frozen CLIP vision encoder using dual-pathway decoder, bypassing text prompts for efficient depth estimation.


<details>
  <summary>Details</summary>
Motivation: CLIP excels at semantic understanding but struggles with geometric perception. Existing methods use indirect textual prompts, which is inefficient. Need a direct way to extract geometric knowledge from CLIP's visual features.

Method: Proposes SPACE-CLIP with dual-pathway decoder: semantic pathway interprets high-level features with FiLM conditioning, structural pathway extracts fine-grained spatial details from early layers. Hierarchical fusion combines both streams.

Result: Extensive experiments on KITTI benchmark show SPACE-CLIP dramatically outperforms previous CLIP-based methods. Ablation studies confirm synergistic fusion of dual pathways is critical to success.

Conclusion: SPACE-CLIP offers efficient blueprint for repurposing large vision models, serving as integrable spatial perception module for next-gen embodied AI systems like VLA models.

Abstract: Contrastive Language-Image Pre-training (CLIP) has accomplished extraordinary success for semantic understanding but inherently struggles to perceive geometric structure. Existing methods attempt to bridge this gap by querying CLIP with textual prompts, a process that is often indirect and inefficient. This paper introduces a fundamentally different approach using a dual-pathway decoder. We present SPACE-CLIP, an architecture that unlocks and interprets latent geometric knowledge directly from a frozen CLIP vision encoder, completely bypassing the text encoder and its associated textual prompts. A semantic pathway interprets high-level features, dynamically conditioned on global context using feature-wise linear modulation (FiLM). In addition, a structural pathway extracts fine-grained spatial details from early layers. These complementary streams are hierarchically fused, enabling a robust synthesis of semantic context and precise geometry. Extensive experiments on the KITTI benchmark show that SPACE-CLIP dramatically outperforms previous CLIP-based methods. Our ablation studies validate that the synergistic fusion of our dual pathways is critical to this success. SPACE-CLIP offers a new, efficient, and architecturally elegant blueprint for repurposing large-scale vision models. The proposed method is not just a standalone depth estimator, but a readily integrable spatial perception module for the next generation of embodied AI systems, such as vision-language-action (VLA) models. Our model is available at https://github.com/taewan2002/space-clip

</details>


### [70] [Training-Free Text-to-Image Compositional Food Generation via Prompt Grafting](https://arxiv.org/abs/2601.17666)
*Xinyue Pan,Yuhao Chen,Fengqing Zhu*

Main category: cs.CV

TL;DR: Prompt Grafting (PG) is a training-free framework that improves multi-food image generation by combining explicit spatial text cues with implicit layout guidance to prevent food entanglement.


<details>
  <summary>Details</summary>
Motivation: Real-world meal images contain multiple food items, but current text-to-image diffusion models struggle with accurate multi-food generation due to object entanglement where adjacent foods fuse together without clear boundaries. This is important for applications like image-based dietary assessment and recipe visualization.

Method: Prompt Grafting (PG) uses a two-stage process: first, a layout prompt establishes distinct regions, then the target prompt is grafted once layout formation stabilizes. The framework combines explicit spatial cues in text with implicit layout guidance during sampling, enabling users to control food entanglement by specifying which items should remain separated or be intentionally mixed.

Result: Across two food datasets, the method significantly improves the presence of target objects and provides qualitative evidence of controllable separation between food items.

Conclusion: Prompt Grafting effectively addresses the challenge of food entanglement in multi-food image generation, offering a training-free solution that enables better control over food separation and mixing for practical applications.

Abstract: Real-world meal images often contain multiple food items, making reliable compositional food image generation important for applications such as image-based dietary assessment, where multi-food data augmentation is needed, and recipe visualization. However, modern text-to-image diffusion models struggle to generate accurate multi-food images due to object entanglement, where adjacent foods (e.g., rice and soup) fuse together because many foods do not have clear boundaries. To address this challenge, we introduce Prompt Grafting (PG), a training-free framework that combines explicit spatial cues in text with implicit layout guidance during sampling. PG runs a two-stage process where a layout prompt first establishes distinct regions and the target prompt is grafted once layout formation stabilizes. The framework enables food entanglement control: users can specify which food items should remain separated or be intentionally mixed by editing the arrangement of layouts. Across two food datasets, our method significantly improves the presence of target objects and provides qualitative evidence of controllable separation.

</details>


### [71] [Uni-RS: A Spatially Faithful Unified Understanding and Generation Model for Remote Sensing](https://arxiv.org/abs/2601.17673)
*Weiyu Zhang,Yuan Hu,Yong Li,Yu Liu*

Main category: cs.CV

TL;DR: Uni-RS addresses spatial reversal curse in remote sensing multimodal models by introducing spatial layout planning, spatial-aware query supervision, and image-caption spatial layout variation to improve spatial faithfulness in text-to-image generation while maintaining strong understanding capabilities.


<details>
  <summary>Details</summary>
Motivation: Remote sensing multimodal models suffer from spatial reversal curse - they can recognize object locations in images but fail to faithfully execute spatial relations during text-to-image generation, where spatial relations are core semantic information in remote sensing.

Method: 1) Explicit Spatial-Layout Planning to transform text instructions into spatial layout plans, decoupling geometric planning from visual synthesis. 2) Spatial-Aware Query Supervision to bias learnable queries toward spatial relations in instructions. 3) Image-Caption Spatial Layout Variation to expose model to systematic geometry-consistent spatial transformations.

Result: Extensive experiments across multiple benchmarks show substantial improvement in spatial faithfulness in text-to-image generation while maintaining strong performance on multimodal understanding tasks like image captioning, visual grounding, and VQA.

Conclusion: Uni-RS is the first unified multimodal model for remote sensing that explicitly addresses spatial asymmetry between understanding and generation, successfully overcoming the spatial reversal curse through explicit spatial planning and supervision mechanisms.

Abstract: Unified remote sensing multimodal models exhibit a pronounced spatial reversal curse: Although they can accurately recognize and describe object locations in images, they often fail to faithfully execute the same spatial relations during text-to-image generation, where such relations constitute core semantic information in remote sensing. Motivated by this observation, we propose Uni-RS, the first unified multimodal model tailored for remote sensing, to explicitly address the spatial asymmetry between understanding and generation. Specifically, we first introduce explicit Spatial-Layout Planning to transform textual instructions into spatial layout plans, decoupling geometric planning from visual synthesis. We then impose Spatial-Aware Query Supervision to bias learnable queries toward spatial relations explicitly specified in the instruction. Finally, we develop Image-Caption Spatial Layout Variation to expose the model to systematic geometry-consistent spatial transformations. Extensive experiments across multiple benchmarks show that our approach substantially improves spatial faithfulness in text-to-image generation, while maintaining strong performance on multimodal understanding tasks like image captioning, visual grounding, and VQA tasks.

</details>


### [72] [StyleDecoupler: Generalizable Artistic Style Disentanglement](https://arxiv.org/abs/2601.17697)
*Zexi Jia,Jinchao Zhang,Jie Zhou*

Main category: cs.CV

TL;DR: StyleDecoupler is a plug-and-play framework that separates artistic style from content using multi-modal vs uni-modal vision model representations, achieving SOTA style retrieval and enabling style analysis applications.


<details>
  <summary>Details</summary>
Motivation: Artistic style is deeply entangled with semantic content in visual representations, making it challenging to isolate pure style features for analysis and applications.

Method: Uses uni-modal vision models (which suppress style for content invariance) as content references, then isolates pure style features from multi-modal embeddings through mutual information minimization. Operates as plug-and-play module on frozen Vision-Language Models without fine-tuning.

Result: Achieves state-of-the-art performance on style retrieval across WeART (new 280K artwork benchmark) and WikiART datasets. Enables applications like style relationship mapping and generative model evaluation.

Conclusion: StyleDecoupler effectively decouples style from content using information-theoretic principles, providing a practical framework for style analysis with released method and dataset.

Abstract: Representing artistic style is challenging due to its deep entanglement with semantic content. We propose StyleDecoupler, an information-theoretic framework that leverages a key insight: multi-modal vision models encode both style and content, while uni-modal models suppress style to focus on content-invariant features. By using uni-modal representations as content-only references, we isolate pure style features from multi-modal embeddings through mutual information minimization. StyleDecoupler operates as a plug-and-play module on frozen Vision-Language Models without fine-tuning. We also introduce WeART, a large-scale benchmark of 280K artworks across 152 styles and 1,556 artists. Experiments show state-of-the-art performance on style retrieval across WeART and WikiART, while enabling applications like style relationship mapping and generative model evaluation. We release our method and dataset at this url.

</details>


### [73] [An AI-enabled tool for quantifying overlapping red blood cell sickling dynamics in microfluidic assays](https://arxiv.org/abs/2601.17703)
*Nikhil Kadivar,Guansheng Li,Jianlu Zheng,John M. Higgins,Ming Dao,George Em Karniadakis,Mengjia Xu*

Main category: cs.CV

TL;DR: AI-driven framework for automated quantification of sickle cell morphological transitions using deep learning segmentation and watershed algorithms to analyze densely packed RBC populations in time-lapse microscopy.


<details>
  <summary>Details</summary>
Motivation: Need for accurate identification of sickle cell morphological transitions under diverse biophysical conditions, especially in densely packed and overlapping cell populations, to understand sickle cell dynamics and improve experimental throughput.

Method: Automated deep learning framework integrating AI-assisted annotation (Roboflow), nnU-Net segmentation model training, watershed algorithm for resolving overlapping cells, and instance counting to quantify RBC populations across varying density regimes.

Result: High segmentation performance with limited labeled data, ability to track temporal evolution of sickle cell fraction, more than double experimental throughput via densely packed suspensions, capture drug-dependent sickling behavior, and reveal mechanobiological signatures.

Conclusion: AI-driven framework establishes scalable, reproducible computational platform for investigating cellular biomechanics and assessing therapeutic efficacy in microphysiological systems.

Abstract: Understanding sickle cell dynamics requires accurate identification of morphological transitions under diverse biophysical conditions, particularly in densely packed and overlapping cell populations. Here, we present an automated deep learning framework that integrates AI-assisted annotation, segmentation, classification, and instance counting to quantify red blood cell (RBC) populations across varying density regimes in time-lapse microscopy data. Experimental images were annotated using the Roboflow platform to generate labeled dataset for training an nnU-Net segmentation model. The trained network enables prediction of the temporal evolution of the sickle cell fraction, while a watershed algorithm resolves overlapping cells to enhance quantification accuracy. Despite requiring only a limited amount of labeled data for training, the framework achieves high segmentation performance, effectively addressing challenges associated with scarce manual annotations and cell overlap. By quantitatively tracking dynamic changes in RBC morphology, this approach can more than double the experimental throughput via densely packed cell suspensions, capture drug-dependent sickling behavior, and reveal distinct mechanobiological signatures of cellular morphological evolution. Overall, this AI-driven framework establishes a scalable and reproducible computational platform for investigating cellular biomechanics and assessing therapeutic efficacy in microphysiological systems.

</details>


### [74] [Advancing Structured Priors for Sparse-Voxel Surface Reconstruction](https://arxiv.org/abs/2601.17720)
*Ting-Hsun Chi,Chu-Rong Chen,Chi-Tun Hsu,Hsuan-Ting Lin,Sheng-Yu Huang,Cheng Sun,Yu-Chiang Frank Wang*

Main category: cs.CV

TL;DR: The paper proposes a hybrid approach combining 3D Gaussian Splatting and sparse-voxel rasterization for better surface reconstruction, using smart voxel initialization and refined depth supervision.


<details>
  <summary>Details</summary>
Motivation: Current explicit representations for radiance field surface reconstruction have complementary weaknesses: 3D Gaussian Splatting converges quickly but has limited surface fidelity due to point-like parameterization, while sparse-voxel rasterization provides better geometry but suffers from slow convergence due to uniform dense-grid initialization.

Method: 1) A voxel initialization method that places voxels at plausible locations with appropriate levels of detail, creating a strong starting point for optimization. 2) Refined depth geometry supervision that converts multi-view cues into direct per-ray depth regularization to enhance depth consistency without blurring edges.

Result: Experiments on standard benchmarks show improvements over prior methods in geometric accuracy, better fine-structure recovery, more complete surfaces, while maintaining fast convergence.

Conclusion: The proposed hybrid approach successfully combines the strengths of both 3D Gaussian Splatting and sparse-voxel rasterization, achieving better surface reconstruction with improved geometry and faster convergence.

Abstract: Reconstructing accurate surfaces with radiance fields has progressed rapidly, yet two promising explicit representations, 3D Gaussian Splatting and sparse-voxel rasterization, exhibit complementary strengths and weaknesses. 3D Gaussian Splatting converges quickly and carries useful geometric priors, but surface fidelity is limited by its point-like parameterization. Sparse-voxel rasterization provides continuous opacity fields and crisp geometry, but its typical uniform dense-grid initialization slows convergence and underutilizes scene structure. We combine the advantages of both by introducing a voxel initialization method that places voxels at plausible locations and with appropriate levels of detail, yielding a strong starting point for per-scene optimization. To further enhance depth consistency without blurring edges, we propose refined depth geometry supervision that converts multi-view cues into direct per-ray depth regularization. Experiments on standard benchmarks demonstrate improvements over prior methods in geometric accuracy, better fine-structure recovery, and more complete surfaces, while maintaining fast convergence.

</details>


### [75] [Implicit Neural Representation-Based Continuous Single Image Super Resolution: An Empirical Study](https://arxiv.org/abs/2601.17723)
*Tayyab Nasir,Daochang Liu,Ajmal Mian*

Main category: cs.CV

TL;DR: Systematic empirical study of implicit neural representation (INR) methods for arbitrary-scale image super-resolution reveals that recent complex methods offer only marginal gains, training configurations significantly impact performance, scaling laws apply, and a proposed loss function improves texture fidelity.


<details>
  <summary>Details</summary>
Motivation: No comprehensive empirical study exists for INR-based arbitrary-scale super-resolution (ASSR) methods. There's a need to systematically evaluate existing techniques, understand the effects of training recipes, benchmark performance, identify saturation limits, and highlight promising research directions.

Method: Conducted rigorous empirical analysis comparing existing INR methods across diverse settings, aggregated performance on multiple image quality metrics, created unified framework and code repository for reproducible comparisons, investigated impact of controlled training configurations, and examined a new loss function that penalizes intensity variations while preserving edges and textures.

Result: Four key findings: (1) Recent complex INR methods provide only marginal improvements over earlier methods; (2) Model performance strongly correlates with training configurations (previously overlooked); (3) Proposed loss enhances texture fidelity across architectures; (4) Scaling laws apply to INR-based ASSR, showing predictable gains with increased model complexity and data diversity.

Conclusion: Training configurations and objective design are crucial factors in INR-based ASSR that have been overlooked. The study provides benchmark insights, reveals saturation points, and offers practical guidance for future research, emphasizing that careful training recipe design matters more than architectural complexity.

Abstract: Implicit neural representation (INR) has become the standard approach for arbitrary-scale image super-resolution (ASSR). To date, no empirical study has systematically examined the effectiveness of existing methods, nor investigated the effects of different training recipes, such as scaling laws, objective design, and optimization strategies. A rigorous empirical analysis is essential not only for benchmarking performance and revealing true gains but also for establishing the current state of ASSR, identifying saturation limits, and highlighting promising directions. We fill this gap by comparing existing techniques across diverse settings and presenting aggregated performance results on multiple image quality metrics. We contribute a unified framework and code repository to facilitate reproducible comparisons. Furthermore, we investigate the impact of carefully controlled training configurations on perceptual image quality and examine a new loss function that penalizes intensity variations while preserving edges, textures, and finer details during training. We conclude the following key insights that have been previously overlooked: (1) Recent, more complex INR methods provide only marginal improvements over earlier methods. (2) Model performance is strongly correlated to training configurations, a factor overlooked in prior works. (3) The proposed loss enhances texture fidelity across architectures, emphasizing the role of objective design for targeted perceptual gains. (4) Scaling laws apply to INR-based ASSR, confirming predictable gains with increased model complexity and data diversity.

</details>


### [76] [Flatten The Complex: Joint B-Rep Generation via Compositional $k$-Cell Particles](https://arxiv.org/abs/2601.17733)
*Junran Lu,Yuanqi Li,Hengji Li,Jie Guo,Yanwen Guo*

Main category: cs.CV

TL;DR: A novel method for generative modeling of B-Rep CAD models using compositional k-cell particles that decouples hierarchy and enables joint topology-geometry generation with global context awareness.


<details>
  <summary>Details</summary>
Motivation: Current B-Rep generative modeling methods use cascaded sequences that fail to fully exploit geometric relationships between cells (adjacency, sharing), limiting context awareness and error recovery. The inherent heterogeneity of B-Reps as geometric cell complexes makes generative modeling challenging.

Method: Reformulates B-Reps into sets of compositional k-cell particles where adjacent cells share identical latents at interfaces. Uses multi-modal flow matching framework for unconditional generation and conditional tasks (3D reconstruction from single-view/point cloud). Decouples rigid hierarchy to unify vertices, edges, and faces.

Result: Produces high-fidelity CAD models with superior validity and editability compared to state-of-the-art methods. Enables direct synthesis of non-manifold structures (wireframes) and supports downstream tasks like local in-painting.

Conclusion: The particle-based representation successfully addresses limitations of hierarchical approaches by promoting geometric coupling along shared boundaries, enabling joint topology-geometry generation with global context awareness for B-Rep CAD modeling.

Abstract: Boundary Representation (B-Rep) is the widely adopted standard
  in Computer-Aided Design (CAD) and manufacturing. However, generative modeling of B-Reps remains a formidable challenge due to their inherent heterogeneity as geometric cell complexes, which entangles topology with geometry across cells of varying orders (i.e., $k$-cells such as vertices, edges, faces). Previous methods typically rely on cascaded sequences to handle this hierarchy, which fails to fully exploit the geometric relationships between cells, such as adjacency and sharing, limiting context awareness and error recovery. To fill this gap, we introduce a novel paradigm that reformulates B-Reps into sets of compositional $k$-cell particles. Our approach encodes each topological entity as a composition of particles, where adjacent cells share identical latents at their interfaces, thereby promoting geometric coupling along shared boundaries. By decoupling the rigid hierarchy, our representation unifies vertices, edges, and faces, enabling the joint generation of topology and geometry with global context awareness.
  We synthesize these particle sets using a multi-modal flow matching framework to handle unconditional generation as well as precise conditional tasks, such as 3D reconstruction from single-view or point cloud. Furthermore, the explicit and localized nature of our representation naturally extends to downstream tasks like local in-painting and enables the direct synthesis of non-manifold structures (e.g., wireframes). Extensive experiments demonstrate that our method produces high-fidelity CAD models with superior validity and editability compared to state-of-the-art methods.

</details>


### [77] [The Script is All You Need: An Agentic Framework for Long-Horizon Dialogue-to-Cinematic Video Generation](https://arxiv.org/abs/2601.17737)
*Chenyu Mu,Xin He,Qu Yang,Wanshun Chen,Jiadi Yao,Huang Liu,Zihao Yi,Bo Zhao,Xingyu Chen,Ruotian Ma,Fanghua Ye,Erkun Yang,Cheng Deng,Zhaopeng Tu,Xiaolong Li,Linus*

Main category: cs.CV

TL;DR: Novel agentic framework for dialogue-to-cinematic-video generation that bridges semantic gap between creative ideas and cinematic execution through script translation and orchestrated video generation.


<details>
  <summary>Details</summary>
Motivation: Current video generation models struggle with long-form coherent narratives from high-level concepts like dialogue, revealing a "semantic gap" between creative ideas and cinematic execution.

Method: End-to-end agentic framework with ScripterAgent (translates dialogue to executable cinematic scripts), DirectorAgent (orchestrates video models using cross-scene continuous generation), and ScriptBench benchmark with rich multimodal context. Uses AI-powered CriticAgent and Visual-Script Alignment metric for evaluation.

Result: Framework significantly improves script faithfulness and temporal fidelity across all tested video models. Analysis reveals trade-off between visual spectacle and strict script adherence in current SOTA models.

Conclusion: The framework bridges the semantic gap in dialogue-to-video generation, providing valuable insights for automated filmmaking and revealing important trade-offs in current video generation technology.

Abstract: Recent advances in video generation have produced models capable of synthesizing stunning visual content from simple text prompts. However, these models struggle to generate long-form, coherent narratives from high-level concepts like dialogue, revealing a ``semantic gap'' between a creative idea and its cinematic execution. To bridge this gap, we introduce a novel, end-to-end agentic framework for dialogue-to-cinematic-video generation. Central to our framework is ScripterAgent, a model trained to translate coarse dialogue into a fine-grained, executable cinematic script. To enable this, we construct ScriptBench, a new large-scale benchmark with rich multimodal context, annotated via an expert-guided pipeline. The generated script then guides DirectorAgent, which orchestrates state-of-the-art video models using a cross-scene continuous generation strategy to ensure long-horizon coherence. Our comprehensive evaluation, featuring an AI-powered CriticAgent and a new Visual-Script Alignment (VSA) metric, shows our framework significantly improves script faithfulness and temporal fidelity across all tested video models. Furthermore, our analysis uncovers a crucial trade-off in current SOTA models between visual spectacle and strict script adherence, providing valuable insights for the future of automated filmmaking.

</details>


### [78] [Learning Sewing Patterns via Latent Flow Matching of Implicit Fields](https://arxiv.org/abs/2601.17740)
*Cong Cao,Ren Li,Corentin Dumery,Hao Li*

Main category: cs.CV

TL;DR: A novel implicit representation method for sewing pattern modeling using signed/unsigned distance fields and latent flow matching for accurate generation and image-based estimation.


<details>
  <summary>Details</summary>
Motivation: Sewing patterns are fundamental for fashion design and applications, but automated pattern generation remains challenging due to the high variability in panel geometry and seam arrangements.

Method: Uses implicit representation with signed distance fields for panel boundaries and unsigned distance fields for seam endpoints, encoded into continuous latent space. Combines latent flow matching for panel combination distributions and stitching prediction module for seam relations.

Result: Enables accurate modeling and generation of complex sewing patterns, improves pattern estimation from images compared to existing approaches, and supports applications like pattern completion and refitting.

Conclusion: Provides a practical tool for digital fashion design through a differentiable, implicit representation approach that effectively handles the complexity of sewing pattern structures.

Abstract: Sewing patterns define the structural foundation of garments and are essential for applications such as fashion design, fabrication, and physical simulation. Despite progress in automated pattern generation, accurately modeling sewing patterns remains difficult due to the broad variability in panel geometry and seam arrangements. In this work, we introduce a sewing pattern modeling method based on an implicit representation. We represent each panel using a signed distance field that defines its boundary and an unsigned distance field that identifies seam endpoints, and encode these fields into a continuous latent space that enables differentiable meshing. A latent flow matching model learns distributions over panel combinations in this representation, and a stitching prediction module recovers seam relations from extracted edge segments. This formulation allows accurate modeling and generation of sewing patterns with complex structures. We further show that it can be used to estimate sewing patterns from images with improved accuracy relative to existing approaches, and supports applications such as pattern completion and refitting, providing a practical tool for digital fashion design.

</details>


### [79] [Frequency-aware Neural Representation for Videos](https://arxiv.org/abs/2601.17741)
*Jun Zhu,Xinfeng Zhang,Lv Tang,Junhao Jiang,Gai Zhang,Jia Wang*

Main category: cs.CV

TL;DR: FaNeRV is a frequency-aware neural representation for video compression that addresses spectral bias in INRs by explicitly decoupling low- and high-frequency components for better reconstruction quality.


<details>
  <summary>Details</summary>
Motivation: Existing INR-based video compression frameworks suffer from spectral bias that favors low-frequency components, leading to over-smoothed reconstructions and suboptimal rate-distortion performance.

Method: 1) Multi-resolution supervision strategy for progressive capture of global structures and fine-grained textures; 2) Dynamic high-frequency injection mechanism for adaptive emphasis on challenging regions; 3) Frequency-decomposed network module for improved feature modeling across spectral bands.

Result: Extensive experiments show FaNeRV significantly outperforms state-of-the-art INR methods and achieves competitive rate-distortion performance against traditional codecs on standard benchmarks.

Conclusion: FaNeRV effectively addresses spectral bias in INR-based video compression through frequency-aware design, enabling efficient and faithful video reconstruction with improved performance over existing methods.

Abstract: Implicit Neural Representations (INRs) have emerged as a promising paradigm for video compression. However, existing INR-based frameworks typically suffer from inherent spectral bias, which favors low-frequency components and leads to over-smoothed reconstructions and suboptimal rate-distortion performance. In this paper, we propose FaNeRV, a Frequency-aware Neural Representation for videos, which explicitly decouples low- and high-frequency components to enable efficient and faithful video reconstruction. FaNeRV introduces a multi-resolution supervision strategy that guides the network to progressively capture global structures and fine-grained textures through staged supervision . To further enhance high-frequency reconstruction, we propose a dynamic high-frequency injection mechanism that adaptively emphasizes challenging regions. In addition, we design a frequency-decomposed network module to improve feature modeling across different spectral bands. Extensive experiments on standard benchmarks demonstrate that FaNeRV significantly outperforms state-of-the-art INR methods and achieves competitive rate-distortion performance against traditional codecs.

</details>


### [80] [Video Compression with Hierarchical Temporal Neural Representation](https://arxiv.org/abs/2601.17743)
*Jun Zhu,Xinfeng Zhang,Lv Tang,Junhao Jiang,Gai Zhang,Jia Wang*

Main category: cs.CV

TL;DR: TeNeRV is a hierarchical temporal neural representation for video compression that captures both short- and long-term dependencies through inter-frame feature fusion and GoP-adaptive modulation, outperforming existing INR-based methods.


<details>
  <summary>Details</summary>
Motivation: Current INR-based video compression methods treat temporal dimension as independent input, limiting their ability to capture complex temporal dependencies in videos.

Method: TeNeRV uses two key components: 1) Inter-Frame Feature Fusion (IFF) module for local temporal coherence and fine-grained motion capture, and 2) GoP-Adaptive Modulation (GAM) mechanism that partitions videos into Groups-of-Pictures and learns group-specific priors to modulate network parameters.

Result: Extensive experiments show TeNeRV consistently outperforms existing INR-based methods in rate-distortion performance.

Conclusion: The hierarchical temporal approach effectively captures both short- and long-term dependencies in video compression, validating the effectiveness of the proposed TeNeRV method.

Abstract: Video compression has recently benefited from implicit neural representations (INRs), which model videos as continuous functions. INRs offer compact storage and flexible reconstruction, providing a promising alternative to traditional codecs. However, most existing INR-based methods treat the temporal dimension as an independent input, limiting their ability to capture complex temporal dependencies. To address this, we propose a Hierarchical Temporal Neural Representation for Videos, TeNeRV. TeNeRV integrates short- and long-term dependencies through two key components. First, an Inter-Frame Feature Fusion (IFF) module aggregates features from adjacent frames, enforcing local temporal coherence and capturing fine-grained motion. Second, a GoP-Adaptive Modulation (GAM) mechanism partitions videos into Groups-of-Pictures and learns group-specific priors. The mechanism modulates network parameters, enabling adaptive representations across different GoPs. Extensive experiments demonstrate that TeNeRV consistently outperforms existing INR-based methods in rate-distortion performance, validating the effectiveness of our proposed approach.

</details>


### [81] [Bridging Supervision Gaps: A Unified Framework for Remote Sensing Change Detection](https://arxiv.org/abs/2601.17747)
*Kaixuan Jiang,Chen Wu,Zhenghui Zhao,Chengxi Han*

Main category: cs.CV

TL;DR: UniCD is a unified change detection framework that handles supervised, weakly-supervised, and unsupervised tasks through a coupled architecture with shared encoder and multi-branch collaborative learning.


<details>
  <summary>Details</summary>
Motivation: Pixel-level change labels are expensive to acquire in remote sensing, and existing models struggle to adapt to diverse annotation availability scenarios. There's a need for a unified approach that can handle different supervision levels.

Method: UniCD uses a shared encoder with three supervision-specific branches: 1) Supervised branch with spatial-temporal awareness module (STAM) for bi-temporal feature fusion, 2) Weakly-supervised branch with change representation regularization (CRR) for coherent change modeling, and 3) Unsupervised branch with semantic prior-driven change inference (SPCI) that transforms unsupervised tasks into controlled weakly-supervised optimization.

Result: UniCD achieves optimal performance across all three tasks on mainstream datasets. It shows significant accuracy improvements in weakly and unsupervised scenarios, surpassing current state-of-the-art by 12.72% and 12.37% respectively on LEVIR-CD dataset.

Conclusion: UniCD successfully addresses the challenge of diverse annotation availability in change detection by providing a unified framework that collaboratively handles different supervision levels through deep coupling of heterogeneous supervision signals.

Abstract: Change detection (CD) aims to identify surface changes from multi-temporal remote sensing imagery. In real-world scenarios, Pixel-level change labels are expensive to acquire, and existing models struggle to adapt to scenarios with diverse annotation availability. To tackle this challenge, we propose a unified change detection framework (UniCD), which collaboratively handles supervised, weakly-supervised, and unsupervised tasks through a coupled architecture. UniCD eliminates architectural barriers through a shared encoder and multi-branch collaborative learning mechanism, achieving deep coupling of heterogeneous supervision signals. Specifically, UniCD consists of three supervision-specific branches. In the supervision branch, UniCD introduces the spatial-temporal awareness module (STAM), achieving efficient synergistic fusion of bi-temporal features. In the weakly-supervised branch, we construct change representation regularization (CRR), which steers model convergence from coarse-grained activations toward coherent and separable change modeling. In the unsupervised branch, we propose semantic prior-driven change inference (SPCI), which transforms unsupervised tasks into controlled weakly-supervised path optimization. Experiments on mainstream datasets demonstrate that UniCD achieves optimal performance across three tasks. It exhibits significant accuracy improvements in weakly and unsupervised scenarios, surpassing current state-of-the-art by 12.72% and 12.37% on LEVIR-CD, respectively.

</details>


### [82] [MV-S2V: Multi-View Subject-Consistent Video Generation](https://arxiv.org/abs/2601.17756)
*Ziyang Song,Xinyu Gong,Bangya Liu,Zelin Zhao*

Main category: cs.CV

TL;DR: Proposes MV-S2V for multi-view subject-to-video generation with 3D consistency, using synthetic data and TS-RoPE to distinguish subjects/views.


<details>
  <summary>Details</summary>
Motivation: Existing S2V methods are limited to single-view references, reducing the task to S2I + I2V pipeline and failing to exploit full video subject control potential. Multi-view references are needed for true 3D-level subject consistency.

Method: 1) Develops synthetic data curation pipeline for customized training data, complemented by small-scale real-world dataset. 2) Introduces Temporally Shifted RoPE (TS-RoPE) to distinguish between cross-subject and cross-view references in conditional generation.

Result: Achieves superior 3D subject consistency with multi-view reference images and high-quality visual outputs, establishing new direction for subject-driven video generation.

Conclusion: Proposes and addresses the challenging MV-S2V task, enabling video synthesis from multiple reference views with 3D-level subject consistency through novel data curation and TS-RoPE techniques.

Abstract: Existing Subject-to-Video Generation (S2V) methods have achieved high-fidelity and subject-consistent video generation, yet remain constrained to single-view subject references. This limitation renders the S2V task reducible to an S2I + I2V pipeline, failing to exploit the full potential of video subject control. In this work, we propose and address the challenging Multi-View S2V (MV-S2V) task, which synthesizes videos from multiple reference views to enforce 3D-level subject consistency. Regarding the scarcity of training data, we first develop a synthetic data curation pipeline to generate highly customized synthetic data, complemented by a small-scale real-world captured dataset to boost the training of MV-S2V. Another key issue lies in the potential confusion between cross-subject and cross-view references in conditional generation. To overcome this, we further introduce Temporally Shifted RoPE (TS-RoPE) to distinguish between different subjects and distinct views of the same subject in reference conditioning. Our framework achieves superior 3D subject consistency w.r.t. multi-view reference images and high-quality visual outputs, establishing a new meaningful direction for subject-driven video generation. Our project page is available at <a href="https://szy-young.github.io/mv-s2v">this URL</a>

</details>


### [83] [Agreement-Driven Multi-View 3D Reconstruction for Live Cattle Weight Estimation](https://arxiv.org/abs/2601.17791)
*Rabin Dulal,Wenfeng Jia,Lihong Zheng,Jane Quinn*

Main category: cs.CV

TL;DR: A non-contact method for cattle weight estimation using 3D reconstruction from multi-view RGB images with SAM 3D-based fusion and ensemble regression, achieving practical farm performance.


<details>
  <summary>Details</summary>
Motivation: Traditional cattle weight estimation methods involve manual handling that impacts productivity and economics. There's a need for cost-effective, non-contact alternatives that minimize animal stress and operational disruption.

Method: Uses multi-view RGB images with SAM 3D-based agreement-guided fusion to generate 3D point clouds per animal, then applies ensemble regression models (classical and deep learning) under low-data conditions.

Result: SAM 3D with multi-view agreement fusion outperforms other 3D generation methods. Classical ensemble models provide most consistent performance (R² = 0.69 ± 0.10, MAPE = 2.22 ± 0.56%), making it practical for farm implementation.

Conclusion: Improving 3D reconstruction quality is more critical than increasing model complexity for scalable farm deployment where producing large 3D datasets is challenging. The approach offers practical on-farm implementation potential.

Abstract: Accurate cattle live weight estimation is vital for livestock management, welfare, and productivity. Traditional methods, such as manual weighing using a walk-over weighing system or proximate measurements using body condition scoring, involve manual handling of stock and can impact productivity from both a stock and economic perspective. To address these issues, this study investigated a cost-effective, non-contact method for live weight calculation in cattle using 3D reconstruction. The proposed pipeline utilized multi-view RGB images with SAM 3D-based agreement-guided fusion, followed by ensemble regression. Our approach generates a single 3D point cloud per animal and compares classical ensemble models with deep learning models under low-data conditions. Results show that SAM 3D with multi-view agreement fusion outperforms other 3D generation methods, while classical ensemble models provide the most consistent performance for practical farm scenarios (R$^2$ = 0.69 $\pm$ 0.10, MAPE = 2.22 $\pm$ 0.56 \%), making this practical for on-farm implementation. These findings demonstrate that improving reconstruction quality is more critical than increasing model complexity for scalable deployment on farms where producing a large volume of 3D data is challenging.

</details>


### [84] [ViTCoP: Accelerating Large Vision-Language Models via Visual and Textual Semantic Collaborative Pruning](https://arxiv.org/abs/2601.17818)
*Wen Luo,Peng Chen,Xiaotao Huang,LiQun Huang*

Main category: cs.CV

TL;DR: ViTCoP: A collaborative pruning framework that combines vision encoder redundancy filtering with LLM step-wise co-pruning to efficiently reduce visual token redundancy in LVLMs while preserving critical information.


<details>
  <summary>Details</summary>
Motivation: Large Vision-Language Models have high computational costs due to significant redundancy in visual tokens. Existing pruning methods either lose critical visual information prematurely (pruning in vision encoder) or create information redundancy among selected tokens (pruning in LLMs).

Method: Visual and Textual Semantic Collaborative Pruning (ViTCoP) framework that: 1) Combines redundancy filtering in vision encoder with step-wise co-pruning within LLM based on hierarchical characteristics; 2) Uses L2 norm of K-vectors as token saliency metric in LLM for compatibility with acceleration techniques like FlashAttention.

Result: State-of-the-art performance surpassing existing methods on both image and video understanding tasks. Significantly reduces model inference latency and GPU memory consumption. Performance advantage becomes more pronounced under extreme pruning rates.

Conclusion: ViTCoP effectively addresses limitations of existing pruning methods by collaboratively pruning across vision encoder and LLM, achieving superior efficiency-performance trade-off while maintaining compatibility with modern acceleration techniques.

Abstract: Large Vision-Language Models (LVLMs) incur high computational costs due to significant redundancy in their visual tokens. To effectively reduce this cost, researchers have proposed various visual token pruning methods. However, existing methods are generally limited, either losing critical visual information prematurely due to pruning in the vision encoder, or leading to information redundancy among the selected tokens due to pruning in the Large Language Models (LLMs). To address these challenges, we propose a Visual and Textual Semantic Collaborative Pruning framework (ViTCoP) that combines redundancy filtering in the vision encoder with step-wise co-pruning within the LLM based on its hierarchical characteristics, to efficiently preserve critical and informationally diverse visual tokens. Meanwhile, to ensure compatibility with acceleration techniques like FlashAttention, we introduce the L2 norm of K-vectors as the token saliency metric in the LLM. Extensive experiments on various Large Vision-Language Models demonstrate that ViTCoP not only achieves state-of-the-art performance surpassing existing methods on both image and video understanding tasks, but also significantly reduces model inference latency and GPU memory consumption. Notably, its performance advantage over other methods becomes even more pronounced under extreme pruning rates.

</details>


### [85] [VAE-REPA: Variational Autoencoder Representation Alignment for Efficient Diffusion Training](https://arxiv.org/abs/2601.17830)
*Mengmeng Wang,Dengyang Jiang,Liuzhuozheng Li,Yucheng Lin,Guojiang Shen,Xiangjie Kong,Yong Liu,Guang Dai,Jingdong Wang*

Main category: cs.CV

TL;DR: Lightweight intrinsic guidance framework using pre-trained VAE features to accelerate diffusion transformer training without external dependencies.


<details>
  <summary>Details</summary>
Motivation: Existing methods for accelerating diffusion transformer training (like REPA and SRA) incur heavy computational overhead due to external representation encoders or dual-model setups. There's a need for efficient training convergence without these dependencies.

Method: Proposes a framework that leverages pre-trained VAE features to provide visual priors (texture details, structural patterns, semantic information). Aligns intermediate latent features of diffusion transformers with VAE features via lightweight projection layer supervised by feature alignment loss.

Result: Improves generation quality and training convergence speed compared to vanilla diffusion transformers, matches or outperforms state-of-the-art acceleration methods, with only 4% extra GFLOPs and zero additional cost for external guidance models.

Conclusion: The proposed framework provides an efficient, lightweight solution for accelerating diffusion transformer training by leveraging intrinsic VAE features, eliminating the need for external dependencies while maintaining strong performance.

Abstract: Denoising-based diffusion transformers, despite their strong generation performance, suffer from inefficient training convergence. Existing methods addressing this issue, such as REPA (relying on external representation encoders) or SRA (requiring dual-model setups), inevitably incur heavy computational overhead during training due to external dependencies. To tackle these challenges, this paper proposes \textbf{\namex}, a lightweight intrinsic guidance framework for efficient diffusion training. \name leverages off-the-shelf pre-trained Variational Autoencoder (VAE) features: their reconstruction property ensures inherent encoding of visual priors like rich texture details, structural patterns, and basic semantic information. Specifically, \name aligns the intermediate latent features of diffusion transformers with VAE features via a lightweight projection layer, supervised by a feature alignment loss. This design accelerates training without extra representation encoders or dual-model maintenance, resulting in a simple yet effective pipeline. Extensive experiments demonstrate that \name improves both generation quality and training convergence speed compared to vanilla diffusion transformers, matches or outperforms state-of-the-art acceleration methods, and incurs merely 4\% extra GFLOPs with zero additional cost for external guidance models.

</details>


### [86] [Geometry-Grounded Gaussian Splatting](https://arxiv.org/abs/2601.17835)
*Baowen Zhang,Chenxing Jiang,Heng Li,Shaojie Shen,Ping Tan*

Main category: cs.CV

TL;DR: The paper presents Geometry-Grounded Gaussian Splatting, a method that establishes Gaussian primitives as stochastic solids to enable direct shape extraction with improved multi-view consistency and reduced sensitivity to floaters.


<details>
  <summary>Details</summary>
Motivation: While Gaussian Splatting shows impressive quality and efficiency in novel view synthesis, shape extraction from Gaussian primitives remains challenging due to inadequate geometry parameterization and approximation, leading to poor multi-view consistency and sensitivity to floaters.

Method: The paper provides a rigorous theoretical derivation establishing Gaussian primitives as a specific type of stochastic solids. This framework enables direct treatment of Gaussian primitives as explicit geometric representations. Using the volumetric nature of stochastic solids, the method efficiently renders high-quality depth maps for fine-grained geometry extraction.

Result: Experiments show that the method achieves the best shape reconstruction results among all Gaussian Splatting-based methods on public datasets.

Conclusion: The theoretical framework provides a principled foundation for Geometry-Grounded Gaussian Splatting by enabling direct shape extraction from Gaussian primitives as explicit geometric representations, overcoming previous limitations in multi-view consistency and floater sensitivity.

Abstract: Gaussian Splatting (GS) has demonstrated impressive quality and efficiency in novel view synthesis. However, shape extraction from Gaussian primitives remains an open problem. Due to inadequate geometry parameterization and approximation, existing shape reconstruction methods suffer from poor multi-view consistency and are sensitive to floaters. In this paper, we present a rigorous theoretical derivation that establishes Gaussian primitives as a specific type of stochastic solids. This theoretical framework provides a principled foundation for Geometry-Grounded Gaussian Splatting by enabling the direct treatment of Gaussian primitives as explicit geometric representations. Using the volumetric nature of stochastic solids, our method efficiently renders high-quality depth maps for fine-grained geometry extraction. Experiments show that our method achieves the best shape reconstruction results among all Gaussian Splatting-based methods on public datasets.

</details>


### [87] [SynMind: Reducing Semantic Hallucination in fMRI-Based Image Reconstruction](https://arxiv.org/abs/2601.17857)
*Lan Yang,Minghan Yang,Ke Li,Honggang Zhang,Kaiyue Pang,Yi-Zhe Song*

Main category: cs.CV

TL;DR: SynMind: A new fMRI image reconstruction framework that uses explicit semantic descriptions from fMRI signals to improve semantic alignment in reconstructed images, outperforming state-of-the-art methods while using smaller models.


<details>
  <summary>Details</summary>
Motivation: Current fMRI-based image reconstruction methods produce photo-realistic images but suffer from severe semantic misalignment - they hallucinate or replace salient objects despite good visual quality. The problem is that existing methods rely too heavily on entangled visual embeddings that prioritize low-level appearance cues over explicit semantic identity.

Method: Parse fMRI signals into rich, sentence-level semantic descriptions using grounded Vision-Language Models (VLMs) to generate synthetic, human-like textual representations capturing object identities and spatial organization. Then integrate these explicit semantic encodings with visual priors to condition a pretrained diffusion model (SynMind framework).

Result: SynMind outperforms state-of-the-art methods across most quantitative metrics. It surpasses competing methods based on SDXL while using the much smaller Stable Diffusion 1.4 and a single consumer GPU. Human evaluations confirm reconstructions are more consistent with human visual perception. Neurovisualization shows SynMind engages broader, more semantically relevant brain regions.

Conclusion: Explicit semantic interpretation of fMRI signals through text-aligned representations significantly improves semantic alignment in image reconstruction, addressing a key limitation of current methods while being computationally efficient.

Abstract: Recent advances in fMRI-based image reconstruction have achieved remarkable photo-realistic fidelity. Yet, a persistent limitation remains: while reconstructed images often appear naturalistic and holistically similar to the target stimuli, they frequently suffer from severe semantic misalignment -- salient objects are often replaced or hallucinated despite high visual quality. In this work, we address this limitation by rethinking the role of explicit semantic interpretation in fMRI decoding. We argue that existing methods rely too heavily on entangled visual embeddings which prioritize low-level appearance cues -- such as texture and global gist -- over explicit semantic identity. To overcome this, we parse fMRI signals into rich, sentence-level semantic descriptions that mirror the hierarchical and compositional nature of human visual understanding. We achieve this by leveraging grounded VLMs to generate synthetic, human-like, multi-granularity textual representations that capture object identities and spatial organization. Built upon this foundation, we propose SynMind, a framework that integrates these explicit semantic encodings with visual priors to condition a pretrained diffusion model. Extensive experiments demonstrate that SynMind outperforms state-of-the-art methods across most quantitative metrics. Notably, by offloading semantic reasoning to our text-alignment module, SynMind surpasses competing methods based on SDXL while using the much smaller Stable Diffusion 1.4 and a single consumer GPU. Large-scale human evaluations further confirm that SynMind produces reconstructions more consistent with human visual perception. Neurovisualization analyses reveal that SynMind engages broader and more semantically relevant brain regions, mitigating the over-reliance on high-level visual areas.

</details>


### [88] [Domain Generalization with Quantum Enhancement for Medical Image Classification: A Lightweight Approach for Cross-Center Deployment](https://arxiv.org/abs/2601.17862)
*Jingsong Xia,Siqi Wang*

Main category: cs.CV

TL;DR: A lightweight domain generalization framework with quantum-enhanced collaborative learning improves medical AI model robustness across unseen clinical domains without requiring multi-center labeled data.


<details>
  <summary>Details</summary>
Motivation: Medical AI models perform well in single-center settings but degrade in real-world cross-center deployment due to domain shift, limiting clinical generalizability. There's a need for robust generalization to unseen target domains without relying on real multi-center labeled data.

Method: Uses MobileNetV2-based domain-invariant encoder with three components: (1) multi-domain imaging shift simulation (brightness, contrast, sharpening, noise perturbations), (2) domain-adversarial training with gradient reversal, and (3) lightweight quantum feature enhancement layer with parameterized quantum circuits. Also includes test-time adaptation during inference.

Result: Significantly outperforms baseline models without domain generalization or quantum enhancement on unseen domains, achieving reduced domain-specific performance variance and improved AUC and sensitivity on simulated multi-center medical imaging datasets.

Conclusion: The framework demonstrates clinical potential for quantum-enhanced domain generalization under constrained computational resources and provides a feasible paradigm for hybrid quantum-classical medical imaging systems.

Abstract: Medical image artificial intelligence models often achieve strong performance in single-center or single-device settings, yet their effectiveness frequently deteriorates in real-world cross-center deployment due to domain shift, limiting clinical generalizability. To address this challenge, we propose a lightweight domain generalization framework with quantum-enhanced collaborative learning, enabling robust generalization to unseen target domains without relying on real multi-center labeled data. Specifically, a MobileNetV2-based domain-invariant encoder is constructed and optimized through three key components: (1) multi-domain imaging shift simulation using brightness, contrast, sharpening, and noise perturbations to emulate heterogeneous acquisition conditions; (2) domain-adversarial training with gradient reversal to suppress domain-discriminative features; and (3) a lightweight quantum feature enhancement layer that applies parameterized quantum circuits for nonlinear feature mapping and entanglement modeling. In addition, a test-time adaptation strategy is employed during inference to further alleviate distribution shifts. Experiments on simulated multi-center medical imaging datasets demonstrate that the proposed method significantly outperforms baseline models without domain generalization or quantum enhancement on unseen domains, achieving reduced domain-specific performance variance and improved AUC and sensitivity. These results highlight the clinical potential of quantum-enhanced domain generalization under constrained computational resources and provide a feasible paradigm for hybrid quantum--classical medical imaging systems.

</details>


### [89] [MV-SAM: Multi-view Promptable Segmentation using Pointmap Guidance](https://arxiv.org/abs/2601.17866)
*Yoonwoo Jeong,Cheng Sun,Yu-Chiang Frank Wang,Minsu Cho,Jaesung Choe*

Main category: cs.CV

TL;DR: MV-SAM is a multi-view segmentation framework that achieves 3D consistency using pointmaps from unposed images, eliminating the need for explicit 3D networks or annotated 3D data.


<details>
  <summary>Details</summary>
Motivation: Existing promptable segmentation models like SAM lack 3D awareness, leading to inconsistent results across views and requiring costly per-scene optimization for 3D consistency.

Method: MV-SAM extends SAM by lifting image embeddings into 3D point embeddings using pointmaps (3D points from unposed images), and uses a transformer with cross-attention to decode 3D prompt embeddings, aligning 2D interactions with 3D geometry.

Result: Outperforms SAM2-Video and achieves comparable performance with per-scene optimization baselines on NVOS, SPIn-NeRF, ScanNet++, uCo3D, and DL3DV benchmarks, with good cross-domain generalization.

Conclusion: MV-SAM provides an effective framework for 3D-consistent multi-view segmentation without requiring explicit 3D networks or annotated 3D data, leveraging pointmaps to bridge 2D segmentation with 3D geometry.

Abstract: Promptable segmentation has emerged as a powerful paradigm in computer vision, enabling users to guide models in parsing complex scenes with prompts such as clicks, boxes, or textual cues. Recent advances, exemplified by the Segment Anything Model (SAM), have extended this paradigm to videos and multi-view images. However, the lack of 3D awareness often leads to inconsistent results, necessitating costly per-scene optimization to enforce 3D consistency. In this work, we introduce MV-SAM, a framework for multi-view segmentation that achieves 3D consistency using pointmaps -- 3D points reconstructed from unposed images by recent visual geometry models. Leveraging the pixel-point one-to-one correspondence of pointmaps, MV-SAM lifts images and prompts into 3D space, eliminating the need for explicit 3D networks or annotated 3D data. Specifically, MV-SAM extends SAM by lifting image embeddings from its pretrained encoder into 3D point embeddings, which are decoded by a transformer using cross-attention with 3D prompt embeddings. This design aligns 2D interactions with 3D geometry, enabling the model to implicitly learn consistent masks across views through 3D positional embeddings. Trained on the SA-1B dataset, our method generalizes well across domains, outperforming SAM2-Video and achieving comparable performance with per-scene optimization baselines on NVOS, SPIn-NeRF, ScanNet++, uCo3D, and DL3DV benchmarks. Code will be released.

</details>


### [90] [VidLaDA: Bidirectional Diffusion Large Language Models for Efficient Video Understanding](https://arxiv.org/abs/2601.17868)
*Zhihao He,Tieyuan Chen,Kangyu Wang,Ziran Qin,Yang Shao,Chaofan Gan,Shijie Li,Zuxuan Wu,Weiyao Lin*

Main category: cs.CV

TL;DR: VidLaDA is a Video LLM using Diffusion Language Model with bidirectional attention to overcome autoregressive models' causal masking biases. MARS-Cache accelerates inference via visual cache refreshing and chunk attention, achieving 12x speedup while maintaining accuracy.


<details>
  <summary>Details</summary>
Motivation: Standard autoregressive Video LLMs suffer from causal masking biases that hinder global spatiotemporal modeling, leading to suboptimal understanding efficiency. There's a need for models that can capture bidirectional dependencies in videos without the limitations of causal attention.

Method: Proposes VidLaDA, a Video LLM based on Diffusion Language Model that utilizes bidirectional attention to capture bidirectional dependencies. Introduces MARS-Cache framework that accelerates inference by combining asynchronous visual cache refreshing with frame-wise chunk attention, effectively pruning redundancy while preserving global connectivity via anchor tokens.

Result: Extensive experiments show VidLaDA outperforms diffusion baselines and rivals state-of-the-art autoregressive models (e.g., Qwen2.5-VL and LLaVA-Video). MARS-Cache delivers over 12x speedup without compromising reasoning accuracy.

Conclusion: VidLaDA with MARS-Cache provides an effective solution to the limitations of autoregressive Video LLMs, offering improved global spatiotemporal modeling through bidirectional attention and efficient inference acceleration through cache-based optimization.

Abstract: Standard Autoregressive Video LLMs inevitably suffer from causal masking biases that hinder global spatiotemporal modeling, leading to suboptimal understanding efficiency. We propose VidLaDA, a Video LLM based on Diffusion Language Model utilizing bidirectional attention to capture bidirectional dependencies. To further tackle the inference bottleneck of diffusion decoding on massive video tokens, we introduce MARS-Cache. This framework accelerates inference by combining asynchronous visual cache refreshing with frame-wise chunk attention, effectively pruning redundancy while preserving global connectivity via anchor tokens. Extensive experiments show VidLaDA outperforms diffusion baselines and rivals state-of-the-art autoregressive models (e.g., Qwen2.5-VL and LLaVA-Video), with MARS-Cache delivering over 12x speedup without compromising reasoning accuracy. Code and checkpoints are open-sourced at https://github.com/ziHoHe/VidLaDA.

</details>


### [91] [Quran-MD: A Fine-Grained Multilingual Multimodal Dataset of the Quran](https://arxiv.org/abs/2601.17880)
*Muhammad Umar Salman,Mohammad Areeb Qazi,Mohammed Talha Alam*

Main category: cs.CV

TL;DR: Quran MD is a multimodal dataset integrating Arabic text, English translation, transliteration, and audio recordings from 32 reciters at both verse and word levels for computational Quranic studies.


<details>
  <summary>Details</summary>
Motivation: To create a comprehensive resource that bridges textual and audio modalities of the Quran, capturing its rich oral tradition and enabling computational analysis of recitation, pronunciation, and semantics.

Method: Compiled a multimodal dataset with verse-level data (Arabic text, English translation, transliteration) and word-level aligned audio recordings from 32 distinct reciters, capturing diverse recitation styles and dialectical nuances.

Result: Created Quran MD dataset available on Hugging Face, providing fine-grained multimodal Quranic data that supports NLP, speech recognition, TTS, linguistic analysis, and digital Islamic studies applications.

Conclusion: The dataset enables advanced computational approaches to Quranic recitation and study, supporting research and community applications including ASR, tajweed detection, multimodal embeddings, and personalized tutoring systems.

Abstract: We present Quran MD, a comprehensive multimodal dataset of the Quran that integrates textual, linguistic, and audio dimensions at the verse and word levels. For each verse (ayah), the dataset provides its original Arabic text, English translation, and phonetic transliteration. To capture the rich oral tradition of Quranic recitation, we include verse-level audio from 32 distinct reciters, reflecting diverse recitation styles and dialectical nuances. At the word level, each token is paired with its corresponding Arabic script, English translation, transliteration, and an aligned audio recording, allowing fine-grained analysis of pronunciation, phonology, and semantic context. This dataset supports various applications, including natural language processing, speech recognition, text-to-speech synthesis, linguistic analysis, and digital Islamic studies. Bridging text and audio modalities across multiple reciters, this dataset provides a unique resource to advance computational approaches to Quranic recitation and study. Beyond enabling tasks such as ASR, tajweed detection, and Quranic TTS, it lays the foundation for multimodal embeddings, semantic retrieval, style transfer, and personalized tutoring systems that can support both research and community applications. The dataset is available at https://huggingface.co/datasets/Buraaq/quran-audio-text-dataset

</details>


### [92] [PEAfowl: Perception-Enhanced Multi-View Vision-Language-Action for Bimanual Manipulation](https://arxiv.org/abs/2601.17885)
*Qingyu Fan,Zhaoxiang Li,Yi Lu,Wang Chen,Qiu Shen,Xiao-xiao Long,Yinghao Cai,Tao Lu,Shuo Wang,Xun Cao*

Main category: cs.CV

TL;DR: PEAfowl is a perception-enhanced multi-view vision-language-action policy for bimanual manipulation that improves spatial reasoning through 3D-aware feature fusion and instruction grounding via text-aware readout, achieving significant performance gains over baselines.


<details>
  <summary>Details</summary>
Motivation: Existing vision-language-action models fail to generalize well for bimanual manipulation in cluttered scenes due to weak 3D spatial understanding from view-agnostic feature fusion and coarse instruction grounding from global language conditioning.

Method: PEAfowl introduces: 1) 3D-aware spatial reasoning by predicting per-token depth distributions, performing differentiable 3D lifting, and aggregating local cross-view neighbors; 2) Text-aware instruction grounding using Perceiver-style readout over frozen CLIP features; 3) Training-only depth distillation from a pretrained teacher to supervise depth predictions without inference overhead.

Result: On RoboTwin 2.0 with domain randomization, PEAfowl improves the strongest baseline by 23.0 percentage points in success rate. Real-robot experiments demonstrate reliable sim-to-real transfer and consistent improvements from depth distillation.

Conclusion: PEAfowl effectively addresses key limitations in existing VLA models for bimanual manipulation through geometrically grounded spatial representations and fine-grained instruction grounding, enabling robust performance in cluttered environments with reliable sim-to-real transfer.

Abstract: Bimanual manipulation in cluttered scenes requires policies that remain stable under occlusions, viewpoint and scene variations. Existing vision-language-action models often fail to generalize because (i) multi-view features are fused via view-agnostic token concatenation, yielding weak 3D-consistent spatial understanding, and (ii) language is injected as global conditioning, resulting in coarse instruction grounding.
  In this paper, we introduce PEAfowl, a perception-enhanced multi-view VLA policy for bimanual manipulation. For spatial reasoning, PEAfowl predicts per-token depth distributions, performs differentiable 3D lifting, and aggregates local cross-view neighbors to form geometrically grounded, cross-view consistent representations. For instruction grounding, we propose to replace global conditioning with a Perceiver-style text-aware readout over frozen CLIP visual features, enabling iterative evidence accumulation. To overcome noisy and incomplete commodity depth without adding inference overhead, we apply training-only depth distillation from a pretrained depth teacher to supervise the depth-distribution head, providing perception front-end with geometry-aware priors.
  On RoboTwin 2.0 under domain-randomized setting, PEAfowl improves the strongest baseline by 23.0 pp in success rate, and real-robot experiments further demonstrate reliable sim-to-real transfer and consistent improvements from depth distillation.
  Project website: https://peafowlvla.github.io/.

</details>


### [93] [Masked Depth Modeling for Spatial Perception](https://arxiv.org/abs/2601.17895)
*Bin Tan,Changjiang Sun,Xiage Qin,Hanat Adai,Zelin Fu,Tianxiang Zhou,Han Zhang,Yinghao Xu,Xing Zhu,Yujun Shen,Nan Xue*

Main category: cs.CV

TL;DR: LingBot-Depth is a depth completion model that refines inaccurate depth maps from RGB-D cameras using visual context and masked depth modeling, outperforming commercial sensors in precision and coverage.


<details>
  <summary>Details</summary>
Motivation: RGB-D cameras face hardware limitations and imaging challenges (specular/texture-less surfaces) that produce inaccurate depth measurements. These inaccuracies can be viewed as "masked" signals reflecting underlying geometric ambiguities that need to be resolved.

Method: Proposes LingBot-Depth with two key components: 1) masked depth modeling that leverages visual context to refine depth maps by treating sensor inaccuracies as masked signals, and 2) an automated data curation pipeline for scalable training using 3M RGB-depth pairs (2M real + 1M simulated data).

Result: The model outperforms top-tier RGB-D cameras in both depth precision and pixel coverage. Experimental results on downstream tasks show it provides aligned latent representations across RGB and depth modalities.

Conclusion: LingBot-Depth effectively addresses depth sensor limitations through learned refinement, offering improved spatial perception for applications like autonomous driving and robotics. The release of code, checkpoints, and dataset supports community advancement in spatial perception.

Abstract: Spatial visual perception is a fundamental requirement in physical-world applications like autonomous driving and robotic manipulation, driven by the need to interact with 3D environments. Capturing pixel-aligned metric depth using RGB-D cameras would be the most viable way, yet it usually faces obstacles posed by hardware limitations and challenging imaging conditions, especially in the presence of specular or texture-less surfaces. In this work, we argue that the inaccuracies from depth sensors can be viewed as "masked" signals that inherently reflect underlying geometric ambiguities. Building on this motivation, we present LingBot-Depth, a depth completion model which leverages visual context to refine depth maps through masked depth modeling and incorporates an automated data curation pipeline for scalable training. It is encouraging to see that our model outperforms top-tier RGB-D cameras in terms of both depth precision and pixel coverage. Experimental results on a range of downstream tasks further suggest that LingBot-Depth offers an aligned latent representation across RGB and depth modalities. We release the code, checkpoint, and 3M RGB-depth pairs (including 2M real data and 1M simulated data) to the community of spatial perception.

</details>


### [94] [Revisiting 3D Reconstruction Kernels as Low-Pass Filters](https://arxiv.org/abs/2601.17900)
*Shengjun Zhang,Min Chen,Yibo Wei,Mingyu Dong,Yueqi Duan*

Main category: cs.CV

TL;DR: The paper proposes using Jinc kernel as an ideal low-pass filter for 3D reconstruction, addressing spectral overlap issues from previous kernels, and introduces modulated kernels for better spatial efficiency.


<details>
  <summary>Details</summary>
Motivation: Previous 3D reconstruction kernels (Gaussians, Exponential functions, Student's t distributions) have unideal low-pass properties that cause high-frequency components to overlap with low-frequency components in the spectrum due to periodic spectral extension from discrete sampling.

Method: Introduce Jinc kernel which has instantaneous drop to zero magnitude at cutoff frequency (ideal low-pass filter). To address Jinc's slow decay in spatial domain, propose modulated kernels that balance spatial efficiency and frequency-domain fidelity.

Result: Experimental results demonstrate the effectiveness of Jinc and modulated kernels, achieving superior rendering performance by reconciling spatial efficiency and frequency-domain fidelity.

Conclusion: Jinc kernel provides ideal low-pass filtering for 3D reconstruction, and modulated kernels offer an effective balance between spatial efficiency and frequency fidelity, outperforming previous kernel approaches.

Abstract: 3D reconstruction is to recover 3D signals from the sampled discrete 2D pixels, with the goal to converge continuous 3D spaces. In this paper, we revisit 3D reconstruction from the perspective of signal processing, identifying the periodic spectral extension induced by discrete sampling as the fundamental challenge. Previous 3D reconstruction kernels, such as Gaussians, Exponential functions, and Student's t distributions, serve as the low pass filters to isolate the baseband spectrum. However, their unideal low-pass property results in the overlap of high-frequency components with low-frequency components in the discrete-time signal's spectrum. To this end, we introduce Jinc kernel with an instantaneous drop to zero magnitude exactly at the cutoff frequency, which is corresponding to the ideal low pass filters. As Jinc kernel suffers from low decay speed in the spatial domain, we further propose modulated kernels to strick an effective balance, and achieves superior rendering performance by reconciling spatial efficiency and frequency-domain fidelity. Experimental results have demonstrated the effectiveness of our Jinc and modulated kernels.

</details>


### [95] [Feature-Space Generative Models for One-Shot Class-Incremental Learning](https://arxiv.org/abs/2601.17905)
*Jack Foster,Kirill Paramonov,Mete Ozay,Umberto Michieli*

Main category: cs.CV

TL;DR: Gen1S improves few-shot class-incremental learning by mapping embeddings to residual space and using generative models to learn structural priors from base classes for better novel class recognition.


<details>
  <summary>Details</summary>
Motivation: FSCIL is challenging when models only get single samples per novel class with no further training allowed after base training, making generalization to novel classes difficult. The paper hypothesizes that base and novel class embeddings share structural similarity that can be leveraged.

Method: Map original embedding space to residual space by subtracting class prototypes, then use generative models (VAE or diffusion) to learn multi-modal distribution of residuals over base classes. This structural prior helps recognize novel classes.

Result: Gen1S consistently improves novel class recognition over state-of-the-art across multiple benchmarks and backbone architectures.

Conclusion: Learning structural priors of base class residuals through generative modeling effectively improves few-shot class-incremental learning, especially in challenging 1-shot scenarios with no post-base training.

Abstract: Few-shot class-incremental learning (FSCIL) is a paradigm where a model, initially trained on a dataset of base classes, must adapt to an expanding problem space by recognizing novel classes with limited data. We focus on the challenging FSCIL setup where a model receives only a single sample (1-shot) for each novel class and no further training or model alterations are allowed after the base training phase. This makes generalization to novel classes particularly difficult. We propose a novel approach predicated on the hypothesis that base and novel class embeddings have structural similarity. We map the original embedding space into a residual space by subtracting the class prototype (i.e., the average class embedding) of input samples. Then, we leverage generative modeling with VAE or diffusion models to learn the multi-modal distribution of residuals over the base classes, and we use this as a valuable structural prior to improve recognition of novel classes. Our approach, Gen1S, consistently improves novel class recognition over the state of the art across multiple benchmarks and backbone architectures.

</details>


### [96] [Benchmarking Direct Preference Optimization for Medical Large Vision-Language Models](https://arxiv.org/abs/2601.17918)
*Dain Kim,Jiwoo Lee,Jaehoon Yun,Yong Hoe Koo,Qingyu Chen,Hyunjae Kim,Jaewoo Kang*

Main category: cs.CV

TL;DR: First comprehensive study of DPO variants for medical LVLMs reveals inconsistent gains, failure to fix visual errors, and proposes targeted preference construction for 3.6% improvement.


<details>
  <summary>Details</summary>
Motivation: LVLMs show promise for medical use but face alignment and reliability issues. DPO is effective for refining responses but lacks rigorous evaluation in high-stakes medical contexts, creating a gap in empirical groundwork for methodological advances.

Method: Comprehensive examination of nine DPO variants across two medical LVLMs (LLaVA-Med and HuatuoGPT-Vision). Introduces targeted preference construction strategy specifically addressing visual misinterpretation errors observed in existing DPO models.

Result: Current DPO approaches show inconsistent gains over supervised fine-tuning, with efficacy varying across tasks and backbones. They often fail to resolve fundamental visual misinterpretation errors. The proposed targeted preference construction yields 3.6% improvement over strongest DPO baseline on visual QA tasks.

Conclusion: DPO variants have critical limitations in medical contexts, particularly in addressing visual errors. Targeted preference construction shows promise for improvement. Complete framework released to support future research in medical VLM alignment.

Abstract: Large Vision-Language Models (LVLMs) hold significant promise for medical applications, yet their deployment is often constrained by insufficient alignment and reliability. While Direct Preference Optimization (DPO) has emerged as a potent framework for refining model responses, its efficacy in high-stakes medical contexts remains underexplored, lacking the rigorous empirical groundwork necessary to guide future methodological advances. To bridge this gap, we present the first comprehensive examination of diverse DPO variants within the medical domain, evaluating nine distinct formulations across two medical LVLMs: LLaVA-Med and HuatuoGPT-Vision. Our results reveal several critical limitations: current DPO approaches often yield inconsistent gains over supervised fine-tuning, with their efficacy varying significantly across different tasks and backbones. Furthermore, they frequently fail to resolve fundamental visual misinterpretation errors. Building on these insights, we present a targeted preference construction strategy as a proof-of-concept that explicitly addresses visual misinterpretation errors frequently observed in existing DPO models. This design yields a 3.6% improvement over the strongest existing DPO baseline on visual question-answering tasks. To support future research, we release our complete framework, including all training data, model checkpoints, and our codebase at https://github.com/dmis-lab/med-vlm-dpo.

</details>


### [97] [RemEdit: Efficient Diffusion Editing with Riemannian Geometry](https://arxiv.org/abs/2601.17927)
*Eashan Adhikarla,Brian D. Davison*

Main category: cs.CV

TL;DR: RemEdit is a diffusion-based image editing framework that improves the trade-off between semantic fidelity and inference speed through Riemannian manifold navigation and task-specific attention pruning.


<details>
  <summary>Details</summary>
Motivation: Current controllable image generation faces a critical trade-off between semantic fidelity and inference speed, with existing methods struggling to maintain both editing accuracy and real-time performance.

Method: 1) Navigates latent space as Riemannian manifold using mamba-based module for geodesic path computation, 2) Uses dual-SLERP blending and VLM prompt enrichment for refined control, 3) Implements task-specific attention pruning with lightweight pruning head to retain essential tokens.

Result: RemEdit surpasses prior state-of-the-art editing frameworks while maintaining real-time performance under 50% pruning, establishing new benchmarks for practical image editing.

Conclusion: RemEdit successfully addresses the fidelity-speed trade-off in controllable image generation through synergistic innovations in manifold navigation and attention optimization, enabling both accurate semantic edits and real-time performance.

Abstract: Controllable image generation is fundamental to the success of modern generative AI, yet it faces a critical trade-off between semantic fidelity and inference speed. The RemEdit diffusion-based framework addresses this trade-off with two synergistic innovations. First, for editing fidelity, we navigate the latent space as a Riemannian manifold. A mamba-based module efficiently learns the manifold's structure, enabling direct and accurate geodesic path computation for smooth semantic edits. This control is further refined by a dual-SLERP blending technique and a goal-aware prompt enrichment pass from a Vision-Language Model. Second, for additional acceleration, we introduce a novel task-specific attention pruning mechanism. A lightweight pruning head learns to retain tokens essential to the edit, enabling effective optimization without the semantic degradation common in content-agnostic approaches. RemEdit surpasses prior state-of-the-art editing frameworks while maintaining real-time performance under 50% pruning. Consequently, RemEdit establishes a new benchmark for practical and powerful image editing. Source code: https://www.github.com/eashanadhikarla/RemEdit.

</details>


### [98] [From Specialist to Generalist: Unlocking SAM's Learning Potential on Unlabeled Medical Images](https://arxiv.org/abs/2601.17934)
*Vi Vu,Thanh-Huy Nguyen,Tien-Thinh Nguyen,Ba-Thinh Lam,Hoang-Thien Nguyen,Tianyang Wang,Xingjian Li,Min Xu*

Main category: cs.CV

TL;DR: SC-SAM is a specialist-generalist framework that combines U-Net and SAM in a bidirectional co-training loop for semi-supervised medical image segmentation, achieving state-of-the-art results by leveraging unlabeled data through reciprocal guidance.


<details>
  <summary>Details</summary>
Motivation: Foundation models like SAM struggle with medical image adaptation due to domain shift, scarce labels, and PEFT's inability to use unlabeled data. While U-Net excels in semi-supervised medical learning, its potential to assist PEFT SAM has been overlooked.

Method: SC-SAM creates a specialist-generalist framework where U-Net provides point-based prompts and pseudo-labels to guide SAM's adaptation, while SAM serves as a generalist supervisor to regularize U-Net, forming a bidirectional co-training loop that effectively exploits unlabeled data.

Result: The method achieves state-of-the-art results across prostate MRI and polyp segmentation benchmarks, outperforming other semi-supervised SAM variants and even medical foundation models like MedSAM.

Conclusion: The work demonstrates the value of specialist-generalist cooperation for label-efficient medical image segmentation, showing that combining the strengths of specialized models (U-Net) with general foundation models (SAM) through reciprocal guidance yields superior performance.

Abstract: Foundation models like the Segment Anything Model (SAM) show strong generalization, yet adapting them to medical images remains difficult due to domain shift, scarce labels, and the inability of Parameter-Efficient Fine-Tuning (PEFT) to exploit unlabeled data. While conventional models like U-Net excel in semi-supervised medical learning, their potential to assist a PEFT SAM has been largely overlooked. We introduce SC-SAM, a specialist-generalist framework where U-Net provides point-based prompts and pseudo-labels to guide SAM's adaptation, while SAM serves as a powerful generalist supervisor to regularize U-Net. This reciprocal guidance forms a bidirectional co-training loop that allows both models to effectively exploit the unlabeled data. Across prostate MRI and polyp segmentation benchmarks, our method achieves state-of-the-art results, outperforming other existing semi-supervised SAM variants and even medical foundation models like MedSAM, highlighting the value of specialist-generalist cooperation for label-efficient medical image segmentation. Our code is available at https://github.com/vnlvi2k3/SC-SAM.

</details>


### [99] [DTC: A Deformable Transposed Convolution Module for Medical Image Segmentation](https://arxiv.org/abs/2601.17939)
*Chengkun Sun,Jinqian Pan,Renjie Liang,Zhengkang Fan,Xin Miao,Jiang Bian,Jie Xu*

Main category: cs.CV

TL;DR: Proposes Deformable Transposed Convolution (DTC), a novel upsampling method that learns dynamic sampling positions to improve feature reconstruction in medical image segmentation, outperforming fixed-position methods like transposed convolution and linear interpolation.


<details>
  <summary>Details</summary>
Motivation: Conventional upsampling methods in UNet architectures use fixed sampling positions (transposed convolution applies kernels to predetermined locations, linear interpolation uses fixed coordinates), which may fail to capture structural information beyond these positions and can lead to artifacts or loss of detail in medical image segmentation.

Method: Introduces Deformable Transposed Convolution (DTC) inspired by deformable convolutions. DTC learns dynamic coordinates (sampling positions) to generate high-resolution feature maps, enabling adaptive sampling beyond fixed positions for both 2D and 3D medical image segmentation tasks.

Result: Experiments on 3D datasets (BTCV15) and 2D datasets (ISIC18, BUSI) show that DTC can be effectively integrated into existing medical image segmentation models, consistently improving the decoder's feature reconstruction and detail recovery capability compared to conventional upsampling methods.

Conclusion: DTC provides a more flexible upsampling approach that learns adaptive sampling positions, addressing limitations of fixed-position methods and enhancing feature reconstruction in medical image segmentation architectures.

Abstract: In medical image segmentation, particularly in UNet-like architectures, upsampling is primarily used to transform smaller feature maps into larger ones, enabling feature fusion between encoder and decoder features and supporting multi-scale prediction. Conventional upsampling methods, such as transposed convolution and linear interpolation, operate on fixed positions: transposed convolution applies kernel elements to predetermined pixel or voxel locations, while linear interpolation assigns values based on fixed coordinates in the original feature map. These fixed-position approaches may fail to capture structural information beyond predefined sampling positions and can lead to artifacts or loss of detail. Inspired by deformable convolutions, we propose a novel upsampling method, Deformable Transposed Convolution (DTC), which learns dynamic coordinates (i.e., sampling positions) to generate high-resolution feature maps for both 2D and 3D medical image segmentation tasks. Experiments on 3D (e.g., BTCV15) and 2D datasets (e.g., ISIC18, BUSI) demonstrate that DTC can be effectively integrated into existing medical image segmentation models, consistently improving the decoder's feature reconstruction and detail recovery capability.

</details>


### [100] [FlowMorph: Physics-Consistent Self-Supervision for Label-Free Single-Cell Mechanics in Microfluidic Videos](https://arxiv.org/abs/2601.17947)
*Bora Yimenicioglu,Vishal Manikanden*

Main category: cs.CV

TL;DR: FlowMorph: A physics-consistent self-supervised framework that learns a label-free scalar mechanics proxy for red blood cells from microfluidic videos, enabling deformability analysis without manual annotations.


<details>
  <summary>Details</summary>
Motivation: Red blood cell mechanical properties are important biomarkers for diseases, but existing microfluidic assays rely on supervised segmentation or hand-crafted features and don't incorporate the underlying Stokes-flow physics governing RBC shape evolution.

Method: FlowMorph models each cell with a low-dimensional parametric contour, advances boundary points through a differentiable "capsule-in-flow" model combining laminar advection and curvature-regularized elastic relaxation. It optimizes a loss function coupling silhouette overlap, intra-cellular flow agreement, area conservation, wall constraints, and temporal smoothness using only automatically derived silhouettes and optical flow.

Result: Achieves mean silhouette IoU of 0.905 on physics-rich videos, improves area conservation and wall violations over data-driven baselines. Scalar mechanics proxy k separates tank-treading from flipping dynamics with AUC of 0.863. With only 200 RT-DC events for calibration, predicts apparent Young's modulus with MAE of 0.118 MPa on 600 held-out cells and degrades gracefully under experimental variations.

Conclusion: FlowMorph provides a physics-consistent, self-supervised framework for learning RBC mechanical properties from microfluidic videos without manual annotations, enabling robust deformability analysis across different experimental conditions.

Abstract: Mechanical properties of red blood cells (RBCs) are promising biomarkers for hematologic and systemic disease, motivating microfluidic assays that probe deformability at throughputs of $10^3$--$10^6$ cells per experiment. However, existing pipelines rely on supervised segmentation or hand-crafted kymographs and rarely encode the laminar Stokes-flow physics that governs RBC shape evolution. We introduce FlowMorph, a physics-consistent self-supervised framework that learns a label-free scalar mechanics proxy $k$ for each tracked RBC from short brightfield microfluidic videos. FlowMorph models each cell by a low-dimensional parametric contour, advances boundary points through a differentiable ''capsule-in-flow'' combining laminar advection and curvature-regularized elastic relaxation, and optimizes a loss coupling silhouette overlap, intra-cellular flow agreement, area conservation, wall constraints, and temporal smoothness, using only automatically derived silhouettes and optical flow.
  Across four public RBC microfluidic datasets, FlowMorph achieves a mean silhouette IoU of $0.905$ on physics-rich videos with provided velocity fields and markedly improves area conservation and wall violations over purely data-driven baselines. On $\sim 1.5\times 10^5$ centered sequences, the scalar $k$ alone separates tank-treading from flipping dynamics with an AUC of $0.863$. Using only $200$ real-time deformability cytometry (RT-DC) events for calibration, a monotone map $E=g(k)$ predicts apparent Young's modulus with a mean absolute error of $0.118$\,MPa on $600$ held-out cells and degrades gracefully under shifts in channel geometry, optics, and frame rate.

</details>


### [101] [UPLiFT: Efficient Pixel-Dense Feature Upsampling with Local Attenders](https://arxiv.org/abs/2601.17950)
*Matthew Walmer,Saksham Suri,Anirud Aggarwal,Abhinav Shrivastava*

Main category: cs.CV

TL;DR: UPLiFT is a universal pixel-dense lightweight feature transform that achieves state-of-the-art performance with lower inference costs than cross-attention methods through iterative upsampling with a novel Local Attender operator.


<details>
  <summary>Details</summary>
Motivation: Current cross-attention-based feature upsampling methods risk inheriting the efficiency scaling problems of the backbones they upscale, while earlier iterative upsampling approaches have been overlooked despite their potential for better efficiency.

Method: Proposes UPLiFT architecture with an efficient Local Attender operator that uses an alternative attentional pooling formulation defined fully locally, enabling stable features throughout iterative upsampling.

Result: UPLiFT achieves state-of-the-art performance with lower inference costs than existing pixel-dense feature upsamplers and shows competitive performance with state-of-the-art Coupled Flow Matching models for VAE feature upsampling in generative tasks.

Conclusion: Iterative upsampling methods can compete with cross-attention approaches, and UPLiFT offers a versatile and efficient approach to creating denser features with better efficiency-performance trade-offs.

Abstract: The space of task-agnostic feature upsampling has emerged as a promising area of research to efficiently create denser features from pre-trained visual backbones. These methods act as a shortcut to achieve dense features for a fraction of the cost by learning to map low-resolution features to high-resolution versions. While early works in this space used iterative upsampling approaches, more recent works have switched to cross-attention-based methods, which risk falling into the same efficiency scaling problems of the backbones they are upsampling. In this work, we demonstrate that iterative upsampling methods can still compete with cross-attention-based methods; moreover, they can achieve state-of-the-art performance with lower inference costs. We propose UPLiFT, an architecture for Universal Pixel-dense Lightweight Feature Transforms. We also propose an efficient Local Attender operator to overcome the limitations of prior iterative feature upsampling methods. This operator uses an alternative attentional pooling formulation defined fully locally. We show that our Local Attender allows UPLiFT to maintain stable features throughout upsampling, enabling state-of-the-art performance with lower inference costs than existing pixel-dense feature upsamplers. In addition, we apply UPLiFT to generative downstream tasks and show that it achieves competitive performance with state-of-the-art Coupled Flow Matching models for VAE feature upsampling. Altogether, UPLiFT offers a versatile and efficient approach to creating denser features.

</details>


### [102] [Domain-Expert-Guided Hybrid Mixture-of-Experts for Medical AI: Integrating Data-Driven Learning with Clinical Priors](https://arxiv.org/abs/2601.17977)
*Jinchen Gu,Nan Zhao,Lei Qiu,Lu Zhang*

Main category: cs.CV

TL;DR: DKGH-MoE combines data-driven MoE with domain-expert-guided MoE using clinician gaze patterns to improve medical image analysis with limited data.


<details>
  <summary>Details</summary>
Motivation: MoE models have limited effectiveness in specialized domains like medicine due to small datasets, while clinical practice offers rich expert knowledge (physician gaze patterns, diagnostic heuristics) that models cannot reliably learn from limited data alone.

Method: Propose Domain-Knowledge-Guided Hybrid MoE (DKGH-MoE) - a plug-and-play, interpretable module that integrates: 1) data-driven MoE to extract novel features from raw imaging data, and 2) domain-expert-guided MoE that incorporates clinical priors (clinician eye-gaze cues) to emphasize diagnostically relevant regions.

Result: By integrating domain expert insights with data-driven features, DKGH-MoE improves both performance and interpretability in medical image analysis tasks.

Conclusion: Combining data-driven experts (capturing novel patterns) with domain-expert-guided experts (encoding accumulated clinical insights) provides complementary strengths for robust and clinically meaningful learning in medical applications with limited data.

Abstract: Mixture-of-Experts (MoE) models increase representational capacity with modest computational cost, but their effectiveness in specialized domains such as medicine is limited by small datasets. In contrast, clinical practice offers rich expert knowledge, such as physician gaze patterns and diagnostic heuristics, that models cannot reliably learn from limited data. Combining data-driven experts, which capture novel patterns, with domain-expert-guided experts, which encode accumulated clinical insights, provides complementary strengths for robust and clinically meaningful learning. To this end, we propose Domain-Knowledge-Guided Hybrid MoE (DKGH-MoE), a plug-and-play and interpretable module that unifies data-driven learning with domain expertise. DKGH-MoE integrates a data-driven MoE to extract novel features from raw imaging data, and a domain-expert-guided MoE incorporates clinical priors, specifically clinician eye-gaze cues, to emphasize regions of high diagnostic relevance. By integrating domain expert insights with data-driven features, DKGH-MoE improves both performance and interpretability.

</details>


### [103] [MorphXAI: An Explainable Framework for Morphological Analysis of Parasites in Blood Smear Images](https://arxiv.org/abs/2601.18001)
*Aqsa Yousaf,Sint Sint Win,Megan Coffee,Habeeb Olufowobi*

Main category: cs.CV

TL;DR: MorphXAI is an explainable AI framework that combines parasite detection with fine-grained morphological analysis to provide biologically meaningful explanations for clinical diagnosis.


<details>
  <summary>Details</summary>
Motivation: Current deep learning models for parasite detection lack interpretability, offering only visual heatmaps that don't capture the morphological traits clinicians actually use for diagnosis. This limits their clinical usefulness in low-resource settings where expert knowledge is scarce.

Method: MorphXAI integrates morphological supervision directly into the prediction pipeline, enabling simultaneous parasite localization and characterization of clinically relevant attributes like shape, curvature, visible dot count, flagellum presence, and developmental stage.

Result: The framework improves detection performance over baseline models and provides structured, biologically meaningful explanations. A clinician-annotated dataset of three parasite species (Leishmania, Trypanosoma brucei, and Trypanosoma cruzi) was created as a benchmark.

Conclusion: MorphXAI addresses the interpretability gap in automated parasite detection by providing morphological explanations that align with clinical diagnostic reasoning, potentially improving the clinical adoption of AI tools in low-resource settings.

Abstract: Parasitic infections remain a pressing global health challenge, particularly in low-resource settings where diagnosis still depends on labor-intensive manual inspection of blood smears and the availability of expert domain knowledge. While deep learning models have shown strong performance in automating parasite detection, their clinical usefulness is constrained by limited interpretability. Existing explainability methods are largely restricted to visual heatmaps or attention maps, which highlight regions of interest but fail to capture the morphological traits that clinicians rely on for diagnosis. In this work, we present MorphXAI, an explainable framework that unifies parasite detection with fine-grained morphological analysis. MorphXAI integrates morphological supervision directly into the prediction pipeline, enabling the model to localize parasites while simultaneously characterizing clinically relevant attributes such as shape, curvature, visible dot count, flagellum presence, and developmental stage. To support this task, we curate a clinician-annotated dataset of three parasite species (Leishmania, Trypanosoma brucei, and Trypanosoma cruzi) with detailed morphological labels, establishing a new benchmark for interpretable parasite analysis. Experimental results show that MorphXAI not only improves detection performance over the baseline but also provides structured, biologically meaningful explanations.

</details>


### [104] [Strip-Fusion: Spatiotemporal Fusion for Multispectral Pedestrian Detection](https://arxiv.org/abs/2601.18008)
*Asiegbu Miracle Kanu-Asiegbu,Nitin Jotwani,Xiaoxiao Du*

Main category: cs.CV

TL;DR: Strip-Fusion: A spatial-temporal fusion network for multispectral pedestrian detection that handles misalignment, lighting variations, and occlusion using temporally adaptive convolutions and KL divergence loss.


<details>
  <summary>Details</summary>
Motivation: Existing multispectral pedestrian detection methods focus mainly on spatial fusion and neglect temporal information. Additionally, RGB and thermal image pairs in benchmarks may not be perfectly aligned, and pedestrians are challenging to detect due to varying lighting conditions and occlusion.

Method: Proposes Strip-Fusion pipeline with temporally adaptive convolutions to dynamically weigh spatial-temporal features, a KL divergence loss to mitigate modality imbalance between visible and thermal inputs, and a novel post-processing algorithm to reduce false positives.

Result: Competitive performance on KAIST and CVC-14 benchmarks, with significant improvements over previous state-of-the-art methods on challenging conditions like heavy occlusion and misalignment.

Conclusion: Strip-Fusion effectively addresses key limitations in multispectral pedestrian detection by incorporating temporal information, handling modality misalignment, and improving robustness to challenging conditions.

Abstract: Pedestrian detection is a critical task in robot perception. Multispectral modalities (visible light and thermal) can boost pedestrian detection performance by providing complementary visual information. Several gaps remain with multispectral pedestrian detection methods. First, existing approaches primarily focus on spatial fusion and often neglect temporal information. Second, RGB and thermal image pairs in multispectral benchmarks may not always be perfectly aligned. Pedestrians are also challenging to detect due to varying lighting conditions, occlusion, etc. This work proposes Strip-Fusion, a spatial-temporal fusion network that is robust to misalignment in input images, as well as varying lighting conditions and heavy occlusions. The Strip-Fusion pipeline integrates temporally adaptive convolutions to dynamically weigh spatial-temporal features, enabling our model to better capture pedestrian motion and context over time. A novel Kullback-Leibler divergence loss was designed to mitigate modality imbalance between visible and thermal inputs, guiding feature alignment toward the more informative modality during training. Furthermore, a novel post-processing algorithm was developed to reduce false positives. Extensive experimental results show that our method performs competitively for both the KAIST and the CVC-14 benchmarks. We also observed significant improvements compared to previous state-of-the-art on challenging conditions such as heavy occlusion and misalignment.

</details>


### [105] [Leveraging Persistence Image to Enhance Robustness and Performance in Curvilinear Structure Segmentation](https://arxiv.org/abs/2601.18045)
*Zhuangzhi Gao,Feixiang Zhou,He Zhao,Xiuju Chen,Xiaoxin Li,Qinkai Yu,Yitian Zhao,Alena Shantsila,Gregory Y. H. Lip,Eduard Shantsila,Yalin Zheng*

Main category: cs.CV

TL;DR: PIs-Regressor learns persistence images from data to embed topological features directly into network architecture (Topology SegNet), improving curvilinear structure segmentation without handcrafted loss functions.


<details>
  <summary>Details</summary>
Motivation: Topological properties like connectivity improve segmentation accuracy but are challenging to extract from persistence diagrams due to non-differentiability and computational cost. Existing methods use handcrafted loss functions that generalize poorly.

Method: Propose PIs-Regressor module that learns persistence images (differentiable topological representations) directly from data, combined with Topology SegNet that fuses these features in both downsampling and upsampling stages, integrating topology into network architecture itself.

Result: Experimental results on three curvilinear benchmarks show state-of-the-art performance in both pixel-level accuracy and topological fidelity. The approach enhances model robustness against challenges like overexposure and blurring in medical imaging.

Conclusion: Directly incorporating topological information into network architecture rather than auxiliary losses leads to more robust segmentation. The flexible design can be combined with other topology-based methods to further enhance performance.

Abstract: Segmenting curvilinear structures in medical images is essential for analyzing morphological patterns in clinical applications. Integrating topological properties, such as connectivity, improves segmentation accuracy and consistency. However, extracting and embedding such properties - especially from Persistence Diagrams (PD) - is challenging due to their non-differentiability and computational cost. Existing approaches mostly encode topology through handcrafted loss functions, which generalize poorly across tasks. In this paper, we propose PIs-Regressor, a simple yet effective module that learns persistence image (PI) - finite, differentiable representations of topological features - directly from data. Together with Topology SegNet, which fuses these features in both downsampling and upsampling stages, our framework integrates topology into the network architecture itself rather than auxiliary losses. Unlike existing methods that depend heavily on handcrafted loss functions, our approach directly incorporates topological information into the network structure, leading to more robust segmentation. Our design is flexible and can be seamlessly combined with other topology-based methods to further enhance segmentation performance. Experimental results show that integrating topological features enhances model robustness, effectively handling challenges like overexposure and blurring in medical imaging. Our approach on three curvilinear benchmarks demonstrate state-of-the-art performance in both pixel-level accuracy and topological fidelity.

</details>


### [106] [Semi-Supervised Hyperspectral Image Classification with Edge-Aware Superpixel Label Propagation and Adaptive Pseudo-Labeling](https://arxiv.org/abs/2601.18049)
*Yunfei Qiu,Qiqiong Ma,Tianhua Lv,Li Fang,Shudong Zhou,Wei Yao*

Main category: cs.CV

TL;DR: A semi-supervised HSI classification framework using edge-aware superpixel label propagation and dynamic pseudo-label learning to address boundary diffusion and label instability issues.


<details>
  <summary>Details</summary>
Motivation: Address challenges in semi-supervised HSI classification: high annotation costs, limited samples, boundary label diffusion, and pseudo-label instability.

Method: 1) Edge-Aware Superpixel Label Propagation (EASLP) with edge intensity penalty and neighborhood correction; 2) Dynamic History-Fused Prediction (DHP) with historical prediction weighting; 3) Adaptive Tripartite Sample Categorization (ATSC) for hierarchical sample utilization; 4) Dynamic Reliability-Enhanced Pseudo-Label Framework (DREPL) combining DHP and ATSC.

Result: Superior classification performance demonstrated on four benchmark datasets, achieving spatio-temporal consistency optimization.

Conclusion: The proposed framework effectively mitigates boundary label diffusion and pseudo-label instability, enhancing classification robustness and learning efficiency in semi-supervised HSI classification.

Abstract: Significant progress has been made in semi-supervised hyperspectral image (HSI) classification regarding feature extraction and classification performance. However, due to high annotation costs and limited sample availability, semi-supervised learning still faces challenges such as boundary label diffusion and pseudo-label instability. To address these issues, this paper proposes a novel semi-supervised hyperspectral classification framework integrating spatial prior information with a dynamic learning mechanism. First, we design an Edge-Aware Superpixel Label Propagation (EASLP) module. By integrating edge intensity penalty with neighborhood correction strategy, it mitigates label diffusion from superpixel segmentation while enhancing classification robustness in boundary regions. Second, we introduce a Dynamic History-Fused Prediction (DHP) method. By maintaining historical predictions and dynamically weighting them with current results, DHP smoothens pseudo-label fluctuations and improves temporal consistency and noise resistance. Concurrently, incorporating condifence and consistency measures, the Adaptive Tripartite Sample Categorization (ATSC) strategy implements hierarchical utilization of easy, ambiguous, and hard samples, leading to enhanced pseudo-label quality and learning efficiency. The Dynamic Reliability-Enhanced Pseudo-Label Framework (DREPL), composed of DHP and ATSC, strengthens pseudo-label stability across temporal and sample domains. Through synergizes operation with EASLP, it achieves spatio-temporal consistency optimization. Evaluations on four benchmark datasets demonstrate its capability to maintain superior classification performance.

</details>


### [107] [Cross-Domain Transfer with Self-Supervised Spectral-Spatial Modeling for Hyperspectral Image Classification](https://arxiv.org/abs/2601.18088)
*Jianshu Chao,Tianhua Lv,Qiqiong Ma,Yunfei Qiu,Li Fang,Huifang Shen,Wei Yao*

Main category: cs.CV

TL;DR: Self-supervised cross-domain transfer framework for hyperspectral data that learns spectral-spatial representations without source labels and adapts efficiently with few target samples.


<details>
  <summary>Details</summary>
Motivation: Existing self-supervised methods for hyperspectral representation rely on source domain annotations and suffer from distribution shifts, leading to poor generalization in target domains. There's a need for label-free cross-domain transfer that works with limited target samples.

Method: Two-phase approach: 1) Self-supervised pre-training with Spatial-Spectral Transformer (S2Former) using dual-branch architecture with bidirectional cross-attention for spectral-spatial modeling, plus Frequency Domain Constraint (FDC) for detail preservation. 2) Fine-tuning with Diffusion-Aligned Fine-tuning (DAFT) distillation using teacher-student structure for robust transfer under low-label conditions.

Result: Experimental results show stable classification performance and strong cross-domain adaptability across four hyperspectral datasets, validating effectiveness under resource-constrained conditions.

Conclusion: The proposed framework successfully addresses cross-domain transfer challenges in hyperspectral analysis by enabling label-free representation learning and efficient adaptation with few samples, demonstrating practical value for real-world applications with limited labeled data.

Abstract: Self-supervised learning has demonstrated considerable potential in hyperspectral representation, yet its application in cross-domain transfer scenarios remains under-explored. Existing methods, however, still rely on source domain annotations and are susceptible to distribution shifts, leading to degraded generalization performance in the target domain. To address this, this paper proposes a self-supervised cross-domain transfer framework that learns transferable spectral-spatial joint representations without source labels and achieves efficient adaptation under few samples in the target domain. During the self-supervised pre-training phase, a Spatial-Spectral Transformer (S2Former) module is designed. It adopts a dual-branch spatial-spectral transformer and introduces a bidirectional cross-attention mechanism to achieve spectral-spatial collaborative modeling: the spatial branch enhances structural awareness through random masking, while the spectral branch captures fine-grained differences. Both branches mutually guide each other to improve semantic consistency. We further propose a Frequency Domain Constraint (FDC) to maintain frequency-domain consistency through real Fast Fourier Transform (rFFT) and high-frequency magnitude loss, thereby enhancing the model's capability to discern fine details and boundaries. During the fine-tuning phase, we introduce a Diffusion-Aligned Fine-tuning (DAFT) distillation mechanism. This aligns semantic evolution trajectories through a teacher-student structure, enabling robust transfer learning under low-label conditions. Experimental results demonstrate stable classification performance and strong cross-domain adaptability across four hyperspectral datasets, validating the method's effectiveness under resource-constrained conditions.

</details>


### [108] [Text-Pass Filter: An Efficient Scene Text Detector](https://arxiv.org/abs/2601.18098)
*Chuang Yang,Haozhao Ma,Xu Han,Yuan Yuan,Qi Wang*

Main category: cs.CV

TL;DR: TPF introduces a Text-Pass Filter for arbitrary-shaped text detection that directly segments whole texts using feature-filter pairs, avoiding limitations of shrink-mask methods while naturally separating adhesive texts without complex post-processing.


<details>
  <summary>Details</summary>
Motivation: Existing shrink-mask expansion methods lose visual features of text margins and confuse foreground/background differences, creating intrinsic limitations for text feature recognition. The paper aims to address these issues with a more efficient approach.

Method: TPF simulates band-pass filter concepts to create unique feature-filter pairs for each text. It includes Reinforcement Ensemble Unit (REU) to handle large aspect ratio texts and enhance feature consistency, and Foreground Prior Unit (FPU) to improve foreground/background discrimination.

Result: Experiments demonstrate the effectiveness of REU and FPU components while showing TPF's superiority over existing methods, particularly in handling adhesive texts and arbitrary shapes without complex post-processing.

Conclusion: TPF provides an efficient, real-time capable solution for arbitrary-shaped text detection that avoids limitations of shrink-mask methods by directly segmenting whole texts using feature-filter pairs with specialized units for feature enhancement and discrimination.

Abstract: To pursue an efficient text assembling process, existing methods detect texts via the shrink-mask expansion strategy. However, the shrinking operation loses the visual features of text margins and confuses the foreground and background difference, which brings intrinsic limitations to recognize text features. We follow this issue and design Text-Pass Filter (TPF) for arbitrary-shaped text detection. It segments the whole text directly, which avoids the intrinsic limitations. It is noteworthy that different from previous whole text region-based methods, TPF can separate adhesive texts naturally without complex decoding or post-processing processes, which makes it possible for real-time text detection. Concretely, we find that the band-pass filter allows through components in a specified band of frequencies, called its passband but blocks components with frequencies above or below this band. It provides a natural idea for extracting whole texts separately. By simulating the band-pass filter, TPF constructs a unique feature-filter pair for each text. In the inference stage, every filter extracts the corresponding matched text by passing its pass-feature and blocking other features. Meanwhile, considering the large aspect ratio problem of ribbon-like texts makes it hard to recognize texts wholly, a Reinforcement Ensemble Unit (REU) is designed to enhance the feature consistency of the same text and to enlarge the filter's recognition field to help recognize whole texts. Furthermore, a Foreground Prior Unit (FPU) is introduced to encourage TPF to discriminate the difference between the foreground and background, which improves the feature-filter pair quality. Experiments demonstrate the effectiveness of REU and FPU while showing the TPF's superiority.

</details>


### [109] [Computational Framework for Estimating Relative Gaussian Blur Kernels between Image Pairs](https://arxiv.org/abs/2601.18099)
*Akbar Saadat*

Main category: cs.CV

TL;DR: A zero-training forward computational framework for real-time Gaussian defocus estimation using analytic expressions and similarity filtering.


<details>
  <summary>Details</summary>
Motivation: To enable real-time applications of Gaussian defocus estimation without requiring training, building on previous verification work for Gaussian models.

Method: Uses discrete calculation of analytic expressions for defocused images from sharper ones, handles multiple solutions via similarity filtering over neighboring points, and supports partial blur cases.

Result: Achieves MAE below 1.7% in synthetic blur estimation and intensity discrepancy under 2% when applying extracted defocus filters to less blurred images.

Conclusion: The framework successfully enables real-time Gaussian defocus estimation with high accuracy without requiring training, validated on real images.

Abstract: Following the earlier verification for Gaussian model in \cite{ASaa2026}, this paper introduces a zero training forward computational framework for the model to realize it in real time applications. The framework is based on discrete calculation of the analytic expression of the defocused image from the sharper one for the application range of the standard deviation of the Gaussian kernels and selecting the best matches. The analytic expression yields multiple solutions at certain image points, but is filtered down to a single solution using similarity measures over neighboring points.The framework is structured to handle cases where two given images are partial blurred versions of each other. Experimental evaluations on real images demonstrate that the proposed framework achieves a mean absolute error (MAE) below $1.7\%$ in estimating synthetic blur values. Furthermore, the discrepancy between actual blurred image intensities and their corresponding estimates remains under $2\%$, obtained by applying the extracted defocus filters to less blurred images.

</details>


### [110] [Spatial-Conditioned Reasoning in Long-Egocentric Videos](https://arxiv.org/abs/2601.18100)
*James Tribble,Hao Wang,Si-En Hong,Chaoyi Zhou,Ashish Bastola,Siyu Huang,Abolfazl Razi*

Main category: cs.CV

TL;DR: VLMs struggle with spatial reasoning in long egocentric videos; adding depth maps improves spatial understanding without model changes.


<details>
  <summary>Details</summary>
Motivation: Long-horizon egocentric video poses challenges for visual navigation due to viewpoint drift and lack of persistent geometric context. While VLMs perform well on image/short-video reasoning, their spatial reasoning in long sequences remains limited.

Method: 1) Introduced Sanpo-D, a fine-grained re-annotation of Google Sanpo dataset for navigation-oriented spatial queries; 2) Benchmarked multiple VLMs; 3) Fused depth maps with RGB frames to examine input-level inductive bias; 4) Evaluated impact on spatial reasoning without modifying model architectures or inference procedures.

Result: Revealed trade-off between general-purpose accuracy and spatial specialization. Depth-aware and spatially grounded representations improved performance on safety-critical tasks like pedestrian and obstruction detection.

Conclusion: Explicit spatial signals (like depth maps) can enhance VLM-based video understanding for navigation tasks without architectural changes, showing promise for safety-critical applications.

Abstract: Long-horizon egocentric video presents significant challenges for visual navigation due to viewpoint drift and the absence of persistent geometric context. Although recent vision-language models perform well on image and short-video reasoning, their spatial reasoning capability in long egocentric sequences remains limited. In this work, we study how explicit spatial signals influence VLM-based video understanding without modifying model architectures or inference procedures. We introduce Sanpo-D, a fine-grained re-annotation of the Google Sanpo dataset, and benchmark multiple VLMs on navigation-oriented spatial queries. To examine input-level inductive bias, we further fuse depth maps with RGB frames and evaluate their impact on spatial reasoning. Our results reveal a trade-off between general-purpose accuracy and spatial specialization, showing that depth-aware and spatially grounded representations can improve performance on safety-critical tasks such as pedestrian and obstruction detection.

</details>


### [111] [LungCRCT: Causal Representation based Lung CT Processing for Lung Cancer Treatment](https://arxiv.org/abs/2601.18118)
*Daeyoung Kim*

Main category: cs.CV

TL;DR: LungCRCT: A causal representation learning framework for lung cancer analysis that enables causal intervention simulations and achieves 93.91% AUC in tumor classification.


<details>
  <summary>Details</summary>
Motivation: Lung cancer is a leading cause of mortality with early detection challenges due to symptom overlap with other respiratory diseases. While LDCT scans and AI models have improved detection, current deep learning approaches lack interpretability and causal analysis capabilities for treatment evaluation.

Method: Proposes LungCRCT framework using graph autoencoder-based causal discovery with distance correlation disentanglement and entropy-based image reconstruction refinement to extract causal representations of lung cancer progression mechanisms.

Result: Achieves 93.91% AUC in malignant tumor classification while enabling causal intervention analysis for lung cancer treatments and creating lightweight downstream models.

Conclusion: LungCRCT provides an interpretable causal analysis framework that goes beyond traditional classification to enable treatment simulation and intervention analysis, addressing key limitations of current deep learning approaches in lung cancer diagnostics.

Abstract: Due to silence in early stages, lung cancer has been one of the most leading causes of mortality in cancer patients world-wide. Moreover, major symptoms of lung cancer are hard to differentiate with other respiratory disease symptoms such as COPD, further leading patients to overlook cancer progression in early stages. Thus, to enhance survival rates in lung cancer, early detection from consistent proactive respiratory system monitoring becomes crucial. One of the most prevalent and effective methods for lung cancer monitoring would be low-dose computed tomography(LDCT) chest scans, which led to remarkable enhancements in lung cancer detection or tumor classification tasks under rapid advancements and applications of computer vision based AI models such as EfficientNet or ResNet in image processing. However, though advanced CNN models under transfer learning or ViT based models led to high performing lung cancer detections, due to its intrinsic limitations in terms of correlation dependence and low interpretability due to complexity, expansions of deep learning models to lung cancer treatment analysis or causal intervention analysis simulations are still limited. Therefore, this research introduced LungCRCT: a latent causal representation learning based lung cancer analysis framework that retrieves causal representations of factors within the physical causal mechanism of lung cancer progression. With the use of advanced graph autoencoder based causal discovery algorithms with distance Correlation disentanglement and entropy-based image reconstruction refinement, LungCRCT not only enables causal intervention analysis for lung cancer treatments, but also leads to robust, yet extremely light downstream models in malignant tumor classification tasks with an AUC score of 93.91%.

</details>


### [112] [Forward Consistency Learning with Gated Context Aggregation for Video Anomaly Detection](https://arxiv.org/abs/2601.18135)
*Jiahao Lyu,Minghua Zhao,Xuewen Huang,Yifei Chen,Shuangli Du,Jing Hu,Cheng Shi,Zhiyong Lv*

Main category: cs.CV

TL;DR: FoGA is a lightweight video anomaly detection model with only 2M parameters that uses forward consistency learning with gated context aggregation for efficient edge deployment, achieving 155 FPS while outperforming state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: Existing VAD methods rely on large models unsuitable for edge devices, and prediction-based approaches only use single-frame future prediction errors, missing richer temporal constraints from longer-term forward information.

Method: U-Net-based architecture with feature extraction on consecutive frames generating both immediate and forward predictions, gated context aggregation module in skip connections for dynamic feature fusion, joint optimization with forward consistency loss, and hybrid anomaly measurement integrating errors from both immediate and forward frames.

Result: Extensive experiments show FoGA substantially outperforms state-of-the-art methods while running up to 155 FPS, achieving excellent trade-off between performance and efficiency with only 2M parameters.

Conclusion: FoGA provides an effective lightweight VAD solution suitable for edge devices through forward consistency learning and gated context aggregation, balancing high performance with computational efficiency.

Abstract: As a crucial element of public security, video anomaly detection (VAD) aims to measure deviations from normal patterns for various events in real-time surveillance systems. However, most existing VAD methods rely on large-scale models to pursue extreme accuracy, limiting their feasibility on resource-limited edge devices. Moreover, mainstream prediction-based VAD detects anomalies using only single-frame future prediction errors, overlooking the richer constraints from longer-term temporal forward information. In this paper, we introduce FoGA, a lightweight VAD model that performs Forward consistency learning with Gated context Aggregation, containing about 2M parameters and tailored for potential edge devices. Specifically, we propose a Unet-based method that performs feature extraction on consecutive frames to generate both immediate and forward predictions. Then, we introduce a gated context aggregation module into the skip connections to dynamically fuse encoder and decoder features at the same spatial scale. Finally, the model is jointly optimized with a novel forward consistency loss, and a hybrid anomaly measurement strategy is adopted to integrate errors from both immediate and forward frames for more accurate detection. Extensive experiments demonstrate the effectiveness of the proposed method, which substantially outperforms state-of-the-art competing methods, running up to 155 FPS. Hence, our FoGA achieves an excellent trade-off between performance and the efficiency metric.

</details>


### [113] [Agentic Very Long Video Understanding](https://arxiv.org/abs/2601.18157)
*Aniket Rege,Arka Sadhu,Yuliang Li,Kejie Li,Ramya Korlakai Vinayak,Yuning Chai,Yong Jae Lee,Hyo Jin Kim*

Main category: cs.CV

TL;DR: EGAgent: An entity scene graph-based agentic framework for long-horizon video understanding in always-on AI assistants, achieving SOTA on EgoLifeQA and competitive performance on Video-MME (Long).


<details>
  <summary>Details</summary>
Motivation: Always-on personal AI assistants (smart glasses) require understanding continuous, longitudinal egocentric video streams spanning days/weeks, but existing methods have limited context windows and lack compositional reasoning over long videos.

Method: EGAgent framework uses entity scene graphs representing people, places, objects, and their relationships over time, with a planning agent equipped with tools for structured graph search/reasoning and hybrid visual/audio search capabilities.

Result: Achieves state-of-the-art performance on EgoLifeQA (57.5%) and competitive performance on Video-MME (Long) (74.1%) for complex longitudinal video understanding tasks.

Conclusion: EGAgent enables detailed, cross-modal, temporally coherent reasoning over long video streams, advancing capabilities for always-on AI assistants that need to understand continuous personal experiences.

Abstract: The advent of always-on personal AI assistants, enabled by all-day wearable devices such as smart glasses, demands a new level of contextual understanding, one that goes beyond short, isolated events to encompass the continuous, longitudinal stream of egocentric video. Achieving this vision requires advances in long-horizon video understanding, where systems must interpret and recall visual and audio information spanning days or even weeks. Existing methods, including large language models and retrieval-augmented generation, are constrained by limited context windows and lack the ability to perform compositional, multi-hop reasoning over very long video streams. In this work, we address these challenges through EGAgent, an enhanced agentic framework centered on entity scene graphs, which represent people, places, objects, and their relationships over time. Our system equips a planning agent with tools for structured search and reasoning over these graphs, as well as hybrid visual and audio search capabilities, enabling detailed, cross-modal, and temporally coherent reasoning. Experiments on the EgoLifeQA and Video-MME (Long) datasets show that our method achieves state-of-the-art performance on EgoLifeQA (57.5%) and competitive performance on Video-MME (Long) (74.1%) for complex longitudinal video understanding tasks.

</details>


### [114] [TempDiffReg: Temporal Diffusion Model for Non-Rigid 2D-3D Vascular Registration](https://arxiv.org/abs/2601.18168)
*Zehua Liu,Shihao Zou,Jincai Huang,Yanfang Zhang,Chao Tong,Weixin Si*

Main category: cs.CV

TL;DR: A coarse-to-fine 2D-3D vessel registration method for TACE procedures using SA-PnP for global alignment and TempDiffReg (temporal diffusion model) for iterative vessel deformation, achieving superior accuracy over SOTA methods.


<details>
  <summary>Details</summary>
Motivation: TACE is challenging due to complex vascular navigation and anatomical variability. Accurate 2D-3D vessel registration is essential for guiding microcatheters and instruments during TACE procedures to enable precise localization and optimal therapeutic targeting.

Method: Coarse-to-fine registration strategy: 1) SA-PnP (structure-aware perspective n-point) for global alignment between 2D and 3D vessel structures, 2) TempDiffReg, a temporal diffusion model that performs vessel deformation iteratively using temporal context to capture anatomical variations and local structural changes.

Result: Achieved MSE of 0.63 mm and MAE of 0.51 mm in registration accuracy, representing 66.7% lower MSE and 17.7% lower MAE compared to most competitive existing approaches. Outperformed SOTA methods in both accuracy and anatomical plausibility using data from 23 patients (626 paired multi-frame samples).

Conclusion: The proposed method has potential to assist less-experienced clinicians in safely and efficiently performing complex TACE procedures, enhancing surgical outcomes and patient care. Code and data are publicly available.

Abstract: Transarterial chemoembolization (TACE) is a preferred treatment option for hepatocellular carcinoma and other liver malignancies, yet it remains a highly challenging procedure due to complex intra-operative vascular navigation and anatomical variability. Accurate and robust 2D-3D vessel registration is essential to guide microcatheter and instruments during TACE, enabling precise localization of vascular structures and optimal therapeutic targeting. To tackle this issue, we develop a coarse-to-fine registration strategy. First, we introduce a global alignment module, structure-aware perspective n-point (SA-PnP), to establish correspondence between 2D and 3D vessel structures. Second, we propose TempDiffReg, a temporal diffusion model that performs vessel deformation iteratively by leveraging temporal context to capture complex anatomical variations and local structural changes. We collected data from 23 patients and constructed 626 paired multi-frame samples for comprehensive evaluation. Experimental results demonstrate that the proposed method consistently outperforms state-of-the-art (SOTA) methods in both accuracy and anatomical plausibility. Specifically, our method achieves a mean squared error (MSE) of 0.63 mm and a mean absolute error (MAE) of 0.51 mm in registration accuracy, representing 66.7\% lower MSE and 17.7\% lower MAE compared to the most competitive existing approaches. It has the potential to assist less-experienced clinicians in safely and efficiently performing complex TACE procedures, ultimately enhancing both surgical outcomes and patient care. Code and data are available at: \textcolor{blue}{https://github.com/LZH970328/TempDiffReg.git}

</details>


### [115] [YOLO-DS: Fine-Grained Feature Decoupling via Dual-Statistic Synergy Operator for Object Detection](https://arxiv.org/abs/2601.18172)
*Lin Huang,Yujuan Tan,Weisheng Li,Shitai Shan,Liu Liu,Bo Liu,Linlin Shen,Jing Yu,Yue Niu*

Main category: cs.CV

TL;DR: YOLO-DS introduces a Dual-Statistic Synergy Operator and gating modules to model heterogeneous object responses, improving YOLO detection performance with minimal latency increase.


<details>
  <summary>Details</summary>
Motivation: Existing YOLO detectors lack explicit modeling of heterogeneous object responses within shared feature channels, which limits further performance gains despite their good accuracy-efficiency balance.

Method: Proposes YOLO-DS with Dual-Statistic Synergy Operator (DSO) that decouples object features by jointly modeling channel-wise mean and peak-to-mean difference. Includes two lightweight gating modules: Dual-Statistic Synergy Gating (DSG) for adaptive channel-wise feature selection, and Multi-Path Segmented Gating (MSG) for depth-wise feature weighting.

Result: Outperforms YOLOv8 across five model scales (N, S, M, L, X) on MS-COCO benchmark, achieving AP gains of 1.1% to 1.7% with only minimal increase in inference latency.

Conclusion: YOLO-DS effectively addresses the limitation of modeling heterogeneous object responses in YOLO detectors, demonstrating superior capability in discriminating heterogeneous objects with high efficiency through extensive validation studies.

Abstract: One-stage object detection, particularly the YOLO series, strikes a favorable balance between accuracy and efficiency. However, existing YOLO detectors lack explicit modeling of heterogeneous object responses within shared feature channels, which limits further performance gains. To address this, we propose YOLO-DS, a framework built around a novel Dual-Statistic Synergy Operator (DSO). The DSO decouples object features by jointly modeling the channel-wise mean and the peak-to-mean difference. Building upon the DSO, we design two lightweight gating modules: the Dual-Statistic Synergy Gating (DSG) module for adaptive channel-wise feature selection, and the Multi-Path Segmented Gating (MSG) module for depth-wise feature weighting. On the MS-COCO benchmark, YOLO-DS consistently outperforms YOLOv8 across five model scales (N, S, M, L, X), achieving AP gains of 1.1% to 1.7% with only a minimal increase in inference latency. Extensive visualization, ablation, and comparative studies validate the effectiveness of our approach, demonstrating its superior capability in discriminating heterogeneous objects with high efficiency.

</details>


### [116] [\textsc{NaVIDA}: Vision-Language Navigation with Inverse Dynamics Augmentation](https://arxiv.org/abs/2601.18188)
*Weiye Zhu,Zekai Zhang,Xiangchen Wang,Hewei Pan,Teng Wang,Tiantian Geng,Rongtao Xu,Feng Zheng*

Main category: cs.CV

TL;DR: NaVIDA introduces a VLN framework that learns vision-action causality through inverse dynamics supervision and hierarchical action chunking to improve navigation stability and generalization.


<details>
  <summary>Details</summary>
Motivation: Current VLN methods lack explicit modeling of how actions causally transform visual observations, leading to unstable behaviors, weak generalization, and cumulative trajectory errors.

Method: NaVIDA couples policy learning with action-grounded visual dynamics using chunk-based inverse-dynamics supervision and hierarchical probabilistic action chunking (HPAC) for longer-range planning, plus entropy-guided adaptive execution.

Result: Achieves superior navigation performance with fewer parameters (3B vs 8B) compared to state-of-the-art methods, validated through extensive experiments and real-world robot evaluations.

Conclusion: Learning explicit vision-action causality through inverse dynamics and hierarchical chunking significantly improves VLN agent stability, generalization, and practical feasibility.

Abstract: Vision-and-Language Navigation (VLN) requires agents to interpret natural language instructions and act coherently in visually rich environments. However, most existing methods rely on reactive state-action mappings without explicitly modeling how actions causally transform subsequent visual observations. Lacking such vision-action causality, agents cannot anticipate the visual changes induced by its own actions, leading to unstable behaviors, weak generalization, and cumulative error along trajectory. To address these issues, we introduce \textsc{NaVIDA} (\textbf{Nav}igation with \textbf{I}nverse \textbf{D}ynamics \textbf{A}ugmentation), a unified VLN framework that couples policy learning with action-grounded visual dynamics and adaptive execution. \textsc{NaVIDA} augments training with chunk-based inverse-dynamics supervision to learn causal relationship between visual changes and corresponding actions. To structure this supervision and extend the effective planning range, \textsc{NaVIDA} employs hierarchical probabilistic action chunking (HPAC), which organizes trajectories into multi-step chunks and provides discriminative, longer-range visual-change cues. To further curb error accumulation and stabilize behavior at inference, an entropy-guided mechanism adaptively sets the execution horizon of action chunks. Extensive experiments show that \textsc{NaVIDA} achieves superior navigation performance compared to state-of-the-art methods with fewer parameters (3B vs. 8B). Real-world robot evaluations further validate the practical feasibility and effectiveness of our approach. Code and data will be available upon acceptance.

</details>


### [117] [Multi-Perspective Subimage CLIP with Keyword Guidance for Remote Sensing Image-Text Retrieval](https://arxiv.org/abs/2601.18190)
*Yifan Li,Shiying Wang,Jianqiang Huang*

Main category: cs.CV

TL;DR: MPS-CLIP is a parameter-efficient VLP framework for remote sensing image-text retrieval that shifts from global alignment to keyword-guided fine-grained matching using LLM-extracted keywords and SAM-generated sub-perspectives.


<details>
  <summary>Details</summary>
Motivation: Existing VLP models for RSITR rely on coarse-grained global alignment that overlooks dense, multi-scale semantics in overhead imagery, while full fine-tuning is computationally expensive and risks catastrophic forgetting.

Method: Uses LLM to extract semantic keywords, guides SAM to generate relevant sub-perspectives, introduces Gated Global Attention adapter for efficient backbone adaptation, aggregates local cues via Multi-Perspective Representation module, and optimizes with hybrid contrastive and weighted triplet losses.

Result: Achieves state-of-the-art performance with 35.18% and 48.40% mean Recall on RSICD and RSITMD benchmarks, significantly outperforming full fine-tuning baselines and recent competitive methods.

Conclusion: MPS-CLIP successfully addresses limitations of global alignment in RSITR through parameter-efficient fine-grained matching, demonstrating superior performance while mitigating computational costs and forgetting issues.

Abstract: Vision-Language Pre-training (VLP) models like CLIP have significantly advanced Remote Sensing Image-Text Retrieval (RSITR). However, existing methods predominantly rely on coarse-grained global alignment, which often overlooks the dense, multi-scale semantics inherent in overhead imagery. Moreover, adapting these heavy models via full fine-tuning incurs prohibitive computational costs and risks catastrophic forgetting. To address these challenges, we propose MPS-CLIP, a parameter-efficient framework designed to shift the retrieval paradigm from global matching to keyword-guided fine-grained alignment. Specifically, we leverage a Large Language Model (LLM) to extract core semantic keywords, guiding the Segment Anything Model (SamGeo) to generate semantically relevant sub-perspectives. To efficiently adapt the frozen backbone, we introduce a Gated Global Attention (G^2A) adapter, which captures global context and long-range dependencies with minimal overhead. Furthermore, a Multi-Perspective Representation (MPR) module aggregates these local cues into robust multi-perspective embeddings. The framework is optimized via a hybrid objective combining multi-perspective contrastive and weighted triplet losses, which dynamically selects maximum-response perspectives to suppress noise and enforce precise semantic matching. Extensive experiments on the RSICD and RSITMD benchmarks demonstrate that MPS-CLIP achieves state-of-the-art performance with 35.18% and 48.40% mean Recall (mR), respectively, significantly outperforming full fine-tuning baselines and recent competitive methods. Code is available at https://github.com/Lcrucial1f/MPS-CLIP.

</details>


### [118] [MindCine: Multimodal EEG-to-Video Reconstruction with Large-Scale Pretrained Models](https://arxiv.org/abs/2601.18192)
*Tian-Yi Zhou,Xuan-Hao Liu,Bao-Liang Lu,Wei-Long Zheng*

Main category: cs.CV

TL;DR: MindCine: A novel EEG-to-video reconstruction framework using multimodal joint learning and pre-trained large EEG models to overcome single modality limitations and data scarcity issues.


<details>
  <summary>Details</summary>
Motivation: Reconstructing human dynamic visual perception from EEG signals is valuable due to EEG's non-invasiveness and high temporal resolution, but current methods face challenges with single modality alignment (only text) and data scarcity issues that hinder training convergence.

Method: Proposes MindCine framework with: 1) multimodal joint learning strategy incorporating beyond-text modalities, 2) pre-trained large EEG model to alleviate data scarcity for semantic decoding, and 3) specifically designed Seq2Seq model with causal attention for perceptual information decoding.

Result: Extensive experiments show the model outperforms state-of-the-art methods both qualitatively and quantitatively, demonstrating effectiveness of complementary multimodal strengths and enhanced reconstruction performance through large-scale EEG models.

Conclusion: MindCine successfully addresses key challenges in EEG-to-video reconstruction by leveraging multimodal learning and pre-trained models, achieving high-fidelity video reconstructions even with limited EEG-video data.

Abstract: Reconstructing human dynamic visual perception from electroencephalography (EEG) signals is of great research significance since EEG's non-invasiveness and high temporal resolution. However, EEG-to-video reconstruction remains challenging due to: 1) Single Modality: existing studies solely align EEG signals with the text modality, which ignores other modalities and are prone to suffer from overfitting problems; 2) Data Scarcity: current methods often have difficulty training to converge with limited EEG-video data. To solve the above problems, we propose a novel framework MindCine to achieve high-fidelity video reconstructions on limited data. We employ a multimodal joint learning strategy to incorporate beyond-text modalities in the training stage and leverage a pre-trained large EEG model to relieve the data scarcity issue for decoding semantic information, while a Seq2Seq model with causal attention is specifically designed for decoding perceptual information. Extensive experiments demonstrate that our model outperforms state-of-the-art methods both qualitatively and quantitatively. Additionally, the results underscore the effectiveness of the complementary strengths of different modalities and demonstrate that leveraging a large-scale EEG model can further enhance reconstruction performance by alleviating the challenges associated with limited data.

</details>


### [119] [QualiRAG: Retrieval-Augmented Generation for Visual Quality Understanding](https://arxiv.org/abs/2601.18195)
*Linhan Cao,Wei Sun,Weixia Zhang,Xiangyang Zhu,Kaiwei Zhang,Jun Jia,Dandan Zhu,Guangtao Zhai,Xiongkuo Min*

Main category: cs.CV

TL;DR: QualiRAG is a training-free RAG framework that leverages LMMs' latent perceptual knowledge for visual quality assessment, using dynamic knowledge generation and retrieval instead of supervised fine-tuning.


<details>
  <summary>Details</summary>
Motivation: Current VQA approaches require labor-intensive supervised fine-tuning or reinforcement learning on curated datasets, which are prone to biases and lack interpretability. There's a need for training-free methods that can provide fine-grained spatiotemporal perception and contextual understanding.

Method: QualiRAG uses a RAG framework that dynamically generates auxiliary knowledge by decomposing questions into structured requests. It constructs four complementary knowledge sources: visual metadata, subject localization, global quality summaries, and local quality descriptions, followed by relevance-aware retrieval for evidence-grounded reasoning.

Result: QualiRAG achieves substantial improvements over open-source general-purpose LMMs and VQA-finetuned LMMs on visual quality understanding tasks, and delivers competitive performance on visual quality comparison tasks without any task-specific training.

Conclusion: QualiRAG demonstrates that training-free RAG frameworks can effectively leverage LMMs' latent perceptual knowledge for robust visual quality assessment, providing interpretable quality understanding without the need for labor-intensive annotation or dataset-specific biases.

Abstract: Visual quality assessment (VQA) is increasingly shifting from scalar score prediction toward interpretable quality understanding -- a paradigm that demands \textit{fine-grained spatiotemporal perception} and \textit{auxiliary contextual information}. Current approaches rely on supervised fine-tuning or reinforcement learning on curated instruction datasets, which involve labor-intensive annotation and are prone to dataset-specific biases. To address these challenges, we propose \textbf{QualiRAG}, a \textit{training-free} \textbf{R}etrieval-\textbf{A}ugmented \textbf{G}eneration \textbf{(RAG)} framework that systematically leverages the latent perceptual knowledge of large multimodal models (LMMs) for visual quality perception. Unlike conventional RAG that retrieves from static corpora, QualiRAG dynamically generates auxiliary knowledge by decomposing questions into structured requests and constructing four complementary knowledge sources: \textit{visual metadata}, \textit{subject localization}, \textit{global quality summaries}, and \textit{local quality descriptions}, followed by relevance-aware retrieval for evidence-grounded reasoning. Extensive experiments show that QualiRAG achieves substantial improvements over open-source general-purpose LMMs and VQA-finetuned LMMs on visual quality understanding tasks, and delivers competitive performance on visual quality comparison tasks, demonstrating robust quality assessment capabilities without any task-specific training. The code will be publicly available at https://github.com/clh124/QualiRAG.

</details>


### [120] [HomoFM: Deep Homography Estimation with Flow Matching](https://arxiv.org/abs/2601.18222)
*Mengfan He,Liangzheng Sun,Chunyu Li,Ziyang Meng*

Main category: cs.CV

TL;DR: HomoFM introduces flow matching technique to homography estimation, formulating it as velocity field learning with domain adaptation via gradient reversal layer for robustness across domains.


<details>
  <summary>Details</summary>
Motivation: Existing deep homography estimation methods struggle with complex geometric transformations and domain generalization (multimodal matching, varying illumination). Current approaches treat it as direct regression or iterative refinement, limiting their ability to capture complex transformations.

Method: Proposes HomoFM framework that formulates homography estimation as velocity field learning using flow matching technique. Models continuous point-wise velocity field to transform noisy distributions into registered coordinates via conditional flow trajectory. Integrates gradient reversal layer (GRL) into feature extraction backbone for domain adaptation to learn domain-invariant representations.

Result: Extensive experiments show HomoFM outperforms state-of-the-art methods in both estimation accuracy and robustness on standard benchmarks. The method demonstrates effectiveness in handling domain shifts like multimodal matching and varying illumination scenarios.

Conclusion: HomoFM successfully introduces flow matching to homography estimation, providing a novel velocity field learning approach with domain adaptation that achieves superior performance and robustness compared to existing methods.

Abstract: Deep homography estimation has broad applications in computer vision and robotics. Remarkable progresses have been achieved while the existing methods typically treat it as a direct regression or iterative refinement problem and often struggling to capture complex geometric transformations or generalize across different domains. In this work, we propose HomoFM, a new framework that introduces the flow matching technique from generative modeling into the homography estimation task for the first time. Unlike the existing methods, we formulate homography estimation problem as a velocity field learning problem. By modeling a continuous and point-wise velocity field that transforms noisy distributions into registered coordinates, the proposed network recovers high-precision transformations through a conditional flow trajectory. Furthermore, to address the challenge of domain shifts issue, e.g., the cases of multimodal matching or varying illumination scenarios, we integrate a gradient reversal layer (GRL) into the feature extraction backbone. This domain adaptation strategy explicitly constrains the encoder to learn domain-invariant representations, significantly enhancing the network's robustness. Extensive experiments demonstrate the effectiveness of the proposed method, showing that HomoFM outperforms state-of-the-art methods in both estimation accuracy and robustness on standard benchmarks. Code and data resource are available at https://github.com/hmf21/HomoFM.

</details>


### [121] [Facial Emotion Recognition on FER-2013 using an EfficientNetB2-Based Approach](https://arxiv.org/abs/2601.18228)
*Sahil Naik,Soham Bagayatkar,Pavankumar Singh*

Main category: cs.CV

TL;DR: Lightweight EfficientNetB2-based facial emotion recognition pipeline achieves 68.78% accuracy on FER-2013 with 10x fewer parameters than VGG16, using advanced training techniques for real-time applications.


<details>
  <summary>Details</summary>
Motivation: Real-world facial emotion recognition faces challenges like low image quality, lighting variations, pose changes, background distractions, small inter-class variations, noisy labels, and class imbalance. Existing large CNN models (VGG, ResNet) are computationally expensive and memory-intensive, limiting practicality for real-time applications.

Method: Proposes a lightweight pipeline based on EfficientNetB2 with two-stage warm-up and fine-tuning strategy. Uses AdamW optimization with decoupled weight decay, label smoothing (ε=0.06) to reduce annotation noise, clipped class weights to mitigate class imbalance, dropout, mixed-precision training, and extensive real-time data augmentation. Trained with stratified 87.5%/12.5% train-validation split.

Result: Achieves 68.78% test accuracy on FER-2013 dataset with nearly ten times fewer parameters than VGG16-based baselines. Experimental results show stable training, strong generalization, and per-class metrics demonstrating effectiveness.

Conclusion: The proposed lightweight EfficientNetB2-based approach with advanced training techniques provides an efficient solution for facial emotion recognition, making it suitable for real-time and edge-based applications while maintaining competitive accuracy.

Abstract: Detection of human emotions based on facial images in real-world scenarios is a difficult task due to low image quality, variations in lighting, pose changes, background distractions, small inter-class variations, noisy crowd-sourced labels, and severe class imbalance, as observed in the FER-2013 dataset of 48x48 grayscale images. Although recent approaches using large CNNs such as VGG and ResNet achieve reasonable accuracy, they are computationally expensive and memory-intensive, limiting their practicality for real-time applications. We address these challenges using a lightweight and efficient facial emotion recognition pipeline based on EfficientNetB2, trained using a two-stage warm-up and fine-tuning strategy. The model is enhanced with AdamW optimization, decoupled weight decay, label smoothing (epsilon = 0.06) to reduce annotation noise, and clipped class weights to mitigate class imbalance, along with dropout, mixed-precision training, and extensive real-time data augmentation. The model is trained using a stratified 87.5%/12.5% train-validation split while keeping the official test set intact, achieving a test accuracy of 68.78% with nearly ten times fewer parameters than VGG16-based baselines. Experimental results, including per-class metrics and learning dynamics, demonstrate stable training and strong generalization, making the proposed approach suitable for real-time and edge-based applications.

</details>


### [122] [V-Loop: Visual Logical Loop Verification for Hallucination Detection in Medical Visual Question Answering](https://arxiv.org/abs/2601.18240)
*Mengyuan Jin,Zehui Liao,Yong Xia*

Main category: cs.CV

TL;DR: V-Loop is a training-free framework that detects hallucinations in medical VQA by creating a bidirectional reasoning loop to verify factual correctness through visual attention consistency.


<details>
  <summary>Details</summary>
Motivation: MLLMs in medical VQA are prone to hallucinations (responses contradicting visual facts), which is dangerous in high-stakes medical scenarios. Existing uncertainty-based methods are indirect and don't verify factual correctness of specific answers.

Method: V-Loop creates a visually grounded logical loop: 1) MLLM produces answer for primary input, 2) extracts semantic units from QA pair, 3) generates verification question by conditioning on answer to re-query question, 4) enforces visual attention consistency to ensure both questions use same image evidence, 5) if verification answer matches expected content, loop closes (factual); otherwise flagged as hallucinated.

Result: V-Loop consistently outperforms existing introspective methods across multiple medical VQA benchmarks and MLLMs, remains highly efficient, and further boosts uncertainty-based approaches when combined.

Conclusion: V-Loop provides an effective, training-free solution for hallucination detection in medical VQA by directly verifying factual correctness through bidirectional reasoning and visual attention consistency, addressing limitations of indirect uncertainty-based approaches.

Abstract: Multimodal Large Language Models (MLLMs) have shown remarkable capability in assisting disease diagnosis in medical visual question answering (VQA). However, their outputs remain vulnerable to hallucinations (i.e., responses that contradict visual facts), posing significant risks in high-stakes medical scenarios. Recent introspective detection methods, particularly uncertainty-based approaches, offer computational efficiency but are fundamentally indirect, as they estimate predictive uncertainty for an image-question pair rather than verifying the factual correctness of a specific answer. To address this limitation, we propose Visual Logical Loop Verification (V-Loop), a training-free and plug-and-play framework for hallucination detection in medical VQA. V-Loop introduces a bidirectional reasoning process that forms a visually grounded logical loop to verify factual correctness. Given an input, the MLLM produces an answer for the primary input pair. V-Loop extracts semantic units from the primary QA pair, generates a verification question by conditioning on the answer unit to re-query the question unit, and enforces visual attention consistency to ensure answering both primary question and verification question rely on the same image evidence. If the verification answer matches the expected semantic content, the logical loop closes, indicating factual grounding; otherwise, the primary answer is flagged as hallucinated. Extensive experiments on multiple medical VQA benchmarks and MLLMs show that V-Loop consistently outperforms existing introspective methods, remains highly efficient, and further boosts uncertainty-based approaches when used in combination.

</details>


### [123] [Vision-Language-Model-Guided Differentiable Ray Tracing for Fast and Accurate Multi-Material RF Parameter Estimation](https://arxiv.org/abs/2601.18242)
*Zerui Kang,Yishen Lim,Zhouyou Gu,Seung-Woo Ko,Tony Q. S. Quek,Jihong Park*

Main category: cs.CV

TL;DR: VLM-guided framework accelerates RF material parameter estimation by using vision-language models to provide semantic priors for initialization and measurement placement in differentiable ray tracing.


<details>
  <summary>Details</summary>
Motivation: Accurate RF material parameters are crucial for 6G electromagnetic digital twins, but current gradient-based inverse ray tracing methods are sensitive to initialization and computationally expensive with limited measurements.

Method: Uses a vision-language model to parse scene images, infer material categories, and map them to quantitative priors via ITU-R material table for initialization. VLM also selects informative transmitter/receiver placements to create diverse, material-discriminative paths. Then performs gradient-based refinement using measured received signal strengths in a differentiable ray tracing engine.

Result: Achieves 2-4× faster convergence and 10-100× lower final parameter error compared to uniform/random initialization and random placement baselines. Achieves sub-0.1% mean relative error with only a few receivers. VLM-guided placement reduces measurements needed for accurate recovery.

Conclusion: Semantic priors from vision-language models effectively guide physics-based optimization for fast and reliable RF material estimation, demonstrating practical benefits for 6G electromagnetic digital twins.

Abstract: Accurate radio-frequency (RF) material parameters are essential for electromagnetic digital twins in 6G systems, yet gradient-based inverse ray tracing (RT) remains sensitive to initialization and costly under limited measurements. This paper proposes a vision-language-model (VLM) guided framework that accelerates and stabilizes multi-material parameter estimation in a differentiable RT (DRT) engine. A VLM parses scene images to infer material categories and maps them to quantitative priors via an ITU-R material table, yielding informed conductivity initializations. The VLM further selects informative transmitter/receiver placements that promote diverse, material-discriminative paths. Starting from these priors, the DRT performs gradient-based refinement using measured received signal strengths. Experiments in NVIDIA Sionna on indoor scenes show 2-4$\times$ faster convergence and 10-100$\times$ lower final parameter error compared with uniform or random initialization and random placement baselines, achieving sub-0.1\% mean relative error with only a few receivers. Complexity analyses indicate per-iteration time scales near-linearly with the number of materials and measurement setups, while VLM-guided placement reduces the measurements required for accurate recovery. Ablations over RT depth and ray counts confirm further accuracy gains without significant per-iteration overhead. Results demonstrate that semantic priors from VLMs effectively guide physics-based optimization for fast and reliable RF material estimation.

</details>


### [124] [A multimodal vision foundation model for generalizable knee pathology](https://arxiv.org/abs/2601.18250)
*Kang Yu,Dingyu Wang,Zimu Yuan,Nan Zhou,Jiajun Liu,Jiaxin Liu,Shanggui Liu,Yaoyan Zheng,Huishu Yuan,Di Huang,Dong Jiang*

Main category: cs.CV

TL;DR: OrthoFoundation is a multimodal vision foundation model for musculoskeletal imaging that achieves SOTA performance across 14 tasks with exceptional cross-anatomy generalization, using self-supervised learning on 1.2M knee images.


<details>
  <summary>Details</summary>
Motivation: Current AI in orthopedics uses fragmented, task-specific supervised learning that requires extensive annotations and lacks generalizability. Foundation models are limited by scarce large-scale musculoskeletal datasets.

Method: Built a 1.2M unlabeled knee X-ray/MRI dataset from internal/public sources. Used Dinov3 backbone with self-supervised contrastive learning to capture robust radiological representations.

Result: Achieved SOTA across 14 downstream tasks: superior accuracy in X-ray osteoarthritis diagnosis, ranked first in MRI structural injury detection. Matched supervised baselines with only 50% labeled data. Showed exceptional cross-anatomy generalization to hip, shoulder, ankle despite knee-only pre-training.

Conclusion: OrthoFoundation represents significant advancement toward general-purpose AI for musculoskeletal imaging by learning joint-agnostic radiological semantics, overcoming limitations of conventional models to reduce annotation burdens and enhance diagnostic accuracy.

Abstract: Musculoskeletal disorders represent a leading cause of global disability, creating an urgent demand for precise interpretation of medical imaging. Current artificial intelligence (AI) approaches in orthopedics predominantly rely on task-specific, supervised learning paradigms. These methods are inherently fragmented, require extensive annotated datasets, and often lack generalizability across different modalities and clinical scenarios. The development of foundation models in this field has been constrained by the scarcity of large-scale, curated, and open-source musculoskeletal datasets. To address these challenges, we introduce OrthoFoundation, a multimodal vision foundation model optimized for musculoskeletal pathology. We constructed a pre-training dataset of 1.2 million unlabeled knee X-ray and MRI images from internal and public databases. Utilizing a Dinov3 backbone, the model was trained via self-supervised contrastive learning to capture robust radiological representations. OrthoFoundation achieves state-of-the-art (SOTA) performance across 14 downstream tasks. It attained superior accuracy in X-ray osteoarthritis diagnosis and ranked first in MRI structural injury detection. The model demonstrated remarkable label efficiency, matching supervised baselines using only 50% of labeled data. Furthermore, despite being pre-trained on knee images, OrthoFoundation exhibited exceptional cross-anatomy generalization to the hip, shoulder, and ankle. OrthoFoundation represents a significant advancement toward general-purpose AI for musculoskeletal imaging. By learning fundamental, joint-agnostic radiological semantics from large-scale multimodal data, it overcomes the limitations of conventional models, which provides a robust framework for reducing annotation burdens and enhancing diagnostic accuracy in clinical practice.

</details>


### [125] [Co-PLNet: A Collaborative Point-Line Network for Prompt-Guided Wireframe Parsing](https://arxiv.org/abs/2601.18252)
*Chao Wang,Xuanying Li,Cheng Dai,Jinglei Feng,Yuxiang Luo,Yuqi Ouyang,Hao Qin*

Main category: cs.CV

TL;DR: Co-PLNet: A point-line collaborative framework for wireframe parsing that exchanges spatial cues between line and junction detection via prompt encoding and cross-guidance decoding, improving accuracy and consistency.


<details>
  <summary>Details</summary>
Motivation: Existing wireframe parsing methods predict lines and junctions separately and reconcile them post-hoc, causing mismatches and reduced robustness. There's a need for a more integrated approach that maintains consistency between these geometric elements.

Method: Co-PLNet uses a point-line collaborative framework with: 1) Point-Line Prompt Encoder (PLP-Encoder) that converts early detections into spatial prompts by encoding geometric attributes into compact, spatially aligned maps; 2) Cross-Guidance Line Decoder (CGL-Decoder) that refines predictions with sparse attention conditioned on complementary prompts, enforcing point-line consistency.

Result: Experiments on Wireframe and YorkUrban datasets show consistent improvements in accuracy and robustness, together with favorable real-time efficiency, demonstrating effectiveness for structured geometry perception.

Conclusion: The collaborative point-line framework successfully addresses the mismatch problem in wireframe parsing by enabling spatial cue exchange between line and junction detection, resulting in improved geometric consistency and practical efficiency for downstream applications like SLAM.

Abstract: Wireframe parsing aims to recover line segments and their junctions to form a structured geometric representation useful for downstream tasks such as Simultaneous Localization and Mapping (SLAM). Existing methods predict lines and junctions separately and reconcile them post-hoc, causing mismatches and reduced robustness. We present Co-PLNet, a point-line collaborative framework that exchanges spatial cues between the two tasks, where early detections are converted into spatial prompts via a Point-Line Prompt Encoder (PLP-Encoder), which encodes geometric attributes into compact and spatially aligned maps. A Cross-Guidance Line Decoder (CGL-Decoder) then refines predictions with sparse attention conditioned on complementary prompts, enforcing point-line consistency and efficiency. Experiments on Wireframe and YorkUrban show consistent improvements in accuracy and robustness, together with favorable real-time efficiency, demonstrating our effectiveness for structured geometry perception.

</details>


### [126] [Depth to Anatomy: Learning Internal Organ Locations from Surface Depth Images](https://arxiv.org/abs/2601.18260)
*Eytan Kats,Kai Geissler,Daniel Mensing,Jochen G. Hirsch,Stefan Heldman,Mattias P. Heinrich*

Main category: cs.CV

TL;DR: A learning-based framework predicts 3D locations and shapes of multiple internal organs from single 2D depth images for automated patient positioning in radiology.


<details>
  <summary>Details</summary>
Motivation: Automated patient positioning is important for optimizing scanning procedures and improving patient throughput in radiology. Depth information from RGB-D cameras offers a promising approach to estimate internal organ positions for more accurate and efficient positioning.

Method: Proposes a learning-based framework that directly predicts 3D locations and shapes of multiple internal organs from single 2D depth images of the body surface. Uses a large-scale dataset of full-body MRI scans to synthesize depth images paired with anatomical segmentations, training a unified convolutional neural network architecture without requiring explicit surface reconstruction.

Result: The method accurately localizes a diverse set of anatomical structures including bones and soft tissues. Experimental results demonstrate the potential of integrating depth sensors into radiology workflows.

Conclusion: The framework shows promise for streamlining scanning procedures and enhancing patient experience through automated patient positioning by leveraging depth sensors to estimate internal anatomy from surface depth images.

Abstract: Automated patient positioning plays an important role in optimizing scanning procedure and improving patient throughput. Leveraging depth information captured by RGB-D cameras presents a promising approach for estimating internal organ positions, thereby enabling more accurate and efficient positioning. In this work, we propose a learning-based framework that directly predicts the 3D locations and shapes of multiple internal organs from single 2D depth images of the body surface. Utilizing a large-scale dataset of full-body MRI scans, we synthesize depth images paired with corresponding anatomical segmentations to train a unified convolutional neural network architecture. Our method accurately localizes a diverse set of anatomical structures, including bones and soft tissues, without requiring explicit surface reconstruction. Experimental results demonstrate the potential of integrating depth sensors into radiology workflows to streamline scanning procedures and enhance patient experience through automated patient positioning.

</details>


### [127] [Revisiting Aerial Scene Classification on the AID Benchmark](https://arxiv.org/abs/2601.18263)
*Subhajeet Das,Susmita Ghosh,Abhiroop Chatterjee*

Main category: cs.CV

TL;DR: Literature review of ML methods for aerial image classification plus proposed Aerial-Y-Net with spatial attention and multi-scale fusion, achieving 91.72% accuracy on AID dataset.


<details>
  <summary>Details</summary>
Motivation: Aerial images are crucial for urban planning and environmental preservation but are heterogeneous with various structures (buildings, forests, mountains, unoccupied lands), making robust scene classification challenging.

Method: 1) Literature review covering handcrafted features (SIFT, LBP), traditional CNNs (VGG, GoogLeNet), and advanced deep hybrid networks. 2) Proposed Aerial-Y-Net: spatial attention-enhanced CNN with multi-scale feature fusion mechanism as an attention-based model.

Result: Aerial-Y-Net achieves 91.72% accuracy on the AID dataset, outperforming several baseline architectures.

Conclusion: The study provides comprehensive review of aerial image classification methods and demonstrates effectiveness of attention-based models with multi-scale feature fusion for handling complexities in aerial imagery.

Abstract: Aerial images play a vital role in urban planning and environmental preservation, as they consist of various structures, representing different types of buildings, forests, mountains, and unoccupied lands. Due to its heterogeneous nature, developing robust models for scene classification remains a challenge. In this study, we conduct a literature review of various machine learning methods for aerial image classification. Our survey covers a range of approaches from handcrafted features (e.g., SIFT, LBP) to traditional CNNs (e.g., VGG, GoogLeNet), and advanced deep hybrid networks. In this connection, we have also designed Aerial-Y-Net, a spatial attention-enhanced CNN with multi-scale feature fusion mechanism, which acts as an attention-based model and helps us to better understand the complexities of aerial images. Evaluated on the AID dataset, our model achieves 91.72% accuracy, outperforming several baseline architectures.

</details>


### [128] [Contextual Range-View Projection for 3D LiDAR Point Clouds](https://arxiv.org/abs/2601.18301)
*Seyedali Mousavi,Seyedhamidreza Mousavi,Masoud Daneshtalab*

Main category: cs.CV

TL;DR: The paper proposes two novel range-view projection methods (CAP and CWAP) that incorporate contextual information beyond just depth to address the many-to-one conflict in LiDAR point cloud projection, improving semantic segmentation performance.


<details>
  <summary>Details</summary>
Motivation: Current depth-based selection strategies for range-view projection discard important contextual information by always selecting the closest point, ignoring semantic relevance and object structure, which leads to loss of valuable information for downstream tasks.

Method: Two mechanisms: 1) Centerness-Aware Projection (CAP) adjusts point depths based on distance from instance center to prioritize central points over boundary/background points; 2) Class-Weighted-Aware Projection (CWAP) prioritizes object classes using user-defined weights for flexible projection strategies.

Result: On SemanticKITTI dataset, CAP preserves more instance points and achieves up to 3.1% mIoU improvement over baseline. CWAP enhances performance of targeted classes with negligible impact on other classes.

Conclusion: Incorporating contextual information (instance centers and class labels) into range-view projection selection rules significantly improves semantic segmentation performance compared to traditional depth-only approaches.

Abstract: Range-view projection provides an efficient method for transforming 3D LiDAR point clouds into 2D range image representations, enabling effective processing with 2D deep learning models. However, a major challenge in this projection is the many-to-one conflict, where multiple 3D points are mapped onto the same pixel in the range image, requiring a selection strategy. Existing approaches typically retain the point with the smallest depth (closest to the LiDAR), disregarding semantic relevance and object structure, which leads to the loss of important contextual information. In this paper, we extend the depth-based selection rule by incorporating contextual information from both instance centers and class labels, introducing two mechanisms: \textit{Centerness-Aware Projection (CAP)} and \textit{Class-Weighted-Aware Projection (CWAP)}. In CAP, point depths are adjusted according to their distance from the instance center, thereby prioritizing central instance points over noisy boundary and background points. In CWAP, object classes are prioritized through user-defined weights, offering flexibility in the projection strategy. Our evaluations on the SemanticKITTI dataset show that CAP preserves more instance points during projection, achieving up to a 3.1\% mIoU improvement compared to the baseline. Furthermore, CWAP enhances the performance of targeted classes while having a negligible impact on the performance of other classes

</details>


### [129] [SwipeGen: Bridging the Execution Gap in GUI Agents via Human-like Swipe Synthesis](https://arxiv.org/abs/2601.18305)
*Xuan Wang,Siyuan Su,Quantong Fu,Yongxiang Hu,Yangfan Zhou*

Main category: cs.CV

TL;DR: The paper addresses limitations in GUI agents' swipe interaction execution by proposing SwipeGen for synthesizing human-like swipes and GUISwiper for improved execution, achieving 214% improvement over baselines.


<details>
  <summary>Details</summary>
Motivation: While GUI agents have improved in perception and task grounding, their execution capability, particularly for swipe interactions, has become a bottleneck. Existing agents use overly simplified swipe strategies that fail to replicate human-like behavior, limiting task completion effectiveness.

Method: 1) Decompose human swipe gestures into multiple quantifiable dimensions; 2) Propose SwipeGen, an automated pipeline to synthesize human-like swipe interactions through GUI exploration; 3) Create the first benchmark for evaluating GUI agents' swipe execution capability; 4) Develop GUISwiper, a GUI agent with enhanced interaction execution using synthesized data.

Result: GUISwiper achieves 69.07% swipe execution accuracy, representing a 214% improvement over existing VLM baselines. The paper also releases the first benchmark for evaluating swipe execution capability in GUI agents.

Conclusion: The proposed approach successfully addresses the swipe execution bottleneck in GUI agents by synthesizing human-like interactions and developing an enhanced agent, significantly improving execution accuracy and enabling more realistic GUI automation.

Abstract: With the widespread adoption of Graphical User Interface (GUI) agents for automating GUI interaction tasks, substantial research focused on improving GUI perception to ground task instructions into concrete action steps. However, the step execution capability of these agents has gradually emerged as a new bottleneck for task completion. In particular, existing GUI agents often adopt overly simplified strategies for handling swipe interactions, preventing them from accurately replicating human-like behavior. To address this limitation, we decompose human swipe gestures into multiple quantifiable dimensions and propose an automated pipeline SwipeGen to synthesize human-like swipe interactions through GUI exploration. Based on this pipeline, we construct and release the first benchmark for evaluating the swipe execution capability of GUI agents. Furthermore, leveraging the synthesized data, we propose GUISwiper, a GUI agent with enhanced interaction execution capabilities. Experimental results demonstrate that GUISwiper achieves a swipe execution accuracy of 69.07%, representing a 214% improvement over existing VLM baselines.

</details>


### [130] [A Tumor Aware DenseNet Swin Hybrid Learning with Boosted and Hierarchical Feature Spaces for Large-Scale Brain MRI Classification](https://arxiv.org/abs/2601.18330)
*Muhammad Ali Shah,Muhammad Mansoor Alam,Saddam Hussain Khan*

Main category: cs.CV

TL;DR: EDSH framework combines DenseNet and Swin Transformer for brain tumor MRI analysis, achieving 98.50% accuracy on large-scale dataset through two tumor-aware setups that capture both local texture patterns and global contextual dependencies.


<details>
  <summary>Details</summary>
Motivation: Brain tumor MRI analysis requires capturing both fine-grained texture patterns (for irregular gliomas) and long-range contextual dependencies (for well-defined tumors). Existing standalone CNNs or Vision Transformers alone are insufficient for addressing class-specific diagnostic challenges across different tumor types.

Method: Two tumor-aware experimental setups: 1) Boosted Feature Space (BFS) with independently customized DenseNet and Swin branches for complementary local/global representations, dimension alignment, fusion, and boosting for glioma detection. 2) Hierarchical DenseNet-Swin architecture with Deep Feature Extraction and Dual Residual connections, where DenseNet handles local features and Swin_t models global tumor morphology for meningioma/pituitary classification. DenseNet is customized for MRI spatial characteristics, Swin_t uses task-aligned patch embedding and shifted-window self-attention.

Result: Extensive evaluation on large-scale MRI dataset (40,260 images across four tumor classes) shows consistent superiority over standalone CNNs, Vision Transformers, and hybrids. Achieves 98.50% accuracy and recall on test unseen dataset.

Conclusion: The EDSH framework effectively addresses class-specific diagnostic challenges in brain tumor MRI analysis by jointly capturing local texture patterns and global contextual dependencies through customized hybrid architecture, demonstrating state-of-the-art performance on large-scale dataset.

Abstract: This study proposes an efficient Densely Swin Hybrid (EDSH) framework for brain tumor MRI analysis, designed to jointly capture fine grained texture patterns and long range contextual dependencies. Two tumor aware experimental setups are introduced to address class-specific diagnostic challenges. The first setup employs a Boosted Feature Space (BFS), where independently customized DenseNet and Swint branches learn complementary local and global representations that are dimension aligned, fused, and boosted, enabling highly sensitive detection of diffuse glioma patterns by successfully learning the features of irregular shape, poorly defined mass, and heterogeneous texture. The second setup adopts a hierarchical DenseNet Swint architecture with Deep Feature Extraction have Dual Residual connections (DFE and DR), in which DenseNet serves as a stem CNN for structured local feature learning, while Swin_t models global tumor morphology, effectively suppressing false negatives in meningioma and pituitary tumor classification by learning the features of well defined mass, location (outside brain) and enlargments in tumors (dural tail or upward extension). DenseNet is customized at the input level to match MRI spatial characteristics, leveraging dense residual connectivity to preserve texture information and mitigate vanishing-gradient effects. In parallel, Swint is tailored through task aligned patch embedding and shifted-window self attention to efficiently capture hierarchical global dependencies. Extensive evaluation on a large-scale MRI dataset (stringent 40,260 images across four tumor classes) demonstrates consistent superiority over standalone CNNs, Vision Transformers, and hybrids, achieving 98.50 accuracy and recall on the test unseen dataset.

</details>


### [131] [PPISP: Physically-Plausible Compensation and Control of Photometric Variations in Radiance Field Reconstruction](https://arxiv.org/abs/2601.18336)
*Isaac Deutsch,Nicolas Moënne-Loccoz,Gavriel State,Zan Gojcic*

Main category: cs.CV

TL;DR: PPISP introduces a physically-based ISP correction module that disentangles camera-intrinsic and capture-dependent effects for robust multi-view 3D reconstruction, achieving state-of-the-art performance with interpretable control.


<details>
  <summary>Details</summary>
Motivation: Current multi-view 3D reconstruction methods are sensitive to photometric inconsistencies from camera optics and ISP variations. Existing mitigation strategies lack physical grounding and generalize poorly to novel views.

Method: Proposes Physically-Plausible ISP (PPISP) correction module that disentangles camera-intrinsic and capture-dependent effects through physically based transformations. Includes a PPISP controller trained on input views to predict ISP parameters for novel viewpoints, similar to auto exposure/white balance in real cameras.

Result: PPISP achieves state-of-the-art performance on standard benchmarks, enables realistic evaluation on novel views without ground-truth images, provides intuitive control, and supports metadata integration when available.

Conclusion: PPISP offers a physically-grounded, interpretable solution to photometric inconsistencies in multi-view 3D reconstruction that generalizes well to novel views and supports practical camera-like behavior.

Abstract: Multi-view 3D reconstruction methods remain highly sensitive to photometric inconsistencies arising from camera optical characteristics and variations in image signal processing (ISP). Existing mitigation strategies such as per-frame latent variables or affine color corrections lack physical grounding and generalize poorly to novel views. We propose the Physically-Plausible ISP (PPISP) correction module, which disentangles camera-intrinsic and capture-dependent effects through physically based and interpretable transformations. A dedicated PPISP controller, trained on the input views, predicts ISP parameters for novel viewpoints, analogous to auto exposure and auto white balance in real cameras. This design enables realistic and fair evaluation on novel views without access to ground-truth images. PPISP achieves SoTA performance on standard benchmarks, while providing intuitive control and supporting the integration of metadata when available. The source code is available at: https://github.com/nv-tlabs/ppisp

</details>


### [132] [Beyond Rigid: Benchmarking Non-Rigid Video Editing](https://arxiv.org/abs/2601.18340)
*Bingzheng Qu,Kehai Chen,Xuefeng Bai,Jun Yu,Min Zhang*

Main category: cs.CV

TL;DR: NRVBench is the first comprehensive benchmark for evaluating non-rigid video editing, featuring a curated dataset, a novel evaluation metric, and a training-free baseline method that addresses physical distortion and temporal flicker issues.


<details>
  <summary>Details</summary>
Motivation: Despite progress in text-driven video editing, generating coherent non-rigid deformations remains challenging due to physical distortion and temporal flicker. Current methods lack proper evaluation for complex dynamics in non-rigid motion editing.

Method: 1) Created NRVBench benchmark with 180 non-rigid motion videos from six physics-based categories, 2,340 task instructions, and 360 multiple-choice questions. 2) Proposed NRVE-Acc evaluation metric using Vision-Language Models to assess physical compliance, temporal consistency, and instruction alignment. 3) Introduced VM-Edit, a training-free baseline with dual-region denoising mechanism for structure-aware control.

Result: Extensive experiments show current methods have shortcomings in maintaining physical plausibility, while the proposed VM-Edit method achieves excellent performance across both standard and proposed metrics.

Conclusion: NRVBench serves as a standard testing platform for advancing physics-aware video editing, addressing the critical challenge of coherent non-rigid deformations in video editing.

Abstract: Despite the remarkable progress in text-driven video editing, generating coherent non-rigid deformations remains a critical challenge, often plagued by physical distortion and temporal flicker. To bridge this gap, we propose NRVBench, the first dedicated and comprehensive benchmark designed to evaluate non-rigid video editing. First, we curate a high-quality dataset consisting of 180 non-rigid motion videos from six physics-based categories, equipped with 2,340 fine-grained task instructions and 360 multiple-choice questions. Second, we propose NRVE-Acc, a novel evaluation metric based on Vision-Language Models that can rigorously assess physical compliance, temporal consistency, and instruction alignment, overcoming the limitations of general metrics in capturing complex dynamics. Third, we introduce a training-free baseline, VM-Edit, which utilizes a dual-region denoising mechanism to achieve structure-aware control, balancing structural preservation and dynamic deformation. Extensive experiments demonstrate that while current methods have shortcomings in maintaining physical plausibility, our method achieves excellent performance across both standard and proposed metrics. We believe the benchmark could serve as a standard testing platform for advancing physics-aware video editing.

</details>


### [133] [Q-Bench-Portrait: Benchmarking Multimodal Large Language Models on Portrait Image Quality Perception](https://arxiv.org/abs/2601.18346)
*Sijing Wu,Yunhao Li,Zicheng Zhang,Qi Jia,Xinyue Li,Huiyu Duan,Xiongkuo Min,Guangtao Zhai*

Main category: cs.CV

TL;DR: Q-Bench-Portrait is the first comprehensive benchmark for evaluating MLLMs on portrait image quality perception, covering diverse image sources, quality dimensions, and question formats, revealing current models' limitations compared to human judgment.


<details>
  <summary>Details</summary>
Motivation: Current MLLMs have shown strong performance on generic low-level vision tasks, but their ability to perceive and assess portrait images - which have unique structural and perceptual properties - remains largely unexplored and unmeasured.

Method: Created Q-Bench-Portrait benchmark with 2,765 image-question-answer triplets featuring: (1) diverse portrait sources (natural, synthetic distortion, AI-generated, artistic, computer graphics), (2) comprehensive quality dimensions (technical distortions, AIGC-specific distortions, aesthetics), and (3) varied question formats (single/multiple-choice, true/false, open-ended) at global and local levels. Evaluated 20 open-source and 5 closed-source MLLMs.

Result: Evaluation revealed that while current MLLMs demonstrate some competence in portrait image perception, their performance remains limited and imprecise, with a clear gap relative to human judgments. Models show deficiencies in accurately assessing portrait-specific quality aspects.

Conclusion: Q-Bench-Portrait fills a critical gap in evaluating MLLMs' portrait perception capabilities and should foster further research to enhance both general-purpose and domain-specific MLLMs' ability to understand and assess portrait images.

Abstract: Recent advances in multimodal large language models (MLLMs) have demonstrated impressive performance on existing low-level vision benchmarks, which primarily focus on generic images. However, their capabilities to perceive and assess portrait images, a domain characterized by distinct structural and perceptual properties, remain largely underexplored. To this end, we introduce Q-Bench-Portrait, the first holistic benchmark specifically designed for portrait image quality perception, comprising 2,765 image-question-answer triplets and featuring (1) diverse portrait image sources, including natural, synthetic distortion, AI-generated, artistic, and computer graphics images; (2) comprehensive quality dimensions, covering technical distortions, AIGC-specific distortions, and aesthetics; and (3) a range of question formats, including single-choice, multiple-choice, true/false, and open-ended questions, at both global and local levels. Based on Q-Bench-Portrait, we evaluate 20 open-source and 5 closed-source MLLMs, revealing that although current models demonstrate some competence in portrait image perception, their performance remains limited and imprecise, with a clear gap relative to human judgments. We hope that the proposed benchmark will foster further research into enhancing the portrait image perception capabilities of both general-purpose and domain-specific MLLMs.

</details>


### [134] [OREHAS: A fully automated deep-learning pipeline for volumetric endolymphatic hydrops quantification in MRI](https://arxiv.org/abs/2601.18368)
*Caterina Fuster-Barceló,Claudia Castrillón,Laura Rodrigo-Muñoz,Victor Manuel Vega-Suárez,Nicolás Pérez-Fernández,Gorka Bastarrika,Arrate Muñoz-Barrutia*

Main category: cs.CV

TL;DR: OREHAS is the first fully automatic pipeline for volumetric quantification of endolymphatic hydrops from routine MRI, achieving accurate segmentation with minimal supervision and outperforming existing clinical software.


<details>
  <summary>Details</summary>
Motivation: Current methods for quantifying endolymphatic hydrops (EH) from MRI require manual intervention, are operator-dependent, and lack consistency. There's a need for automated, reliable quantification that can support large-scale studies and clinical diagnosis.

Method: OREHAS integrates three components: slice classification, inner ear localization, and sequence-specific segmentation into a single workflow. It computes per-ear endolymphatic-to-vestibular volume ratios (ELR) directly from whole MRI volumes, trained with only 3-6 annotated slices per patient.

Result: Achieved Dice scores of 0.90 for SPACE-MRC and 0.75 for REAL-IR. In external validation, closely matched expert ground truth (VSI = 74.3%) and substantially outperformed clinical syngo.via software (VSI = 42.5%). Produced more physiologically realistic endolymphatic volumes across 19 test patients.

Conclusion: OREHAS enables reliable, reproducible EH quantification from standard MRI with limited supervision, reducing operator dependence and ensuring methodological consistency. Provides foundation for large-scale studies and recalibrating clinical diagnostic thresholds.

Abstract: We present OREHAS (Optimized Recognition & Evaluation of volumetric Hydrops in the Auditory System), the first fully automatic pipeline for volumetric quantification of endolymphatic hydrops (EH) from routine 3D-SPACE-MRC and 3D-REAL-IR MRI. The system integrates three components -- slice classification, inner ear localization, and sequence-specific segmentation -- into a single workflow that computes per-ear endolymphatic-to-vestibular volume ratios (ELR) directly from whole MRI volumes, eliminating the need for manual intervention.
  Trained with only 3 to 6 annotated slices per patient, OREHAS generalized effectively to full 3D volumes, achieving Dice scores of 0.90 for SPACE-MRC and 0.75 for REAL-IR. In an external validation cohort with complete manual annotations, OREHAS closely matched expert ground truth (VSI = 74.3%) and substantially outperformed the clinical syngo.via software (VSI = 42.5%), which tended to overestimate endolymphatic volumes. Across 19 test patients, vestibular measurements from OREHAS were consistent with syngo.via, while endolymphatic volumes were systematically smaller and more physiologically realistic.
  These results show that reliable and reproducible EH quantification can be achieved from standard MRI using limited supervision. By combining efficient deep-learning-based segmentation with a clinically aligned volumetric workflow, OREHAS reduces operator dependence, ensures methodological consistency. Besides, the results are compatible with established imaging protocols. The approach provides a robust foundation for large-scale studies and for recalibrating clinical diagnostic thresholds based on accurate volumetric measurements of the inner ear.

</details>


### [135] [Gaze Prediction in Virtual Reality Without Eye Tracking Using Visual and Head Motion Cues](https://arxiv.org/abs/2601.18372)
*Christos Petrou,Harris Partaourides,Athanasios Balomenos,Yannis Kopsinis,Sotirios Chatzis*

Main category: cs.CV

TL;DR: A novel gaze prediction framework for VR that combines HMD motion signals with visual saliency cues to predict future gaze directions without direct eye tracking.


<details>
  <summary>Details</summary>
Motivation: Direct eye tracking is often unavailable in VR due to hardware limitations or privacy concerns, yet gaze prediction is critical for reducing latency and enabling techniques like foveated rendering.

Method: Combines HMD motion signals with visual saliency cues from video frames using UniSal (lightweight saliency encoder), fuses these features, and processes them through time-series prediction modules (TSMixer or LSTM architectures).

Result: Outperforms baselines (Center-of-HMD and Mean Gaze) on EHTask dataset and in deployment on commercial VR hardware, demonstrating effective gaze prediction without direct eye tracking.

Conclusion: The framework effectively reduces perceptual lag and enhances natural interaction in VR environments where direct eye tracking is constrained, showing the viability of predictive gaze modeling as an alternative to direct eye tracking.

Abstract: Gaze prediction plays a critical role in Virtual Reality (VR) applications by reducing sensor-induced latency and enabling computationally demanding techniques such as foveated rendering, which rely on anticipating user attention. However, direct eye tracking is often unavailable due to hardware limitations or privacy concerns. To address this, we present a novel gaze prediction framework that combines Head-Mounted Display (HMD) motion signals with visual saliency cues derived from video frames. Our method employs UniSal, a lightweight saliency encoder, to extract visual features, which are then fused with HMD motion data and processed through a time-series prediction module. We evaluate two lightweight architectures, TSMixer and LSTM, for forecasting future gaze directions. Experiments on the EHTask dataset, along with deployment on commercial VR hardware, show that our approach consistently outperforms baselines such as Center-of-HMD and Mean Gaze. These results demonstrate the effectiveness of predictive gaze modeling in reducing perceptual lag and enhancing natural interaction in VR environments where direct eye tracking is constrained.

</details>


### [136] [Estimation of geometric transformation matrices using grid-shaped pilot signals](https://arxiv.org/abs/2601.18385)
*Rinka Kawano,Masaki Kawamura*

Main category: cs.CV

TL;DR: Proposed a watermarking method using grid-shaped pilot signals to estimate geometric transformations (scaling, rotation, shearing, cropping) for accurate synchronization in watermark extraction.


<details>
  <summary>Details</summary>
Motivation: Existing watermarking methods lack robustness against cropping attacks, which change image origins and make synchronization difficult. Accurate detection of watermark embedding positions is critical for proper extraction from pirated images that undergo geometric distortions.

Method: Embed a grid-shaped pilot signal with distinct horizontal and vertical values. When image is transformed, analyze grid distortion using Radon transform to estimate transformation matrix. Different encoding of horizontal/vertical grid lines helps determine orientation and reduce ambiguity.

Result: Method accurately estimates transformation matrices with low error under both single and composite attacks including anisotropic scaling, rotation, shearing, and cropping.

Conclusion: The proposed pilot signal-based approach enables robust synchronization against geometric transformations including cropping, addressing a key limitation of existing watermarking methods.

Abstract: Digital watermarking techniques are essential to prevent unauthorized use of images. Since pirated images are often geometrically distorted by operations such as scaling and cropping, accurate synchronization - detecting the embedding position of the watermark - is critical for proper extraction. In particular, cropping changes the origin of the image, making synchronization difficult. However, few existing methods are robust against cropping. To address this issue, we propose a watermarking method that estimates geometric transformations applied to a stego image using a pilot signal, allowing synchronization even after cropping. A grid-shaped pilot signal with distinct horizontal and vertical values is embedded in the image. When the image is transformed, the grid is also distorted. By analyzing this distortion, the transformation matrix can be estimated. Applying the Radon transform to the distorted image allows estimation of the grid angles and intervals. In addition, since the horizontal and vertical grid lines are encoded differently, the grid orientation can be determined, which reduces ambiguity. To validate our method, we performed simulations with anisotropic scaling, rotation, shearing, and cropping. The results show that the proposed method accurately estimates transformation matrices with low error under both single and composite attacks.

</details>


### [137] [ARMOR: Agentic Reasoning for Methods Orchestration and Reparameterization for Robust Adversarial Attacks](https://arxiv.org/abs/2601.18386)
*Gabriel Lee Jun Rong,Christos Korgialas,Dion Jia Xu Ho,Pai Chet Ng,Xiaoxiao Miao,Konstantinos N. Plataniotis*

Main category: cs.CV

TL;DR: ARMOR framework uses VLM-guided agents to orchestrate multiple adversarial attacks adaptively, achieving better transferability and fooling both white-box and blind targets.


<details>
  <summary>Details</summary>
Motivation: Existing automated attack suites are static with fixed sequences, lacking strategic adaptation and semantic awareness, limiting their effectiveness against diverse targets.

Method: ARMOR orchestrates three adversarial primitives (CW, JSMA, STA) via VLM-guided agents that collaboratively generate perturbations through a shared "Mixing Desk." LLMs adaptively tune and reparameterize parallel attack agents in real-time to exploit image-specific semantic vulnerabilities.

Result: On standard benchmarks, ARMOR achieves improved cross-architecture transfer and reliably fools both settings, delivering blended output for blind targets and selecting best attack or blended attacks for white-box targets using confidence-and-SSIM score.

Conclusion: ARMOR provides a dynamic, semantically-aware adversarial attack framework that outperforms static ensembles through adaptive orchestration of multiple attack strategies.

Abstract: Existing automated attack suites operate as static ensembles with fixed sequences, lacking strategic adaptation and semantic awareness. This paper introduces the Agentic Reasoning for Methods Orchestration and Reparameterization (ARMOR) framework to address these limitations. ARMOR orchestrates three canonical adversarial primitives, Carlini-Wagner (CW), Jacobian-based Saliency Map Attack (JSMA), and Spatially Transformed Attacks (STA) via Vision Language Models (VLM)-guided agents that collaboratively generate and synthesize perturbations through a shared ``Mixing Desk". Large Language Models (LLMs) adaptively tune and reparameterize parallel attack agents in a real-time, closed-loop system that exploits image-specific semantic vulnerabilities. On standard benchmarks, ARMOR achieves improved cross-architecture transfer and reliably fools both settings, delivering a blended output for blind targets and selecting the best attack or blended attacks for white-box targets using a confidence-and-SSIM score.

</details>


### [138] [Efficient Complex-Valued Vision Transformers for MRI Classification Directly from k-Space](https://arxiv.org/abs/2601.18392)
*Moritz Rempe,Lukas T. Rotkopf,Marco Schlimbach,Helmut Becker,Fabian Hörst,Johannes Haubold,Philipp Dammann,Kevin Kröninger,Jens Kleesiek*

Main category: cs.CV

TL;DR: kViT: A complex-valued Vision Transformer for direct k-space MRI classification that bypasses image reconstruction, using radial patching to match MRI physics and achieving competitive performance with 68× VRAM reduction.


<details>
  <summary>Details</summary>
Motivation: Current deep learning for MRI works on reconstructed magnitude images, discarding phase information and requiring expensive transforms. Standard architectures (convolutions, grid patches) are ill-suited for global, non-local k-space data.

Method: Proposes kViT - a complex-valued Vision Transformer for direct k-space classification. Introduces radial k-space patching strategy that respects spectral energy distribution of frequency-domain data.

Result: Achieves classification performance competitive with state-of-the-art image-domain baselines (ResNet, EfficientNet, ViT). Shows superior robustness to high acceleration factors and reduces VRAM consumption during training by up to 68× compared to standard methods.

Conclusion: Establishes pathway for resource-efficient, direct-from-scanner AI analysis by enabling classification directly on k-space data without image reconstruction, offering computational efficiency and robustness advantages.

Abstract: Deep learning applications in Magnetic Resonance Imaging (MRI) predominantly operate on reconstructed magnitude images, a process that discards phase information and requires computationally expensive transforms. Standard neural network architectures rely on local operations (convolutions or grid-patches) that are ill-suited for the global, non-local nature of raw frequency-domain (k-Space) data. In this work, we propose a novel complex-valued Vision Transformer (kViT) designed to perform classification directly on k-Space data. To bridge the geometric disconnect between current architectures and MRI physics, we introduce a radial k-Space patching strategy that respects the spectral energy distribution of the frequency-domain. Extensive experiments on the fastMRI and in-house datasets demonstrate that our approach achieves classification performance competitive with state-of-the-art image-domain baselines (ResNet, EfficientNet, ViT). Crucially, kViT exhibits superior robustness to high acceleration factors and offers a paradigm shift in computational efficiency, reducing VRAM consumption during training by up to 68$\times$ compared to standard methods. This establishes a pathway for resource-efficient, direct-from-scanner AI analysis.

</details>


### [139] [Larger than memory image processing](https://arxiv.org/abs/2601.18407)
*Jon Sporring,David Stansby*

Main category: cs.CV

TL;DR: A streaming architecture for petascale image analysis that minimizes I/O through slice-based processing, formal sweep execution, and a DSL for automatic pipeline optimization.


<details>
  <summary>Details</summary>
Motivation: Address performance bottlenecks in analyzing extremely large (petascale) image datasets (1.4PB EM volumes, 150TB organ atlases) where performance is fundamentally I/O-bound rather than compute-bound.

Method: Propose slice-based streaming architecture over either 2D slice stacks or 3D chunked layouts, formalize sweep-based execution with windowed operations and overlap-aware tiling, and introduce a DSL that performs compile-time/run-time pipeline analysis for automatic optimization.

Result: Achieves near-linear I/O scans and predictable memory footprints, delivering substantial throughput gains for extremely large images without requiring full-volume residency in memory.

Conclusion: Streaming passes over data with intelligent scheduling and a DSL for automatic optimization provide an effective approach for I/O-bound petascale image analysis, integrating with existing tooling while reframing processing as pipelines that privilege sequential access patterns.

Abstract: This report addresses larger-than-memory image analysis for petascale datasets such as 1.4 PB electron-microscopy volumes and 150 TB human-organ atlases. We argue that performance is fundamentally I/O-bound. We show that structuring analysis as streaming passes over data is crucial. For 3D volumes, two representations are popular: stacks of 2D slices (e.g., directories or multi-page TIFF) and 3D chunked layouts (e.g., Zarr/HDF5). While for a few algorithms, chunked layout on disk is crucial to keep disk I/O at a minimum, we show how the slice-based streaming architecture can be built on top of either image representation in a manner that minimizes disk I/O. This is in particular advantageous for algorithms relying on neighbouring values, since the slicing streaming architecture is 1D, which implies that there are only 2 possible sweeping orders, both of which are aligned with the order in which images are read from the disk. This is in contrast to 3D chunks, in which any sweep cannot be done without accessing each chunk at least 9 times. We formalize this with sweep-based execution (natural 2D/3D orders), windowed operations, and overlap-aware tiling to minimize redundant access. Building on these principles, we introduce a domain-specific language (DSL) that encodes algorithms with intrinsic knowledge of their optimal streaming and memory use; the DSL performs compile-time and run-time pipeline analyses to automatically select window sizes, fuse stages, tee and zip streams, and schedule passes for limited-RAM machines, yielding near-linear I/O scans and predictable memory footprints. The approach integrates with existing tooling for segmentation and morphology but reframes pre/post-processing as pipelines that privilege sequential read/write patterns, delivering substantial throughput gains for extremely large images without requiring full-volume residency in memory.

</details>


### [140] [Comparative Evaluation of Machine Learning Algorithms for Affective State Recognition from Children's Drawings](https://arxiv.org/abs/2601.18414)
*Aura Loredana Dan*

Main category: cs.CV

TL;DR: Comparative evaluation of MobileNet, EfficientNet, and VGG16 for emotion classification from children's drawings to aid early autism assessment.


<details>
  <summary>Details</summary>
Motivation: Early assessment of emotional states in children with autism spectrum disorder (ASD) is challenging due to intrusive, subjective, and inconsistent conventional methods. Children's drawings offer a non-intrusive window into their affective states, but automated emotion recognition from drawings needs robust and efficient computational approaches.

Method: Three deep learning architectures (MobileNet, EfficientNet, VGG16) were evaluated using transfer learning on a dataset of children's drawings annotated with emotional labels by psychological experts. The study employed a unified experimental framework to analyze classification performance, robustness, and computational efficiency.

Result: The comparative evaluation revealed important trade-offs between lightweight (MobileNet) and deeper (VGG16) architectures. EfficientNet showed balanced performance. Results highlight practical considerations for mobile and real-time applications in drawing-based affective computing.

Conclusion: Deep learning models can effectively classify emotions from children's drawings, offering a non-intrusive approach for early ASD assessment. The choice of architecture involves trade-offs between accuracy, computational efficiency, and practical deployment considerations, particularly for mobile healthcare applications.

Abstract: Autism spectrum disorder (ASD) represents a neurodevelopmental condition characterized by difficulties in expressing emotions and communication, particularly during early childhood. Understanding the affective state of children at an early age remains challenging, as conventional assessment methods are often intrusive, subjective, or difficult to apply consistently. This paper builds upon previous work on affective state recognition from children's drawings by presenting a comparative evaluation of machine learning models for emotion classification. Three deep learning architectures -- MobileNet, EfficientNet, and VGG16 -- are evaluated within a unified experimental framework to analyze classification performance, robustness, and computational efficiency. The models are trained using transfer learning on a dataset of children's drawings annotated with emotional labels provided by psychological experts. The results highlight important trade-offs between lightweight and deeper architectures when applied to drawing-based affective computing tasks, particularly in mobile and real-time application contexts.

</details>


### [141] [On Procrustes Contamination in Machine Learning Applications of Geometric Morphometrics](https://arxiv.org/abs/2601.18448)
*Lloyd Austin Courtenay*

Main category: cs.CV

TL;DR: Standard GPA alignment before train-test split causes statistical contamination in ML analyses of geometric morphometrics; a new realignment method fixes this by aligning test data to training set.


<details>
  <summary>Details</summary>
Motivation: Current practice of aligning all specimens via Generalized Procrustes Analysis (GPA) before splitting data into training and test sets introduces statistical dependence and contaminates downstream predictive models in machine learning applications of geometric morphometrics.

Method: Used controlled 2D and 3D simulations across varying sample sizes, landmark densities, and allometric patterns to characterize GPA-induced contamination. Proposed a novel realignment procedure where test specimens are aligned to the training set prior to model fitting. Analyzed spatial autocorrelation among landmarks using linear and convolutional regression models.

Result: Simulations revealed a robust "diagonal" pattern in sample-size vs. landmark-space reflecting RMSE scaling under isotropic variation, with slopes analytically derived from Procrustes tangent space degrees of freedom. Demonstrated performance degradation when landmark spatial relationships are ignored, highlighting importance of spatial autocorrelation.

Conclusion: Establishes need for careful preprocessing in ML applications of GMM, provides practical guidelines for realignment to eliminate cross-sample dependency, and clarifies fundamental statistical constraints inherent to Procrustes shape space.

Abstract: Geometric morphometrics (GMM) is widely used to quantify shape variation, more recently serving as input for machine learning (ML) analyses. Standard practice aligns all specimens via Generalized Procrustes Analysis (GPA) prior to splitting data into training and test sets, potentially introducing statistical dependence and contaminating downstream predictive models. Here, the effects of GPA-induced contamination are formally characterised using controlled 2D and 3D simulations across varying sample sizes, landmark densities, and allometric patterns. A novel realignment procedure is proposed, whereby test specimens are aligned to the training set prior to model fitting, eliminating cross-sample dependency. Simulations reveal a robust "diagonal" in sample-size vs. landmark-space, reflecting the scaling of RMSE under isotropic variation, with slopes analytically derived from the degrees of freedom in Procrustes tangent space. The importance of spatial autocorrelation among landmarks is further demonstrated using linear and convolutional regression models, highlighting performance degradation when landmark relationships are ignored. This work establishes the need for careful preprocessing in ML applications of GMM, provides practical guidelines for realignment, and clarifies fundamental statistical constraints inherent to Procrustes shape space.

</details>


### [142] [3DGesPolicy: Phoneme-Aware Holistic Co-Speech Gesture Generation Based on Action Control](https://arxiv.org/abs/2601.18451)
*Xuanmeng Sha,Liyun Zhang,Tomohiro Mashita,Naoya Chiba,Yuki Uranishi*

Main category: cs.CV

TL;DR: 3DGesPolicy: A novel action-based framework using diffusion policy from robotics to generate holistic co-speech gestures with coordinated body motion and facial expressions, addressing semantic incoherence and spatial instability in existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing methods for generating holistic co-speech gestures suffer from semantically incoherent coordination between body motion and facial expressions, and spatially unstable meaningless movements due to part-decomposed or frame-level regression approaches.

Method: Reformulates holistic gesture generation as a continuous trajectory control problem using diffusion policy from robotics. Models frame-to-frame variations as unified holistic actions, and introduces a Gesture-Audio-Phoneme (GAP) fusion module to deeply integrate multi-modal signals for fine-grained alignment.

Result: Extensive experiments on BEAT2 dataset demonstrate 3DGesPolicy outperforms state-of-the-art methods in generating natural, expressive, and highly speech-aligned holistic gestures with both spatial and semantic coherence.

Conclusion: 3DGesPolicy effectively addresses the limitations of existing methods by learning inter-frame holistic gesture motion patterns and ensuring coherent movement trajectories that adhere to realistic motion manifolds, while achieving structured alignment between speech semantics, body motion, and facial expressions.

Abstract: Generating holistic co-speech gestures that integrate full-body motion with facial expressions suffers from semantically incoherent coordination on body motion and spatially unstable meaningless movements due to existing part-decomposed or frame-level regression methods, We introduce 3DGesPolicy, a novel action-based framework that reformulates holistic gesture generation as a continuous trajectory control problem through diffusion policy from robotics. By modeling frame-to-frame variations as unified holistic actions, our method effectively learns inter-frame holistic gesture motion patterns and ensures both spatially and semantically coherent movement trajectories that adhere to realistic motion manifolds. To further bridge the gap in expressive alignment, we propose a Gesture-Audio-Phoneme (GAP) fusion module that can deeply integrate and refine multi-modal signals, ensuring structured and fine-grained alignment between speech semantics, body motion, and facial expressions. Extensive quantitative and qualitative experiments on the BEAT2 dataset demonstrate the effectiveness of our 3DGesPolicy across other state-of-the-art methods in generating natural, expressive, and highly speech-aligned holistic gestures.

</details>


### [143] [Fair-Eye Net: A Fair, Trustworthy, Multimodal Integrated Glaucoma Full Chain AI System](https://arxiv.org/abs/2601.18464)
*Wenbin Wei,Suyuan Yao,Cheng Huang,Xiangyu Gao*

Main category: cs.CV

TL;DR: Fair-Eye Net is a multimodal AI system for glaucoma screening and progression monitoring that integrates multiple data types with fairness constraints to reduce diagnostic disparities across demographic groups.


<details>
  <summary>Details</summary>
Motivation: Current glaucoma screening and progression assessment methods are subjective, fragmented, and inconsistent, with limited access to quality imaging and specialist expertise. There are significant disparities in diagnosis rates across demographic groups, compromising healthcare equity.

Method: Developed a dual-stream heterogeneous fusion architecture integrating fundus photos, OCT structural metrics, VF functional indices, and demographic factors. Uses uncertainty-aware hierarchical gating for selective prediction and safe referral, with fairness constraints to reduce missed diagnoses in disadvantaged subgroups through multitask learning.

Result: Achieved AUC of 0.912 with 96.7% specificity, reduced racial false-negativity disparity by 73.4% (from 12.31% to 3.28%), maintained stable cross-domain performance, and enabled 3-12 months early risk alerts with 92% sensitivity and 88% specificity.

Conclusion: Fair-Eye Net offers a reproducible path for clinical translation and large-scale deployment by optimizing fairness as a primary goal alongside clinical reliability, advancing global eye health equity through integrated screening, follow-up, and risk alerting.

Abstract: Glaucoma is a top cause of irreversible blindness globally, making early detection and longitudinal follow-up pivotal to preventing permanent vision loss. Current screening and progression assessment, however, rely on single tests or loosely linked examinations, introducing subjectivity and fragmented care. Limited access to high-quality imaging tools and specialist expertise further compromises consistency and equity in real-world use. To address these gaps, we developed Fair-Eye Net, a fair, reliable multimodal AI system closing the clinical loop from glaucoma screening to follow-up and risk alerting. It integrates fundus photos, OCT structural metrics, VF functional indices, and demographic factors via a dual-stream heterogeneous fusion architecture, with an uncertainty-aware hierarchical gating strategy for selective prediction and safe referral. A fairness constraint reduces missed diagnoses in disadvantaged subgroups. Experimental results show it achieved an AUC of 0.912 (96.7% specificity), cut racial false-negativity disparity by 73.4% (12.31% to 3.28%), maintained stable cross-domain performance, and enabled 3-12 months of early risk alerts (92% sensitivity, 88% specificity). Unlike post hoc fairness adjustments, Fair-Eye Net optimizes fairness as a primary goal with clinical reliability via multitask learning, offering a reproducible path for clinical translation and large-scale deployment to advance global eye health equity.

</details>


### [144] [DisasterInsight: A Multimodal Benchmark for Function-Aware and Grounded Disaster Assessment](https://arxiv.org/abs/2601.18493)
*Sara Tehrani,Yonghao Xu,Leif Haglund,Amanda Berg,Michael Felsberg*

Main category: cs.CV

TL;DR: DisasterInsight is a new multimodal benchmark for evaluating vision-language models on realistic disaster analysis tasks using building-centered satellite imagery, with DI-Chat as a domain-adapted baseline model.


<details>
  <summary>Details</summary>
Motivation: Existing remote sensing benchmarks focus on coarse labels and image-level recognition, lacking functional understanding and instruction robustness needed for real humanitarian disaster response workflows.

Method: Restructured xBD dataset into ~112K building-centered instances, created instruction-diverse evaluation across multiple tasks, and developed DI-Chat by fine-tuning existing VLM backbones with LoRA on disaster-specific instruction data.

Result: DI-Chat shows significant improvements in damage-level and disaster-type classification and report generation quality, but building-function classification remains challenging for all models. Performance gaps revealed across tasks, especially in damage understanding and structured reporting.

Conclusion: DisasterInsight provides a unified benchmark for studying grounded multimodal reasoning in disaster imagery, addressing critical needs for timely satellite imagery interpretation in humanitarian response.

Abstract: Timely interpretation of satellite imagery is critical for disaster response, yet existing vision-language benchmarks for remote sensing largely focus on coarse labels and image-level recognition, overlooking the functional understanding and instruction robustness required in real humanitarian workflows. We introduce DisasterInsight, a multimodal benchmark designed to evaluate vision-language models (VLMs) on realistic disaster analysis tasks. DisasterInsight restructures the xBD dataset into approximately 112K building-centered instances and supports instruction-diverse evaluation across multiple tasks, including building-function classification, damage-level and disaster-type classification, counting, and structured report generation aligned with humanitarian assessment guidelines.
  To establish domain-adapted baselines, we propose DI-Chat, obtained by fine-tuning existing VLM backbones on disaster-specific instruction data using parameter-efficient Low-Rank Adaptation (LoRA). Extensive experiments on state-of-the-art generic and remote-sensing VLMs reveal substantial performance gaps across tasks, particularly in damage understanding and structured report generation. DI-Chat achieves significant improvements on damage-level and disaster-type classification as well as report generation quality, while building-function classification remains challenging for all evaluated models. DisasterInsight provides a unified benchmark for studying grounded multimodal reasoning in disaster imagery.

</details>


### [145] [From Cold Start to Active Learning: Embedding-Based Scan Selection for Medical Image Segmentation](https://arxiv.org/abs/2601.18532)
*Devon Levy,Bar Assayag,Laura Gaspar,Ilan Shimshoni,Bella Specktor-Fadida*

Main category: cs.CV

TL;DR: Proposed novel active learning framework combining foundation-model embeddings with clustering for cold-start sampling, followed by uncertainty-based selection with spatial diversity, improving medical image segmentation accuracy across multiple datasets.


<details>
  <summary>Details</summary>
Motivation: Manual segmentation annotation is time-consuming and requires expertise, creating a bottleneck in disease monitoring. Active learning can reduce annotation burden by prioritizing informative samples, but existing methods need improvement for better cold-start initialization and sample selection.

Method: Two-stage approach: 1) Cold-start phase uses foundation-model embeddings with clustering (including automatic cluster number selection and proportional sampling) to create diverse initial training set. 2) Active learning phase integrates uncertainty-based selection (entropy) with spatial diversity to guide sample selection. The method provides interpretable visualization of feature-space distributions.

Result: Consistent improvements across three medical imaging datasets (CheXmask, Montgomery, SynthStrip). On CheXmask: cold-start improved Dice from 0.918 to 0.929, Hausdorff distance from 32.41 to 27.66 mm; AL improved Dice from 0.919 to 0.939, Hausdorff from 30.10 to 19.16 mm. Montgomery: cold-start improved Dice from 0.928 to 0.950, Hausdorff from 14.22 to 9.38 mm. SynthStrip: cold-start reduced Hausdorff from 9.43 to 8.69 mm; AL improved Dice from 0.816 to 0.826, Hausdorff from 7.76 to 6.38 mm.

Conclusion: The proposed active learning framework effectively reduces annotation burden while improving segmentation accuracy in low-data regimes. The combination of foundation-model embeddings with clustering for cold-start and uncertainty with spatial diversity for active learning consistently outperforms baseline methods across different medical imaging modalities.

Abstract: Accurate segmentation annotations are critical for disease monitoring, yet manual labeling remains a major bottleneck due to the time and expertise required. Active learning (AL) alleviates this burden by prioritizing informative samples for annotation, typically through a diversity-based cold-start phase followed by uncertainty-driven selection. We propose a novel cold-start sampling strategy that combines foundation-model embeddings with clustering, including automatic selection of the number of clusters and proportional sampling across clusters, to construct a diverse and representative initial training. This is followed by an uncertainty-based AL framework that integrates spatial diversity to guide sample selection. The proposed method is intuitive and interpretable, enabling visualization of the feature-space distribution of candidate samples. We evaluate our approach on three datasets spanning X-ray and MRI modalities. On the CheXmask dataset, the cold-start strategy outperforms random selection, improving Dice from 0.918 to 0.929 and reducing the Hausdorff distance from 32.41 to 27.66 mm. In the AL setting, combined entropy and diversity selection improves Dice from 0.919 to 0.939 and reduces the Hausdorff distance from 30.10 to 19.16 mm. On the Montgomery dataset, cold-start gains are substantial, with Dice improving from 0.928 to 0.950 and Hausdorff distance decreasing from 14.22 to 9.38 mm. On the SynthStrip dataset, cold-start selection slightly affects Dice but reduces the Hausdorff distance from 9.43 to 8.69 mm, while active learning improves Dice from 0.816 to 0.826 and reduces the Hausdorff distance from 7.76 to 6.38 mm. Overall, the proposed framework consistently outperforms baseline methods in low-data regimes, improving segmentation accuracy.

</details>


### [146] [GenAgent: Scaling Text-to-Image Generation via Agentic Multimodal Reasoning](https://arxiv.org/abs/2601.18543)
*Kaixun Jiang,Yuzheng Wang,Junjie Zhou,Pandeng Li,Zhihang Liu,Chen-Wei Xie,Zhaoyu Chen,Yun Zheng,Wenqiang Zhang*

Main category: cs.CV

TL;DR: GenAgent is an agentic multimodal model that decouples visual understanding and generation capabilities, using the model for understanding and treating image generators as invokable tools, enabling autonomous multi-turn interactions with multimodal chains-of-thought for iterative refinement.


<details>
  <summary>Details</summary>
Motivation: Unified visual understanding and generation models face expensive training costs and understanding-generation trade-offs. Existing modular systems are constrained by static pipelines, lacking autonomous multi-turn interaction capabilities for iterative refinement.

Method: Agentic framework decoupling understanding (handled by multimodal model) and generation (image generators as invokable tools). Two-stage training: 1) supervised fine-tuning on tool invocation and reflection data, 2) end-to-end agentic reinforcement learning with pointwise (image quality) and pairwise (reflection accuracy) rewards, plus trajectory resampling for multi-turn exploration.

Result: Significant performance boosts: +23.6% on GenEval++ and +14% on WISE over base generator (FLUX.1-dev). Demonstrates cross-tool generalization, test-time scaling with consistent improvements across interaction rounds, and task-adaptive reasoning.

Conclusion: GenAgent provides an effective agentic framework that unifies visual understanding and generation while avoiding training costs and trade-offs of unified models, enabling autonomous multi-turn interactions with strong performance gains and valuable emergent properties.

Abstract: We introduce GenAgent, unifying visual understanding and generation through an agentic multimodal model. Unlike unified models that face expensive training costs and understanding-generation trade-offs, GenAgent decouples these capabilities through an agentic framework: understanding is handled by the multimodal model itself, while generation is achieved by treating image generation models as invokable tools. Crucially, unlike existing modular systems constrained by static pipelines, this design enables autonomous multi-turn interactions where the agent generates multimodal chains-of-thought encompassing reasoning, tool invocation, judgment, and reflection to iteratively refine outputs. We employ a two-stage training strategy: first, cold-start with supervised fine-tuning on high-quality tool invocation and reflection data to bootstrap agent behaviors; second, end-to-end agentic reinforcement learning combining pointwise rewards (final image quality) and pairwise rewards (reflection accuracy), with trajectory resampling for enhanced multi-turn exploration. GenAgent significantly boosts base generator(FLUX.1-dev) performance on GenEval++ (+23.6\%) and WISE (+14\%). Beyond performance gains, our framework demonstrates three key properties: 1) cross-tool generalization to generators with varying capabilities, 2) test-time scaling with consistent improvements across interaction rounds, and 3) task-adaptive reasoning that automatically adjusts to different tasks. Our code will be available at \href{https://github.com/deep-kaixun/GenAgent}{this url}.

</details>


### [147] [REMAC: Reference-Based Martian Asymmetrical Image Compression](https://arxiv.org/abs/2601.18547)
*Qing Ding,Mai Xu,Shengxi Li,Xin Deng,Xin Zou*

Main category: cs.CV

TL;DR: REMAC is a reference-based Martian image compression method that reduces encoder complexity by 43.51% while improving compression performance by leveraging inter-image similarities from reference images.


<details>
  <summary>Details</summary>
Motivation: Two critical issues hinder existing learned compression methods for Martian images: 1) They overlook highly-limited computational resources on Mars, and 2) They don't utilize strong inter-image similarities across Martian images to advance compression performance.

Method: Proposes reference-based Martian asymmetrical image compression (REMAC) that shifts computational complexity from encoder to decoder. Uses reference-guided entropy module and ref-decoder to leverage inter-image similarities, and deep multi-scale architecture with enlarged receptive field to exploit intra-image similarities. Includes latent feature recycling mechanism to alleviate computational constraints.

Result: REMAC reduces encoder complexity by 43.51% compared to state-of-the-art method while achieving BD-PSNR gain of 0.2664 dB.

Conclusion: REMAC effectively addresses Martian image compression challenges by shifting computational burden to resource-rich Earth-based decoders while leveraging both intra- and inter-image similarities for superior compression performance.

Abstract: To expedite space exploration on Mars, it is indispensable to develop an efficient Martian image compression method for transmitting images through the constrained Mars-to-Earth communication channel. Although the existing learned compression methods have achieved promising results for natural images from earth, there remain two critical issues that hinder their effectiveness for Martian image compression: 1) They overlook the highly-limited computational resources on Mars; 2) They do not utilize the strong \textit{inter-image} similarities across Martian images to advance image compression performance. Motivated by our empirical analysis of the strong \textit{intra-} and \textit{inter-image} similarities from the perspective of texture, color, and semantics, we propose a reference-based Martian asymmetrical image compression (REMAC) approach, which shifts computational complexity from the encoder to the resource-rich decoder and simultaneously improves compression performance. To leverage \textit{inter-image} similarities, we propose a reference-guided entropy module and a ref-decoder that utilize useful information from reference images, reducing redundant operations at the encoder and achieving superior compression performance. To exploit \textit{intra-image} similarities, the ref-decoder adopts a deep, multi-scale architecture with enlarged receptive field size to model long-range spatial dependencies. Additionally, we develop a latent feature recycling mechanism to further alleviate the extreme computational constraints on Mars. Experimental results show that REMAC reduces encoder complexity by 43.51\% compared to the state-of-the-art method, while achieving a BD-PSNR gain of 0.2664 dB.

</details>


### [148] [Automated Landmark Detection for assessing hip conditions: A Cross-Modality Validation of MRI versus X-ray](https://arxiv.org/abs/2601.18555)
*Roberto Di Via,Vito Paolo Pastore,Francesca Odone,Siôn Glyn-Jones,Irina Voiculescu*

Main category: cs.CV

TL;DR: Automated landmark detection on MRI achieves equivalent accuracy to X-ray for cam-type FAI assessment, enabling volumetric analysis and integration into routine MRI workflows.


<details>
  <summary>Details</summary>
Motivation: Current FAI screening relies on angle measurements from X-rays, but assessing the full impingement area requires 3D MRI views. The study aims to validate whether MRI can achieve equivalent clinical accuracy to X-rays for FAI assessment.

Method: Matched-cohort validation study with 89 patients (paired MRI/X-ray) using standard heatmap regression architectures for landmark detection. The approach focuses on coronal views of 3D MRI volumes for cam-type impingement assessment.

Result: MRI achieves equivalent localization and diagnostic accuracy to X-rays for cam-type FAI. The method demonstrates clinical feasibility for FAI assessment in MRI coronal views, enabling potential volumetric analysis through additional landmark placement.

Conclusion: Automated FAI assessment can be integrated into routine MRI workflows, with MRI providing equivalent diagnostic accuracy to X-rays while offering the potential for 3D volumetric analysis of impingement areas.

Abstract: Many clinical screening decisions are based on angle measurements. In particular, FemoroAcetabular Impingement (FAI) screening relies on angles traditionally measured on X-rays. However, assessing the height and span of the impingement area requires also a 3D view through an MRI scan. The two modalities inform the surgeon on different aspects of the condition. In this work, we conduct a matched-cohort validation study (89 patients, paired MRI/X-ray) using standard heatmap regression architectures to assess cross-modality clinical equivalence. Seen that landmark detection has been proven effective on X-rays, we show that MRI also achieves equivalent localisation and diagnostic accuracy for cam-type impingement. Our method demonstrates clinical feasibility for FAI assessment in coronal views of 3D MRI volumes, opening the possibility for volumetric analysis through placing further landmarks. These results support integrating automated FAI assessment into routine MRI workflows. Code is released at https://github.com/Malga-Vision/Landmarks-Hip-Conditions

</details>


### [149] [Generative Diffusion Augmentation with Quantum-Enhanced Discrimination for Medical Image Diagnosis](https://arxiv.org/abs/2601.18556)
*Jingsong Xia,Siqi Wang*

Main category: cs.CV

TL;DR: SDA-QEC framework combines simplified diffusion augmentation with quantum-enhanced classification to handle severe class imbalance in medical image datasets, achieving state-of-the-art performance on coronary angiography classification.


<details>
  <summary>Details</summary>
Motivation: Real-world medical datasets often have severe class imbalance where positive samples outnumber negative samples, leading to biased models with low recall for minority classes. This compromises diagnostic accuracy and poses clinical misdiagnosis risks.

Method: Proposes SDA-QEC: Simplified Diffusion Augmentation with Quantum-Enhanced Classification. Uses lightweight diffusion augmentor to generate synthetic samples for minority classes, then embeds quantum feature layer within MobileNetV2 architecture for enhanced discriminative capability through high-dimensional feature mapping in Hilbert space.

Result: Achieves 98.33% accuracy, 98.78% AUC, and 98.33% F1-score on coronary angiography image classification, outperforming classical baselines (ResNet18, MobileNetV2, DenseNet121, VGG16). Notably achieves balanced 98.33% sensitivity and 98.33% specificity.

Conclusion: Validates feasibility of integrating generative augmentation with quantum-enhanced modeling for medical imaging tasks. Offers novel pathway for developing reliable medical AI systems in small-sample, highly imbalanced, high-risk diagnostic scenarios.

Abstract: In biomedical engineering, artificial intelligence has become a pivotal tool for enhancing medical diagnostics, particularly in medical image classification tasks such as detecting pneumonia from chest X-rays and breast cancer screening. However, real-world medical datasets frequently exhibit severe class imbalance, where positive samples substantially outnumber negative samples, leading to biased models with low recall rates for minority classes. This imbalance not only compromises diagnostic accuracy but also poses clinical misdiagnosis risks. To address this challenge, we propose SDA-QEC (Simplified Diffusion Augmentation with Quantum-Enhanced Classification), an innovative framework that integrates simplified diffusion-based data augmentation with quantum-enhanced feature discrimination. Our approach employs a lightweight diffusion augmentor to generate high-quality synthetic samples for minority classes, rebalancing the training distribution. Subsequently, a quantum feature layer embedded within MobileNetV2 architecture enhances the model's discriminative capability through high-dimensional feature mapping in Hilbert space. Comprehensive experiments on coronary angiography image classification demonstrate that SDA-QEC achieves 98.33% accuracy, 98.78% AUC, and 98.33% F1-score, significantly outperforming classical baselines including ResNet18, MobileNetV2, DenseNet121, and VGG16. Notably, our framework simultaneously attains 98.33% sensitivity and 98.33% specificity, achieving a balanced performance critical for clinical deployment. The proposed method validates the feasibility of integrating generative augmentation with quantum-enhanced modeling in real-world medical imaging tasks, offering a novel research pathway for developing highly reliable medical AI systems in small-sample, highly imbalanced, and high-risk diagnostic scenarios.

</details>


### [150] [AI-enabled Satellite Edge Computing: A Single-Pixel Feature based Shallow Classification Model for Hyperspectral Imaging](https://arxiv.org/abs/2601.18560)
*Li Fang,Tianyu Li,Yanghong Lin,Shudong Zhou,Wei Yao*

Main category: cs.CV

TL;DR: An AI-enabled satellite edge computing paradigm for hyperspectral image classification using lightweight non-deep learning with few-shot learning to enable autonomous decision-making on resource-constrained satellites.


<details>
  <summary>Details</summary>
Motivation: Hyperspectral imaging satellites provide valuable data but face downlink transmission bottlenecks for time-sensitive applications like disaster monitoring. There's a need for onboard autonomous decision-making capabilities despite satellite resource constraints and potential sensor/image quality issues.

Method: Lightweight non-deep learning framework with few-shot learning strategy. Two-stage pixel-wise label propagation using only spectral features: 1) initial labels via anchor-pixel affinity matrix, 2) top-k pruned sparse graph with closed-form solution replacing iterative computations. Rank constraint-based graph clustering for anchor label determination.

Result: Proposed method enables efficient hyperspectral image classification directly on satellites, addressing transmission bottlenecks while handling sensor failures, bad pixels, and mixed noise without requiring spatial structural information like deep neural networks.

Conclusion: The AI-enabled satellite edge computing paradigm with lightweight pixel-wise label propagation provides an effective solution for autonomous hyperspectral image classification on resource-constrained satellite platforms, overcoming downlink limitations for time-critical applications.

Abstract: As the important component of the Earth observation system, hyperspectral imaging satellites provide high-fidelity and enriched information for the formulation of related policies due to the powerful spectral measurement capabilities. However, the transmission speed of the satellite downlink has become a major bottleneck in certain applications, such as disaster monitoring and emergency mapping, which demand a fast response ability. We propose an efficient AI-enabled Satellite Edge Computing paradigm for hyperspectral image classification, facilitating the satellites to attain autonomous decision-making. To accommodate the resource constraints of satellite platforms, the proposed method adopts a lightweight, non-deep learning framework integrated with a few-shot learning strategy. Moreover, onboard processing on satellites could be faced with sensor failure and scan pattern errors, which result in degraded image quality with bad/misaligned pixels and mixed noise. To address these challenges, we develop a novel two-stage pixel-wise label propagation scheme that utilizes only intrinsic spectral features at the single pixel level without the necessity to consider spatial structural information as requested by deep neural networks. In the first stage, initial pixel labels are obtained by propagating selected anchor labels through the constructed anchor-pixel affinity matrix. Subsequently, a top-k pruned sparse graph is generated by directly computing pixel-level similarities. In the second stage, a closed-form solution derived from the sparse graph is employed to replace iterative computations. Furthermore, we developed a rank constraint-based graph clustering algorithm to determine the anchor labels.

</details>


### [151] [Self-Refining Video Sampling](https://arxiv.org/abs/2601.18577)
*Sangwon Jang,Taekyung Ki,Jaehyeong Jo,Saining Xie,Jaehong Yoon,Sung Ju Hwang*

Main category: cs.CV

TL;DR: Self-refining video sampling improves physical realism in video generation by using the pre-trained generator as its own iterative refiner without external verifiers or additional training.


<details>
  <summary>Details</summary>
Motivation: Modern video generators struggle with complex physical dynamics and lack physical realism. Existing approaches using external verifiers or augmented data training are computationally expensive and limited in capturing fine-grained motion.

Method: Interpret the pre-trained video generator as a denoising autoencoder to enable iterative inner-loop refinement at inference time. Introduce uncertainty-aware refinement strategy that selectively refines regions based on self-consistency to prevent over-refinement artifacts.

Result: Significant improvements in motion coherence and physics alignment, achieving over 70% human preference compared to default sampler and guidance-based sampler.

Conclusion: Self-refining video sampling provides an effective, training-free approach to enhance physical realism in video generation by leveraging the generator's own capabilities for iterative refinement.

Abstract: Modern video generators still struggle with complex physical dynamics, often falling short of physical realism. Existing approaches address this using external verifiers or additional training on augmented data, which is computationally expensive and still limited in capturing fine-grained motion. In this work, we present self-refining video sampling, a simple method that uses a pre-trained video generator trained on large-scale datasets as its own self-refiner. By interpreting the generator as a denoising autoencoder, we enable iterative inner-loop refinement at inference time without any external verifier or additional training. We further introduce an uncertainty-aware refinement strategy that selectively refines regions based on self-consistency, which prevents artifacts caused by over-refinement. Experiments on state-of-the-art video generators demonstrate significant improvements in motion coherence and physics alignment, achieving over 70\% human preference compared to the default sampler and guidance-based sampler.

</details>


### [152] [GimmBO: Interactive Generative Image Model Merging via Bayesian Optimization](https://arxiv.org/abs/2601.18585)
*Chenxi Liu,Selena Ling,Alec Jacobson*

Main category: cs.CV

TL;DR: GimmBO: Interactive exploration of adapter merging for image generation using Preferential Bayesian Optimization to efficiently navigate high-dimensional design spaces.


<details>
  <summary>Details</summary>
Motivation: Current manual slider-based tuning for exploring merged adapter combinations scales poorly and makes weight selection difficult, even with limited adapter sets (20-30). There's a need for better exploration methods in the vast continuous design space of merged diffusion model adapters.

Method: Proposes GimmBO using Preferential Bayesian Optimization (PBO) with a two-stage BO backend that improves sampling efficiency and convergence in high-dimensional spaces, addressing sparsity and constrained weight ranges observed in real-world usage.

Result: Evaluation with simulated users and user study shows improved convergence, high success rates, and consistent gains over BO and line-search baselines. The framework demonstrates flexibility through several extensions.

Conclusion: GimmBO effectively supports interactive exploration of adapter merging for image generation, overcoming limitations of manual tuning and enabling efficient navigation of high-dimensional design spaces.

Abstract: Fine-tuning-based adaptation is widely used to customize diffusion-based image generation, leading to large collections of community-created adapters that capture diverse subjects and styles. Adapters derived from the same base model can be merged with weights, enabling the synthesis of new visual results within a vast and continuous design space. To explore this space, current workflows rely on manual slider-based tuning, an approach that scales poorly and makes weight selection difficult, even when the candidate set is limited to 20-30 adapters. We propose GimmBO to support interactive exploration of adapter merging for image generation through Preferential Bayesian Optimization (PBO). Motivated by observations from real-world usage, including sparsity and constrained weight ranges, we introduce a two-stage BO backend that improves sampling efficiency and convergence in high-dimensional spaces. We evaluate our approach with simulated users and a user study, demonstrating improved convergence, high success rates, and consistent gains over BO and line-search baselines, and further show the flexibility of the framework through several extensions.

</details>


### [153] [AGSP-DSA: An Adaptive Graph Signal Processing Framework for Robust Multimodal Fusion with Dynamic Semantic Alignment](https://arxiv.org/abs/2601.18589)
*KV Karthikeya,Ashok Kumar Das,Shantanu Pal,Vivekananda Bhat K,Arun Sekar Rajasekaran*

Main category: cs.CV

TL;DR: AGSP-DSA framework for multimodal fusion uses dual-graph construction, spectral filtering, and semantic attention to achieve state-of-the-art performance on sentiment analysis, event recognition, and multimedia classification tasks.


<details>
  <summary>Details</summary>
Motivation: To perform robust multimodal data fusion over heterogeneous sources (text, audio, images) by addressing the challenges of learning both intra-modal and inter-modal relations while handling missing modalities.

Method: Uses dual-graph construction for intra-modal and inter-modal relations, spectral graph filtering to boost informative signals, Multi-scale GCNs for node embedding, and semantic-aware attention mechanism for dynamic modality contribution based on contextual relevance.

Result: Achieves state-of-the-art performance: 95.3% accuracy, 0.936 F1-score, 0.924 mAP on CMU-MOSEI (2.6% improvement over MM-GNN); 93.4% accuracy, 0.911 F1-score on AVE; 91.8% accuracy, 0.886 F1-score on MM-IMDB, demonstrating good generalization and robustness in missing modality settings.

Conclusion: AGSP-DSA effectively promotes multimodal learning across various applications including sentiment analysis, event recognition, and multimedia classification, showing efficiency in handling heterogeneous multimodal data fusion.

Abstract: In this paper, we introduce an Adaptive Graph Signal Processing with Dynamic Semantic Alignment (AGSP DSA) framework to perform robust multimodal data fusion over heterogeneous sources, including text, audio, and images. The requested approach uses a dual-graph construction to learn both intra-modal and inter-modal relations, spectral graph filtering to boost the informative signals, and effective node embedding with Multi-scale Graph Convolutional Networks (GCNs). Semantic aware attention mechanism: each modality may dynamically contribute to the context with respect to contextual relevance. The experimental outcomes on three benchmark datasets, including CMU-MOSEI, AVE, and MM-IMDB, show that AGSP-DSA performs as the state of the art. More precisely, it achieves 95.3% accuracy, 0.936 F1-score, and 0.924 mAP on CMU-MOSEI, improving MM-GNN by 2.6 percent in accuracy. It gets 93.4% accuracy and 0.911 F1-score on AVE and 91.8% accuracy and 0.886 F1-score on MM-IMDB, which demonstrate good generalization and robustness in the missing modality setting. These findings verify the efficiency of AGSP-DSA in promoting multimodal learning in sentiment analysis, event recognition and multimedia classification.

</details>


### [154] [EFSI-DETR: Efficient Frequency-Semantic Integration for Real-Time Small Object Detection in UAV Imagery](https://arxiv.org/abs/2601.18597)
*Yu Xia,Chang Liu,Tianqi Xiang,Zhigang Tu*

Main category: cs.CV

TL;DR: EFSI-DETR is a novel real-time small object detection framework for UAV imagery that integrates dynamic frequency-spatial guidance with efficient semantic feature enhancement to address challenges in feature representation and multi-scale fusion.


<details>
  <summary>Details</summary>
Motivation: Real-time small object detection in UAV imagery is challenging due to limited feature representation and ineffective multi-scale fusion. Existing methods underutilize frequency information and rely on static convolutional operations, constraining rich feature representations and hindering effective exploitation of deep semantic features.

Method: EFSI-DETR consists of: (1) Dynamic Frequency-Spatial Unified Synergy Network (DyFusNet) that jointly exploits frequency and spatial cues for robust multi-scale feature fusion, (2) Efficient Semantic Feature Concentrator (ESFC) for deep semantic extraction with minimal computational cost, and (3) Fine-grained Feature Retention (FFR) strategy to incorporate spatially rich shallow features during fusion to preserve fine-grained details.

Result: Extensive experiments on VisDrone and CODrone benchmarks show state-of-the-art performance with real-time efficiency: 1.6% improvement in AP and 5.8% improvement in APₛ on VisDrone, while achieving 188 FPS inference speed on a single RTX 4090 GPU.

Conclusion: EFSI-DETR effectively addresses small object detection challenges in UAV imagery by integrating dynamic frequency-spatial guidance with efficient semantic enhancement, achieving superior performance with real-time inference capabilities.

Abstract: Real-time small object detection in Unmanned Aerial Vehicle (UAV) imagery remains challenging due to limited feature representation and ineffective multi-scale fusion. Existing methods underutilize frequency information and rely on static convolutional operations, which constrain the capacity to obtain rich feature representations and hinder the effective exploitation of deep semantic features. To address these issues, we propose EFSI-DETR, a novel detection framework that integrates efficient semantic feature enhancement with dynamic frequency-spatial guidance. EFSI-DETR comprises two main components: (1) a Dynamic Frequency-Spatial Unified Synergy Network (DyFusNet) that jointly exploits frequency and spatial cues for robust multi-scale feature fusion, (2) an Efficient Semantic Feature Concentrator (ESFC) that enables deep semantic extraction with minimal computational cost. Furthermore, a Fine-grained Feature Retention (FFR) strategy is adopted to incorporate spatially rich shallow features during fusion to preserve fine-grained details, crucial for small object detection in UAV imagery. Extensive experiments on VisDrone and CODrone benchmarks demonstrate that our EFSI-DETR achieves the state-of-the-art performance with real-time efficiency, yielding improvement of \textbf{1.6}\% and \textbf{5.8}\% in AP and AP$_{s}$ on VisDrone, while obtaining \textbf{188} FPS inference speed on a single RTX 4090 GPU.

</details>


### [155] [Scale-Aware Self-Supervised Learning for Segmentation of Small and Sparse Structures](https://arxiv.org/abs/2601.18619)
*Jorge Quesada,Ghassan AlRegib*

Main category: cs.CV

TL;DR: Scale-aware SSL adaptation using small-window cropping improves segmentation of small, sparse objects in scientific imaging domains like seismic fault detection and neuroimaging cell delineation.


<details>
  <summary>Details</summary>
Motivation: Standard SSL methods are tuned for large, homogeneous regions but perform poorly on small, sparse, or irregular objects in segmentation tasks. There's a need for SSL approaches that better handle fine-scale structures.

Method: Proposed scale-aware SSL adaptation that integrates small-window cropping into the augmentation pipeline to focus on fine-scale structures during pretraining. This approach zooms in on detailed features rather than large regions.

Result: Consistent improvements over standard and state-of-the-art baselines under label constraints: 13% accuracy improvement for seismic fault segmentation and 5% improvement for neuroimaging cell delineation. Large-scale features saw little benefit.

Conclusion: SSL effectiveness critically depends on object scale. Aligning SSL design with object size and sparsity is a general principle for building more effective representation learning pipelines across scientific imaging domains.

Abstract: Self-supervised learning (SSL) has emerged as a powerful strategy for representation learning under limited annotation regimes, yet its effectiveness remains highly sensitive to many factors, especially the nature of the target task. In segmentation, existing pipelines are typically tuned to large, homogeneous regions, but their performance drops when objects are small, sparse, or locally irregular. In this work, we propose a scale-aware SSL adaptation that integrates small-window cropping into the augmentation pipeline, zooming in on fine-scale structures during pretraining. We evaluate this approach across two domains with markedly different data modalities: seismic imaging, where the goal is to segment sparse faults, and neuroimaging, where the task is to delineate small cellular structures. In both settings, our method yields consistent improvements over standard and state-of-the-art baselines under label constraints, improving accuracy by up to 13% for fault segmentation and 5% for cell delineation. In contrast, large-scale features such as seismic facies or tissue regions see little benefit, underscoring that the value of SSL depends critically on the scale of the target objects. Our findings highlight the need to align SSL design with object size and sparsity, offering a general principle for buil ding more effective representation learning pipelines across scientific imaging domains.

</details>


### [156] [Adaptive Domain Shift in Diffusion Models for Cross-Modality Image Translation](https://arxiv.org/abs/2601.18623)
*Zihao Wang,Yuzhou Chen,Shaogang Ren*

Main category: cs.CV

TL;DR: The paper proposes a new diffusion-based image translation method that uses spatially varying mixing fields and target-consistent restoration to improve efficiency and quality over standard global linear transfer approaches.


<details>
  <summary>Details</summary>
Motivation: Standard diffusion approaches for cross-modal image translation rely on single global linear transfer between domains, which forces the sampler to traverse off-manifold regions, increasing correction burden and causing semantic drift (fixed-schedule domain transfer problem).

Method: The model predicts a spatially varying mixing field at every reverse step and injects an explicit target-consistent restoration term into the drift. This in-step guidance keeps updates on-manifold and shifts the model's role from global alignment to local residual correction. The approach includes a continuous-time formulation with exact solution and a practical first-order sampler that preserves marginal consistency.

Result: Empirical results across translation tasks in medical imaging, remote sensing, and electroluminescence semantic mapping show improved structural fidelity and semantic consistency while converging in fewer denoising steps.

Conclusion: The proposed framework successfully addresses the limitations of fixed-schedule domain transfer by embedding domain-shift dynamics directly into the generative process, leading to more efficient and higher-quality cross-modal image translation.

Abstract: Cross-modal image translation remains brittle and inefficient. Standard diffusion approaches often rely on a single, global linear transfer between domains. We find that this shortcut forces the sampler to traverse off-manifold, high-cost regions, inflating the correction burden and inviting semantic drift. We refer to this shared failure mode as fixed-schedule domain transfer. In this paper, we embed domain-shift dynamics directly into the generative process. Our model predicts a spatially varying mixing field at every reverse step and injects an explicit, target-consistent restoration term into the drift. This in-step guidance keeps large updates on-manifold and shifts the model's role from global alignment to local residual correction. We provide a continuous-time formulation with an exact solution form and derive a practical first-order sampler that preserves marginal consistency. Empirically, across translation tasks in medical imaging, remote sensing, and electroluminescence semantic mapping, our framework improves structural fidelity and semantic consistency while converging in fewer denoising steps.

</details>


### [157] [CONQUER: Context-Aware Representation with Query Enhancement for Text-Based Person Search](https://arxiv.org/abs/2601.18625)
*Zequn Xie*

Main category: cs.CV

TL;DR: CONQUER is a two-stage framework for text-based person search that improves cross-modal alignment during training and adaptively refines queries at inference, achieving state-of-the-art performance on multiple benchmarks.


<details>
  <summary>Details</summary>
Motivation: Text-Based Person Search (TBPS) faces challenges due to cross-modal discrepancies between text and images, and ambiguous/incomplete user queries, limiting its effectiveness for real-world public safety applications.

Method: Two-stage framework: 1) Training stage uses multi-granularity encoding, complementary pair mining, and context-guided optimal transport matching to learn robust embeddings; 2) Inference stage employs plug-and-play query enhancement module with anchor selection and attribute-driven enrichment to refine vague queries.

Result: CONQUER consistently outperforms strong baselines on CUHK-PEDES, ICFG-PEDES, and RSTPReid datasets in both Rank-1 accuracy and mAP, with notable improvements in cross-domain and incomplete-query scenarios.

Conclusion: CONQUER provides a practical and effective solution for real-world TBPS deployment by addressing both training-time cross-modal alignment and inference-time query refinement challenges.

Abstract: Text-Based Person Search (TBPS) aims to retrieve pedestrian images from large galleries using natural language descriptions. This task, essential for public safety applications, is hindered by cross-modal discrepancies and ambiguous user queries. We introduce CONQUER, a two-stage framework designed to address these challenges by enhancing cross-modal alignment during training and adaptively refining queries at inference. During training, CONQUER employs multi-granularity encoding, complementary pair mining, and context-guided optimal matching based on Optimal Transport to learn robust embeddings. At inference, a plug-and-play query enhancement module refines vague or incomplete queries via anchor selection and attribute-driven enrichment, without requiring retraining of the backbone. Extensive experiments on CUHK-PEDES, ICFG-PEDES, and RSTPReid demonstrate that CONQUER consistently outperforms strong baselines in both Rank-1 accuracy and mAP, yielding notable improvements in cross-domain and incomplete-query scenarios. These results highlight CONQUER as a practical and effective solution for real-world TBPS deployment. Source code is available at https://github.com/zqxie77/CONQUER.

</details>


### [158] [Splat-Portrait: Generalizing Talking Heads with Gaussian Splatting](https://arxiv.org/abs/2601.18633)
*Tong Shi,Melonie de Almeida,Daniela Ivanova,Nicolas Pugeault,Paul Henderson*

Main category: cs.CV

TL;DR: Splat-Portrait: A Gaussian-splatting-based method for 3D talking head generation that learns to disentangle a portrait into static 3D reconstruction and background, then generates lip motion from audio without motion priors.


<details>
  <summary>Details</summary>
Motivation: Previous 3D talking head methods rely on domain-specific heuristics and warping-based facial motion priors, leading to inaccurate 3D avatar reconstructions that undermine animation realism.

Method: Uses Gaussian splatting to automatically disentangle a single portrait into static 3D reconstruction (static Gaussian Splatting) and 2D background. Generates natural lip motion conditioned on audio without motion priors. Training uses only 2D reconstruction and score-distillation losses, no 3D supervision or landmarks.

Result: Superior performance on talking head generation and novel view synthesis, achieving better visual quality compared to previous works.

Conclusion: Splat-Portrait effectively addresses 3D head reconstruction and lip motion synthesis challenges without relying on motion priors or 3D supervision, producing more realistic animations.

Abstract: Talking Head Generation aims at synthesizing natural-looking talking videos from speech and a single portrait image. Previous 3D talking head generation methods have relied on domain-specific heuristics such as warping-based facial motion representation priors to animate talking motions, yet still produce inaccurate 3D avatar reconstructions, thus undermining the realism of generated animations. We introduce Splat-Portrait, a Gaussian-splatting-based method that addresses the challenges of 3D head reconstruction and lip motion synthesis. Our approach automatically learns to disentangle a single portrait image into a static 3D reconstruction represented as static Gaussian Splatting, and a predicted whole-image 2D background. It then generates natural lip motion conditioned on input audio, without any motion driven priors. Training is driven purely by 2D reconstruction and score-distillation losses, without 3D supervision nor landmarks. Experimental results demonstrate that Splat-Portrait exhibits superior performance on talking head generation and novel view synthesis, achieving better visual quality compared to previous works. Our project code and supplementary documents are public available at https://github.com/stonewalking/Splat-portrait.

</details>


### [159] [Are Video Generation Models Geographically Fair? An Attraction-Centric Evaluation of Global Visual Knowledge](https://arxiv.org/abs/2601.18698)
*Xiao Liu,Jiawei Zhang*

Main category: cs.CV

TL;DR: The paper introduces GAP, a framework to evaluate geographic equity in text-to-video models, finding Sora 2 shows surprisingly uniform visual knowledge across global regions despite expectations of bias.


<details>
  <summary>Details</summary>
Motivation: To investigate whether text-to-video generation models encode geographically equitable visual knowledge, addressing concerns about geographic bias in AI systems and their global deployment.

Method: Developed Geo-Attraction Landmark Probing (GAP) framework with complementary metrics (structural alignment, keypoint-based alignment, VLM judgments) and created GEOATTRACTION-500 benchmark of 500 globally distributed tourist attractions.

Result: Sora 2 exhibits relatively uniform geographically grounded visual knowledge across regions, development levels, and cultural groupings, with only weak dependence on attraction popularity - contrary to expectations of strong geographic bias.

Conclusion: Current text-to-video models express global visual knowledge more evenly than expected, highlighting promise for globally deployed applications while emphasizing need for continued evaluation as systems evolve.

Abstract: Recent advances in text-to-video generation have produced visually compelling results, yet it remains unclear whether these models encode geographically equitable visual knowledge. In this work, we investigate the geo-equity and geographically grounded visual knowledge of text-to-video models through an attraction-centric evaluation. We introduce Geo-Attraction Landmark Probing (GAP), a systematic framework for assessing how faithfully models synthesize tourist attractions from diverse regions, and construct GEOATTRACTION-500, a benchmark of 500 globally distributed attractions spanning varied regions and popularity levels. GAP integrates complementary metrics that disentangle overall video quality from attraction-specific knowledge, including global structural alignment, fine-grained keypoint-based alignment, and vision-language model judgments, all validated against human evaluation. Applying GAP to the state-of-the-art text-to-video model Sora 2, we find that, contrary to common assumptions of strong geographic bias, the model exhibits a relatively uniform level of geographically grounded visual knowledge across regions, development levels, and cultural groupings, with only weak dependence on attraction popularity. These results suggest that current text-to-video models express global visual knowledge more evenly than expected, highlighting both their promise for globally deployed applications and the need for continued evaluation as such systems evolve.

</details>


### [160] [Low Cost, High Efficiency: LiDAR Place Recognition in Vineyards with Matryoshka Representation Learning](https://arxiv.org/abs/2601.18714)
*Judith Vilella-Cantos,Mauro Martini,Marcello Chiaberge,Mónica Ballesta,David Valiente*

Main category: cs.CV

TL;DR: MinkUNeXt-VINE: A lightweight deep learning method for place recognition in vineyards using sparse LiDAR data, achieving state-of-the-art performance with efficient real-time processing.


<details>
  <summary>Details</summary>
Motivation: Agricultural environments like vineyards are challenging for robot localization due to their unstructured nature and lack of distinctive landmarks. Current place recognition methods struggle in these settings despite existing work on classification and segmentation.

Method: Proposes MinkUNeXt-VINE, a lightweight deep learning approach with specialized pre-processing and Matryoshka Representation Learning multi-loss strategy. Designed to work with low-cost, sparse LiDAR inputs and produce low-dimensionality outputs for real-time efficiency.

Result: Surpasses state-of-the-art methods in vineyard environments, demonstrates robust performance with low-cost/low-resolution LiDAR data, and shows efficient trade-off between performance and computational requirements. Validated on two extensive long-term vineyard datasets with different LiDAR sensors.

Conclusion: The method provides an effective solution for place recognition in agricultural settings, balancing performance with practical constraints of real-time operation and cost-effective sensor requirements. Code is publicly available for reproduction.

Abstract: Localization in agricultural environments is challenging due to their unstructured nature and lack of distinctive landmarks. Although agricultural settings have been studied in the context of object classification and segmentation, the place recognition task for mobile robots is not trivial in the current state of the art. In this study, we propose MinkUNeXt-VINE, a lightweight, deep-learning-based method that surpasses state-of-the-art methods in vineyard environments thanks to its pre-processing and Matryoshka Representation Learning multi-loss approach. Our method prioritizes enhanced performance with low-cost, sparse LiDAR inputs and lower-dimensionality outputs to ensure high efficiency in real-time scenarios. Additionally, we present a comprehensive ablation study of the results on various evaluation cases and two extensive long-term vineyard datasets employing different LiDAR sensors. The results demonstrate the efficiency of the trade-off output produced by this approach, as well as its robust performance on low-cost and low-resolution input data. The code is publicly available for reproduction.

</details>


### [161] [SeNeDiF-OOD: Semantic Nested Dichotomy Fusion for Out-of-Distribution Detection Methodology in Open-World Classification. A Case Study on Monument Style Classification](https://arxiv.org/abs/2601.18739)
*Ignacio Antequera-Sánchez,Juan Luis Suárez-Díaz,Rosana Montes,Francisco Herrera*

Main category: cs.CV

TL;DR: SeNeDiF-OOD: A hierarchical Semantic Nested Dichotomy Fusion framework for OOD detection that decomposes detection into binary fusion nodes to handle heterogeneous OOD data, validated on MonuMAI architectural style recognition system.


<details>
  <summary>Details</summary>
Motivation: Current OOD detection methods struggle with heterogeneous OOD data (low-level corruption to semantic shifts) in open-world environments, and single-stage detectors often fail to address this complexity.

Method: Proposes SeNeDiF-OOD based on Semantic Nested Dichotomy Fusion - decomposes detection task into hierarchical structure of binary fusion nodes, each layer integrates decision boundaries aligned with specific semantic abstraction levels.

Result: Extensive experimental evaluation on MonuMAI architectural style recognition system shows hierarchical fusion methodology significantly outperforms traditional baselines, effectively filters diverse OOD categories while preserving in-distribution performance.

Conclusion: SeNeDiF-OOD provides effective solution for heterogeneous OOD detection in open-world environments through hierarchical semantic decomposition and fusion, validated on real-world architectural recognition application.

Abstract: Out-of-distribution (OOD) detection is a fundamental requirement for the reliable deployment of artificial intelligence applications in open-world environments. However, addressing the heterogeneous nature of OOD data, ranging from low-level corruption to semantic shifts, remains a complex challenge that single-stage detectors often fail to resolve. To address this issue, we propose SeNeDiF-OOD, a novel methodology based on Semantic Nested Dichotomy Fusion. This framework decomposes the detection task into a hierarchical structure of binary fusion nodes, where each layer is designed to integrate decision boundaries aligned with specific levels of semantic abstraction. To validate the proposed framework, we present a comprehensive case study using MonuMAI, a real-world architectural style recognition system exposed to an open environment. This application faces a diverse range of inputs, including non-monument images, unknown architectural styles, and adversarial attacks, making it an ideal testbed for our proposal. Through extensive experimental evaluation in this domain, results demonstrate that our hierarchical fusion methodology significantly outperforms traditional baselines, effectively filtering these diverse OOD categories while preserving in-distribution performance.

</details>
