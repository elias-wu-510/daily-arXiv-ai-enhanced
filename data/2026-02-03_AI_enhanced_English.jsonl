{"id": "2602.00095", "categories": ["cs.CV", "cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2602.00095", "abs": "https://arxiv.org/abs/2602.00095", "authors": ["Weiyu Sun", "Liangliang Chen", "Yongnuo Cai", "Huiru Xie", "Yi Zeng", "Ying Zhang"], "title": "EDU-CIRCUIT-HW: Evaluating Multimodal Large Language Models on Real-World University-Level STEM Student Handwritten Solutions", "comment": null, "summary": "Multimodal Large Language Models (MLLMs) hold significant promise for revolutionizing traditional education and reducing teachers' workload. However, accurately interpreting unconstrained STEM student handwritten solutions with intertwined mathematical formulas, diagrams, and textual reasoning poses a significant challenge due to the lack of authentic and domain-specific benchmarks. Additionally, current evaluation paradigms predominantly rely on the outcomes of downstream tasks (e.g., auto-grading), which often probe only a subset of the recognized content, thereby failing to capture the MLLMs' understanding of complex handwritten logic as a whole. To bridge this gap, we release EDU-CIRCUIT-HW, a dataset consisting of 1,300+ authentic student handwritten solutions from a university-level STEM course. Utilizing the expert-verified verbatim transcriptions and grading reports of student solutions, we simultaneously evaluate various MLLMs' upstream recognition fidelity and downstream auto-grading performance. Our evaluation uncovers an astonishing scale of latent failures within MLLM-recognized student handwritten content, highlighting the models' insufficient reliability for auto-grading and other understanding-oriented applications in high-stakes educational settings. In solution, we present a case study demonstrating that leveraging identified error patterns to preemptively detect and rectify recognition errors, with only minimal human intervention (approximately 4% of the total solutions), can significantly enhance the robustness of the deployed AI-enabled grading system on unseen student solutions.", "AI": {"tldr": "EDU-CIRCUIT-HW dataset reveals MLLMs' poor performance in recognizing authentic student handwritten STEM solutions, with latent recognition errors undermining auto-grading reliability.", "motivation": "Current MLLMs struggle with authentic student handwritten STEM solutions containing intertwined math formulas, diagrams, and text. Existing benchmarks lack domain-specific authentic data, and evaluation focuses only on downstream tasks like auto-grading, failing to assess holistic understanding of handwritten logic.", "method": "Released EDU-CIRCUIT-HW dataset with 1,300+ authentic student handwritten solutions from university STEM courses. Used expert-verified transcriptions and grading reports to simultaneously evaluate MLLMs' upstream recognition fidelity and downstream auto-grading performance.", "result": "Evaluation uncovered astonishing scale of latent failures in MLLM-recognized content, showing insufficient reliability for auto-grading in high-stakes educational settings. Case study demonstrated that using error patterns to detect/rectify recognition errors with minimal human intervention (4% of solutions) significantly enhanced grading system robustness.", "conclusion": "MLLMs currently lack reliability for authentic student handwritten solution interpretation in STEM education. Proactive error detection and correction with minimal human oversight can improve system robustness, but fundamental recognition challenges remain for high-stakes educational applications."}}
{"id": "2602.00096", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.00096", "abs": "https://arxiv.org/abs/2602.00096", "authors": ["Zhengqing Gao", "Ziwen Li", "Xin Wang", "Jiaxin Huang", "Zhenyang Ren", "Mingkai Shao", "Hanlue Zhang", "Tianyu Huang", "Yongkang Cheng", "Yandong Guo", "Runqi Lin", "Yuanyuan Wang", "Tongliang Liu", "Kun Zhang", "Mingming Gong"], "title": "Mirage2Matter: A Physically Grounded Gaussian World Model from Video", "comment": null, "summary": "The scalability of embodied intelligence is fundamentally constrained by the scarcity of real-world interaction data. While simulation platforms provide a promising alternative, existing approaches often suffer from a substantial visual and physical gap to real environments and rely on expensive sensors, precise robot calibration, or depth measurements, limiting their practicality at scale. We present Simulate Anything, a graphics-driven world modeling and simulation framework that enables efficient generation of high-fidelity embodied training data using only multi-view environment videos and off-the-shelf assets. Our approach reconstructs real-world environments into a photorealistic scene representation using 3D Gaussian Splatting (3DGS), seamlessly capturing fine-grained geometry and appearance from video. We then leverage generative models to recover a physically realistic representation and integrate it into a simulation environment via a precision calibration target, enabling accurate scale alignment between the reconstructed scene and the real world. Together, these components provide a unified, editable, and physically grounded world model. Vision Language Action (VLA) models trained on our simulated data achieve strong zero-shot performance on downstream tasks, matching or even surpassing results obtained with real-world data, highlighting the potential of reconstruction-driven world modeling for scalable and practical embodied intelligence training.", "AI": {"tldr": "Simulate Anything is a graphics-driven world modeling framework that uses multi-view videos and 3D Gaussian Splatting to create high-fidelity simulated training data for embodied AI, achieving performance comparable to real-world data.", "motivation": "The scarcity of real-world interaction data limits embodied intelligence scalability. Existing simulation approaches suffer from visual/physical gaps, require expensive sensors/calibration, and lack practicality at scale.", "method": "Uses 3D Gaussian Splatting to reconstruct photorealistic scenes from multi-view videos, then applies generative models to recover physically realistic representations. Integrates into simulation via precision calibration targets for accurate scale alignment, creating unified, editable world models.", "result": "Vision Language Action models trained on the simulated data achieve strong zero-shot performance on downstream tasks, matching or even surpassing results obtained with real-world data.", "conclusion": "The framework demonstrates the potential of reconstruction-driven world modeling for scalable and practical embodied intelligence training, bridging the gap between simulation and reality."}}
{"id": "2602.00104", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.00104", "abs": "https://arxiv.org/abs/2602.00104", "authors": ["Zhuohong Chen", "Zhengxian Wu", "Zirui Liao", "Shenao Jiang", "Hangrui Xu", "Yang Chen", "Chaokui Su", "Xiaoyu Liu", "Haoqian Wang"], "title": "R3G: A Reasoning--Retrieval--Reranking Framework for Vision-Centric Answer Generation", "comment": null, "summary": "Vision-centric retrieval for VQA requires retrieving images to supply missing visual cues and integrating them into the reasoning process. However, selecting the right images and integrating them effectively into the model's reasoning remains challenging.To address this challenge, we propose R3G, a modular Reasoning-Retrieval-Reranking framework.It first produces a brief reasoning plan that specifies the required visual cues, then adopts a two-stage strategy, with coarse retrieval followed by fine-grained reranking, to select evidence images.On MRAG-Bench, R3G improves accuracy across six MLLM backbones and nine sub-scenarios, achieving state-of-the-art overall performance. Ablations show that sufficiency-aware reranking and reasoning steps are complementary, helping the model both choose the right images and use them well. We release code and data at https://github.com/czh24/R3G.", "AI": {"tldr": "R3G is a modular Reasoning-Retrieval-Reranking framework for vision-centric VQA that uses reasoning plans to guide two-stage image retrieval and achieves SOTA performance on MRAG-Bench.", "motivation": "Vision-centric VQA requires retrieving relevant images to provide missing visual information, but current approaches struggle with selecting appropriate images and effectively integrating them into reasoning processes.", "method": "R3G framework: 1) Generates reasoning plan specifying needed visual cues, 2) Two-stage retrieval: coarse retrieval followed by fine-grained reranking to select evidence images, 3) Uses sufficiency-aware reranking and reasoning steps.", "result": "Improves accuracy across six MLLM backbones and nine sub-scenarios on MRAG-Bench, achieving state-of-the-art overall performance. Ablations show reasoning and reranking are complementary.", "conclusion": "R3G effectively addresses vision-centric VQA retrieval challenges by combining reasoning-guided retrieval with two-stage selection, enabling better image selection and utilization."}}
{"id": "2602.00105", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.00105", "abs": "https://arxiv.org/abs/2602.00105", "authors": ["Wing Chan", "Richard Allen"], "title": "HYPE-EDIT-1: Benchmark for Measuring Reliability in Frontier Image Editing Models", "comment": "14 pages, 5 figures, for code and data, see https://github.com/sourceful-official/hype-edit-1-benchmark", "summary": "Public demos of image editing models are typically best-case samples; real workflows pay for retries and review time. We introduce HYPE-EDIT-1, a 100-task benchmark of reference-based marketing/design edits with binary pass/fail judging. For each task we generate 10 independent outputs to estimate per-attempt pass rate, pass@10, expected attempts under a retry cap, and an effective cost per successful edit that combines model price with human review time. We release 50 public tasks and maintain a 50-task held-out private split for server-side evaluation, plus a standardized JSON schema and tooling for VLM and human-based judging. Across the evaluated models, per-attempt pass rates span 34-83 percent and effective cost per success spans USD 0.66-1.42. Models that have low per-image pricing are more expensive when you consider the total effective cost of retries and human reviews.", "AI": {"tldr": "HYPE-EDIT-1 is a 100-task benchmark for evaluating image editing models on real-world marketing/design workflows, measuring not just quality but also practical costs including retries and human review time.", "motivation": "Public demos of image editing models show best-case samples, but real workflows involve costs from retries and review time. There's a need for benchmarks that capture these practical considerations beyond just quality metrics.", "method": "Created a 100-task benchmark with reference-based marketing/design edits using binary pass/fail judging. For each task, generate 10 independent outputs to estimate per-attempt pass rate, pass@10, expected attempts under retry cap, and effective cost per successful edit combining model price with human review time.", "result": "Across evaluated models, per-attempt pass rates span 34-83% and effective cost per success spans USD $0.66-$1.42. Models with low per-image pricing become more expensive when considering total effective costs of retries and human reviews.", "conclusion": "The benchmark reveals that focusing only on per-image pricing is misleading; effective cost including retries and human review time provides a more realistic measure of model performance for practical applications."}}
{"id": "2602.00107", "categories": ["cs.CV", "cs.RO", "eess.IV"], "pdf": "https://arxiv.org/pdf/2602.00107", "abs": "https://arxiv.org/abs/2602.00107", "authors": ["Yuan Gao", "Xinyu Guo", "Wenjing Xie", "Zifan Wang", "Hongwen Yu", "Gongyang Li", "Shugong Xu"], "title": "Efficient UAV trajectory prediction: A multi-modal deep diffusion framework", "comment": "in Chinese language", "summary": "To meet the requirements for managing unauthorized UAVs in the low-altitude economy, a multi-modal UAV trajectory prediction method based on the fusion of LiDAR and millimeter-wave radar information is proposed. A deep fusion network for multi-modal UAV trajectory prediction, termed the Multi-Modal Deep Fusion Framework, is designed. The overall architecture consists of two modality-specific feature extraction networks and a bidirectional cross-attention fusion module, aiming to fully exploit the complementary information of LiDAR and radar point clouds in spatial geometric structure and dynamic reflection characteristics. In the feature extraction stage, the model employs independent but structurally identical feature encoders for LiDAR and radar. After feature extraction, the model enters the Bidirectional Cross-Attention Mechanism stage to achieve information complementarity and semantic alignment between the two modalities. To verify the effectiveness of the proposed model, the MMAUD dataset used in the CVPR 2024 UG2+ UAV Tracking and Pose-Estimation Challenge is adopted as the training and testing dataset. Experimental results show that the proposed multi-modal fusion model significantly improves trajectory prediction accuracy, achieving a 40% improvement compared to the baseline model. In addition, ablation experiments are conducted to demonstrate the effectiveness of different loss functions and post-processing strategies in improving model performance. The proposed model can effectively utilize multi-modal data and provides an efficient solution for unauthorized UAV trajectory prediction in the low-altitude economy.", "AI": {"tldr": "A multi-modal UAV trajectory prediction method using LiDAR and millimeter-wave radar fusion achieves 40% accuracy improvement over baseline models.", "motivation": "To address the need for managing unauthorized UAVs in the low-altitude economy by improving trajectory prediction accuracy through multi-modal sensor fusion.", "method": "Proposes a Multi-Modal Deep Fusion Framework with two modality-specific feature extraction networks (LiDAR and radar) and a bidirectional cross-attention fusion module to exploit complementary spatial geometric and dynamic reflection characteristics.", "result": "Achieves 40% improvement in trajectory prediction accuracy compared to baseline models on the MMAUD dataset from CVPR 2024 UG2+ challenge. Ablation studies confirm effectiveness of different loss functions and post-processing strategies.", "conclusion": "The proposed multi-modal fusion model effectively utilizes complementary sensor data and provides an efficient solution for unauthorized UAV trajectory prediction in low-altitude applications."}}
{"id": "2602.00108", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.00108", "abs": "https://arxiv.org/abs/2602.00108", "authors": ["Ren\u00e9 Peinl", "Vincent Tischler", "Patrick Schr\u00f6der", "Christian Groth"], "title": "SITUATE -- Synthetic Object Counting Dataset for VLM training", "comment": "accepted at 21st International Conference on Computer Vision Theory and Applications", "summary": "We present SITUATE, a novel dataset designed for training and evaluating Vision Language Models on counting tasks with spatial constraints. The dataset bridges the gap between simple 2D datasets like VLMCountBench and often ambiguous real-life datasets like TallyQA, which lack control over occlusions and spatial composition. Experiments show that our dataset helps to improve generalization for out-of-distribution images, since a finetune of Qwen VL 2.5 7B on SITUATE improves accuracy on the Pixmo count test data, but not vice versa. We cross validate this by comparing the model performance across established other counting benchmarks and against an equally sized fine-tuning set derived from Pixmo count.", "AI": {"tldr": "SITUATE is a new dataset for training VLMs on counting tasks with spatial constraints, bridging the gap between simple 2D datasets and ambiguous real-life datasets.", "motivation": "Existing counting datasets have limitations: simple 2D datasets like VLMCountBench lack real-world complexity, while real-life datasets like TallyQA lack control over occlusions and spatial composition. There's a need for a dataset that bridges this gap for better VLM training on counting tasks.", "method": "The authors created the SITUATE dataset specifically designed for counting tasks with spatial constraints. They conducted experiments by fine-tuning Qwen VL 2.5 7B on SITUATE and comparing performance against other datasets, including cross-validation across established counting benchmarks and comparison with equally sized fine-tuning sets from Pixmo count.", "result": "Fine-tuning on SITUATE improves accuracy on Pixmo count test data, but fine-tuning on Pixmo count does not improve performance on SITUATE. The dataset helps improve generalization for out-of-distribution images, as validated through cross-benchmark comparisons.", "conclusion": "SITUATE is an effective dataset for training VLMs on counting tasks with spatial constraints, offering better generalization capabilities than existing datasets and demonstrating asymmetric transfer benefits when used for fine-tuning."}}
{"id": "2602.00109", "categories": ["cs.CV", "eess.IV"], "pdf": "https://arxiv.org/pdf/2602.00109", "abs": "https://arxiv.org/abs/2602.00109", "authors": ["John J. Howard", "Richard O. Plesh", "Yevgeniy B. Sirotin", "Jerry L. Tipton", "Arun R. Vemury"], "title": "Robustness of Presentation Attack Detection in Remote Identity Validation Scenarios", "comment": "Accepted to the IEEE/CVF WACV 2026 Workshop on Generative, Adversarial and Presentation Attacks in Biometrics (GAPBio). 8 pages, 6 figures, 4 tables", "summary": "Presentation attack detection (PAD) subsystems are an important part of effective and user-friendly remote identity validation (RIV) systems. However, ensuring robust performance across diverse environmental and procedural conditions remains a critical challenge. This paper investigates the impact of low-light conditions and automated image acquisition on the robustness of commercial PAD systems using a scenario test of RIV. Our results show that PAD systems experience a significant decline in performance when utilized in low-light or auto-capture scenarios, with a model-predicted increase in error rates by a factor of about four under low-light conditions and a doubling of those odds under auto-capture workflows. Specifically, only one of the tested systems was robust to these perturbations, maintaining a maximum bona fide presentation classification error rate below 3% across all scenarios. Our findings emphasize the importance of testing across diverse environments to ensure robust and reliable PAD performance in real-world applications.", "AI": {"tldr": "Commercial PAD systems show significant performance degradation in low-light and auto-capture scenarios, with error rates increasing 4x and 2x respectively, highlighting the need for diverse environmental testing.", "motivation": "Presentation attack detection (PAD) is crucial for remote identity validation systems, but ensuring robust performance across diverse environmental and procedural conditions remains a critical challenge that needs investigation.", "method": "The paper investigates the impact of low-light conditions and automated image acquisition on commercial PAD systems using scenario testing of remote identity validation (RIV) systems.", "result": "PAD systems experience significant performance decline in low-light (4x error rate increase) and auto-capture (2x error rate increase) scenarios. Only one tested system maintained robust performance with <3% error rate across all scenarios.", "conclusion": "Testing across diverse environments is essential to ensure robust and reliable PAD performance in real-world applications, as most commercial systems are vulnerable to environmental and procedural variations."}}
{"id": "2602.00110", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.00110", "abs": "https://arxiv.org/abs/2602.00110", "authors": ["Yu Li", "Guilherme N. DeSouza", "Praveen Rao", "Chi-Ren Shyu"], "title": "Observing Health Outcomes Using Remote Sensing Imagery and Geo-Context Guided Visual Transformer", "comment": "Submitted to IEEE Transactions on Geoscience and Remote Sensing", "summary": "Visual transformers have driven major progress in remote sensing image analysis, particularly in object detection and segmentation. Recent vision-language and multimodal models further extend these capabilities by incorporating auxiliary information, including captions, question and answer pairs, and metadata, which broadens applications beyond conventional computer vision tasks. However, these models are typically optimized for semantic alignment between visual and textual content rather than geospatial understanding, and therefore are not suited for representing or reasoning with structured geospatial layers. In this study, we propose a novel model that enhances remote sensing imagery processing with guidance from auxiliary geospatial information. Our approach introduces a geospatial embedding mechanism that transforms diverse geospatial data into embedding patches that are spatially aligned with image patches. To facilitate cross-modal interaction, we design a guided attention module that dynamically integrates multimodal information by computing attention weights based on correlations with auxiliary data, thereby directing the model toward the most relevant regions. In addition, the module assigns distinct roles to individual attention heads, allowing the model to capture complementary aspects of the guidance information and improving the interpretability of its predictions. Experimental results demonstrate that the proposed framework outperforms existing pretrained geospatial foundation models in predicting disease prevalence, highlighting its effectiveness in multimodal geospatial understanding.", "AI": {"tldr": "A novel transformer model that enhances remote sensing image analysis by incorporating geospatial data through aligned embeddings and guided attention mechanisms, outperforming existing geospatial foundation models in disease prevalence prediction.", "motivation": "Current vision-language models for remote sensing focus on semantic alignment between visual and textual content but lack proper geospatial understanding. They cannot effectively represent or reason with structured geospatial layers, limiting their applicability to tasks requiring spatial reasoning.", "method": "Proposes a model with: 1) Geospatial embedding mechanism that transforms diverse geospatial data into embedding patches spatially aligned with image patches; 2) Guided attention module that dynamically integrates multimodal information by computing attention weights based on correlations with auxiliary data; 3) Role assignment to individual attention heads to capture complementary aspects of guidance information and improve interpretability.", "result": "The proposed framework outperforms existing pretrained geospatial foundation models in predicting disease prevalence, demonstrating its effectiveness in multimodal geospatial understanding.", "conclusion": "The model successfully bridges the gap between visual transformers and geospatial reasoning by incorporating structured geospatial data through novel embedding and attention mechanisms, enabling more effective multimodal geospatial analysis for applications like disease prevalence prediction."}}
{"id": "2602.00111", "categories": ["cs.CV", "eess.IV"], "pdf": "https://arxiv.org/pdf/2602.00111", "abs": "https://arxiv.org/abs/2602.00111", "authors": ["Haiyu Yang", "Heidi Lesscher", "Enhong Liu", "Miel Hostens"], "title": "From Manual Observation to Automated Monitoring: Space Allowance Effects on Play Behaviour in Group-Housed Dairy Calves", "comment": null, "summary": "Play behaviour serves as a positive welfare indicator in dairy calves, yet the influence of space allowance under commercial conditions remains poorly characterized, particularly at intermediate-to-high allowances (6-20 m2 per calf). This study investigated the relationship between space allowance and play behaviour in 60 group-housed dairy calves across 14 commercial farms in the Netherlands (space range: 2.66-17.98 m2 per calf), and developed an automated computer vision pipeline for scalable monitoring. Video observations were analyzed using a detailed ethogram, with play expressed as percentage of observation period (%OP). Statistical analysis employed linear mixed models with farm as a random effect. A computer vision pipeline was trained on manual annotations from 108 hours on 6 farms and validated on held-out test data. The computer vision classifier achieved 97.6% accuracy with 99.4% recall for active play detection. Calves spent on average 1.0% of OP playing reflecting around 10 minutes per 17-hour period. The space-play relationship was non-linear, with highest play levels at 8-10 m2 per calf (1.6% OP) and lowest at 6-8 m2 and 12-14 m2 (<0.6% OP). Space remained significant after controlling for age, health, and group size. In summary, these findings suggest that 8-10 m2 per calf represents a practical target balancing welfare benefits with economic feasibility, and demonstrate that automated monitoring can scale small annotation projects to continuous welfare assessment systems.", "AI": {"tldr": "Automated computer vision pipeline detects dairy calf play behavior with 97.6% accuracy; optimal space allowance for play is 8-10 m\u00b2 per calf, showing non-linear relationship with welfare benefits.", "motivation": "Play behavior is a positive welfare indicator in dairy calves, but the influence of space allowance under commercial conditions (especially at intermediate-to-high allowances of 6-20 m\u00b2 per calf) remains poorly characterized. There's also a need for scalable automated monitoring systems for continuous welfare assessment.", "method": "Studied 60 group-housed dairy calves across 14 commercial farms in the Netherlands with space range of 2.66-17.98 m\u00b2 per calf. Used video observations analyzed with detailed ethogram (play expressed as % of observation period). Employed linear mixed models with farm as random effect. Developed automated computer vision pipeline trained on 108 hours of manual annotations from 6 farms and validated on held-out test data.", "result": "Computer vision classifier achieved 97.6% accuracy with 99.4% recall for active play detection. Calves spent average 1.0% of observation period playing (~10 minutes per 17-hour period). Space-play relationship was non-linear: highest play levels at 8-10 m\u00b2 per calf (1.6% OP), lowest at 6-8 m\u00b2 and 12-14 m\u00b2 (<0.6% OP). Space remained significant after controlling for age, health, and group size.", "conclusion": "8-10 m\u00b2 per calf represents a practical target balancing welfare benefits with economic feasibility. Automated computer vision monitoring can scale small annotation projects to continuous welfare assessment systems for commercial dairy operations."}}
{"id": "2602.00113", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.00113", "abs": "https://arxiv.org/abs/2602.00113", "authors": ["S. Kalaycioglu", "C. Hong", "K. Zhai", "H. Xie", "J. N. Wong"], "title": "AI-Driven Three-Dimensional Reconstruction and Quantitative Analysis for Burn Injury Assessment", "comment": "11 pages and 5 figures", "summary": "Accurate, reproducible burn assessment is critical for treatment planning, healing monitoring, and medico-legal documentation, yet conventional visual inspection and 2D photography are subjective and limited for longitudinal comparison. This paper presents an AI-enabled burn assessment and management platform that integrates multi-view photogrammetry, 3D surface reconstruction, and deep learning-based segmentation within a structured clinical workflow. Using standard multi-angle images from consumer-grade cameras, the system reconstructs patient-specific 3D burn surfaces and maps burn regions onto anatomy to compute objective metrics in real-world units, including surface area, TBSA, depth-related geometric proxies, and volumetric change. Successive reconstructions are spatially aligned to quantify healing progression over time, enabling objective tracking of wound contraction and depth reduction. The platform also supports structured patient intake, guided image capture, 3D analysis and visualization, treatment recommendations, and automated report generation. Simulation-based evaluation demonstrates stable reconstructions, consistent metric computation, and clinically plausible longitudinal trends, supporting a scalable, non-invasive approach to objective, geometry-aware burn assessment and decision support in acute and outpatient care.", "AI": {"tldr": "AI platform for objective burn assessment using 3D reconstruction from standard photos to compute burn metrics and track healing over time.", "motivation": "Current burn assessment methods (visual inspection, 2D photography) are subjective and inadequate for longitudinal comparison, creating challenges for treatment planning, healing monitoring, and medico-legal documentation.", "method": "Integrates multi-view photogrammetry, 3D surface reconstruction, and deep learning-based segmentation within clinical workflow. Uses standard multi-angle images from consumer cameras to reconstruct patient-specific 3D burn surfaces and map burn regions onto anatomy.", "result": "System computes objective metrics in real-world units (surface area, TBSA, depth-related geometric proxies, volumetric change) and enables spatial alignment of successive reconstructions to quantify healing progression over time.", "conclusion": "Simulation-based evaluation shows stable reconstructions, consistent metric computation, and clinically plausible longitudinal trends, supporting a scalable, non-invasive approach to objective, geometry-aware burn assessment and decision support."}}
{"id": "2602.00114", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.00114", "abs": "https://arxiv.org/abs/2602.00114", "authors": ["Yunwei Bai", "Ying Kiat Tan", "Yao Shu", "Tsuhan Chen"], "title": "1S-DAug: One-Shot Data Augmentation for Robust Few-Shot Generalization", "comment": null, "summary": "Few-shot learning (FSL) challenges model generalization to novel classes based on just a few shots of labeled examples, a testbed where traditional test-time augmentations fail to be effective. We introduce 1S-DAug, a one-shot generative augmentation operator that synthesizes diverse yet faithful variants from just one example image at test time. 1S-DAug couples traditional geometric perturbations with controlled noise injection and a denoising diffusion process conditioned on the original image. The generated images are then encoded and aggregated, alongside the original image, into a combined representation for more robust FSL predictions. Integrated as a training-free model-agnostic plugin, 1S-DAug consistently improves FSL across standard benchmarks of 4 different datasets without any model parameter update, including achieving over 10% proportional accuracy improvement on the miniImagenet 5-way-1-shot benchmark. Codes will be released.", "AI": {"tldr": "1S-DAug is a one-shot generative augmentation method that creates diverse image variants from a single test example using geometric perturbations, noise injection, and diffusion denoising to improve few-shot learning performance without model retraining.", "motivation": "Traditional test-time augmentations fail in few-shot learning scenarios where only a few labeled examples are available for novel classes. There's a need for effective augmentation methods that can work with minimal data at test time to improve model generalization.", "method": "1S-DAug combines geometric perturbations with controlled noise injection and a denoising diffusion process conditioned on the original image. It generates diverse yet faithful image variants from just one example, encodes them, and aggregates representations alongside the original for more robust predictions.", "result": "The method consistently improves few-shot learning across 4 standard benchmark datasets without model parameter updates, achieving over 10% proportional accuracy improvement on the miniImagenet 5-way-1-shot benchmark.", "conclusion": "1S-DAug is an effective training-free, model-agnostic plugin that enhances few-shot learning by generating diverse yet faithful augmentations from single test examples, demonstrating significant performance improvements across multiple benchmarks."}}
{"id": "2602.00115", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.00115", "abs": "https://arxiv.org/abs/2602.00115", "authors": ["David El-Chai Ben-Ezra", "Adar Tal", "Daniel Brisk"], "title": "Event Driven Clustering Algorithm", "comment": "~10 pages, 2 figures", "summary": "This paper introduces a novel asynchronous, event-driven algorithm for real-time detection of small event clusters in event camera data. Like other hierarchical agglomerative clustering algorithms, the algorithm detects the event clusters based on their tempo-spatial distance. However, the algorithm leverages the special asynchronous data structure of event camera, and by a sophisticated, efficient and simple decision-making, enjoys a linear complexity of $O(n)$ where $n$ is the events amount. In addition, the run-time of the algorithm is independent with the dimensions of the pixels array.", "AI": {"tldr": "Novel asynchronous event-driven algorithm for real-time detection of small event clusters in event camera data with linear O(n) complexity.", "motivation": "Need for efficient real-time detection of event clusters in event camera data, leveraging the asynchronous nature of event cameras while maintaining computational efficiency.", "method": "Asynchronous, event-driven hierarchical agglomerative clustering algorithm that detects clusters based on tempo-spatial distance, using sophisticated but simple decision-making to achieve linear complexity.", "result": "Algorithm achieves O(n) linear complexity where n is number of events, with runtime independent of pixel array dimensions, enabling real-time performance.", "conclusion": "The proposed algorithm provides an efficient solution for real-time event cluster detection in event cameras with linear complexity and dimension-independent runtime."}}
{"id": "2602.00117", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.00117", "abs": "https://arxiv.org/abs/2602.00117", "authors": ["Lamia Lahouel", "Laurynas Lopata", "Simon Gruening", "Gabriele Meoni", "Gaetan Petit", "Sylvain Lobry"], "title": "IC-EO: Interpretable Code-based assistant for Earth Observation", "comment": "15 pages, 1 figure", "summary": "Despite recent advances in computer vision, Earth Observation (EO) analysis remains difficult to perform for the laymen, requiring expert knowledge and technical capabilities. Furthermore, many systems return black-box predictions that are difficult to audit or reproduce. Leveraging recent advances in tool LLMs, this study proposes a conversational, code-generating agent that transforms natural-language queries into executable, auditable Python workflows. The agent operates over a unified easily extendable API for classification, segmentation, detection (oriented bounding boxes), spectral indices, and geospatial operators. With our proposed framework, it is possible to control the results at three levels: (i) tool-level performance on public EO benchmarks; (ii) at the agent-level to understand the capacity to generate valid, hallucination-free code; and (iii) at the task-level on specific use cases. In this work, we select two use-cases of interest: land-composition mapping and post-wildfire damage assessment. The proposed agent outperforms general-purpose LLM/VLM baselines (GPT-4o, LLaVA), achieving 64.2% vs. 51.7% accuracy on land-composition and 50% vs. 0% on post-wildfire analysis, while producing results that are transparent and easy to interpret. By outputting verifiable code, the approach turns EO analysis into a transparent, reproducible process.", "AI": {"tldr": "A conversational AI agent that converts natural language queries into executable Python code for Earth Observation analysis, making EO accessible to non-experts while ensuring transparency and reproducibility.", "motivation": "Earth Observation analysis is currently difficult for non-experts, requiring specialized knowledge and technical skills. Existing systems often provide black-box predictions that lack transparency and are hard to audit or reproduce.", "method": "Proposes a conversational, code-generating agent using tool LLMs that transforms natural language queries into executable Python workflows. The agent operates over a unified, extensible API supporting classification, segmentation, detection, spectral indices, and geospatial operators.", "result": "The agent outperforms general-purpose LLM/VLM baselines (GPT-4o, LLaVA), achieving 64.2% vs. 51.7% accuracy on land-composition mapping and 50% vs. 0% on post-wildfire damage assessment. It generates valid, hallucination-free code that makes results transparent and interpretable.", "conclusion": "By outputting verifiable code instead of black-box predictions, this approach transforms Earth Observation analysis into a transparent, reproducible process accessible to non-experts while maintaining performance advantages over general-purpose AI models."}}
{"id": "2602.00122", "categories": ["cs.CV", "cs.AI", "cs.MM"], "pdf": "https://arxiv.org/pdf/2602.00122", "abs": "https://arxiv.org/abs/2602.00122", "authors": ["Hongzhu Yi", "Yujia Yang", "Yuanxiang Wang", "Zhenyu Guan", "Jiahuan Chen", "Chenxi Bao", "Tiankun Yang", "Yixuan Yuan", "Tianyu Zong", "Xinming Wang", "Tao Yu", "Ruiwen Tao", "Haijin Liang", "Jin Ma", "Jinwen Luo", "Yeshani Xinyu Zuo", "Jungang Xu"], "title": "VDE Bench: Evaluating The Capability of Image Editing Models to Modify Visual Documents", "comment": null, "summary": "In recent years, multimodal image editing models have achieved substantial progress, enabling users to manipulate visual content through natural language in a flexible and interactive manner. Nevertheless, an important yet insufficiently explored research direction remains visual document image editing, which involves modifying textual content within images while faithfully preserving the original text style and background context. Existing approaches, including AnyText, GlyphControl, and TextCtrl, predominantly focus on English-language scenarios and documents with relatively sparse textual layouts, thereby failing to adequately address dense, structurally complex documents or non-Latin scripts such as Chinese. To bridge this gap, we propose \\textbf{V}isual \\textbf{D}oc \\textbf{E}dit Bench(VDE Bench), a rigorously human-annotated and evaluated benchmark specifically designed to assess image editing models on multilingual and complex visual document editing tasks. The benchmark comprises a high-quality dataset encompassing densely textual documents in both English and Chinese, including academic papers, posters, presentation slides, examination materials, and newspapers. Furthermore, we introduce a decoupled evaluation framework that systematically quantifies editing performance at the OCR parsing level, enabling fine-grained assessment of text modification accuracy. Based on this benchmark, we conduct a comprehensive evaluation of representative state-of-the-art image editing models. Manual verification demonstrates a strong consistency between human judgments and automated evaluation metrics. VDE Bench constitutes the first systematic benchmark for evaluating image editing models on multilingual and densely textual visual documents.", "AI": {"tldr": "VDE Bench is a new benchmark for evaluating image editing models on multilingual and complex visual document editing tasks, focusing on dense textual documents in English and Chinese.", "motivation": "Existing multimodal image editing models lack proper evaluation for visual document image editing, especially for dense, structurally complex documents and non-Latin scripts like Chinese. Current approaches focus on English and sparse layouts, creating a gap in assessing models for real-world document editing scenarios.", "method": "The authors propose VDE Bench: 1) A human-annotated benchmark dataset with dense textual documents in English and Chinese (academic papers, posters, slides, exams, newspapers), 2) A decoupled evaluation framework that systematically quantifies editing performance at OCR parsing level for fine-grained assessment.", "result": "Comprehensive evaluation of state-of-the-art image editing models shows strong consistency between human judgments and automated evaluation metrics. VDE Bench is the first systematic benchmark for evaluating image editing on multilingual and densely textual visual documents.", "conclusion": "VDE Bench addresses a critical gap in evaluating visual document editing models for complex, multilingual scenarios, providing a rigorous benchmark that enables systematic assessment of text modification accuracy while preserving original style and context."}}
{"id": "2602.00124", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.00124", "abs": "https://arxiv.org/abs/2602.00124", "authors": ["Divya Acharya", "Pierre Bernab'e", "Antoine Chevrot", "Helge Spieker", "Arnaud Gotlieb", "Bruno Legeard"], "title": "Context-Aware Autoencoders for Anomaly Detection in Maritime Surveillance", "comment": null, "summary": "The detection of anomalies is crucial to ensuring the safety and security of maritime vessel traffic surveillance. Although autoencoders are popular for anomaly detection, their effectiveness in identifying collective and contextual anomalies is limited, especially in the maritime domain, where anomalies depend on vessel-specific contexts derived from self-reported AIS messages. To address these limitations, we propose a novel solution: the context-aware autoencoder. By integrating context-specific thresholds, our method improves detection accuracy and reduces computational cost. We compare four context-aware autoencoder variants and a conventional autoencoder using a case study focused on fishing status anomalies in maritime surveillance. Results demonstrate the significant impact of context on reconstruction loss and anomaly detection. The context-aware autoencoder outperforms others in detecting anomalies in time series data. By incorporating context-specific thresholds and recognizing the importance of context in anomaly detection, our approach offers a promising solution to improve accuracy in maritime vessel traffic surveillance systems.", "AI": {"tldr": "Context-aware autoencoder improves anomaly detection in maritime vessel traffic by incorporating vessel-specific context thresholds, outperforming conventional autoencoders in detecting fishing status anomalies.", "motivation": "Conventional autoencoders have limited effectiveness in detecting collective and contextual anomalies in maritime surveillance, where anomalies depend on vessel-specific contexts from AIS messages. There's a need for better anomaly detection to ensure maritime safety and security.", "method": "Proposed a context-aware autoencoder that integrates context-specific thresholds. Compared four variants of context-aware autoencoders against a conventional autoencoder, focusing on fishing status anomalies in maritime surveillance.", "result": "Context-aware autoencoder outperforms conventional autoencoders in detecting anomalies in time series data. Results show significant impact of context on reconstruction loss and anomaly detection accuracy, with improved detection and reduced computational cost.", "conclusion": "Incorporating context-specific thresholds and recognizing context importance offers a promising solution to improve anomaly detection accuracy in maritime vessel traffic surveillance systems."}}
{"id": "2602.00126", "categories": ["cs.CV", "eess.IV"], "pdf": "https://arxiv.org/pdf/2602.00126", "abs": "https://arxiv.org/abs/2602.00126", "authors": ["Dmytro Filatov", "Valentyn Fedorov", "Vira Filatova", "Andrii Zelenchuk"], "title": "D3R-Net: Dual-Domain Denoising Reconstruction Network for Robust Industrial Anomaly Detection", "comment": "9 pages", "summary": "Unsupervised anomaly detection (UAD) is a key ingredient of automated visual inspection in modern manufacturing. The reconstruction-based methods appeal because they have basic architectural design and they process data quickly but they produce oversmoothed results for high-frequency details. As a result, subtle defects are partially reconstructed rather than highlighted, which limits segmentation accuracy. We build on this line of work and introduce D3R-Net, a Dual-Domain Denoising Reconstruction framework that couples a self-supervised 'healing' task with frequency-aware regularization. During training, the network receives synthetically corrupted normal images and is asked to reconstruct the clean targets, which prevents trivial identity mapping and pushes the model to learn the manifold of defect-free textures. In addition to the spatial mean squared error, we employ a Fast Fourier Transform (FFT) magnitude loss that encourages consistency in the frequency domain. The implementation also allows an optional structural similarity (SSIM) term, which we study in an ablation. On the MVTec AD Hazelnut benchmark, D3R-Net with the FFT loss improves localization consistency over a spatial-only baseline: PRO AUC increases from 0.603 to 0.687, while image-level ROC AUC remains robust. Evaluated across fifteen MVTec categories, the FFT variant raises the average pixel ROC AUC from 0.733 to 0.751 and PRO AUC from 0.417 to 0.468 compared to the MSE-only baseline, at roughly 20 FPS on a single GPU. The network is trained from scratch and uses a lightweight convolutional autoencoder backbone, providing a practical alternative to heavy pre-trained feature embedding methods.", "AI": {"tldr": "D3R-Net improves unsupervised anomaly detection by combining spatial reconstruction with frequency-aware regularization to better preserve high-frequency defect details.", "motivation": "Existing reconstruction-based UAD methods produce oversmoothed results that fail to highlight subtle defects, limiting segmentation accuracy. They need to better preserve high-frequency details for improved anomaly localization.", "method": "Dual-Domain Denoising Reconstruction framework with self-supervised 'healing' task and frequency-aware regularization. Uses synthetic corruption of normal images during training, spatial MSE loss, FFT magnitude loss for frequency consistency, and optional SSIM term.", "result": "On MVTec AD Hazelnut: PRO AUC improved from 0.603 to 0.687 with FFT loss. Across 15 MVTec categories: average pixel ROC AUC increased from 0.733 to 0.751, PRO AUC from 0.417 to 0.468. Runs at ~20 FPS on single GPU.", "conclusion": "D3R-Net provides a practical lightweight alternative to heavy pre-trained methods, effectively combining spatial and frequency domain information to improve anomaly localization while maintaining fast inference speeds."}}
{"id": "2602.00131", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2602.00131", "abs": "https://arxiv.org/abs/2602.00131", "authors": ["Fraser Robinson", "Souren Pashangpour", "Matthew Lisondra", "Goldie Nejat"], "title": "PovNet+: A Deep Learning Architecture for Socially Assistive Robots to Learn and Assist with Multiple Activities of Daily Living", "comment": "Submitted to Advanced Robotics (Taylor & Francis)", "summary": "A significant barrier to the long-term deployment of autonomous socially assistive robots is their inability to both perceive and assist with multiple activities of daily living (ADLs). In this paper, we present the first multimodal deep learning architecture, POVNet+, for multi-activity recognition for socially assistive robots to proactively initiate assistive behaviors. Our novel architecture introduces the use of both ADL and motion embedding spaces to uniquely distinguish between a known ADL being performed, a new unseen ADL, or a known ADL being performed atypically in order to assist people in real scenarios. Furthermore, we apply a novel user state estimation method to the motion embedding space to recognize new ADLs while monitoring user performance. This ADL perception information is used to proactively initiate robot assistive interactions. Comparison experiments with state-of-the-art human activity recognition methods show our POVNet+ method has higher ADL classification accuracy. Human-robot interaction experiments in a cluttered living environment with multiple users and the socially assistive robot Leia using POVNet+ demonstrate the ability of our multi-modal ADL architecture in successfully identifying different seen and unseen ADLs, and ADLs being performed atypically, while initiating appropriate assistive human-robot interactions.", "AI": {"tldr": "POVNet+ is a multimodal deep learning architecture for socially assistive robots to recognize multiple activities of daily living (ADLs), distinguish between known/unknown/atypical ADLs, and proactively initiate assistive interactions.", "motivation": "Current socially assistive robots lack the ability to perceive and assist with multiple ADLs, which is a barrier to long-term deployment. There's a need for robots that can recognize various daily activities and proactively provide appropriate assistance.", "method": "POVNet+ uses multimodal deep learning with ADL and motion embedding spaces to distinguish between known ADLs, unseen ADLs, and atypically performed ADLs. It incorporates user state estimation in the motion embedding space to recognize new ADLs while monitoring user performance.", "result": "POVNet+ achieves higher ADL classification accuracy than state-of-the-art human activity recognition methods. Human-robot interaction experiments in cluttered living environments with multiple users demonstrate successful identification of seen/unseen ADLs and atypically performed ADLs, with appropriate assistive interactions.", "conclusion": "POVNet+ enables socially assistive robots to effectively perceive multiple ADLs, distinguish between different activity types, and proactively initiate appropriate assistive behaviors, addressing a key barrier to long-term autonomous deployment."}}
{"id": "2602.00132", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.00132", "abs": "https://arxiv.org/abs/2602.00132", "authors": ["Jiao Li", "Jian Lang", "Xikai Tang", "Wenzheng Shu", "Ting Zhong", "Qiang Gao", "Yong Wang", "Leiting Chen", "Fan Zhou"], "title": "Shedding the Facades, Connecting the Domains: Detecting Shifting Multimodal Hate Video with Test-Time Adaptation", "comment": "Accepted by AAAI2026 main track", "summary": "Hate Video Detection (HVD) is crucial for online ecosystems. Existing methods assume identical distributions between training (source) and inference (target) data. However, hateful content often evolves into irregular and ambiguous forms to evade censorship, resulting in substantial semantic drift and rendering previously trained models ineffective. Test-Time Adaptation (TTA) offers a solution by adapting models during inference to narrow the cross-domain gap, while conventional TTA methods target mild distribution shifts and struggle with the severe semantic drift in HVD. To tackle these challenges, we propose SCANNER, the first TTA framework tailored for HVD. Motivated by the insight that, despite the evolving nature of hateful manifestations, their underlying cores remain largely invariant (i.e., targeting is still based on characteristics like gender, race, etc), we leverage these stable cores as a bridge to connect the source and target domains. Specifically, SCANNER initially reveals the stable cores from the ambiguous layout in evolving hateful content via a principled centroid-guided alignment mechanism. To alleviate the impact of outlier-like samples that are weakly correlated with centroids during the alignment process, SCANNER enhances the prior by incorporating a sample-level adaptive centroid alignment strategy, promoting more stable adaptation. Furthermore, to mitigate semantic collapse from overly uniform outputs within clusters, SCANNER introduces an intra-cluster diversity regularization that encourages the cluster-wise semantic richness. Experiments show that SCANNER outperforms all baselines, with an average gain of 4.69% in Macro-F1 over the best.", "AI": {"tldr": "SCANNER is a novel Test-Time Adaptation framework for Hate Video Detection that addresses severe semantic drift in evolving hateful content by leveraging stable core concepts as domain bridges.", "motivation": "Hateful content evolves into irregular forms to evade censorship, causing semantic drift that makes existing models ineffective. Traditional TTA methods struggle with these severe distribution shifts in Hate Video Detection.", "method": "SCANNER uses stable hate cores (gender, race, etc.) as domain bridges with: 1) centroid-guided alignment to reveal stable cores, 2) sample-level adaptive centroid alignment for outlier handling, and 3) intra-cluster diversity regularization to prevent semantic collapse.", "result": "SCANNER outperforms all baselines with an average gain of 4.69% in Macro-F1 over the best existing method.", "conclusion": "The framework successfully addresses severe semantic drift in HVD by leveraging invariant hate cores and adaptive alignment strategies, demonstrating significant performance improvements over conventional TTA approaches."}}
{"id": "2602.00135", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.00135", "abs": "https://arxiv.org/abs/2602.00135", "authors": ["Pengcheng Zheng", "Chaoning Zhang", "Jiarong Mo", "GuoHui Li", "Jiaquan Zhang", "Jiahao Zhang", "Sihan Cao", "Sheng Zheng", "Caiyan Qin", "Guoqing Wang", "Yang Yang"], "title": "LLaVA-FA: Learning Fourier Approximation for Compressing Large Multimodal Models", "comment": "Accepted by ICLR 2026", "summary": "Large multimodal models (LMMs) have achieved impressive performance on various vision-language tasks, but their substantial computational and memory costs hinder their practical deployment. Existing compression methods often decouple low-rank decomposition and quantization, leading to compounded reconstruction errors, especially in multimodal architectures with cross-modal redundancy. To address this issue, we propose LLaVA-FA, a novel efficient LMM that performs joint low-rank plus quantization approximation in the frequency domain. By leveraging the de-correlation and conjugate symmetry properties of Fourier transform, LLaVA-FA achieves more compact and accurate weight representations. Furthermore, we introduce PolarQuant, a polar-coordinate quantization method tailored for complex matrices, and an optional diagonal calibration (ODC) scheme that eliminates the need for large-scale calibration data. Extensive experimental results demonstrate that our proposed LLaVA-FA outperforms existing efficient multimodal models across multiple benchmarks while maintaining minimal activated parameters and low computational costs, validating its effectiveness as a powerful solution for compressing LMMs.", "AI": {"tldr": "LLaVA-FA is a novel efficient large multimodal model that uses joint low-rank plus quantization approximation in the frequency domain to compress vision-language models with minimal performance loss.", "motivation": "Large multimodal models have high computational and memory costs that hinder practical deployment. Existing compression methods often decouple low-rank decomposition and quantization, leading to compounded reconstruction errors, especially in multimodal architectures with cross-modal redundancy.", "method": "Proposes LLaVA-FA that performs joint low-rank plus quantization approximation in the frequency domain, leveraging Fourier transform properties for compact weight representations. Introduces PolarQuant (polar-coordinate quantization for complex matrices) and optional diagonal calibration scheme that eliminates need for large-scale calibration data.", "result": "LLaVA-FA outperforms existing efficient multimodal models across multiple benchmarks while maintaining minimal activated parameters and low computational costs.", "conclusion": "LLaVA-FA provides an effective solution for compressing large multimodal models, validating the effectiveness of joint frequency-domain approximation with specialized quantization methods."}}
{"id": "2602.00144", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.00144", "abs": "https://arxiv.org/abs/2602.00144", "authors": ["Xuan Rao", "Mingming Ha", "Bo Zhao", "Derong Liu", "Cesare Alippi"], "title": "Scalable Analytic Classifiers with Associative Drift Compensation for Class-Incremental Learning of Vision Transformers", "comment": null, "summary": "Class-incremental learning (CIL) with Vision Transformers (ViTs) faces a major computational bottleneck during the classifier reconstruction phase, where most existing methods rely on costly iterative stochastic gradient descent (SGD). We observe that analytic Regularized Gaussian Discriminant Analysis (RGDA) provides a Bayes-optimal alternative with accuracy comparable to SGD-based classifiers; however, its quadratic inference complexity limits its use in large-scale CIL scenarios. To overcome this, we propose Low-Rank Factorized RGDA (LR-RGDA), a scalable classifier that combines RGDA's expressivity with the efficiency of linear classifiers. By exploiting the low-rank structure of the covariance via the Woodbury matrix identity, LR-RGDA decomposes the discriminant function into a global affine term refined by a low-rank quadratic perturbation, reducing the inference complexity from $\\mathcal{O}(Cd^2)$ to $\\mathcal{O}(d^2 + Crd^2)$, where $C$ is the class number, $d$ the feature dimension, and $r \\ll d$ the subspace rank. To mitigate representation drift caused by backbone updates, we further introduce Hopfield-based Distribution Compensator (HopDC), a training-free mechanism that uses modern continuous Hopfield Networks to recalibrate historical class statistics through associative memory dynamics on unlabeled anchors, accompanied by a theoretical bound on the estimation error. Extensive experiments on diverse CIL benchmarks demonstrate that our framework achieves state-of-the-art performance, providing a scalable solution for large-scale class-incremental learning with ViTs. Code: https://github.com/raoxuan98-hash/lr_rgda_hopdc.", "AI": {"tldr": "LR-RGDA + HopDC: A scalable class-incremental learning framework for Vision Transformers that replaces costly SGD classifier reconstruction with low-rank factorized RGDA and compensates for representation drift using Hopfield networks.", "motivation": "Class-incremental learning with Vision Transformers faces computational bottlenecks in classifier reconstruction using iterative SGD. While analytic RGDA offers Bayes-optimal accuracy comparable to SGD, its quadratic inference complexity limits scalability for large-scale CIL scenarios.", "method": "Proposes Low-Rank Factorized RGDA (LR-RGDA) that exploits low-rank covariance structure via Woodbury identity to decompose discriminant function into global affine term plus low-rank quadratic perturbation, reducing inference complexity from O(Cd\u00b2) to O(d\u00b2 + Crd\u00b2). Also introduces Hopfield-based Distribution Compensator (HopDC) - a training-free mechanism using continuous Hopfield Networks to recalibrate historical class statistics through associative memory dynamics on unlabeled anchors.", "result": "Extensive experiments on diverse CIL benchmarks demonstrate state-of-the-art performance. The framework provides a scalable solution for large-scale class-incremental learning with ViTs, achieving accuracy comparable to SGD-based methods with significantly reduced computational cost.", "conclusion": "The proposed LR-RGDA + HopDC framework offers an efficient and effective solution for class-incremental learning with Vision Transformers, combining RGDA's expressivity with linear classifier efficiency while addressing representation drift through training-free statistical compensation."}}
{"id": "2602.00145", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.00145", "abs": "https://arxiv.org/abs/2602.00145", "authors": ["Siva Teja Kakileti", "Geetha Manjunath"], "title": "DensiThAI, A Multi-View Deep Learning Framework for Breast Density Estimation using Infrared Images", "comment": null, "summary": "Breast tissue density is a key biomarker of breast cancer risk and a major factor affecting mammographic sensitivity. However, density assessment currently relies almost exclusively on X-ray mammography, an ionizing imaging modality. This study investigates the feasibility of estimating breast density using artificial intelligence over infrared thermal images, offering a non-ionizing imaging approach. The underlying hypothesis is that fibroglandular and adipose tissues exhibit distinct thermophysical and physiological properties, leading to subtle but spatially coherent temperature variations on the breast surface. In this paper, we propose DensiThAI, a multi-view deep learning framework for breast density classification from thermal images. The framework was evaluated on a multi-center dataset of 3,500 women using mammography-derived density labels as reference. Using five standard thermal views, DensiThAI achieved a mean AUROC of 0.73 across 10 random splits, with statistically significant separation between density classes across all splits (p << 0.05). Consistent performance across age cohorts supports the potential of thermal imaging as a non-ionizing approach for breast density assessment with implications for improved patient experience and workflow optimization.", "AI": {"tldr": "DensiThAI: A multi-view deep learning framework that estimates breast density from thermal images, achieving 0.73 AUROC using non-ionizing infrared imaging as an alternative to mammography.", "motivation": "Current breast density assessment relies on ionizing X-ray mammography. The study aims to develop a non-ionizing alternative using thermal imaging, leveraging distinct thermophysical properties of fibroglandular and adipose tissues.", "method": "DensiThAI - a multi-view deep learning framework for breast density classification from thermal images. Uses five standard thermal views and mammography-derived density labels as reference, evaluated on a multi-center dataset of 3,500 women.", "result": "Achieved mean AUROC of 0.73 across 10 random splits, with statistically significant separation between density classes (p << 0.05). Consistent performance across age cohorts demonstrates feasibility.", "conclusion": "Thermal imaging shows potential as a non-ionizing approach for breast density assessment, offering improved patient experience and workflow optimization compared to traditional mammography."}}
{"id": "2602.00148", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.00148", "abs": "https://arxiv.org/abs/2602.00148", "authors": ["Shiqian Li", "Ruihong Shen", "Junfeng Ni", "Chang Pan", "Chi Zhang", "Yixin Zhu"], "title": "Learning Physics-Grounded 4D Dynamics with Neural Gaussian Force Fields", "comment": "43 pages, ICLR 2026", "summary": "Predicting physical dynamics from raw visual data remains a major challenge in AI. While recent video generation models have achieved impressive visual quality, they still cannot consistently generate physically plausible videos due to a lack of modeling of physical laws. Recent approaches combining 3D Gaussian splatting and physics engines can produce physically plausible videos, but are hindered by high computational costs in both reconstruction and simulation, and often lack robustness in complex real-world scenarios. To address these issues, we introduce Neural Gaussian Force Field (NGFF), an end-to-end neural framework that integrates 3D Gaussian perception with physics-based dynamic modeling to generate interactive, physically realistic 4D videos from multi-view RGB inputs, achieving two orders of magnitude faster than prior Gaussian simulators. To support training, we also present GSCollision, a 4D Gaussian dataset featuring diverse materials, multi-object interactions, and complex scenes, totaling over 640k rendered physical videos (~4 TB). Evaluations on synthetic and real 3D scenarios show NGFF's strong generalization and robustness in physical reasoning, advancing video prediction towards physics-grounded world models.", "AI": {"tldr": "NGFF is an end-to-end neural framework that combines 3D Gaussian perception with physics modeling to generate physically realistic 4D videos from multi-view inputs, achieving 100x speedup over prior methods.", "motivation": "Current video generation models lack physical plausibility, while physics-based approaches are computationally expensive and not robust in complex real-world scenarios. There's a need for efficient, physically-grounded video prediction.", "method": "Neural Gaussian Force Field (NGFF) integrates 3D Gaussian perception with physics-based dynamic modeling in an end-to-end neural framework. It uses multi-view RGB inputs and includes GSCollision dataset for training with diverse materials and interactions.", "result": "NGFF achieves two orders of magnitude faster performance than prior Gaussian simulators, shows strong generalization and robustness in physical reasoning on synthetic and real 3D scenarios, and advances physics-grounded world models.", "conclusion": "NGFF successfully addresses the computational and robustness limitations of previous approaches, enabling efficient generation of physically realistic 4D videos and advancing the field toward physics-grounded world models for video prediction."}}
{"id": "2602.00149", "categories": ["cs.CV", "cs.RO", "eess.IV"], "pdf": "https://arxiv.org/pdf/2602.00149", "abs": "https://arxiv.org/abs/2602.00149", "authors": ["Shucong Li", "Xiaoluo Zhou", "Yuqian He", "Zhenyu Liu"], "title": "SDCM: Simulated Densifying and Compensatory Modeling Fusion for Radar-Vision 3-D Object Detection in Internet of Vehicles", "comment": null, "summary": "3-D object detection based on 4-D radar-vision is an important part in Internet of Vehicles (IoV). However, there are two challenges which need to be faced. First, the 4-D radar point clouds are sparse, leading to poor 3-D representation. Second, vision datas exhibit representation degradation under low-light, long distance detection and dense occlusion scenes, which provides unreliable texture information during fusion stage. To address these issues, a framework named SDCM is proposed, which contains Simulated Densifying and Compensatory Modeling Fusion for radar-vision 3-D object detection in IoV. Firstly, considering point generation based on Gaussian simulation of key points obtained from 3-D Kernel Density Estimation (3-D KDE), and outline generation based on curvature simulation, Simulated Densifying (SimDen) module is designed to generate dense radar point clouds. Secondly, considering that radar data could provide more real time information than vision data, due to the all-weather property of 4-D radar. Radar Compensatory Mapping (RCM) module is designed to reduce the affects of vision datas' representation degradation. Thirdly, considering that feature tensor difference values contain the effective information of every modality, which could be extracted and modeled for heterogeneity reduction and modalities interaction, Mamba Modeling Interactive Fusion (MMIF) module is designed for reducing heterogeneous and achieving interactive Fusion. Experiment results on the VoD, TJ4DRadSet and Astyx HiRes 2019 dataset show that SDCM achieves best performance with lower parameter quantity and faster inference speed. Our code will be available.", "AI": {"tldr": "SDCM framework improves 4-D radar-vision 3D object detection for IoV by densifying sparse radar point clouds, compensating for vision degradation, and using Mamba-based interactive fusion.", "motivation": "Address two key challenges in 4-D radar-vision 3D object detection: 1) sparse radar point clouds leading to poor 3D representation, and 2) vision data degradation under low-light, long-distance, and occlusion conditions providing unreliable texture information for fusion.", "method": "Three-module framework: 1) SimDen module uses Gaussian simulation of key points from 3D KDE and curvature simulation to generate dense radar point clouds; 2) RCM module leverages radar's all-weather capability to compensate for vision degradation; 3) MMIF module extracts and models feature tensor differences for heterogeneity reduction and interactive fusion.", "result": "SDCM achieves best performance on VoD, TJ4DRadSet and Astyx HiRes 2019 datasets with lower parameter quantity and faster inference speed compared to existing methods.", "conclusion": "The proposed SDCM framework effectively addresses radar sparsity and vision degradation challenges in 4-D radar-vision 3D object detection, achieving superior performance with efficient computation for IoV applications."}}
{"id": "2602.00151", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.00151", "abs": "https://arxiv.org/abs/2602.00151", "authors": ["Alexander Blezinger", "Wolfgang Nejdl", "Ming Tang"], "title": "Investigating the Impact of Histopathological Foundation Models on Regressive Prediction of Homologous Recombination Deficiency", "comment": "9 pages, 7 figures and 5 tables. Initialy submitted for IJCAI 2026", "summary": "Foundation models pretrained on large-scale histopathology data have found great success in various fields of computational pathology, but their impact on regressive biomarker prediction remains underexplored. In this work, we systematically evaluate histopathological foundation models for regression-based tasks, demonstrated through the prediction of homologous recombination deficiency (HRD) score - a critical biomarker for personalized cancer treatment. Within multiple instance learning frameworks, we extract patch-level features from whole slide images (WSI) using five state-of-the-art foundation models, and evaluate their impact compared to contrastive learning-based features. Models are trained to predict continuous HRD scores based on these extracted features across breast, endometrial, and lung cancer cohorts from two public medical data collections. Extensive experiments demonstrate that models trained on foundation model features consistently outperform the baseline in terms of predictive accuracy and generalization capabilities while exhibiting systematic differences among the foundation models. Additionally, we propose a distribution-based upsampling strategy to mitigate target imbalance in these datasets, significantly improving the recall and balanced accuracy for underrepresented but clinically important patient populations. Furthermore, we investigate the impact of different sampling strategies and instance bagsizes by ablation studies. Our results highlight the benefits of large-scale histopathological pretraining for more precise and transferable regressive biomarker prediction, showcasing its potential to advance AI-driven precision oncology.", "AI": {"tldr": "Foundation models pretrained on large-scale histopathology data outperform contrastive learning baselines for predicting continuous HRD scores across multiple cancer types, with proposed distribution-based upsampling improving performance for underrepresented patient populations.", "motivation": "While foundation models have shown success in computational pathology, their impact on regressive biomarker prediction (like continuous HRD scores) remains underexplored. HRD score is a critical biomarker for personalized cancer treatment, and better prediction methods could advance precision oncology.", "method": "Used multiple instance learning frameworks with five state-of-the-art foundation models to extract patch-level features from whole slide images. Compared foundation model features against contrastive learning-based features. Proposed distribution-based upsampling to address target imbalance. Evaluated across breast, endometrial, and lung cancer cohorts from two public datasets. Conducted ablation studies on sampling strategies and instance bag sizes.", "result": "Foundation model features consistently outperformed baseline in predictive accuracy and generalization across cancer types. Different foundation models showed systematic performance differences. Distribution-based upsampling significantly improved recall and balanced accuracy for underrepresented patient populations. Ablation studies revealed insights about optimal sampling strategies and bag sizes.", "conclusion": "Large-scale histopathological pretraining provides substantial benefits for precise and transferable regressive biomarker prediction, demonstrating potential to advance AI-driven precision oncology through improved HRD score prediction across multiple cancer types."}}
{"id": "2602.00152", "categories": ["cs.CV", "cs.AI", "eess.SP"], "pdf": "https://arxiv.org/pdf/2602.00152", "abs": "https://arxiv.org/abs/2602.00152", "authors": ["Boyu Li", "Kuangji Zuo", "Lincong Li", "Yonghui Wu"], "title": "Real-Time Human Activity Recognition on Edge Microcontrollers: Dynamic Hierarchical Inference with Multi-Spectral Sensor Fusion", "comment": "24 pages, 6 figures. The manusrcipt is under review at Measurement", "summary": "The demand for accurate on-device pattern recognition in edge applications is intensifying, yet existing approaches struggle to reconcile accuracy with computational constraints. To address this challenge, a resource-aware hierarchical network based on multi-spectral fusion and interpretable modules, namely the Hierarchical Parallel Pseudo-image Enhancement Fusion Network (HPPI-Net), is proposed for real-time, on-device Human Activity Recognition (HAR). Deployed on an ARM Cortex-M4 microcontroller for low-power real-time inference, HPPI-Net achieves 96.70% accuracy while utilizing only 22.3 KiB of RAM and 439.5 KiB of ROM after optimization. HPPI-Net employs a two-layer architecture. The first layer extracts preliminary features using Fast Fourier Transform (FFT) spectrograms, while the second layer selectively activates either a dedicated module for stationary activity recognition or a parallel LSTM-MobileNet network (PLMN) for dynamic states. PLMN fuses FFT, Wavelet, and Gabor spectrograms through three parallel LSTM encoders and refines the concatenated features using Efficient Channel Attention (ECA) and Depthwise Separable Convolution (DSC), thereby offering channel-level interpretability while substantially reducing multiply-accumulate operations. Compared with MobileNetV3, HPPI-Net improves accuracy by 1.22% and reduces RAM usage by 71.2% and ROM usage by 42.1%. These results demonstrate that HPPI-Net achieves a favorable accuracy-efficiency trade-off and provides explainable predictions, establishing a practical solution for wearable, industrial, and smart home HAR on memory-constrained edge platforms.", "AI": {"tldr": "HPPI-Net is a hierarchical network for on-device human activity recognition that achieves 96.70% accuracy with minimal memory usage (22.3 KiB RAM, 439.5 KiB ROM) on ARM Cortex-M4 microcontrollers.", "motivation": "There's growing demand for accurate on-device pattern recognition in edge applications, but existing approaches struggle to balance accuracy with computational constraints, especially for memory-limited edge platforms.", "method": "Two-layer hierarchical architecture: first layer extracts features using FFT spectrograms; second layer selectively activates either stationary activity module or parallel LSTM-MobileNet network (PLMN) for dynamic states. PLMN fuses FFT, Wavelet, and Gabor spectrograms through parallel LSTM encoders, then refines features with Efficient Channel Attention and Depthwise Separable Convolution.", "result": "Achieves 96.70% accuracy with only 22.3 KiB RAM and 439.5 KiB ROM. Compared to MobileNetV3, improves accuracy by 1.22% while reducing RAM usage by 71.2% and ROM usage by 42.1%.", "conclusion": "HPPI-Net achieves favorable accuracy-efficiency trade-off with explainable predictions, providing a practical solution for wearable, industrial, and smart home HAR on memory-constrained edge platforms."}}
{"id": "2602.00153", "categories": ["cs.CV", "cs.AI", "eess.IV"], "pdf": "https://arxiv.org/pdf/2602.00153", "abs": "https://arxiv.org/abs/2602.00153", "authors": ["Axel Duch\u00e9", "Cl\u00e9ment Chatelain", "Gilles Gasso"], "title": "See Without Decoding: Motion-Vector-Based Tracking in Compressed Video", "comment": null, "summary": "We propose a lightweight compressed-domain tracking model that operates directly on video streams, without requiring full RGB video decoding. Using motion vectors and transform coefficients from compressed data, our deep model propagates object bounding boxes across frames, achieving a computational speed-up of order up to 3.7 with only a slight 4% mAP@0.5 drop vs RGB baseline on MOTS15/17/20 datasets. These results highlight codec-domain motion modeling efficiency for real-time analytics in large monitoring systems.", "AI": {"tldr": "Lightweight compressed-domain tracking model uses motion vectors and transform coefficients from video streams without full RGB decoding, achieving 3.7x speed-up with minimal accuracy loss.", "motivation": "To enable real-time object tracking in large monitoring systems by avoiding computationally expensive RGB video decoding, leveraging existing compressed video data for efficiency.", "method": "Deep model that operates directly on compressed video streams using motion vectors and transform coefficients, propagating object bounding boxes across frames without full RGB decoding.", "result": "Achieves up to 3.7x computational speed-up compared to RGB baseline with only 4% mAP@0.5 drop on MOTS15/17/20 datasets.", "conclusion": "Codec-domain motion modeling is efficient for real-time analytics in large monitoring systems, demonstrating significant speed improvements with minimal accuracy trade-offs."}}
{"id": "2602.00163", "categories": ["cs.CV", "q-bio.NC"], "pdf": "https://arxiv.org/pdf/2602.00163", "abs": "https://arxiv.org/abs/2602.00163", "authors": ["Laura Cif", "Diane Demailly", "Gabriella A. Horv\u00e0th", "Juan Dario Ortigoza Escobar", "Nathalie Dorison", "Mayt\u00e9 Castro Jim\u00e9nez", "C\u00e9cile A. Hubsch", "Thomas Wirth", "Gun-Marie Hariz", "Sophie Huby", "Morgan Dornadic", "Zohra Souei", "Muhammad Mushhood Ur Rehman", "Simone Hemm", "Mehdi Boulayme", "Eduardo M. Moraud", "Jocelyne Bloch", "Xavier Vasques"], "title": "Deep Learning Pose Estimation for Multi-Label Recognition of Combined Hyperkinetic Movement Disorders", "comment": null, "summary": "Hyperkinetic movement disorders (HMDs) such as dystonia, tremor, chorea, myoclonus, and tics are disabling motor manifestations across childhood and adulthood. Their fluctuating, intermittent, and frequently co-occurring expressions hinder clinical recognition and longitudinal monitoring, which remain largely subjective and vulnerable to inter-rater variability. Objective and scalable methods to distinguish overlapping HMD phenotypes from routine clinical videos are still lacking. Here, we developed a pose-based machine-learning framework that converts standard outpatient videos into anatomically meaningful keypoint time series and computes kinematic descriptors spanning statistical, temporal, spectral, and higher-order irregularity-complexity features.", "AI": {"tldr": "A pose-based ML framework converts clinical videos into keypoint time series to objectively analyze hyperkinetic movement disorders using kinematic features.", "motivation": "Hyperkinetic movement disorders (dystonia, tremor, chorea, myoclonus, tics) are disabling but difficult to diagnose and monitor due to their fluctuating, intermittent nature and overlapping symptoms. Current clinical assessment is subjective with high inter-rater variability, lacking objective, scalable methods for phenotype distinction from routine videos.", "method": "Developed a pose-based machine learning framework that converts standard outpatient clinical videos into anatomically meaningful keypoint time series, then computes comprehensive kinematic descriptors including statistical, temporal, spectral, and higher-order irregularity-complexity features.", "result": "Not specified in the provided abstract excerpt - the abstract only describes the problem and proposed method, not the results.", "conclusion": "Not specified in the provided abstract excerpt - the abstract only describes the problem and proposed method, not the conclusions."}}
{"id": "2602.00168", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.00168", "abs": "https://arxiv.org/abs/2602.00168", "authors": ["Ranjan Sapkota", "Manoj Karkee"], "title": "YOLOE-26: Integrating YOLO26 with YOLOE for Real-Time Open-Vocabulary Instance Segmentation", "comment": null, "summary": "This paper presents YOLOE-26, a unified framework that integrates the deployment-optimized YOLO26(or YOLOv26) architecture with the open-vocabulary learning paradigm of YOLOE for real-time open-vocabulary instance segmentation. Building on the NMS-free, end-to-end design of YOLOv26, the proposed approach preserves the hallmark efficiency and determinism of the YOLO family while extending its capabilities beyond closed-set recognition. YOLOE-26 employs a convolutional backbone with PAN/FPN-style multi-scale feature aggregation, followed by end-to-end regression and instance segmentation heads. A key architectural contribution is the replacement of fixed class logits with an object embedding head, which formulates classification as similarity matching against prompt embeddings derived from text descriptions, visual examples, or a built-in vocabulary. To enable efficient open-vocabulary reasoning, the framework incorporates Re-Parameterizable Region-Text Alignment (RepRTA) for zero-overhead text prompting, a Semantic-Activated Visual Prompt Encoder (SAVPE) for example-guided segmentation, and Lazy Region Prompt Contrast for prompt-free inference. All prompting modalities operate within a unified object embedding space, allowing seamless switching between text-prompted, visual-prompted, and fully autonomous segmentation. Extensive experiments demonstrate consistent scaling behavior and favorable accuracy-efficiency trade-offs across model sizes in both prompted and prompt-free settings. The training strategy leverages large-scale detection and grounding datasets with multi-task optimization and remains fully compatible with the Ultralytics ecosystem for training, validation, and deployment. Overall, YOLOE-26 provides a practical and scalable solution for real-time open-vocabulary instance segmentation in dynamic, real-world environments.", "AI": {"tldr": "YOLOE-26 is a real-time open-vocabulary instance segmentation framework that combines YOLOv26's efficient architecture with open-vocabulary learning, enabling text-prompted, visual-prompted, and autonomous segmentation in a unified system.", "motivation": "To extend YOLO's real-time efficiency beyond closed-set recognition to open-vocabulary instance segmentation, enabling flexible object recognition using text descriptions, visual examples, or autonomous detection in dynamic environments.", "method": "Integrates YOLOv26's NMS-free architecture with open-vocabulary learning via object embedding head for similarity matching. Uses RepRTA for zero-overhead text prompting, SAVPE for example-guided segmentation, and Lazy Region Prompt Contrast for prompt-free inference in unified embedding space.", "result": "Demonstrates consistent scaling behavior and favorable accuracy-efficiency trade-offs across model sizes in both prompted and prompt-free settings, with full compatibility with Ultralytics ecosystem for training and deployment.", "conclusion": "YOLOE-26 provides a practical, scalable solution for real-time open-vocabulary instance segmentation that preserves YOLO's efficiency while enabling flexible recognition capabilities for dynamic real-world applications."}}
{"id": "2602.00174", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.00174", "abs": "https://arxiv.org/abs/2602.00174", "authors": ["Jiajun Zhao", "Xuan Yang"], "title": "Intra-Class Subdivision for Pixel Contrastive Learning: Application to Semi-supervised Cardiac Image Segmentation", "comment": "5 pages, 7 figures, accepted by ICASSP 2026", "summary": "We propose an intra-class subdivision pixel contrastive learning (SPCL) framework for cardiac image segmentation to address representation contamination at boundaries. The novel concept ``Unconcerned sample'' is proposed to distinguish pixel representations at the inner and boundary regions within the same class, facilitating a clearer characterization of intra-class variations. A novel boundary contrastive loss for boundary representations is proposed to enhance representation discrimination across boundaries. The advantages of the unconcerned sample and boundary contrastive loss are analyzed theoretically. Experimental results in public cardiac datasets demonstrate that SPCL significantly improves segmentation performance, outperforming existing methods with respect to segmentation quality and boundary precision. Our code is available at https://github.com/Jrstud203/SPCL.", "AI": {"tldr": "SPCL framework improves cardiac image segmentation by addressing boundary representation contamination through intra-class subdivision and boundary contrastive learning.", "motivation": "Address representation contamination at boundaries in cardiac image segmentation, where pixel representations at boundaries often get confused between adjacent classes.", "method": "Proposes intra-class subdivision pixel contrastive learning with \"unconcerned samples\" to distinguish inner vs boundary regions within same class, plus novel boundary contrastive loss for boundary representations.", "result": "SPCL significantly improves segmentation performance on public cardiac datasets, outperforming existing methods in both segmentation quality and boundary precision.", "conclusion": "The proposed SPCL framework effectively addresses boundary representation issues in cardiac segmentation through intra-class subdivision and boundary contrastive learning, achieving state-of-the-art performance."}}
{"id": "2602.00176", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.00176", "abs": "https://arxiv.org/abs/2602.00176", "authors": ["Feng Tian", "Yixuan Li", "Weili Zeng", "Weitian Zhang", "Yichao Yan", "Xiaokang Yang"], "title": "Stabilizing Diffusion Posterior Sampling by Noise--Frequency Continuation", "comment": null, "summary": "Diffusion posterior sampling solves inverse problems by combining a pretrained diffusion prior with measurement-consistency guidance, but it often fails to recover fine details because measurement terms are applied in a manner that is weakly coupled to the diffusion noise level. At high noise, data-consistency gradients computed from inaccurate estimates can be geometrically incongruent with the posterior geometry, inducing early-step drift, spurious high-frequency artifacts, plus sensitivity to schedules and ill-conditioned operators. To address these concerns, we propose a noise--frequency Continuation framework that constructs a continuous family of intermediate posteriors whose likelihood enforces measurement consistency only within a noise-dependent frequency band. This principle is instantiated with a stabilized posterior sampler that combines a diffusion predictor, band-limited likelihood guidance, and a multi-resolution consistency strategy that aggressively commits reliable coarse corrections while conservatively adopting high-frequency details only when they become identifiable. Across super-resolution, inpainting, and deblurring, our method achieves state-of-the-art performance and improves motion deblurring PSNR by up to 5 dB over strong baselines.", "AI": {"tldr": "A noise-frequency continuation framework for diffusion posterior sampling that improves inverse problem solving by applying measurement consistency only within noise-dependent frequency bands, achieving state-of-the-art performance.", "motivation": "Standard diffusion posterior sampling often fails to recover fine details because measurement consistency guidance is weakly coupled to diffusion noise levels, causing early-step drift, spurious artifacts, and sensitivity to schedules and ill-conditioned operators.", "method": "Proposes a noise-frequency continuation framework that creates intermediate posteriors with likelihood enforcing measurement consistency only within noise-dependent frequency bands. Uses stabilized posterior sampler combining diffusion predictor, band-limited likelihood guidance, and multi-resolution consistency strategy.", "result": "Achieves state-of-the-art performance across super-resolution, inpainting, and deblurring tasks. Improves motion deblurring PSNR by up to 5 dB over strong baselines.", "conclusion": "The proposed noise-frequency continuation framework effectively addresses limitations of standard diffusion posterior sampling by better coupling measurement consistency with diffusion noise levels, leading to significant performance improvements in inverse problems."}}
{"id": "2602.00181", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.00181", "abs": "https://arxiv.org/abs/2602.00181", "authors": ["Hang Wu", "Yujun Cai", "Zehao Li", "Haonan Ge", "Bowen Sun", "Junsong Yuan", "Yiwei Wang"], "title": "CamReasoner: Reinforcing Camera Movement Understanding via Structured Spatial Reasoning", "comment": null, "summary": "Understanding camera dynamics is a fundamental pillar of video spatial intelligence. However, existing multimodal models predominantly treat this task as a black-box classification, often confusing physically distinct motions by relying on superficial visual patterns rather than geometric cues. We present CamReasoner, a framework that reformulates camera movement understanding as a structured inference process to bridge the gap between perception and cinematic logic. Our approach centers on the Observation-Thinking-Answer (O-T-A) paradigm, which compels the model to decode spatio-temporal cues such as trajectories and view frustums within an explicit reasoning block. To instill this capability, we construct a Large-scale Inference Trajectory Suite comprising 18k SFT reasoning chains and 38k RL feedback samples. Notably, we are the first to employ RL for logical alignment in this domain, ensuring motion inferences are grounded in physical geometry rather than contextual guesswork. By applying Reinforcement Learning to the Observation-Think-Answer (O-T-A) reasoning paradigm, CamReasoner effectively suppresses hallucinations and achieves state-of-the-art performance across multiple benchmarks.", "AI": {"tldr": "CamReasoner reformulates camera movement understanding as structured inference using Observation-Thinking-Answer paradigm with RL for logical alignment, achieving SOTA performance.", "motivation": "Existing multimodal models treat camera dynamics as black-box classification, confusing physically distinct motions by relying on superficial visual patterns rather than geometric cues. There's a gap between perception and cinematic logic that needs bridging.", "method": "Uses Observation-Thinking-Answer (O-T-A) paradigm to decode spatio-temporal cues (trajectories, view frustums) in explicit reasoning blocks. Constructs Large-scale Inference Trajectory Suite (18k SFT reasoning chains, 38k RL feedback samples). First to employ RL for logical alignment in this domain.", "result": "Effectively suppresses hallucinations and achieves state-of-the-art performance across multiple benchmarks.", "conclusion": "CamReasoner successfully bridges the gap between perception and cinematic logic by reformulating camera movement understanding as structured inference with RL-based logical alignment."}}
{"id": "2602.00192", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.00192", "abs": "https://arxiv.org/abs/2602.00192", "authors": ["Elif Nebioglu", "Emirhan Bilgi\u00e7", "Adrian Popescu"], "title": "AI-Generated Image Detectors Overrely on Global Artifacts: Evidence from Inpainting Exchange", "comment": "21 pages, 15 figures, 6 tables", "summary": "Modern deep learning-based inpainting enables realistic local image manipulation, raising critical challenges for reliable detection. However, we observe that current detectors primarily rely on global artifacts that appear as inpainting side effects, rather than on locally synthesized content. We show that this behavior occurs because VAE-based reconstruction induces a subtle but pervasive spectral shift across the entire image, including unedited regions. To isolate this effect, we introduce Inpainting Exchange (INP-X), an operation that restores original pixels outside the edited region while preserving all synthesized content. We create a 90K test dataset including real, inpainted, and exchanged images to evaluate this phenomenon. Under this intervention, pretrained state-of-the-art detectors, including commercial ones, exhibit a dramatic drop in accuracy (e.g., from 91\\% to 55\\%), frequently approaching chance level. We provide a theoretical analysis linking this behavior to high-frequency attenuation caused by VAE information bottlenecks. Our findings highlight the need for content-aware detection. Indeed, training on our dataset yields better generalization and localization than standard inpainting. Our dataset and code are publicly available at https://github.com/emirhanbilgic/INP-X.", "AI": {"tldr": "Current inpainting detectors rely on global spectral artifacts rather than local synthesized content, making them vulnerable to attacks that restore original pixels outside edited regions.", "motivation": "Modern deep learning inpainting creates realistic local image manipulation, but current detectors focus on global artifacts rather than the actual synthesized content, making them unreliable for detecting local edits.", "method": "Introduces Inpainting Exchange (INP-X) operation that restores original pixels outside edited regions while preserving synthesized content. Creates a 90K test dataset with real, inpainted, and exchanged images to evaluate detector performance.", "result": "State-of-the-art detectors (including commercial ones) show dramatic accuracy drops (e.g., from 91% to 55%) when tested with INP-X, often approaching chance level. Training on the INP-X dataset yields better generalization and localization than standard inpainting detection.", "conclusion": "Current inpainting detectors are vulnerable because they rely on global spectral artifacts rather than local content. The INP-X operation exposes this weakness, highlighting the need for content-aware detection approaches that focus on synthesized regions rather than global image statistics."}}
{"id": "2602.00202", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.00202", "abs": "https://arxiv.org/abs/2602.00202", "authors": ["Shanwen Wang", "Xin Sun", "Danfeng Hong", "Fei Zhou"], "title": "Vision-Language Model Purified Semi-Supervised Semantic Segmentation for Remote Sensing Images", "comment": null, "summary": "The semi-supervised semantic segmentation (S4) can learn rich visual knowledge from low-cost unlabeled images. However, traditional S4 architectures all face the challenge of low-quality pseudo-labels, especially for the teacher-student framework.We propose a novel SemiEarth model that introduces vision-language models (VLMs) to address the S4 issues for the remote sensing (RS) domain. Specifically, we invent a VLM pseudo-label purifying (VLM-PP) structure to purify the teacher network's pseudo-labels, achieving substantial improvements. Especially in multi-class boundary regions of RS images, the VLM-PP module can significantly improve the quality of pseudo-labels generated by the teacher, thereby correctly guiding the student model's learning. Moreover, since VLM-PP equips VLMs with open-world capabilities and is independent of the S4 architecture, it can correct mispredicted categories in low-confidence pseudo-labels whenever a discrepancy arises between its prediction and the pseudo-label. We conducted extensive experiments on multiple RS datasets, which demonstrate that our SemiEarth achieves SOTA performance. More importantly, unlike previous SOTA RS S4 methods, our model not only achieves excellent performance but also offers good interpretability. The code is released at https://github.com/wangshanwen001/SemiEarth.", "AI": {"tldr": "SemiEarth introduces vision-language models to purify pseudo-labels in semi-supervised semantic segmentation for remote sensing, achieving state-of-the-art performance with improved interpretability.", "motivation": "Traditional semi-supervised semantic segmentation architectures suffer from low-quality pseudo-labels, especially in teacher-student frameworks. This is particularly problematic for remote sensing images where multi-class boundary regions are challenging.", "method": "Proposes SemiEarth with a VLM pseudo-label purifying (VLM-PP) structure that uses vision-language models to purify teacher network's pseudo-labels. VLM-PP leverages VLMs' open-world capabilities to correct mispredicted categories in low-confidence pseudo-labels when discrepancies arise between VLM predictions and pseudo-labels.", "result": "Extensive experiments on multiple remote sensing datasets demonstrate that SemiEarth achieves state-of-the-art performance. The model significantly improves pseudo-label quality, especially in multi-class boundary regions, and offers good interpretability unlike previous methods.", "conclusion": "SemiEarth successfully addresses pseudo-label quality issues in semi-supervised semantic segmentation for remote sensing by integrating vision-language models, achieving superior performance while maintaining interpretability. The VLM-PP module is architecture-agnostic and leverages open-world capabilities of VLMs."}}
{"id": "2602.00211", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.00211", "abs": "https://arxiv.org/abs/2602.00211", "authors": ["Zafar Iqbal", "Anwar Ul Haq", "Srimannarayana Grandhi"], "title": "Interpretable Unsupervised Deformable Image Registration via Confidence-bound Multi-Hop Visual Reasoning", "comment": null, "summary": "Unsupervised deformable image registration requires aligning complex anatomical structures without reference labels, making interpretability and reliability critical. Existing deep learning methods achieve considerable accuracy but often lack transparency, leading to error drift and reduced clinical trust. We propose a novel Multi-Hop Visual Chain of Reasoning (VCoR) framework that reformulates registration as a progressive reasoning process. Inspired by the iterative nature of clinical decision-making, each visual reasoning hop integrates a Localized Spatial Refinement (LSR) module to enrich feature representations and a Cross-Reference Attention (CRA) mechanism that leads the iterative refinement process, preserving anatomical consistency. This multi-hop strategy enables robust handling of large deformations and produces a transparent sequence of intermediate predictions with a theoretical bound. Beyond accuracy, our framework offers built-in interpretability by estimating uncertainty via the stability and convergence of deformation fields across hops. Extensive evaluations on two challenging public datasets, DIR-Lab 4D CT (lung) and IXI T1-weighted MRI (brain), demonstrate that VCoR achieves competitive registration accuracy while offering rich intermediate visualizations and confidence measures. By embedding an implicit visual reasoning paradigm, we present an interpretable, reliable, and clinically viable unsupervised medical image registration.", "AI": {"tldr": "VCoR is a novel unsupervised deformable image registration framework that uses multi-hop visual reasoning with localized refinement and cross-reference attention to achieve accurate, interpretable registration with uncertainty estimation.", "motivation": "Existing deep learning registration methods lack transparency and interpretability, leading to error drift and reduced clinical trust. There's a need for registration methods that are not only accurate but also provide interpretability and reliability for clinical applications.", "method": "Proposes Multi-Hop Visual Chain of Reasoning (VCoR) framework that reformulates registration as progressive reasoning. Each hop integrates Localized Spatial Refinement (LSR) to enrich features and Cross-Reference Attention (CRA) to guide iterative refinement while preserving anatomical consistency. The multi-hop approach handles large deformations and provides intermediate predictions with theoretical bounds.", "result": "Extensive evaluations on DIR-Lab 4D CT (lung) and IXI T1-weighted MRI (brain) datasets show VCoR achieves competitive registration accuracy while providing rich intermediate visualizations and confidence measures through uncertainty estimation based on deformation field stability across hops.", "conclusion": "VCoR presents an interpretable, reliable, and clinically viable unsupervised medical image registration framework that embeds implicit visual reasoning, offering both accuracy and transparency for clinical applications."}}
{"id": "2602.00212", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.00212", "abs": "https://arxiv.org/abs/2602.00212", "authors": ["Sathish Krishna Anumula", "Vetrivelan Tamilmani", "Aniruddha Arjun Singh", "Dinesh Rajendran", "Venkata Deepak Namburi"], "title": "Deep Learning Based CNN Model for Automated Detection of Pneumonia from Chest XRay Images", "comment": "17 Pages, 2 Tables, 6 Figures", "summary": "Pneumonia has been one of the major causes of morbidities and mortality in the world and the prevalence of this disease is disproportionately high among the pediatric and elderly populations especially in resources trained areas Fast and precise diagnosis is a prerequisite for successful clinical intervention but due to inter observer variation fatigue among experts and a shortage of qualified radiologists traditional approaches that rely on manual interpretation of chest radiographs are frequently constrained To address these problems this paper introduces a unified automated diagnostic model using a custom Convolutional Neural Network CNN that can recognize pneumonia in chest Xray images with high precision and at minimal computational expense In contrast like other generic transfer learning based models which often possess redundant parameters the offered architecture uses a tailor made depth wise separable convolutional design which is optimized towards textural characteristics of grayscale medical images Contrast Limited Adaptive Histogram Equalization CLAHE and geometric augmentation are two significant preprocessing techniques used to ensure that the system does not experience class imbalance and is more likely to generalize The system is tested using a dataset of 5863 anterior posterior chest Xrays.", "AI": {"tldr": "A custom CNN model for automated pneumonia detection in chest X-rays using depthwise separable convolutions optimized for grayscale medical images, with CLAHE preprocessing and geometric augmentation to handle class imbalance.", "motivation": "Pneumonia causes high morbidity and mortality, especially in pediatric/elderly populations in resource-constrained areas. Traditional manual interpretation of chest radiographs is limited by inter-observer variation, expert fatigue, and shortage of qualified radiologists.", "method": "Custom CNN architecture using depthwise separable convolutional design optimized for grayscale medical images. Preprocessing includes Contrast Limited Adaptive Histogram Equalization (CLAHE) and geometric augmentation to address class imbalance and improve generalization.", "result": "The system was tested on a dataset of 5,863 anterior-posterior chest X-rays and achieved high precision with minimal computational expense.", "conclusion": "The proposed automated diagnostic model offers a fast, precise, and computationally efficient solution for pneumonia detection in chest X-rays, addressing limitations of traditional manual interpretation methods."}}
{"id": "2602.00214", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.00214", "abs": "https://arxiv.org/abs/2602.00214", "authors": ["Juan A. Olmos", "Antoine Manzanera", "Fabio Mart\u00ednez"], "title": "A Geometric Multimodal Foundation Model Integrating Bp-MRI and Clinical Reports in Prostate Cancer Classification", "comment": "Accepted at IEEE International Symposium on Biomedical Imaging (ISBI) 2026", "summary": "Prostate cancer (PCa) is one of the most common cancers in men worldwide. Bi-parametric MRI (bp-MRI) and clinical variables are crucial for PCa identification and improving treatment decisions. However, this process is subjective to expert interpretations. Furthermore, most existing computer-aided diagnosis methods focus on imaging-based models, overlooking the clinical context and suffering from data scarcity, limiting their ability to learn robust representations. We propose a geometric multimodal Foundation Model (FM), named MFM-Geom, that learns representations from bp-MRI and clinical reports, encoding visual findings and information from the context of clinical variables. In the representations classification head, the approach leverages symmetric positive definite (SPD) matrices and Riemannian deep learning to integrate imaging-text representations from a biomedical multimodal FM. Using 10% of the training data, MFM-Geom outperformed baseline class token embedding-based classification (+8.3%, AUC-PR of 90.67). Generalization on external dataset confirmed the robustness of fine-tuning biomedical FM, achieving an AUC-PR of 90.6.", "AI": {"tldr": "MFM-Geom is a geometric multimodal foundation model that combines bi-parametric MRI and clinical reports using Riemannian deep learning on SPD matrices to improve prostate cancer diagnosis with limited data.", "motivation": "Current prostate cancer diagnosis using bp-MRI and clinical variables is subjective and dependent on expert interpretation. Existing computer-aided methods focus only on imaging, ignore clinical context, and suffer from data scarcity, limiting robust representation learning.", "method": "Proposed MFM-Geom, a geometric multimodal foundation model that learns representations from both bp-MRI and clinical reports. Uses symmetric positive definite (SPD) matrices and Riemannian deep learning to integrate imaging-text representations from a biomedical multimodal foundation model.", "result": "Using only 10% of training data, MFM-Geom outperformed baseline class token embedding-based classification by +8.3%, achieving AUC-PR of 90.67%. Generalization on external dataset confirmed robustness with AUC-PR of 90.6.", "conclusion": "The geometric multimodal foundation model effectively integrates imaging and clinical data, demonstrating strong performance with limited training data and good generalization to external datasets for prostate cancer diagnosis."}}
{"id": "2602.00216", "categories": ["cs.CV", "cs.CY", "eess.IV"], "pdf": "https://arxiv.org/pdf/2602.00216", "abs": "https://arxiv.org/abs/2602.00216", "authors": ["Zaldy Pagaduan", "Jason Occidental", "Nathaniel Duro", "Dexielito Badilles", "Eleonor Palconit"], "title": "Development of a Cacao Disease Identification and Management App Using Deep Learning", "comment": "6 pages, 8 figures, preprint", "summary": "Smallholder cacao producers often rely on outdated farming techniques and face significant challenges from pests and diseases, unlike larger plantations with more resources and expertise. In the Philippines, cacao farmers have limited access to data, information, and good agricultural practices. This study addresses these issues by developing a mobile application for cacao disease identification and management that functions offline, enabling use in remote areas where farms are mostly located. The core of the system is a deep learning model trained to identify cacao diseases accurately. The trained model is integrated into the mobile app to support farmers in field diagnosis. The disease identification model achieved a validation accuracy of 96.93% while the model for detecting cacao black pod infection levels achieved 79.49% validation accuracy. Field testing of the application showed an agreement rate of 84.2% compared with expert cacao technician assessments. This approach empowers smallholder farmers by providing accessible, technology-enabled tools to improve cacao crop health and productivity.", "AI": {"tldr": "Mobile app with deep learning model helps Filipino cacao farmers identify diseases offline, achieving 96.93% accuracy for disease identification and 84.2% agreement with expert assessments.", "motivation": "Smallholder cacao producers in the Philippines face challenges from pests/diseases, outdated farming techniques, and limited access to data and good agricultural practices, especially in remote areas with poor connectivity.", "method": "Developed a mobile application with integrated deep learning model for cacao disease identification that works offline. The model was trained to identify diseases and detect black pod infection levels.", "result": "Disease identification model achieved 96.93% validation accuracy; black pod infection level detection achieved 79.49% validation accuracy. Field testing showed 84.2% agreement rate with expert technician assessments.", "conclusion": "The offline mobile app with deep learning provides accessible, technology-enabled tools to empower smallholder cacao farmers, improving crop health and productivity in remote areas."}}
{"id": "2602.00247", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.00247", "abs": "https://arxiv.org/abs/2602.00247", "authors": ["Samyak Jha", "Junho Kim"], "title": "CAPA: Contribution-Aware Pruning and FFN Approximation for Efficient Large Vision-Language Models", "comment": null, "summary": "Efficient inference in Large Vision-Language Models is constrained by the high cost of processing thousands of visual tokens, yet it remains unclear which tokens and computations can be safely removed. While attention scores are commonly used to estimate visual token importance, they are an imperfect proxy for actual contribution. We show that Attention Contribution, which weights attention probabilities by value vector magnitude, provides a more accurate criterion for visual token selection. Our empirical analysis reveals that visual attention sinks are functionally heterogeneous, comprising Probability Dumps with low contribution that can be safely pruned, and Structural Anchors with high contribution essential for maintaining model performance. Further, we identify substantial redundancy in Feed-Forward Networks (FFNs) associated with visual tokens, particularly in intermediate layers where image tokens exhibit linear behavior. Based on our findings, we introduce CAPA (Contribution-Aware Pruning and FFN Approximation), a dual-strategy framework that prunes visual tokens using attention contribution at critical functional transitions and reduces FFN computation through efficient linear approximations. Experiments on various benchmarks across baselines show that CAPA achieves competent efficiency--performance trade-offs with improved robustness.", "AI": {"tldr": "CAPA improves efficiency in Large Vision-Language Models by pruning unimportant visual tokens using Attention Contribution scores and approximating redundant FFN computations with linear approximations.", "motivation": "Current inference in Large Vision-Language Models is inefficient due to processing thousands of visual tokens, but existing methods using attention scores as importance proxies are inaccurate. There's a need for better criteria to identify which tokens and computations can be safely removed without harming performance.", "method": "Introduces CAPA framework with two strategies: 1) Uses Attention Contribution (weighting attention probabilities by value vector magnitude) to identify and prune visual tokens at critical functional transitions, distinguishing between Probability Dumps (low contribution) and Structural Anchors (high contribution). 2) Reduces FFN computation through efficient linear approximations, especially for intermediate layers where image tokens exhibit linear behavior.", "result": "CAPA achieves competent efficiency-performance trade-offs with improved robustness across various benchmarks and baselines, demonstrating that visual attention sinks are functionally heterogeneous and FFNs associated with visual tokens have substantial redundancy.", "conclusion": "Attention Contribution provides a more accurate criterion than attention scores for visual token selection, enabling effective pruning and computation reduction through the CAPA framework, which maintains model performance while significantly improving inference efficiency."}}
{"id": "2602.00249", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.00249", "abs": "https://arxiv.org/abs/2602.00249", "authors": ["Rishav Pramanik", "Ian E. Nielsen", "Jeff Smith", "Saurav Pandit", "Ravi P. Ramachandran", "Zhaozheng Yin"], "title": "SANEval: Open-Vocabulary Compositional Benchmarks with Failure-mode Diagnosis", "comment": null, "summary": "The rapid progress of text-to-image (T2I) models has unlocked unprecedented creative potential, yet their ability to faithfully render complex prompts involving multiple objects, attributes, and spatial relationships remains a significant bottleneck. Progress is hampered by a lack of adequate evaluation methods; current benchmarks are often restricted to closed-set vocabularies, lack fine-grained diagnostic capabilities, and fail to provide the interpretable feedback necessary to diagnose and remedy specific compositional failures. We solve these challenges by introducing SANEval (Spatial, Attribute, and Numeracy Evaluation), a comprehensive benchmark that establishes a scalable new pipeline for open-vocabulary compositional evaluation. SANEval combines a large language model (LLM) for deep prompt understanding with an LLM-enhanced, open-vocabulary object detector to robustly evaluate compositional adherence, unconstrained by a fixed vocabulary. Through extensive experiments on six state-of-the-art T2I models, we demonstrate that SANEval's automated evaluations provide a more faithful proxy for human assessment; our metric achieves a Spearman's rank correlation with statistically different results than those of existing benchmarks across tasks of attribute binding, spatial relations, and numeracy. To facilitate future research in compositional T2I generation and evaluation, we will release the SANEval dataset and our open-source evaluation pipeline.", "AI": {"tldr": "SANEval is a new benchmark for evaluating text-to-image models' compositional capabilities, using LLMs for prompt understanding and open-vocabulary object detection to assess spatial relations, attribute binding, and numeracy.", "motivation": "Current T2I models struggle with complex prompts involving multiple objects, attributes, and spatial relationships, but progress is hampered by inadequate evaluation methods. Existing benchmarks have closed-set vocabularies, lack fine-grained diagnostics, and don't provide interpretable feedback for diagnosing specific compositional failures.", "method": "SANEval establishes a scalable pipeline combining a large language model (LLM) for deep prompt understanding with an LLM-enhanced, open-vocabulary object detector to evaluate compositional adherence without vocabulary constraints. The benchmark focuses on spatial relations, attribute binding, and numeracy tasks.", "result": "Experiments on six state-of-the-art T2I models show SANEval's automated evaluations provide a more faithful proxy for human assessment, achieving statistically different Spearman's rank correlation results than existing benchmarks across attribute binding, spatial relations, and numeracy tasks.", "conclusion": "SANEval addresses limitations of current evaluation methods by providing comprehensive, open-vocabulary compositional evaluation with better correlation to human judgment. The authors will release the dataset and open-source evaluation pipeline to facilitate future research in compositional T2I generation and evaluation."}}
{"id": "2602.00262", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.00262", "abs": "https://arxiv.org/abs/2602.00262", "authors": ["Huanran Li", "Daniel Pimentel-Alarc\u00f3n"], "title": "Subspace Clustering on Incomplete Data with Self-Supervised Contrastive Learning", "comment": null, "summary": "Subspace clustering aims to group data points that lie in a union of low-dimensional subspaces and finds wide application in computer vision, hyperspectral imaging, and recommendation systems. However, most existing methods assume fully observed data, limiting their effectiveness in real-world scenarios with missing entries. In this paper, we propose a contrastive self-supervised framework, Contrastive Subspace Clustering (CSC), designed for clustering incomplete data. CSC generates masked views of partially observed inputs and trains a deep neural network using a SimCLR-style contrastive loss to learn invariant embeddings. These embeddings are then clustered using sparse subspace clustering. Experiments on six benchmark datasets show that CSC consistently outperforms both classical and deep learning baselines, demonstrating strong robustness to missing data and scalability to large datasets.", "AI": {"tldr": "CSC is a contrastive self-supervised framework for subspace clustering of incomplete data, using masked views and contrastive learning to handle missing entries.", "motivation": "Most subspace clustering methods assume fully observed data, which limits their effectiveness in real-world scenarios where data often has missing entries. There's a need for methods that can handle incomplete data while maintaining clustering performance.", "method": "Proposes Contrastive Subspace Clustering (CSC) - a contrastive self-supervised framework that generates masked views of partially observed inputs, trains a deep neural network using SimCLR-style contrastive loss to learn invariant embeddings, and then clusters these embeddings using sparse subspace clustering.", "result": "Experiments on six benchmark datasets show CSC consistently outperforms both classical and deep learning baselines, demonstrating strong robustness to missing data and scalability to large datasets.", "conclusion": "CSC provides an effective solution for subspace clustering with incomplete data, combining contrastive self-supervised learning with traditional subspace clustering techniques to handle real-world data limitations."}}
{"id": "2602.00265", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.00265", "abs": "https://arxiv.org/abs/2602.00265", "authors": ["Dong Liang", "Yuhao Liu", "Jinyuan Jia", "Youjun Zhao", "Rynson W. H. Lau"], "title": "World-Shaper: A Unified Framework for 360\u00b0 Panoramic Editing", "comment": null, "summary": "Being able to edit panoramic images is crucial for creating realistic 360\u00b0 visual experiences. However, existing perspective-based image editing methods fail to model the spatial structure of panoramas. Conventional cube-map decompositions attempt to overcome this problem but inevitably break global consistency due to their mismatch with spherical geometry. Motivated by this insight, we reformulate panoramic editing directly in the equirectangular projection (ERP) domain and present World-Shaper, a unified geometry-aware framework that bridges panoramic generation and editing within a single editing-centric design. To overcome the scarcity of paired data, we adopt a generate-then-edit paradigm, where controllable panoramic generation serves as an auxiliary stage to synthesize diverse paired examples for supervised editing learning. To address geometric distortion, we introduce a geometry-aware learning strategy that explicitly enforces position-aware shape supervision and implicitly internalizes panoramic priors through progressive training. Extensive experiments on our new benchmark, PEBench, demonstrate that our method achieves superior geometric consistency, editing fidelity, and text controllability compared to SOTA methods, enabling coherent and flexible 360\u00b0 visual world creation with unified editing control. Code, model, and data will be released at our project page: https://world-shaper-project.github.io/", "AI": {"tldr": "World-Shaper: A geometry-aware framework for panoramic image editing that works directly in equirectangular projection to maintain global consistency, using a generate-then-edit paradigm with geometry-aware learning.", "motivation": "Existing perspective-based image editing methods fail to model panoramic spatial structure, and conventional cube-map decompositions break global consistency due to mismatch with spherical geometry. There's a need for panoramic editing that preserves geometric consistency.", "method": "Reformulates panoramic editing directly in ERP domain with a unified geometry-aware framework. Uses generate-then-edit paradigm to create synthetic paired data for supervised learning. Introduces geometry-aware learning with explicit position-aware shape supervision and implicit panoramic priors through progressive training.", "result": "Extensive experiments on new PEBench benchmark show superior geometric consistency, editing fidelity, and text controllability compared to SOTA methods, enabling coherent and flexible 360\u00b0 visual world creation.", "conclusion": "World-Shaper successfully bridges panoramic generation and editing with unified control, overcoming geometric distortion issues and enabling realistic 360\u00b0 visual experiences with maintained global consistency."}}
{"id": "2602.00267", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.00267", "abs": "https://arxiv.org/abs/2602.00267", "authors": ["Gemma Canet Tarr\u00e9s", "Manel Baradad", "Francesc Moreno-Noguer", "Yumeng Li"], "title": "PLACID: Identity-Preserving Multi-Object Compositing via Video Diffusion with Synthetic Trajectories", "comment": null, "summary": "Recent advances in generative AI have dramatically improved photorealistic image synthesis, yet they fall short for studio-level multi-object compositing. This task demands simultaneous (i) near-perfect preservation of each item's identity, (ii) precise background and color fidelity, (iii) layout and design elements control, and (iv) complete, appealing displays showcasing all objects. However, current state-of-the-art models often alter object details, omit or duplicate objects, and produce layouts with incorrect relative sizing or inconsistent item presentations. To bridge this gap, we introduce PLACID, a framework that transforms a collection of object images into an appealing multi-object composite. Our approach makes two main contributions. First, we leverage a pretrained image-to-video (I2V) diffusion model with text control to preserve objects consistency, identities, and background details by exploiting temporal priors from videos. Second, we propose a novel data curation strategy that generates synthetic sequences where randomly placed objects smoothly move to their target positions. This synthetic data aligns with the video model's temporal priors during training. At inference, objects initialized at random positions consistently converge into coherent layouts guided by text, with the final frame serving as the composite image. Extensive quantitative evaluations and user studies demonstrate that PLACID surpasses state-of-the-art methods in multi-object compositing, achieving superior identity, background, and color preservation, with less omitted objects and visually appealing results.", "AI": {"tldr": "PLACID is a framework that transforms multiple object images into appealing multi-object composites using video diffusion models and synthetic training data.", "motivation": "Current generative AI models fail at studio-level multi-object compositing, often altering object details, omitting/duplicating objects, and producing incorrect layouts. There's a need for a solution that preserves object identity, background fidelity, layout control, and creates appealing displays.", "method": "Uses pretrained image-to-video diffusion model with text control to preserve object consistency and background details via temporal priors. Introduces synthetic data curation strategy generating sequences where randomly placed objects move to target positions, aligning with video model's temporal priors during training.", "result": "PLACID surpasses state-of-the-art methods in multi-object compositing, achieving superior identity, background, and color preservation with fewer omitted objects and more visually appealing results, as demonstrated through extensive quantitative evaluations and user studies.", "conclusion": "PLACID effectively bridges the gap in studio-level multi-object compositing by leveraging video diffusion models and synthetic training data to produce high-quality, coherent multi-object composites that preserve object identities and background details."}}
{"id": "2602.00268", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.00268", "abs": "https://arxiv.org/abs/2602.00268", "authors": ["Ariel Shaulov", "Eitan Shaar", "Amit Edenzon", "Lior Wolf"], "title": "TokenTrim: Inference-Time Token Pruning for Autoregressive Long Video Generation", "comment": null, "summary": "Auto-regressive video generation enables long video synthesis by iteratively conditioning each new batch of frames on previously generated content. However, recent work has shown that such pipelines suffer from severe temporal drift, where errors accumulate and amplify over long horizons. We hypothesize that this drift does not primarily stem from insufficient model capacity, but rather from inference-time error propagation. Specifically, we contend that drift arises from the uncontrolled reuse of corrupted latent conditioning tokens during auto-regressive inference. To correct this accumulation of errors, we propose a simple, inference-time method that mitigates temporal drift by identifying and removing unstable latent tokens before they are reused for conditioning. For this purpose, we define unstable tokens as latent tokens whose representations deviate significantly from those of the previously generated batch, indicating potential corruption or semantic drift. By explicitly removing corrupted latent tokens from the auto-regressive context, rather than modifying entire spatial regions or model parameters, our method prevents unreliable latent information from influencing future generation steps. As a result, it significantly improves long-horizon temporal consistency without modifying the model architecture, training procedure, or leaving latent space.", "AI": {"tldr": "Proposes an inference-time method to mitigate temporal drift in auto-regressive video generation by identifying and removing unstable latent tokens before they are reused for conditioning.", "motivation": "Auto-regressive video generation suffers from severe temporal drift where errors accumulate and amplify over long horizons, which stems from inference-time error propagation rather than insufficient model capacity.", "method": "Identifies unstable latent tokens (those whose representations deviate significantly from previous batch) and removes them from auto-regressive context before reuse, preventing corrupted latent information from influencing future generation.", "result": "Significantly improves long-horizon temporal consistency without modifying model architecture, training procedure, or leaving latent space.", "conclusion": "A simple inference-time method effectively mitigates temporal drift in auto-regressive video generation by controlling error propagation through selective removal of unstable latent tokens."}}
{"id": "2602.00288", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.00288", "abs": "https://arxiv.org/abs/2602.00288", "authors": ["Baiqi Li", "Kangyi Zhao", "Ce Zhang", "Chancharik Mitra", "Jean de Dieu Nyandwi", "Gedas Bertasius"], "title": "TimeBlind: A Spatio-Temporal Compositionality Benchmark for Video LLMs", "comment": "For code and data, see https://baiqi-li.github.io/timeblind_project/", "summary": "Fine-grained spatio-temporal understanding is essential for video reasoning and embodied AI. Yet, while Multimodal Large Language Models (MLLMs) master static semantics, their grasp of temporal dynamics remains brittle. We present TimeBlind, a diagnostic benchmark for compositional spatio-temporal understanding. Inspired by cognitive science, TimeBlind categorizes fine-grained temporal understanding into three levels: recognizing atomic events, characterizing event properties, and reasoning about event interdependencies. Unlike benchmarks that conflate recognition with temporal reasoning, TimeBlind leverages a minimal-pairs paradigm: video pairs share identical static visual content but differ solely in temporal structure, utilizing complementary questions to neutralize language priors. Evaluating over 20 state-of-the-art MLLMs (e.g., GPT-5, Gemini 3 Pro) on 600 curated instances (2400 video-question pairs), reveals that the Instance Accuracy (correctly distinguishing both videos in a pair) of the best performing MLLM is only 48.2%, far below the human performance (98.2%). These results demonstrate that even frontier models rely heavily on static visual shortcuts rather than genuine temporal logic, positioning TimeBlind as a vital diagnostic tool for next-generation video understanding. Dataset and code are available at https://baiqi-li.github.io/timeblind_project/ .", "AI": {"tldr": "TimeBlind is a diagnostic benchmark that reveals MLLMs' poor temporal understanding despite strong static visual recognition, showing they rely on visual shortcuts rather than genuine temporal reasoning.", "motivation": "Current Multimodal Large Language Models (MLLMs) excel at static visual semantics but have brittle temporal understanding, creating a need for a diagnostic tool to assess compositional spatio-temporal reasoning capabilities.", "method": "TimeBlind uses a minimal-pairs paradigm with video pairs that share identical static visual content but differ only in temporal structure, complemented by questions that neutralize language priors. It categorizes temporal understanding into three cognitive levels: atomic event recognition, event property characterization, and event interdependency reasoning.", "result": "Evaluation of 20+ state-of-the-art MLLMs (including GPT-5, Gemini 3 Pro) on 600 instances (2400 video-question pairs) shows the best model achieves only 48.2% Instance Accuracy (correctly distinguishing both videos in a pair), far below human performance of 98.2%.", "conclusion": "Even frontier MLLMs rely heavily on static visual shortcuts rather than genuine temporal logic, positioning TimeBlind as a crucial diagnostic tool for advancing video understanding and embodied AI systems."}}
{"id": "2602.00289", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.00289", "abs": "https://arxiv.org/abs/2602.00289", "authors": ["Alan Yuille", "Daniel Kersten"], "title": "Computer Vision and Its Relationship to Cognitive Science: A perspective from Bayes Decision Theory", "comment": null, "summary": "This document presents an introduction to computer vision, and its relationship to Cognitive Science, from the perspective of Bayes Decision Theory (Berger 1985). Computer vision is a vast and complex field, so this overview has a narrow scope and provides a theoretical lens which captures many key concepts. BDT is rich enough to include two different approaches: (i) the Bayesian viewpoint, which gives a conceptually attractive framework for vision with concepts that resonate with Cognitive Science (Griffiths et al., 2024), and (ii) the Deep Neural Network approach whose successes in the real world have made Computer Vision into a trillion-dollar industry and which is motivated by the hierarchical structure of the visual ventral stream. The BDT framework relates and captures the strengths and weakness of these two approaches and, by discussing the limitations of BDT, points the way to how they can be combined in a richer framework.", "AI": {"tldr": "The paper introduces computer vision through the lens of Bayes Decision Theory, showing how it connects Bayesian approaches (with cognitive science links) and deep neural networks (with practical industry success), while pointing toward their integration.", "motivation": "To provide a theoretical framework (Bayes Decision Theory) that can unify and analyze two major approaches in computer vision: Bayesian methods (with cognitive science relevance) and deep neural networks (with practical industry impact).", "method": "Using Bayes Decision Theory as an analytical lens to examine and relate Bayesian approaches (conceptually attractive, cognitive science-aligned) and deep neural network approaches (practically successful, biologically inspired by visual ventral stream).", "result": "BDT framework successfully captures the strengths and weaknesses of both Bayesian and deep learning approaches, revealing their complementary nature and limitations within the BDT framework itself.", "conclusion": "The limitations of Bayes Decision Theory point toward the need for a richer framework that can combine the strengths of both Bayesian and deep neural network approaches for more comprehensive computer vision systems."}}
{"id": "2602.00292", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.00292", "abs": "https://arxiv.org/abs/2602.00292", "authors": ["Rory Driscoll", "Alexandros Christoforos", "Chadbourne Davis"], "title": "LogicGaze: Benchmarking Causal Consistency in Visual Narratives via Counterfactual Verification", "comment": null, "summary": "While sequential reasoning enhances the capability of Vision-Language Models (VLMs) to execute complex multimodal tasks, their reliability in grounding these reasoning chains within actual visual evidence remains insufficiently explored. We introduce LogicGaze, a novel benchmark framework designed to rigorously interrogate whether VLMs can validate sequential causal chains against visual inputs, specifically targeting the pervasive issue of hallucination. Curated from 40,000 video segments from ShareGPT4Video and a subset of Flickr30k imagery, LogicGaze integrates causal sequences with visually contradictory yet linguistically plausible perturbations, compelling models to verify the authenticity of each reasoning step. Our tripartite evaluation protocol - Causal Validation, Grounded Narrative Synthesis, and Perturbation Rejection - exposes significant vulnerabilities in state-of-the-art VLMs such as Qwen2.5-VL-72B. LogicGaze advocates for robust, trustworthy multimodal reasoning, with all resources publicly available in an anonymized repository.", "AI": {"tldr": "LogicGaze is a benchmark framework that tests whether Vision-Language Models can validate sequential causal reasoning chains against visual evidence, exposing their vulnerability to hallucinations when faced with linguistically plausible but visually contradictory perturbations.", "motivation": "While VLMs can perform sequential reasoning for complex multimodal tasks, there's insufficient exploration of whether they can properly ground these reasoning chains in actual visual evidence, particularly regarding hallucination issues where models generate plausible-sounding but visually unsupported causal narratives.", "method": "LogicGaze is a benchmark framework curated from 40,000 video segments from ShareGPT4Video and Flickr30k imagery. It integrates causal sequences with visually contradictory but linguistically plausible perturbations, forcing models to verify each reasoning step. The evaluation uses a tripartite protocol: Causal Validation, Grounded Narrative Synthesis, and Perturbation Rejection.", "result": "The benchmark exposes significant vulnerabilities in state-of-the-art VLMs like Qwen2.5-VL-72B, showing they struggle to validate sequential causal chains against visual evidence and are susceptible to hallucination when faced with plausible but contradictory perturbations.", "conclusion": "LogicGaze advocates for more robust and trustworthy multimodal reasoning in VLMs by providing a rigorous testing framework that reveals their limitations in grounding reasoning chains in visual evidence, with all resources made publicly available to advance the field."}}
{"id": "2602.00309", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.00309", "abs": "https://arxiv.org/abs/2602.00309", "authors": ["Samuel Church", "Joshua D. Warner", "Danyal Maqbool", "Xin Tie", "Junjie Hu", "Meghan G. Lubner", "Tyler J. Bradshaw"], "title": "Opportunistic Promptable Segmentation: Leveraging Routine Radiological Annotations to Guide 3D CT Lesion Segmentation", "comment": null, "summary": "The development of machine learning models for CT imaging depends on the availability of large, high-quality, and diverse annotated datasets. Although large volumes of CT images and reports are readily available in clinical picture archiving and communication systems (PACS), 3D segmentations of critical findings are costly to obtain, typically requiring extensive manual annotation by radiologists. On the other hand, it is common for radiologists to provide limited annotations of findings during routine reads, such as line measurements and arrows, that are often stored in PACS as GSPS objects. We posit that these sparse annotations can be extracted along with CT volumes and converted into 3D segmentations using promptable segmentation models, a paradigm we term Opportunistic Promptable Segmentation. To enable this paradigm, we propose SAM2CT, the first promptable segmentation model designed to convert radiologist annotations into 3D segmentations in CT volumes. SAM2CT builds upon SAM2 by extending the prompt encoder to support arrow and line inputs and by introducing Memory-Conditioned Memories (MCM), a memory encoding strategy tailored to 3D medical volumes. On public lesion segmentation benchmarks, SAM2CT outperforms existing promptable segmentation models and similarly trained baselines, achieving Dice similarity coefficients of 0.649 for arrow prompts and 0.757 for line prompts. Applying the model to pre-existing GSPS annotations from a clinical PACS (N = 60), SAM2CT generates 3D segmentations that are clinically acceptable or require only minor adjustments in 87% of cases, as scored by radiologists. Additionally, SAM2CT demonstrates strong zero-shot performance on select Emergency Department findings. These results suggest that large-scale mining of historical GSPS annotations represents a promising and scalable approach for generating 3D CT segmentation datasets.", "AI": {"tldr": "SAM2CT: A promptable segmentation model that converts radiologists' sparse annotations (arrows/lines) in PACS into 3D CT segmentations, enabling large-scale dataset creation from existing clinical data.", "motivation": "3D CT segmentation datasets are costly to create due to manual annotation requirements, while abundant sparse annotations (arrows, lines) exist in clinical PACS systems that could be leveraged for automated segmentation generation.", "method": "SAM2CT extends SAM2 with enhanced prompt encoder for arrow/line inputs and introduces Memory-Conditioned Memories (MCM) for 3D medical volumes, enabling conversion of sparse annotations into 3D segmentations.", "result": "Achieves Dice scores of 0.649 (arrow prompts) and 0.757 (line prompts) on public benchmarks; generates clinically acceptable segmentations in 87% of cases from real PACS data; shows strong zero-shot performance on ED findings.", "conclusion": "Opportunistic Promptable Segmentation using SAM2CT enables scalable mining of historical PACS annotations to create large 3D CT segmentation datasets, addressing data scarcity in medical imaging research."}}
{"id": "2602.00314", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.00314", "abs": "https://arxiv.org/abs/2602.00314", "authors": ["Apostol Vassilev", "Munawar Hasan", "Edward Griffor", "Honglan Jin", "Pavel Piliptchak", "Mahima Arora", "Thoshitha Gamage"], "title": "On the Assessment of Sensitivity of Autonomous Vehicle Perception", "comment": "21 pages, 17 figures", "summary": "The viability of automated driving is heavily dependent on the performance of perception systems to provide real-time accurate and reliable information for robust decision-making and maneuvers. These systems must perform reliably not only under ideal conditions, but also when challenged by natural and adversarial driving factors. Both of these types of interference can lead to perception errors and delays in detection and classification. Hence, it is essential to assess the robustness of the perception systems of automated vehicles (AVs) and explore strategies for making perception more reliable. We approach this problem by evaluating perception performance using predictive sensitivity quantification based on an ensemble of models, capturing model disagreement and inference variability across multiple models, under adverse driving scenarios in both simulated environments and real-world conditions. A notional architecture for assessing perception performance is proposed. A perception assessment criterion is developed based on an AV's stopping distance at a stop sign on varying road surfaces, such as dry and wet asphalt, and vehicle speed. Five state-of-the-art computer vision models are used, including YOLO (v8-v9), DEtection TRansformer (DETR50, DETR101), Real-Time DEtection TRansformer (RT-DETR)in our experiments. Diminished lighting conditions, e.g., resulting from the presence of fog and low sun altitude, have the greatest impact on the performance of the perception models. Additionally, adversarial road conditions such as occlusions of roadway objects increase perception sensitivity and model performance drops when faced with a combination of adversarial road conditions and inclement weather conditions. Also, it is demonstrated that the greater the distance to a roadway object, the greater the impact on perception performance, hence diminished perception robustness.", "AI": {"tldr": "The paper evaluates perception system robustness in automated vehicles using ensemble modeling to assess performance under adverse conditions like weather, lighting, and occlusions, with YOLO and DETR models tested on stop sign detection scenarios.", "motivation": "Automated driving requires reliable perception systems that must perform accurately under both ideal and challenging conditions (natural and adversarial factors). Perception errors and delays can compromise safety, so assessing and improving perception robustness is essential for AV viability.", "method": "Uses predictive sensitivity quantification with an ensemble of five state-of-the-art computer vision models (YOLO v8-v9, DETR50, DETR101, RT-DETR) to capture model disagreement and inference variability. Proposes a notional architecture for perception assessment and develops a criterion based on AV stopping distance at stop signs on varying road surfaces (dry/wet asphalt) at different speeds. Evaluates performance under adverse scenarios in simulated and real-world conditions.", "result": "Diminished lighting conditions (fog, low sun altitude) have the greatest negative impact on perception performance. Adversarial road conditions (occlusions) increase perception sensitivity, and performance drops further with combined adverse conditions. Greater distance to roadway objects reduces perception robustness. The ensemble approach effectively quantifies sensitivity and variability across models.", "conclusion": "Perception system robustness is significantly affected by environmental factors, especially lighting and combined adverse conditions. The proposed ensemble-based assessment framework provides valuable insights for improving perception reliability in AVs, highlighting the need for robust perception systems that can handle real-world driving challenges."}}
{"id": "2602.00340", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.00340", "abs": "https://arxiv.org/abs/2602.00340", "authors": ["Alexandros Christoforos", "Sarah Jenkins", "Michael Brown", "Tuan Pham", "David Chen"], "title": "Bridging the Semantic Chasm: Synergistic Conceptual Anchoring for Generalized Few-Shot and Zero-Shot OOD Perception", "comment": null, "summary": "This manuscript presents a pioneering Synergistic Neural Agents Network (SynerNet) framework designed to mitigate the phenomenon of cross-modal alignment degeneration in Vision-Language Models (VLMs) when encountering Out-of-Distribution (OOD) concepts. Specifically, four specialized computational units - visual perception, linguistic context, nominal embedding, and global coordination - collaboratively rectify modality disparities via a structured message-propagation protocol. The principal contributions encompass a multi-agent latent space nomenclature acquisition framework, a semantic context-interchange algorithm for enhanced few-shot adaptation, and an adaptive dynamic equilibrium mechanism. Empirical evaluations conducted on the VISTA-Beyond benchmark demonstrate that SynerNet yields substantial performance augmentations in both few-shot and zero-shot scenarios, exhibiting precision improvements ranging from 1.2% to 5.4% across a diverse array of domains.", "AI": {"tldr": "SynerNet framework uses four neural agents to fix cross-modal alignment degeneration in VLMs for OOD concepts, improving few-shot and zero-shot performance by 1.2-5.4% on VISTA-Beyond benchmark.", "motivation": "Address the problem of cross-modal alignment degeneration in Vision-Language Models when encountering Out-of-Distribution concepts, which causes performance degradation in few-shot and zero-shot scenarios.", "method": "Synergistic Neural Agents Network (SynerNet) with four specialized units: visual perception, linguistic context, nominal embedding, and global coordination. Uses structured message-propagation protocol, multi-agent latent space nomenclature acquisition, semantic context-interchange algorithm, and adaptive dynamic equilibrium mechanism.", "result": "Substantial performance improvements on VISTA-Beyond benchmark: 1.2% to 5.4% precision gains across diverse domains in both few-shot and zero-shot scenarios.", "conclusion": "SynerNet effectively mitigates cross-modal alignment degeneration for OOD concepts through collaborative neural agents, demonstrating significant performance enhancements in challenging vision-language tasks."}}
{"id": "2602.00344", "categories": ["cs.CV", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.00344", "abs": "https://arxiv.org/abs/2602.00344", "authors": ["Beidi Zhao", "Wenlong Deng", "Xinting Liao", "Yushu Li", "Nazim Shaikh", "Yao Nie", "Xiaoxiao Li"], "title": "When RAG Hurts: Diagnosing and Mitigating Attention Distraction in Retrieval-Augmented LVLMs", "comment": "18 pages, 10 figures", "summary": "While Retrieval-Augmented Generation (RAG) is one of the dominant paradigms for enhancing Large Vision-Language Models (LVLMs) on knowledge-based VQA tasks, recent work attributes RAG failures to insufficient attention towards the retrieved context, proposing to reduce the attention allocated to image tokens. In this work, we identify a distinct failure mode that previous study overlooked: Attention Distraction (AD). When the retrieved context is sufficient (highly relevant or including the correct answer), the retrieved text suppresses the visual attention globally, and the attention on image tokens shifts away from question-relevant regions. This leads to failures on questions the model could originally answer correctly without the retrieved text. To mitigate this issue, we propose MAD-RAG, a training-free intervention that decouples visual grounding from context integration through a dual-question formulation, combined with attention mixing to preserve image-conditioned evidence. Extensive experiments on OK-VQA, E-VQA, and InfoSeek demonstrate that MAD-RAG consistently outperforms existing baselines across different model families, yielding absolute gains of up to 4.76%, 9.20%, and 6.18% over the vanilla RAG baseline. Notably, MAD-RAG rectifies up to 74.68% of failure cases with negligible computational overhead.", "AI": {"tldr": "MAD-RAG addresses Attention Distraction in RAG for vision-language models by decoupling visual grounding from context integration using dual-question formulation and attention mixing, improving performance on knowledge-based VQA tasks.", "motivation": "The paper identifies a new failure mode in RAG for vision-language models called Attention Distraction, where retrieved context suppresses visual attention globally and shifts attention away from question-relevant image regions, even when the retrieved context is sufficient. This causes models to fail on questions they could originally answer correctly without retrieval.", "method": "Proposes MAD-RAG, a training-free intervention that uses dual-question formulation to decouple visual grounding from context integration. It combines this with attention mixing to preserve image-conditioned evidence, allowing the model to maintain proper visual attention while integrating retrieved context.", "result": "Extensive experiments on OK-VQA, E-VQA, and InfoSeek show MAD-RAG consistently outperforms existing baselines across different model families, achieving absolute gains up to 4.76%, 9.20%, and 6.18% over vanilla RAG. It rectifies up to 74.68% of failure cases with negligible computational overhead.", "conclusion": "MAD-RAG effectively addresses the Attention Distraction problem in RAG for vision-language models through a simple yet effective training-free approach that preserves visual grounding while integrating retrieved context, significantly improving performance on knowledge-based VQA tasks."}}
{"id": "2602.00347", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.00347", "abs": "https://arxiv.org/abs/2602.00347", "authors": ["Chongyu Qu", "Zhengyi Lu", "Yuxiang Lai", "Thomas Z. Li", "Junchao Zhu", "Junlin Guo", "Juming Xiong", "Yanfan Zhu", "Yuechen Yang", "Allen J. Luna", "Kim L. Sandler", "Bennett A. Landman", "Yuankai Huo"], "title": "AdaFuse: Adaptive Multimodal Fusion for Lung Cancer Risk Prediction via Reinforcement Learning", "comment": null, "summary": "Multimodal fusion has emerged as a promising paradigm for disease diagnosis and prognosis, integrating complementary information from heterogeneous data sources such as medical images, clinical records, and radiology reports. However, existing fusion methods process all available modalities through the network, either treating them equally or learning to assign different contribution weights, leaving a fundamental question unaddressed: for a given patient, should certain modalities be used at all? We present AdaFuse, an adaptive multimodal fusion framework that leverages reinforcement learning (RL) to learn patient-specific modality selection and fusion strategies for lung cancer risk prediction. AdaFuse formulates multimodal fusion as a sequential decision process, where the policy network iteratively decides whether to incorporate an additional modality or proceed to prediction based on the information already acquired. This sequential formulation enables the model to condition each selection on previously observed modalities and terminate early when sufficient information is available, rather than committing to a fixed subset upfront. We evaluate AdaFuse on the National Lung Screening Trial (NLST) dataset. Experimental results demonstrate that AdaFuse achieves the highest AUC (0.762) compared to the best single-modality baseline (0.732), the best fixed fusion strategy (0.759), and adaptive baselines including DynMM (0.754) and MoE (0.742), while using fewer FLOPs than all triple-modality methods. Our work demonstrates the potential of reinforcement learning for personalized multimodal fusion in medical imaging, representing a shift from uniform fusion strategies toward adaptive diagnostic pipelines that learn when to consult additional modalities and when existing information suffices for accurate prediction.", "AI": {"tldr": "AdaFuse: RL-based adaptive multimodal fusion framework that learns patient-specific modality selection for lung cancer risk prediction, achieving better performance with fewer computations.", "motivation": "Existing multimodal fusion methods process all available modalities equally or with learned weights, but don't address whether certain modalities should be used at all for individual patients. There's a need for personalized modality selection rather than uniform fusion strategies.", "method": "AdaFuse uses reinforcement learning to formulate multimodal fusion as a sequential decision process. A policy network iteratively decides whether to incorporate additional modalities or proceed to prediction based on acquired information, enabling early termination when sufficient.", "result": "On NLST dataset, AdaFuse achieves highest AUC (0.762) vs best single-modality (0.732), best fixed fusion (0.759), and adaptive baselines DynMM (0.754) and MoE (0.742), while using fewer FLOPs than triple-modality methods.", "conclusion": "RL enables personalized multimodal fusion in medical imaging, shifting from uniform strategies toward adaptive diagnostic pipelines that learn when to consult additional modalities vs when existing information suffices."}}
{"id": "2602.00348", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.00348", "abs": "https://arxiv.org/abs/2602.00348", "authors": ["Zhengyi Lu", "Ming Lu", "Chongyu Qu", "Junchao Zhu", "Junlin Guo", "Marilyn Lionts", "Yanfan Zhu", "Yuechen Yang", "Tianyuan Yao", "Jayasai Rajagopal", "Bennett Allan Landman", "Xiao Wang", "Xinqiang Yan", "Yuankai Huo"], "title": "MASC: Metal-Aware Sampling and Correction via Reinforcement Learning for Accelerated MRI", "comment": null, "summary": "Metal implants in MRI cause severe artifacts that degrade image quality and hinder clinical diagnosis. Traditional approaches address metal artifact reduction (MAR) and accelerated MRI acquisition as separate problems. We propose MASC, a unified reinforcement learning framework that jointly optimizes metal-aware k-space sampling and artifact correction for accelerated MRI. To enable supervised training, we construct a paired MRI dataset using physics-based simulation, generating k-space data and reconstructions for phantoms with and without metal implants. This paired dataset provides simulated 3D MRI scans with and without metal implants, where each metal-corrupted sample has an exactly matched clean reference, enabling direct supervision for both artifact reduction and acquisition policy learning. We formulate active MRI acquisition as a sequential decision-making problem, where an artifact-aware Proximal Policy Optimization (PPO) agent learns to select k-space phase-encoding lines under a limited acquisition budget. The agent operates on undersampled reconstructions processed through a U-Net-based MAR network, learning patterns that maximize reconstruction quality. We further propose an end-to-end training scheme where the acquisition policy learns to select k-space lines that best support artifact removal while the MAR network simultaneously adapts to the resulting undersampling patterns. Experiments demonstrate that MASC's learned policies outperform conventional sampling strategies, and end-to-end training improves performance compared to using a frozen pre-trained MAR network, validating the benefit of joint optimization. Cross-dataset experiments on FastMRI with physics-based artifact simulation further confirm generalization to realistic clinical MRI data. The code and models of MASC have been made publicly available: https://github.com/hrlblab/masc", "AI": {"tldr": "MASC is a unified RL framework that jointly optimizes metal-aware k-space sampling and artifact correction for accelerated MRI with metal implants.", "motivation": "Metal implants in MRI cause severe artifacts that degrade image quality, and traditional approaches treat metal artifact reduction (MAR) and accelerated MRI acquisition as separate problems, missing opportunities for joint optimization.", "method": "Uses reinforcement learning with PPO agent for sequential k-space line selection under acquisition budget, paired with U-Net-based MAR network. Creates physics-based simulated dataset with paired metal-corrupted/clean MRI scans for supervised training. End-to-end training jointly optimizes acquisition policy and artifact correction network.", "result": "Learned policies outperform conventional sampling strategies; end-to-end training improves performance over frozen pre-trained MAR network. Cross-dataset experiments on FastMRI with physics-based simulation confirm generalization to realistic clinical MRI data.", "conclusion": "MASC demonstrates the benefit of joint optimization of acquisition and artifact correction for metal-implant MRI, providing a unified framework that outperforms separate approaches and generalizes to clinical data."}}
{"id": "2602.00350", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.00350", "abs": "https://arxiv.org/abs/2602.00350", "authors": ["Ignacy Kolton", "Kacper Marzol", "Pawe\u0142 Batorski", "Marcin Mazur", "Paul Swoboda", "Przemys\u0142aw Spurek"], "title": "ReLAPSe: Reinforcement-Learning-trained Adversarial Prompt Search for Erased concepts in unlearned diffusion models", "comment": null, "summary": "Machine unlearning is a key defense mechanism for removing unauthorized concepts from text-to-image diffusion models, yet recent evidence shows that latent visual information often persists after unlearning. Existing adversarial approaches for exploiting this leakage are constrained by fundamental limitations: optimization-based methods are computationally expensive due to per-instance iterative search. At the same time, reasoning-based and heuristic techniques lack direct feedback from the target model's latent visual representations. To address these challenges, we introduce ReLAPSe, a policy-based adversarial framework that reformulates concept restoration as a reinforcement learning problem. ReLAPSe trains an agent using Reinforcement Learning with Verifiable Rewards (RLVR), leveraging the diffusion model's noise prediction loss as a model-intrinsic and verifiable feedback signal. This closed-loop design directly aligns textual prompt manipulation with latent visual residuals, enabling the agent to learn transferable restoration strategies rather than optimizing isolated prompts. By pioneering the shift from per-instance optimization to global policy learning, ReLAPSe achieves efficient, near-real-time recovery of fine-grained identities and styles across multiple state-of-the-art unlearning methods, providing a scalable tool for rigorous red-teaming of unlearned diffusion models. Some experimental evaluations involve sensitive visual concepts, such as nudity. Code is available at https://github.com/gmum/ReLaPSe", "AI": {"tldr": "ReLAPSe is a reinforcement learning framework that efficiently restores unlearned concepts in diffusion models by learning transferable restoration strategies rather than optimizing individual prompts.", "motivation": "Current adversarial approaches for exploiting leakage in unlearned diffusion models have limitations: optimization-based methods are computationally expensive, while reasoning-based techniques lack direct feedback from the model's latent visual representations.", "method": "ReLAPSe reformulates concept restoration as a reinforcement learning problem using RLVR (Reinforcement Learning with Verifiable Rewards), leveraging the diffusion model's noise prediction loss as model-intrinsic feedback to train an agent for textual prompt manipulation.", "result": "Achieves efficient, near-real-time recovery of fine-grained identities and styles across multiple state-of-the-art unlearning methods, providing scalable red-teaming capabilities.", "conclusion": "ReLAPSe pioneers the shift from per-instance optimization to global policy learning, enabling effective restoration of unlearned concepts and rigorous evaluation of unlearning defenses in diffusion models."}}
{"id": "2602.00381", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.00381", "abs": "https://arxiv.org/abs/2602.00381", "authors": ["Kezia Minni", "Qiang Zhang", "Monoshiz Mahbub Khan", "Zhe Yu"], "title": "Modeling Image-Caption Rating from Comparative Judgments", "comment": null, "summary": "Rating the accuracy of captions in describing images is time-consuming and subjective for humans. In contrast, it is often easier for people to compare two captions and decide which one better matches a given image. In this work, we propose a machine learning framework that models such comparative judgments instead of direct ratings. The model can then be applied to rank unseen image-caption pairs in the same way as a regression model trained on direct ratings. Using the VICR dataset, we extract visual features with ResNet-50 and text features with MiniLM, then train both a regression model and a comparative learning model. While the regression model achieves better performance (Pearson's $\u03c1$: 0.7609 and Spearman's $r_s$: 0.7089), the comparative learning model steadily improves with more data and approaches the regression baseline. In addition, a small-scale human evaluation study comparing absolute rating, pairwise comparison, and same-image comparison shows that comparative annotation yields faster results and has greater agreement among human annotators. These results suggest that comparative learning can effectively model human preferences while significantly reducing the cost of human annotations.", "AI": {"tldr": "Comparative learning framework for image caption evaluation that models pairwise human comparisons instead of direct ratings, reducing annotation cost while maintaining effectiveness.", "motivation": "Direct rating of caption accuracy is time-consuming and subjective for humans, while pairwise comparisons are easier and faster for annotators.", "method": "Propose comparative learning framework that models human comparative judgments; extract visual features with ResNet-50 and text features with MiniLM; train both regression and comparative learning models on VICR dataset.", "result": "Regression model performs better (Pearson's \u03c1: 0.7609, Spearman's r_s: 0.7089), but comparative learning model steadily improves with more data and approaches regression baseline; human evaluation shows comparative annotation is faster with greater inter-annotator agreement.", "conclusion": "Comparative learning can effectively model human preferences while significantly reducing annotation costs, offering a practical alternative to direct rating approaches."}}
{"id": "2602.00385", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.00385", "abs": "https://arxiv.org/abs/2602.00385", "authors": ["Bsher Karbouj", "Adam Michael Altenbuchner", "Joerg Krueger"], "title": "Deep Learning-Based Object Detection for Autonomous Vehicles: A Comparative Study of One-Stage and Two-Stage Detectors on Basic Traffic Objects", "comment": null, "summary": "Object detection is a crucial component in autonomous vehicle systems. It enables the vehicle to perceive and understand its environment by identifying and locating various objects around it. By utilizing advanced imaging and deep learning techniques, autonomous vehicle systems can rapidly and accurately identify objects based on their features. Different deep learning methods vary in their ability to accurately detect and classify objects in autonomous vehicle systems. Selecting the appropriate method significantly impacts system performance, robustness, and efficiency in real-world driving scenarios. While several generic deep learning architectures like YOLO, SSD, and Faster R-CNN have been proposed, guidance on their suitability for specific autonomous driving applications is often limited. The choice of method affects detection accuracy, processing speed, environmental robustness, sensor integration, scalability, and edge case handling. This study provides a comprehensive experimental analysis comparing two prominent object detection models: YOLOv5 (a one-stage detector) and Faster R-CNN (a two-stage detector). Their performance is evaluated on a diverse dataset combining real and synthetic images, considering various metrics including mean Average Precision (mAP), recall, and inference speed. The findings reveal that YOLOv5 demonstrates superior performance in terms of mAP, recall, and training efficiency, particularly as dataset size and image resolution increase. However, Faster R-CNN shows advantages in detecting small, distant objects and performs well in challenging lighting conditions. The models' behavior is also analyzed under different confidence thresholds and in various real-world scenarios, providing insights into their applicability for autonomous driving systems.", "AI": {"tldr": "Experimental comparison of YOLOv5 vs Faster R-CNN for autonomous vehicle object detection, showing YOLOv5 excels in mAP, recall, and training efficiency, while Faster R-CNN better detects small distant objects and handles challenging lighting.", "motivation": "Object detection is critical for autonomous vehicles, but guidance on selecting appropriate deep learning methods (YOLO, SSD, Faster R-CNN) for specific driving applications is limited. The choice significantly impacts system performance, robustness, and efficiency in real-world scenarios.", "method": "Comprehensive experimental analysis comparing two prominent models: YOLOv5 (one-stage detector) and Faster R-CNN (two-stage detector). Performance evaluated on diverse dataset combining real and synthetic images using metrics including mean Average Precision (mAP), recall, and inference speed.", "result": "YOLOv5 demonstrates superior performance in mAP, recall, and training efficiency, especially as dataset size and image resolution increase. Faster R-CNN shows advantages in detecting small, distant objects and performs well in challenging lighting conditions.", "conclusion": "Both models have distinct strengths for autonomous driving applications. YOLOv5 offers better overall performance and efficiency, while Faster R-CNN excels in specific challenging scenarios. The analysis provides insights for selecting appropriate object detection methods based on specific autonomous driving requirements."}}
{"id": "2602.00391", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.00391", "abs": "https://arxiv.org/abs/2602.00391", "authors": ["Alberto Mario Ceballos-Arroyo", "Shrikanth M. Yadav", "Chu-Hsuan Lin", "Jisoo Kim", "Geoffrey S. Young", "Huaizu Jiang", "Lei Qin"], "title": "Robust automatic brain vessel segmentation in 3D CTA scans using dynamic 4D-CTA data", "comment": "16 pages, 8 figures", "summary": "In this study, we develop a novel methodology for annotating the brain vasculature using dynamic 4D-CTA head scans. By using multiple time points from dynamic CTA acquisitions, we subtract bone and soft tissue to enhance the visualization of arteries and veins, reducing the effort required to obtain manual annotations of brain vessels. We then train deep learning models on our ground truth annotations by using the same segmentation for multiple phases from the dynamic 4D-CTA collection, effectively enlarging our dataset by 4 to 5 times and inducing robustness to contrast phases. In total, our dataset comprises 110 training images from 25 patients and 165 test images from 14 patients. In comparison with two similarly-sized datasets for CTA-based brain vessel segmentation, a nnUNet model trained on our dataset can achieve significantly better segmentations across all vascular regions, with an average mDC of 0.846 for arteries and 0.957 for veins in the TopBrain dataset. Furthermore, metrics such as average directed Hausdorff distance (adHD) and topology sensitivity (tSens) reflected similar trends: using our dataset resulted in low error margins (aDHD of 0.304 mm for arteries and 0.078 for veins) and high sensitivity (tSens of 0.877 for arteries and 0.974 for veins), indicating excellent accuracy in capturing vessel morphology. Our code and model weights are available online: https://github.com/alceballosa/robust-vessel-segmentation", "AI": {"tldr": "Novel method for brain vessel segmentation using dynamic 4D-CTA scans with bone/soft tissue subtraction, achieving state-of-the-art performance with high accuracy metrics.", "motivation": "Manual annotation of brain vasculature is labor-intensive. Dynamic 4D-CTA scans provide multiple time points that can be leveraged to enhance vessel visualization and create robust training datasets.", "method": "Develop methodology using dynamic 4D-CTA head scans with bone/soft tissue subtraction to create ground truth annotations. Train deep learning models using same segmentations across multiple phases, effectively expanding dataset 4-5x and increasing robustness to contrast phases.", "result": "Dataset of 110 training images from 25 patients and 165 test images from 14 patients. nnUNet trained on this dataset outperforms similar-sized datasets with mDC of 0.846 (arteries) and 0.957 (veins), low error margins (aDHD 0.304mm arteries, 0.078mm veins), and high topology sensitivity (0.877 arteries, 0.974 veins).", "conclusion": "The proposed dynamic 4D-CTA annotation methodology enables creation of high-quality training datasets for brain vessel segmentation, resulting in superior performance compared to existing approaches with excellent accuracy in capturing vessel morphology."}}
{"id": "2602.00393", "categories": ["cs.CV", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.00393", "abs": "https://arxiv.org/abs/2602.00393", "authors": ["Gabriel Bromonschenkel", "Alessandro L. Koerich", "Thiago M. Paix\u00e3o", "Hil\u00e1rio Tomaz Alves de Oliveira"], "title": "Brazilian Portuguese Image Captioning with Transformers: A Study on Cross-Native-Translated Dataset", "comment": "Accepted to JBCS. 18 pages, 11 figures", "summary": "Image captioning (IC) refers to the automatic generation of natural language descriptions for images, with applications ranging from social media content generation to assisting individuals with visual impairments. While most research has been focused on English-based models, low-resource languages such as Brazilian Portuguese face significant challenges due to the lack of specialized datasets and models. Several studies create datasets by automatically translating existing ones to mitigate resource scarcity. This work addresses this gap by proposing a cross-native-translated evaluation of Transformer-based vision and language models for Brazilian Portuguese IC. We use a version of Flickr30K comprised of captions manually created by native Brazilian Portuguese speakers and compare it to a version with captions automatically translated from English to Portuguese. The experiments include a cross-context approach, where models trained on one dataset are tested on the other to assess the translation impact. Additionally, we incorporate attention maps for model inference interpretation and use the CLIP-Score metric to evaluate the image-description alignment. Our findings show that Swin-DistilBERTimbau consistently outperforms other models, demonstrating strong generalization across datasets. ViTucano, a Brazilian Portuguese pre-trained VLM, surpasses larger multilingual models (GPT-4o, LLaMa 3.2 Vision) in traditional text-based evaluation metrics, while GPT-4 models achieve the highest CLIP-Score, highlighting improved image-text alignment. Attention analysis reveals systematic biases, including gender misclassification, object enumeration errors, and spatial inconsistencies. The datasets and the models generated and analyzed during the current study are available in: https://github.com/laicsiifes/transformer-caption-ptbr.", "AI": {"tldr": "This paper evaluates Transformer-based vision-language models for Brazilian Portuguese image captioning, comparing native vs. translated datasets, and finds Swin-DistilBERTimbau performs best overall while GPT-4 achieves highest image-text alignment.", "motivation": "Addressing the lack of specialized datasets and models for Brazilian Portuguese image captioning, which faces challenges as a low-resource language compared to English-focused research.", "method": "Cross-native-translated evaluation using Flickr30K with native Brazilian Portuguese captions vs. automatically translated captions. Cross-context training/testing, attention map analysis for interpretation, and CLIP-Score for image-description alignment assessment.", "result": "Swin-DistilBERTimbau consistently outperforms other models with strong generalization. ViTucano (Brazilian Portuguese pre-trained VLM) beats larger multilingual models in text metrics, while GPT-4 achieves highest CLIP-Score. Attention analysis reveals systematic biases including gender misclassification and spatial inconsistencies.", "conclusion": "The study provides valuable insights for Brazilian Portuguese IC, showing model performance variations between native and translated datasets, with attention analysis revealing important biases that need addressing in future work."}}
{"id": "2602.00394", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.00394", "abs": "https://arxiv.org/abs/2602.00394", "authors": ["Manoj Reddy Bethi", "Sai Rupa Jhade", "Pravallika Yaganti", "Monoshiz Mahbub Khan", "Zhe Yu"], "title": "Modeling Art Evaluations from Comparative Judgments: A Deep Learning Approach to Predicting Aesthetic Preferences", "comment": null, "summary": "Modeling human aesthetic judgments in visual art presents significant challenges due to individual preference variability and the high cost of obtaining labeled data. To reduce cost of acquiring such labels, we propose to apply a comparative learning framework based on pairwise preference assessments rather than direct ratings. This approach leverages the Law of Comparative Judgment, which posits that relative choices exhibit less cognitive burden and greater cognitive consistency than direct scoring. We extract deep convolutional features from painting images using ResNet-50 and develop both a deep neural network regression model and a dual-branch pairwise comparison model. We explored four research questions: (RQ1) How does the proposed deep neural network regression model with CNN features compare to the baseline linear regression model using hand-crafted features? (RQ2) How does pairwise comparative learning compare to regression-based prediction when lacking access to direct rating values? (RQ3) Can we predict individual rater preferences through within-rater and cross-rater analysis? (RQ4) What is the annotation cost trade-off between direct ratings and comparative judgments in terms of human time and effort? Our results show that the deep regression model substantially outperforms the baseline, achieving up to $328\\%$ improvement in $R^2$. The comparative model approaches regression performance despite having no access to direct rating values, validating the practical utility of pairwise comparisons. However, predicting individual preferences remains challenging, with both within-rater and cross-rater performance significantly lower than average rating prediction. Human subject experiments reveal that comparative judgments require $60\\%$ less annotation time per item, demonstrating superior annotation efficiency for large-scale preference modeling.", "AI": {"tldr": "Deep learning models using pairwise comparisons outperform traditional methods for predicting aesthetic judgments in art, with 60% faster annotation time.", "motivation": "Human aesthetic judgment modeling is challenging due to individual preference variability and high labeling costs. The paper aims to reduce annotation costs by using comparative learning instead of direct ratings.", "method": "Used ResNet-50 for CNN feature extraction, developed deep neural network regression and dual-branch pairwise comparison models. Explored four research questions comparing regression vs. baseline, pairwise vs. regression, individual preference prediction, and annotation cost trade-offs.", "result": "Deep regression model achieved up to 328% improvement in R\u00b2 over baseline. Pairwise comparison model approached regression performance without direct rating access. Individual preference prediction remained challenging. Comparative judgments required 60% less annotation time per item.", "conclusion": "Pairwise comparative learning is an efficient alternative to direct ratings for aesthetic judgment modeling, offering substantial annotation time savings while maintaining competitive prediction performance."}}
{"id": "2602.00395", "categories": ["cs.CV", "cs.LG", "math.OC"], "pdf": "https://arxiv.org/pdf/2602.00395", "abs": "https://arxiv.org/abs/2602.00395", "authors": ["Roger Hsiao", "Yuchen Fang", "Xiangru Huang", "Ruilong Li", "Hesam Rabeti", "Zan Gojcic", "Javad Lavaei", "James Demmel", "Sophia Shao"], "title": "3DGS$^2$-TR: Scalable Second-Order Trust-Region Method for 3D Gaussian Splatting", "comment": null, "summary": "We propose 3DGS$^2$-TR,a second-order optimizer for accelerating the scene training problem in 3D Gaussian Splatting (3DGS). Unlike existing second-order approaches that rely on explicit or dense curvature representations, such as 3DGS-LM (H\u00f6llein et al., 2025) or 3DGS2 (Lan et al., 2025), our method approximates curvature using only the diagonal of the Hessian matrix, efficiently via Hutchinson's method. Our approach is fully matrix-free and has the same complexity as ADAM (Kingma, 2024), $O(n)$ in both computation and memory costs. To ensure stable optimization in the presence of strong nonlinearity in the 3DGS rasterization process, we introduce a parameter-wise trust-region technique based on the squared Hellinger distance, regularizing updates to Gaussian parameters. Under identical parameter initialization and without densification, 3DGS$^2$-TR is able to achieve better reconstruction quality on standard datasets, using 50% fewer training iterations compared to ADAM, while incurring less than 1GB of peak GPU memory overhead (17% more than ADAM and 85% less than 3DGS-LM), enabling scalability to very large scenes and potentially to distributed training settings.", "AI": {"tldr": "3DGS\u00b2-TR: A second-order optimizer for 3D Gaussian Splatting that uses diagonal Hessian approximation via Hutchinson's method with parameter-wise trust-region regularization, achieving 50% faster training with minimal memory overhead.", "motivation": "Existing second-order optimizers for 3DGS (like 3DGS-LM and 3DGS2) rely on explicit or dense curvature representations that are computationally expensive and memory-intensive, limiting scalability to large scenes and distributed training.", "method": "Proposes 3DGS\u00b2-TR that approximates curvature using only the diagonal of the Hessian matrix via Hutchinson's method, making it matrix-free with O(n) complexity. Introduces parameter-wise trust-region technique based on squared Hellinger distance to regularize updates to Gaussian parameters for stable optimization despite nonlinear rasterization.", "result": "Achieves better reconstruction quality with 50% fewer training iterations compared to ADAM, with less than 1GB peak GPU memory overhead (17% more than ADAM, 85% less than 3DGS-LM), enabling scalability to large scenes and distributed training.", "conclusion": "3DGS\u00b2-TR provides an efficient second-order optimization approach for 3DGS that balances computational efficiency with reconstruction quality, offering significant training acceleration with minimal memory overhead compared to existing methods."}}
{"id": "2602.00414", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.00414", "abs": "https://arxiv.org/abs/2602.00414", "authors": ["Trishna Chakraborty", "Udita Ghosh", "Aldair Ernesto Gongora", "Ruben Glatt", "Yue Dong", "Jiachen Li", "Amit K. Roy-Chowdhury", "Chengyu Song"], "title": "Toward Autonomous Laboratory Safety Monitoring with Vision Language Models: Learning to See Hazards Through Scene Structure", "comment": null, "summary": "Laboratories are prone to severe injuries from minor unsafe actions, yet continuous safety monitoring -- beyond mandatory pre-lab safety training -- is limited by human availability. Vision language models (VLMs) offer promise for autonomous laboratory safety monitoring, but their effectiveness in realistic settings is unclear due to the lack of visual evaluation data, as most safety incidents are documented primarily as unstructured text. To address this gap, we first introduce a structured data generation pipeline that converts textual laboratory scenarios into aligned triples of (image, scene graph, ground truth), using large language models as scene graph architects and image generation models as renderers. Our experiments on the synthetic dataset of 1,207 samples across 362 unique scenarios and seven open- and closed-source models show that VLMs perform effectively given textual scene graph, but degrade substantially in visual-only settings indicating difficulty in extracting structured object relationships directly from pixels. To overcome this, we propose a post-training context-engineering approach, scene-graph-guided alignment, to bridge perceptual gaps in VLMs by translating visual inputs into structured scene graphs better aligned with VLM reasoning, improving hazard detection performance in visual only settings.", "AI": {"tldr": "VLMs struggle with visual-only lab safety monitoring but perform well with structured scene graphs; proposed scene-graph-guided alignment improves visual hazard detection.", "motivation": "Laboratories face safety risks from minor unsafe actions, but continuous monitoring is limited by human availability. VLMs could enable autonomous safety monitoring, but their effectiveness in realistic visual settings is unclear due to lack of visual evaluation data.", "method": "1) Created structured data generation pipeline converting text lab scenarios into aligned (image, scene graph, ground truth) triples using LLMs as scene graph architects and image generation models as renderers. 2) Evaluated 7 open/closed-source VLMs on 1,207 samples across 362 unique scenarios. 3) Proposed scene-graph-guided alignment post-training approach to bridge VLM perceptual gaps by translating visual inputs into structured scene graphs.", "result": "VLMs perform effectively with textual scene graphs but degrade substantially in visual-only settings, indicating difficulty extracting structured object relationships directly from pixels. The proposed scene-graph-guided alignment improves hazard detection performance in visual-only settings.", "conclusion": "VLMs need structured scene graph representations to effectively monitor lab safety visually; the proposed alignment method bridges the gap between visual perception and structured reasoning for improved autonomous safety monitoring."}}
{"id": "2602.00420", "categories": ["cs.CV", "cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2602.00420", "abs": "https://arxiv.org/abs/2602.00420", "authors": ["Yihang Chen", "Zhao Xu", "Youyuan Jiang", "Tianle Zheng", "Cho-Jui Hsieh"], "title": "Text is All You Need for Vision-Language Model Jailbreaking", "comment": null, "summary": "Large Vision-Language Models (LVLMs) are increasingly equipped with robust safety safeguards to prevent responses to harmful or disallowed prompts. However, these defenses often focus on analyzing explicit textual inputs or relevant visual scenes. In this work, we introduce Text-DJ, a novel jailbreak attack that bypasses these safeguards by exploiting the model's Optical Character Recognition (OCR) capability. Our methodology consists of three stages. First, we decompose a single harmful query into multiple and semantically related but more benign sub-queries. Second, we pick a set of distraction queries that are maximally irrelevant to the harmful query. Third, we present all decomposed sub-queries and distraction queries to the LVLM simultaneously as a grid of images, with the position of the sub-queries being middle within the grid. We demonstrate that this method successfully circumvents the safety alignment of state-of-the-art LVLMs. We argue this attack succeeds by (1) converting text-based prompts into images, bypassing standard text-based filters, and (2) inducing distractions, where the model's safety protocols fail to link the scattered sub-queries within a high number of irrelevant queries. Overall, our findings expose a critical vulnerability in LVLMs' OCR capabilities that are not robust to dispersed, multi-image adversarial inputs, highlighting the need for defenses for fragmented multimodal inputs.", "AI": {"tldr": "Text-DJ is a novel jailbreak attack that bypasses LVLM safety safeguards by decomposing harmful queries into benign sub-queries, adding distraction queries, and presenting them as a grid of images to exploit OCR vulnerabilities.", "motivation": "Current LVLM safety defenses focus on explicit textual inputs or relevant visual scenes, but fail to address vulnerabilities in OCR capabilities when faced with fragmented multimodal inputs.", "method": "Three-stage approach: 1) Decompose harmful query into multiple semantically related but benign sub-queries, 2) Select maximally irrelevant distraction queries, 3) Present all queries simultaneously as a grid of images with sub-queries positioned in the middle.", "result": "The method successfully circumvents safety alignment of state-of-the-art LVLMs by bypassing text-based filters and overwhelming safety protocols with scattered sub-queries among irrelevant distractions.", "conclusion": "Text-DJ exposes critical vulnerability in LVLMs' OCR capabilities that are not robust to dispersed, multi-image adversarial inputs, highlighting need for defenses against fragmented multimodal inputs."}}
{"id": "2602.00440", "categories": ["cs.CV", "cs.LG", "cs.RO"], "pdf": "https://arxiv.org/pdf/2602.00440", "abs": "https://arxiv.org/abs/2602.00440", "authors": ["Anugunj Naman", "Gaibo Zhang", "Ayushman Singh", "Yaguang Zhang"], "title": "DISK: Dynamic Inference SKipping for World Models", "comment": null, "summary": "We present DISK, a training-free adaptive inference method for autoregressive world models. DISK coordinates two coupled diffusion transformers for video and ego-trajectory via dual-branch controllers with cross-modal skip decisions, preserving motion-appearance consistency without retraining. We extend higher-order latent-difference skip testing to the autoregressive chain-of-forward regime and propagate controller statistics through rollout loops for long-horizon stability. When integrated into closed-loop driving rollouts on 1500 NuPlan and NuScenes samples using an NVIDIA L40S GPU, DISK achieves 2x speedup on trajectory diffusion and 1.6x speedup on video diffusion while maintaining L2 planning error, visual quality (FID/FVD), and NAVSIM PDMS scores, demonstrating practical long-horizon video-and-trajectory prediction at substantially reduced cost.", "AI": {"tldr": "DISK is a training-free adaptive inference method for autoregressive world models that speeds up video and trajectory diffusion by 2x and 1.6x respectively while maintaining performance metrics.", "motivation": "To enable practical long-horizon video-and-trajectory prediction at substantially reduced computational cost without requiring retraining of existing models.", "method": "Coordinates two coupled diffusion transformers for video and ego-trajectory using dual-branch controllers with cross-modal skip decisions. Extends higher-order latent-difference skip testing to autoregressive chain-of-forward regime and propagates controller statistics through rollout loops.", "result": "Achieves 2x speedup on trajectory diffusion and 1.6x speedup on video diffusion while maintaining L2 planning error, visual quality (FID/FVD), and NAVSIM PDMS scores on 1500 NuPlan and NuScenes samples.", "conclusion": "DISK demonstrates practical long-horizon video-and-trajectory prediction at substantially reduced cost while preserving motion-appearance consistency without requiring retraining."}}
{"id": "2602.00450", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.00450", "abs": "https://arxiv.org/abs/2602.00450", "authors": ["Ethan Anderson", "Justin Silva", "Kyle Zheng", "Sameer Pusegaonkar", "Yizhou Wang", "Zheng Tang", "Sujit Biswas"], "title": "Model Optimization for Multi-Camera 3D Detection and Tracking", "comment": null, "summary": "Outside-in multi-camera perception is increasingly important in indoor environments, where networks of static cameras must support multi-target tracking under occlusion and heterogeneous viewpoints. We evaluate Sparse4D, a query-based spatiotemporal 3D detection and tracking framework that fuses multi-view features in a shared world frame and propagates sparse object queries via instance memory. We study reduced input frame rates, post-training quantization (INT8 and FP8), transfer to the WILDTRACK benchmark, and Transformer Engine mixed-precision fine-tuning. To better capture identity stability, we report Average Track Duration (AvgTrackDur), which measures identity persistence in seconds. Sparse4D remains stable under moderate FPS reductions, but below 2 FPS, identity association collapses even when detections are stable. Selective quantization of the backbone and neck offers the best speed-accuracy trade-off, while attention-related modules are consistently sensitive to low precision. On WILDTRACK, low-FPS pretraining yields large zero-shot gains over the base checkpoint, while small-scale fine-tuning provides limited additional benefit. Transformer Engine mixed precision reduces latency and improves camera scalability, but can destabilize identity propagation, motivating stability-aware validation.", "AI": {"tldr": "Sparse4D, a query-based 3D tracking framework, is evaluated under reduced frame rates, quantization, and transfer learning. Performance degrades below 2 FPS, selective quantization works best, and mixed-precision fine-tuning improves speed but can hurt identity stability.", "motivation": "To improve multi-camera perception in indoor environments where static cameras must handle occlusion and heterogeneous viewpoints, and to understand how Sparse4D performs under practical constraints like low frame rates, quantization, and domain transfer.", "method": "Evaluated Sparse4D framework under reduced input frame rates, post-training quantization (INT8 and FP8), transfer to WILDTRACK benchmark, and Transformer Engine mixed-precision fine-tuning. Used Average Track Duration (AvgTrackDur) to measure identity persistence.", "result": "Sparse4D stable under moderate FPS reductions but identity association collapses below 2 FPS. Selective quantization of backbone/neck offers best speed-accuracy trade-off. Low-FPS pretraining yields large zero-shot gains on WILDTRACK. Mixed precision reduces latency but can destabilize identity propagation.", "conclusion": "Sparse4D shows robustness to practical constraints but has critical limitations below 2 FPS. Selective quantization strategies work well, while attention modules are precision-sensitive. Transfer learning benefits from low-FPS pretraining, and mixed-precision optimization requires stability-aware validation."}}
{"id": "2602.00462", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.00462", "abs": "https://arxiv.org/abs/2602.00462", "authors": ["Benno Krojer", "Shravan Nayak", "Oscar Ma\u00f1as", "Vaibhav Adlakha", "Desmond Elliott", "Siva Reddy", "Marius Mosbach"], "title": "LatentLens: Revealing Highly Interpretable Visual Tokens in LLMs", "comment": null, "summary": "Transforming a large language model (LLM) into a Vision-Language Model (VLM) can be achieved by mapping the visual tokens from a vision encoder into the embedding space of an LLM. Intriguingly, this mapping can be as simple as a shallow MLP transformation. To understand why LLMs can so readily process visual tokens, we need interpretability methods that reveal what is encoded in the visual token representations at every layer of LLM processing. In this work, we introduce LatentLens, a novel approach for mapping latent representations to descriptions in natural language. LatentLens works by encoding a large text corpus and storing contextualized token representations for each token in that corpus. Visual token representations are then compared to their contextualized textual representations, with the top-k nearest neighbor representations providing descriptions of the visual token. We evaluate this method on 10 different VLMs, showing that commonly used methods, such as LogitLens, substantially underestimate the interpretability of visual tokens. With LatentLens instead, the majority of visual tokens are interpretable across all studied models and all layers. Qualitatively, we show that the descriptions produced by LatentLens are semantically meaningful and provide more fine-grained interpretations for humans compared to individual tokens. More broadly, our findings contribute new evidence on the alignment between vision and language representations, opening up new directions for analyzing latent representations.", "AI": {"tldr": "LatentLens is a novel interpretability method that maps visual token representations in VLMs to natural language descriptions by comparing them to contextualized textual representations from a large text corpus, revealing that visual tokens are highly interpretable across all layers.", "motivation": "To understand why LLMs can readily process visual tokens when transformed into VLMs, and to develop better interpretability methods that reveal what is encoded in visual token representations at every layer of LLM processing.", "method": "LatentLens encodes a large text corpus and stores contextualized token representations for each token. Visual token representations are then compared to these textual representations, with the top-k nearest neighbors providing natural language descriptions of the visual tokens.", "result": "Evaluation on 10 different VLMs shows that commonly used methods like LogitLens substantially underestimate visual token interpretability. With LatentLens, the majority of visual tokens are interpretable across all studied models and all layers, producing semantically meaningful and fine-grained descriptions.", "conclusion": "LatentLens provides new evidence for the alignment between vision and language representations, opening up new directions for analyzing latent representations in VLMs and demonstrating that visual tokens are highly interpretable when using appropriate methods."}}
{"id": "2602.00463", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.00463", "abs": "https://arxiv.org/abs/2602.00463", "authors": ["Xin Zhang", "Shen Chen", "Jiale Zhou", "Lei Li"], "title": "PSGS: Text-driven Panorama Sliding Scene Generation via Gaussian Splatting", "comment": "Accepted to ICASSP2026", "summary": "Generating realistic 3D scenes from text is crucial for immersive applications like VR, AR, and gaming. While text-driven approaches promise efficiency, existing methods suffer from limited 3D-text data and inconsistent multi-view stitching, resulting in overly simplistic scenes. To address this, we propose PSGS, a two-stage framework for high-fidelity panoramic scene generation. First, a novel two-layer optimization architecture generates semantically coherent panoramas: a layout reasoning layer parses text into structured spatial relationships, while a self-optimization layer refines visual details via iterative MLLM feedback. Second, our panorama sliding mechanism initializes globally consistent 3D Gaussian Splatting point clouds by strategically sampling overlapping perspectives. By incorporating depth and semantic coherence losses during training, we greatly improve the quality and detail fidelity of rendered scenes. Our experiments demonstrate that PSGS outperforms existing methods in panorama generation and produces more appealing 3D scenes, offering a robust solution for scalable immersive content creation.", "AI": {"tldr": "PSGS: A two-stage framework for generating high-fidelity 3D scenes from text using panoramic generation and Gaussian Splatting with improved semantic coherence and visual quality.", "motivation": "Existing text-to-3D scene generation methods suffer from limited 3D-text data and inconsistent multi-view stitching, resulting in overly simplistic scenes. There's a need for better solutions for immersive applications like VR, AR, and gaming.", "method": "Two-stage framework: 1) Two-layer panorama generation with layout reasoning (parses text into spatial relationships) and self-optimization (refines details via iterative MLLM feedback). 2) Panorama sliding mechanism for 3D Gaussian Splatting point clouds with strategic overlapping perspective sampling, plus depth and semantic coherence losses during training.", "result": "PSGS outperforms existing methods in panorama generation and produces more appealing 3D scenes with improved quality and detail fidelity.", "conclusion": "PSGS offers a robust solution for scalable immersive content creation, addressing key limitations in text-to-3D scene generation through its novel two-stage approach with enhanced semantic coherence and visual refinement."}}
{"id": "2602.00470", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.00470", "abs": "https://arxiv.org/abs/2602.00470", "authors": ["Pengyu Chen", "Fangzheng Lyu", "Sicheng Wang", "Cuizhen Wang"], "title": "ZS-TreeSeg: A Zero-Shot Framework for Tree Crown Instance Segmentation", "comment": null, "summary": "Individual tree crown segmentation is an important task in remote sensing for forest biomass estimation and ecological monitoring. However, accurate delineation in dense, overlapping canopies remains a bottleneck. While supervised deep learning methods suffer from high annotation costs and limited generalization, emerging foundation models (e.g., Segment Anything Model) often lack domain knowledge, leading to under-segmentation in dense clusters. To bridge this gap, we propose ZS-TreeSeg, a Zero-Shot framework that adapts from two mature tasks: 1) Canopy Semantic segmentation; and 2) Cells instance segmentation. By modeling tree crowns as star-convex objects within a topological flow field using Cellpose-SAM, the ZS-TreeSeg framework forces the mathematical separation of touching tree crown instances based on vector convergence. Experiments on the NEON and BAMFOREST datasets and visual inspection demonstrate that our framework generalizes robustly across diverse sensor types and canopy densities, which can offer a training-free solution for tree crown instance segmentation and labels generation.", "AI": {"tldr": "ZS-TreeSeg is a zero-shot framework for tree crown segmentation that adapts from canopy semantic segmentation and cells instance segmentation tasks, using topological flow fields to separate overlapping crowns without training.", "motivation": "Current methods for individual tree crown segmentation face limitations: supervised deep learning requires expensive annotations and has poor generalization, while foundation models like SAM lack domain knowledge and under-segment dense, overlapping canopies in forests.", "method": "The framework models tree crowns as star-convex objects within a topological flow field using Cellpose-SAM, forcing mathematical separation of touching tree crown instances based on vector convergence principles adapted from cell instance segmentation.", "result": "Experiments on NEON and BAMFOREST datasets show robust generalization across diverse sensor types and canopy densities, providing a training-free solution for tree crown instance segmentation and label generation.", "conclusion": "ZS-TreeSeg bridges the gap between annotation-heavy supervised methods and domain-agnostic foundation models, offering a practical zero-shot approach for accurate tree crown delineation in dense forest canopies."}}
{"id": "2602.00484", "categories": ["cs.CV", "cs.MM"], "pdf": "https://arxiv.org/pdf/2602.00484", "abs": "https://arxiv.org/abs/2602.00484", "authors": ["Rong-Lin Jian", "Ming-Chi Luo", "Chen-Wei Huang", "Chia-Ming Lee", "Yu-Fan Lin", "Chih-Chung Hsu"], "title": "GTATrack: Winner Solution to SoccerTrack 2025 with Deep-EIoU and Global Tracklet Association", "comment": "Winner Solution of SoccerTrack in ACM Multimedia 2025 Workshop MMSports", "summary": "Multi-object tracking (MOT) in sports is highly challenging due to irregular player motion, uniform appearances, and frequent occlusions. These difficulties are further exacerbated by the geometric distortion and extreme scale variation introduced by static fisheye cameras. In this work, we present GTATrack, a hierarchical tracking framework that win first place in the SoccerTrack Challenge 2025. GTATrack integrates two core components: Deep Expansion IoU (Deep-EIoU) for motion-agnostic online association and Global Tracklet Association (GTA) for trajectory-level refinement. This two-stage design enables both robust short-term matching and long-term identity consistency. Additionally, a pseudo-labeling strategy is used to boost detector recall on small and distorted targets. The synergy between local association and global reasoning effectively addresses identity switches, occlusions, and tracking fragmentation. Our method achieved a winning HOTA score of 0.60 and significantly reduced false positives to 982, demonstrating state-of-the-art accuracy in fisheye-based soccer tracking. Our code is available at https://github.com/ron941/GTATrack-STC2025.", "AI": {"tldr": "GTATrack wins SoccerTrack Challenge 2025 with hierarchical tracking framework combining Deep-EIoU for motion-agnostic association and Global Tracklet Association for trajectory refinement, achieving HOTA 0.60.", "motivation": "Multi-object tracking in sports is challenging due to irregular motion, uniform appearances, frequent occlusions, and fisheye camera distortions causing geometric distortion and extreme scale variation.", "method": "Two-stage hierarchical framework: 1) Deep Expansion IoU (Deep-EIoU) for motion-agnostic online association, 2) Global Tracklet Association (GTA) for trajectory-level refinement, plus pseudo-labeling to boost detector recall on small/distorted targets.", "result": "Achieved winning HOTA score of 0.60 in SoccerTrack Challenge 2025, significantly reduced false positives to 982, demonstrating state-of-the-art accuracy in fisheye-based soccer tracking.", "conclusion": "The synergy between local association (Deep-EIoU) and global reasoning (GTA) effectively addresses identity switches, occlusions, and tracking fragmentation in challenging sports MOT scenarios with fisheye cameras."}}
{"id": "2602.00489", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.00489", "abs": "https://arxiv.org/abs/2602.00489", "authors": ["Sicong Zang", "Tao Sun", "Cairong Yan"], "title": "Refining Strokes by Learning Offset Attributes between Strokes for Flexible Sketch Edit at Stroke-Level", "comment": "Source codes are coming soon", "summary": "Sketch edit at stroke-level aims to transplant source strokes onto a target sketch via stroke expansion or replacement, while preserving semantic consistency and visual fidelity with the target sketch. Recent studies addressed it by relocating source strokes at appropriate canvas positions. However, as source strokes could exhibit significant variations in both size and orientation, we may fail to produce plausible sketch editing results by merely repositioning them without further adjustments. For example, anchoring an oversized source stroke onto the target without proper scaling would fail to produce a semantically coherent outcome. In this paper, we propose SketchMod to refine the source stroke through transformation so as to align it with the target sketch's patterns, further realize flexible sketch edit at stroke-level. As the source stroke refinement is governed by the patterns of the target sketch, we learn three key offset attributes (scale, orientation and position) from the source stroke to another, and align it with the target by: 1) resizing to match spatial proportions by scale, 2) rotating to align with local geometry by orientation, and 3) displacing to meet with semantic layout by position. Besides, a stroke's profiles can be precisely controlled during sketch edit via the exposed captured stroke attributes. Experimental results indicate that SketchMod achieves precise and flexible performances on stroke-level sketch edit.", "AI": {"tldr": "SketchMod refines source strokes through transformation (scale, orientation, position) to align with target sketch patterns for precise stroke-level sketch editing.", "motivation": "Previous methods only reposition source strokes without adjusting for size/orientation variations, leading to implausible editing results when source strokes don't match target sketch patterns.", "method": "Proposes SketchMod that learns three offset attributes (scale, orientation, position) from source to target, then transforms source strokes by: 1) resizing to match spatial proportions, 2) rotating to align with local geometry, and 3) displacing to meet semantic layout.", "result": "Experimental results show SketchMod achieves precise and flexible performances on stroke-level sketch editing.", "conclusion": "Transformation-based stroke refinement enables more plausible sketch editing by aligning source strokes with target sketch patterns through learned scale, orientation, and position adjustments."}}
{"id": "2602.00490", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.00490", "abs": "https://arxiv.org/abs/2602.00490", "authors": ["Chia-Ming Lee", "Yu-Hao Ho", "Yu-Fan Lin", "Jen-Wei Lee", "Li-Wei Kang", "Chih-Chung Hsu"], "title": "HSSDCT: Factorized Spatial-Spectral Correlation for Hyperspectral Image Fusion", "comment": "Accepted by ICASSP 2026", "summary": "Hyperspectral image (HSI) fusion aims to reconstruct a high-resolution HSI (HR-HSI) by combining the rich spectral information of a low-resolution HSI (LR-HSI) with the fine spatial details of a high-resolution multispectral image (HR-MSI). Although recent deep learning methods have achieved notable progress, they still suffer from limited receptive fields, redundant spectral bands, and the quadratic complexity of self-attention, which restrict both efficiency and robustness. To overcome these challenges, we propose the Hierarchical Spatial-Spectral Dense Correlation Network (HSSDCT). The framework introduces two key modules: (i) a Hierarchical Dense-Residue Transformer Block (HDRTB) that progressively enlarges windows and employs dense-residue connections for multi-scale feature aggregation, and (ii) a Spatial-Spectral Correlation Layer (SSCL) that explicitly factorizes spatial and spectral dependencies, reducing self-attention to linear complexity while mitigating spectral redundancy. Extensive experiments on benchmark datasets demonstrate that HSSDCT delivers superior reconstruction quality with significantly lower computational costs, achieving new state-of-the-art performance in HSI fusion. Our code is available at https://github.com/jemmyleee/HSSDCT.", "AI": {"tldr": "HSSDCT is a novel hierarchical transformer network for hyperspectral image fusion that achieves state-of-the-art performance with linear complexity by using hierarchical dense-residue connections and spatial-spectral factorization.", "motivation": "Existing deep learning methods for HSI fusion suffer from limited receptive fields, redundant spectral bands, and the quadratic complexity of self-attention, which restrict both efficiency and robustness.", "method": "Proposes HSSDCT with two key modules: (1) Hierarchical Dense-Residue Transformer Block (HDRTB) that progressively enlarges windows with dense-residue connections for multi-scale feature aggregation, and (2) Spatial-Spectral Correlation Layer (SSCL) that factorizes spatial and spectral dependencies to reduce self-attention to linear complexity while mitigating spectral redundancy.", "result": "Extensive experiments on benchmark datasets demonstrate superior reconstruction quality with significantly lower computational costs, achieving new state-of-the-art performance in HSI fusion.", "conclusion": "HSSDCT effectively addresses the limitations of existing methods by combining hierarchical multi-scale processing with efficient spatial-spectral factorization, delivering both high performance and computational efficiency for hyperspectral image fusion."}}
{"id": "2602.00504", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.00504", "abs": "https://arxiv.org/abs/2602.00504", "authors": ["Jiahe Wu", "Bing Cao", "Qilong Wang", "Qinghua Hu", "Dongdong Li", "Pengfei Zhu"], "title": "RGBX-R1: Visual Modality Chain-of-Thought Guided Reinforcement Learning for Multimodal Grounding", "comment": null, "summary": "Multimodal Large Language Models (MLLM) are primarily pre-trained on the RGB modality, thereby limiting their performance on other modalities, such as infrared, depth, and event data, which are crucial for complex scenarios. To address this, we propose RGBX-R1, a framework to enhance MLLM's perception and reasoning capacities across various X visual modalities. Specifically, we employ an Understand-Associate-Validate (UAV) prompting strategy to construct the Visual Modality Chain-of-Thought (VM-CoT), which aims to expand the MLLMs' RGB understanding capability into X modalities. To progressively enhance reasoning capabilities, we introduce a two-stage training paradigm: Cold-Start Supervised Fine-Tuning (CS-SFT) and Spatio-Temporal Reinforcement Fine-Tuning (ST-RFT). CS-SFT supervises the reasoning process with the guidance of VM-CoT, equipping the MLLM with fundamental modality cognition. Building upon GRPO, ST-RFT employs a Modality-understanding Spatio-Temporal (MuST) reward to reinforce modality reasoning. Notably, we construct the first RGBX-Grounding benchmark, and extensive experiments verify our superiority in multimodal understanding and spatial perception, outperforming baselines by 22.71% on three RGBX grounding tasks.", "AI": {"tldr": "RGBX-R1 enhances MLLMs' perception across non-RGB modalities (infrared, depth, event data) using Visual Modality Chain-of-Thought and two-stage training, achieving 22.71% improvement on RGBX grounding tasks.", "motivation": "Current MLLMs are primarily pre-trained on RGB data, limiting their performance on other important visual modalities (infrared, depth, event data) needed for complex real-world scenarios.", "method": "Proposes RGBX-R1 framework with: 1) UAV prompting strategy to create Visual Modality Chain-of-Thought (VM-CoT) for expanding RGB understanding to X modalities; 2) Two-stage training: Cold-Start Supervised Fine-Tuning (CS-SFT) for fundamental modality cognition, and Spatio-Temporal Reinforcement Fine-Tuning (ST-RFT) with Modality-understanding Spatio-Temporal (MuST) reward based on GRPO.", "result": "Creates first RGBX-Grounding benchmark. Extensive experiments show superiority in multimodal understanding and spatial perception, outperforming baselines by 22.71% on three RGBX grounding tasks.", "conclusion": "RGBX-R1 successfully enhances MLLMs' perception and reasoning across diverse visual modalities beyond RGB, addressing limitations of current models and demonstrating significant performance improvements on multimodal grounding tasks."}}
{"id": "2602.00505", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.00505", "abs": "https://arxiv.org/abs/2602.00505", "authors": ["Jingrui Zhang", "Feng Liang", "Yong Zhang", "Wei Wang", "Runhao Zeng", "Xiping Hu"], "title": "Sparse Shortcuts: Facilitating Efficient Fusion in Multimodal Large Language Models", "comment": null, "summary": "With the remarkable success of large language models (LLMs) in natural language understanding and generation, multimodal large language models (MLLMs) have rapidly advanced in their ability to process data across multiple modalities. While most existing efforts focus on scaling up language models or constructing higher-quality training data, limited attention has been paid to effectively integrating cross-modal knowledge into the language space. In vision-language models, for instance, aligning modalities using only high-level visual features often discards the rich semantic information present in mid- and low-level features, limiting the model's ability of cross-modality understanding. To address this issue, we propose SparseCut, a general cross-modal fusion architecture for MLLMs, introducing sparse shortcut connections between the cross-modal encoder and the LLM. These shortcut connections enable the efficient and hierarchical integration of visual features at multiple levels, facilitating richer semantic fusion without increasing computational overhead. We further introduce an efficient multi-grained feature fusion module, which performs the fusion of visual features before routing them through the shortcuts. This preserves the original language context and does not increase the overall input length, thereby avoiding an increase in computational complexity for the LLM. Experiments demonstrate that SparseCut significantly enhances the performance of MLLMs across various multimodal benchmarks with generality and scalability for different base LLMs.", "AI": {"tldr": "SparseCut introduces sparse shortcut connections between cross-modal encoder and LLM for hierarchical visual feature integration, enhancing MLLM performance without computational overhead.", "motivation": "Existing MLLMs focus on scaling language models or better training data, but neglect effective cross-modal knowledge integration. Vision-language models using only high-level visual features discard rich semantic information from mid- and low-level features, limiting cross-modality understanding.", "method": "Proposes SparseCut with sparse shortcut connections between cross-modal encoder and LLM for hierarchical visual feature integration. Includes efficient multi-grained feature fusion module that fuses visual features before routing through shortcuts, preserving language context without increasing input length.", "result": "SparseCut significantly enhances MLLM performance across various multimodal benchmarks, demonstrating generality and scalability for different base LLMs.", "conclusion": "SparseCut provides an effective cross-modal fusion architecture that enables hierarchical integration of visual features at multiple levels, improving multimodal understanding without computational overhead."}}
{"id": "2602.00508", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.00508", "abs": "https://arxiv.org/abs/2602.00508", "authors": ["Min Shi", "Xiaohui Zeng", "Jiannan Huang", "Yin Cui", "Francesco Ferroni", "Jialuo Li", "Shubham Pachori", "Zhaoshuo Li", "Yogesh Balaji", "Haoxiang Wang", "Tsung-Yi Lin", "Xiao Fu", "Yue Zhao", "Chieh-Yun Chen", "Ming-Yu Liu", "Humphrey Shi"], "title": "DuoGen: Towards General Purpose Interleaved Multimodal Generation", "comment": "Technical Report. Project Page: https://research.nvidia.com/labs/dir/duetgen/", "summary": "Interleaved multimodal generation enables capabilities beyond unimodal generation models, such as step-by-step instructional guides, visual planning, and generating visual drafts for reasoning. However, the quality of existing interleaved generation models under general instructions remains limited by insufficient training data and base model capacity. We present DuoGen, a general-purpose interleaved generation framework that systematically addresses data curation, architecture design, and evaluation. On the data side, we build a large-scale, high-quality instruction-tuning dataset by combining multimodal conversations rewritten from curated raw websites, and diverse synthetic examples covering everyday scenarios. Architecturally, DuoGen leverages the strong visual understanding of a pretrained multimodal LLM and the visual generation capabilities of a diffusion transformer (DiT) pretrained on video generation, avoiding costly unimodal pretraining and enabling flexible base model selection. A two-stage decoupled strategy first instruction-tunes the MLLM, then aligns DiT with it using curated interleaved image-text sequences. Across public and newly proposed benchmarks, DuoGen outperforms prior open-source models in text quality, image fidelity, and image-context alignment, and also achieves state-of-the-art performance on text-to-image and image editing among unified generation models. Data and code will be released at https://research.nvidia.com/labs/dir/duetgen/.", "AI": {"tldr": "DuoGen is a general-purpose interleaved multimodal generation framework that combines text and image generation through systematic data curation, architecture design, and evaluation, achieving state-of-the-art performance.", "motivation": "Existing interleaved generation models have limited quality due to insufficient training data and base model capacity, despite the potential of interleaved multimodal generation for applications like instructional guides, visual planning, and reasoning.", "method": "DuoGen uses a two-stage approach: 1) Builds large-scale instruction-tuning dataset from curated multimodal conversations and synthetic examples, 2) Leverages pretrained multimodal LLM for visual understanding and diffusion transformer for visual generation, avoiding costly unimodal pretraining. Uses decoupled strategy: first instruction-tunes MLLM, then aligns DiT with curated interleaved image-text sequences.", "result": "Outperforms prior open-source models in text quality, image fidelity, and image-context alignment across public and new benchmarks. Achieves state-of-the-art performance on text-to-image and image editing among unified generation models.", "conclusion": "DuoGen provides an effective framework for high-quality interleaved multimodal generation through systematic data curation and architectural design, enabling flexible base model selection while avoiding expensive pretraining costs."}}
{"id": "2602.00516", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.00516", "abs": "https://arxiv.org/abs/2602.00516", "authors": ["Kunal Mahatha", "Jose Dolz", "Christian Desrosiers"], "title": "SPARK: Stochastic Propagation via Affinity-guided Random walK for training-free unsupervised segmentation", "comment": null, "summary": "We argue that existing training-free segmentation methods rely on an implicit and limiting assumption, that segmentation is a spectral graph partitioning problem over diffusion-derived affinities. Such approaches, based on global graph partitioning and eigenvector-based formulations of affinity matrices, suffer from several fundamental drawbacks, they require pre-selecting the number of clusters, induce boundary oversmoothing due to spectral relaxation, and remain highly sensitive to noisy or multi-modal affinity distributions. Moreover, many prior works neglect the importance of local neighborhood structure, which plays a crucial role in stabilizing affinity propagation and preserving fine-grained contours. To address these limitations, we reformulate training-free segmentation as a stochastic flow equilibrium problem over diffusion-induced affinity graphs, where segmentation emerges from a stochastic propagation process that integrates global diffusion attention with local neighborhoods extracted from stable diffusion, yielding a sparse yet expressive affinity structure. Building on this formulation, we introduce a Markov propagation scheme that performs random-walk-based label diffusion with an adaptive pruning strategy that suppresses unreliable transitions while reinforcing confident affinity paths. Experiments across seven widely used semantic segmentation benchmarks demonstrate that our method achieves state-of-the-art zero-shot performance, producing sharper boundaries, more coherent regions, and significantly more stable masks compared to prior spectral-clustering-based approaches.", "AI": {"tldr": "The paper proposes a new training-free segmentation method that reformulates segmentation as a stochastic flow equilibrium problem instead of spectral graph partitioning, using Markov propagation with adaptive pruning to achieve better zero-shot performance.", "motivation": "Existing training-free segmentation methods rely on spectral graph partitioning over diffusion-derived affinities, which has fundamental drawbacks: requires pre-selecting cluster numbers, causes boundary oversmoothing due to spectral relaxation, is sensitive to noisy/multi-modal affinities, and neglects local neighborhood structure that's crucial for stable affinity propagation and fine-grained contours.", "method": "Reformulates segmentation as a stochastic flow equilibrium problem over diffusion-induced affinity graphs. Uses a Markov propagation scheme with random-walk-based label diffusion and adaptive pruning strategy that suppresses unreliable transitions while reinforcing confident affinity paths. Integrates global diffusion attention with local neighborhoods from stable diffusion to create sparse yet expressive affinity structure.", "result": "Achieves state-of-the-art zero-shot performance across seven widely used semantic segmentation benchmarks. Produces sharper boundaries, more coherent regions, and significantly more stable masks compared to prior spectral-clustering-based approaches.", "conclusion": "The stochastic flow equilibrium formulation with Markov propagation and adaptive pruning overcomes limitations of spectral graph partitioning methods, providing better training-free segmentation with improved boundary quality, region coherence, and stability."}}
{"id": "2602.00522", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.00522", "abs": "https://arxiv.org/abs/2602.00522", "authors": ["Chaoran Xu", "Chengkan Lv", "Qiyu Chen", "Feng Zhang", "Zhengtao Zhang"], "title": "MRAD: Zero-Shot Anomaly Detection with Memory-Driven Retrieval", "comment": null, "summary": "Zero-shot anomaly detection (ZSAD) often leverages pretrained vision or vision-language models, but many existing methods use prompt learning or complex modeling to fit the data distribution, resulting in high training or inference cost and limited cross-domain stability. To address these limitations, we propose Memory-Retrieval Anomaly Detection method (MRAD), a unified framework that replaces parametric fitting with a direct memory retrieval. The train-free base model, MRAD-TF, freezes the CLIP image encoder and constructs a two-level memory bank (image-level and pixel-level) from auxiliary data, where feature-label pairs are explicitly stored as keys and values. During inference, anomaly scores are obtained directly by similarity retrieval over the memory bank. Based on the MRAD-TF, we further propose two lightweight variants as enhancements: (i) MRAD-FT fine-tunes the retrieval metric with two linear layers to enhance the discriminability between normal and anomaly; (ii) MRAD-CLIP injects the normal and anomalous region priors from the MRAD-FT as dynamic biases into CLIP's learnable text prompts, strengthening generalization to unseen categories. Across 16 industrial and medical datasets, the MRAD framework consistently demonstrates superior performance in anomaly classification and segmentation, under both train-free and training-based settings. Our work shows that fully leveraging the empirical distribution of raw data, rather than relying only on model fitting, can achieve stronger anomaly detection performance. The code will be publicly released at https://github.com/CROVO1026/MRAD.", "AI": {"tldr": "MRAD is a zero-shot anomaly detection framework that uses memory retrieval instead of parametric fitting, with train-free and lightweight variants achieving strong performance across industrial and medical datasets.", "motivation": "Existing zero-shot anomaly detection methods often use prompt learning or complex modeling that leads to high training/inference costs and limited cross-domain stability. The authors aim to address these limitations by avoiding parametric fitting.", "method": "Proposes MRAD framework with train-free base model (MRAD-TF) that freezes CLIP encoder and constructs two-level memory bank (image & pixel) from auxiliary data. Features are stored as keys, labels as values. Inference uses direct similarity retrieval. Two lightweight variants: MRAD-FT fine-tunes retrieval metric with linear layers; MRAD-CLIP injects region priors as dynamic biases into CLIP text prompts.", "result": "Superior performance across 16 industrial and medical datasets for anomaly classification and segmentation, under both train-free and training-based settings.", "conclusion": "Fully leveraging empirical distribution of raw data through memory retrieval, rather than relying only on model fitting, achieves stronger anomaly detection performance. The approach demonstrates effectiveness in zero-shot anomaly detection."}}
{"id": "2602.00523", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.00523", "abs": "https://arxiv.org/abs/2602.00523", "authors": ["Yujia Tong", "Tian Zhang", "Yunyang Wan", "Kaiwei Lin", "Jingling Yuan", "Chuang Hu"], "title": "SAGE: Accelerating Vision-Language Models via Entropy-Guided Adaptive Speculative Decoding", "comment": null, "summary": "Speculative decoding has emerged as a promising approach to accelerate inference in vision-language models (VLMs) by enabling parallel verification of multiple draft tokens. However, existing methods rely on static tree structures that remain fixed throughout the decoding process, failing to adapt to the varying prediction difficulty across generation steps. This leads to suboptimal acceptance lengths and limited speedup. In this paper, we propose SAGE, a novel framework that dynamically adjusts the speculation tree structure based on real-time prediction uncertainty. Our key insight is that output entropy serves as a natural confidence indicator with strong temporal correlation across decoding steps. SAGE constructs deeper-narrower trees for high-confidence predictions to maximize speculation depth, and shallower-wider trees for uncertain predictions to diversify exploration. SAGE improves acceptance lengths and achieves faster acceleration compared to static tree baselines. Experiments on multiple benchmarks demonstrate the effectiveness of SAGE: without any loss in output quality, it delivers up to $3.36\\times$ decoding speedup for LLaVA-OneVision-72B and $3.18\\times$ for Qwen2.5-VL-72B.", "AI": {"tldr": "SAGE introduces dynamic tree adjustment for speculative decoding in VLMs, using output entropy to adapt tree structure based on prediction uncertainty, achieving up to 3.36\u00d7 speedup without quality loss.", "motivation": "Existing speculative decoding methods use static tree structures that don't adapt to varying prediction difficulty across generation steps, leading to suboptimal acceptance lengths and limited speedup.", "method": "SAGE dynamically adjusts speculation tree structure based on real-time prediction uncertainty, using output entropy as a confidence indicator. It constructs deeper-narrower trees for high-confidence predictions and shallower-wider trees for uncertain predictions.", "result": "SAGE improves acceptance lengths and achieves faster acceleration compared to static tree baselines, delivering up to 3.36\u00d7 decoding speedup for LLaVA-OneVision-72B and 3.18\u00d7 for Qwen2.5-VL-72B without output quality loss.", "conclusion": "Dynamic tree adjustment based on prediction uncertainty significantly improves speculative decoding efficiency in vision-language models, demonstrating the importance of adaptive structures for optimal acceleration."}}
{"id": "2602.00531", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.00531", "abs": "https://arxiv.org/abs/2602.00531", "authors": ["Tianyi Zhang", "Antoine Simoulin", "Kai Li", "Sana Lakdawala", "Shiqing Yu", "Arpit Mittal", "Hongyu Fu", "Yu Lin"], "title": "Enhancing Open-Vocabulary Object Detection through Multi-Level Fine-Grained Visual-Language Alignment", "comment": null, "summary": "Traditional object detection systems are typically constrained to predefined categories, limiting their applicability in dynamic environments. In contrast, open-vocabulary object detection (OVD) enables the identification of objects from novel classes not present in the training set. Recent advances in visual-language modeling have led to significant progress of OVD. However, prior works face challenges in either adapting the single-scale image backbone from CLIP to the detection framework or ensuring robust visual-language alignment. We propose Visual-Language Detection (VLDet), a novel framework that revamps feature pyramid for fine-grained visual-language alignment, leading to improved OVD performance. With the VL-PUB module, VLDet effectively exploits the visual-language knowledge from CLIP and adapts the backbone for object detection through feature pyramid. In addition, we introduce the SigRPN block, which incorporates a sigmoid-based anchor-text contrastive alignment loss to improve detection of novel categories. Through extensive experiments, our approach achieves 58.7 AP for novel classes on COCO2017 and 24.8 AP on LVIS, surpassing all state-of-the-art methods and achieving significant improvements of 27.6% and 6.9%, respectively. Furthermore, VLDet also demonstrates superior zero-shot performance on closed-set object detection.", "AI": {"tldr": "VLDet is a novel open-vocabulary object detection framework that improves visual-language alignment through feature pyramid revamping and sigmoid-based contrastive learning, achieving state-of-the-art performance on novel classes.", "motivation": "Traditional object detection is limited to predefined categories, while open-vocabulary detection (OVD) enables identification of novel classes. Existing methods struggle with adapting CLIP's single-scale backbone to detection frameworks and achieving robust visual-language alignment.", "method": "Proposes VLDet framework with two key components: 1) VL-PUB module that revamps feature pyramid for fine-grained visual-language alignment by exploiting CLIP knowledge, and 2) SigRPN block with sigmoid-based anchor-text contrastive alignment loss to improve novel category detection.", "result": "Achieves 58.7 AP for novel classes on COCO2017 (27.6% improvement) and 24.8 AP on LVIS (6.9% improvement), surpassing all state-of-the-art methods. Also demonstrates superior zero-shot performance on closed-set object detection.", "conclusion": "VLDet effectively addresses visual-language alignment challenges in open-vocabulary detection through feature pyramid adaptation and contrastive learning, achieving significant performance gains on novel categories while maintaining strong closed-set detection capabilities."}}
{"id": "2602.00536", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.00536", "abs": "https://arxiv.org/abs/2602.00536", "authors": ["Yifan Zhang", "Qian Chen", "Yi Liu", "Wengen Li", "Jihong Guan"], "title": "SADER: Structure-Aware Diffusion Framework with DEterministic Resampling for Multi-Temporal Remote Sensing Cloud Removal", "comment": null, "summary": "Cloud contamination severely degrades the usability of remote sensing imagery and poses a fundamental challenge for downstream Earth observation tasks. Recently, diffusion-based models have emerged as a dominant paradigm for remote sensing cloud removal due to their strong generative capability and stable optimization. However, existing diffusion-based approaches often suffer from limited sampling efficiency and insufficient exploitation of structural and temporal priors in multi-temporal remote sensing scenarios. In this work, we propose SADER, a structure-aware diffusion framework for multi-temporal remote sensing cloud removal. SADER first develops a scalable Multi-Temporal Conditional Diffusion Network (MTCDN) to fully capture multi-temporal and multimodal correlations via temporal fusion and hybrid attention. Then, a cloud-aware attention loss is introduced to emphasize cloud-dominated regions by accounting for cloud thickness and brightness discrepancies. In addition, a deterministic resampling strategy is designed for continuous diffusion models to iteratively refine samples under fixed sampling steps by replacing outliers through guided correction. Extensive experiments on multiple multi-temporal datasets demonstrate that SADER consistently outperforms state-of-the-art cloud removal methods across all evaluation metrics. The code of SADER is publicly available at https://github.com/zyfzs0/SADER.", "AI": {"tldr": "SADER is a structure-aware diffusion framework for multi-temporal remote sensing cloud removal that improves sampling efficiency and exploits structural/temporal priors through temporal fusion, cloud-aware attention, and deterministic resampling.", "motivation": "Cloud contamination severely degrades remote sensing imagery usability. Existing diffusion-based approaches suffer from limited sampling efficiency and insufficient exploitation of structural and temporal priors in multi-temporal scenarios.", "method": "Proposes SADER with: 1) Multi-Temporal Conditional Diffusion Network (MTCDN) using temporal fusion and hybrid attention; 2) Cloud-aware attention loss accounting for cloud thickness and brightness; 3) Deterministic resampling strategy for iterative refinement under fixed sampling steps.", "result": "Extensive experiments on multiple multi-temporal datasets show SADER consistently outperforms state-of-the-art cloud removal methods across all evaluation metrics.", "conclusion": "SADER effectively addresses limitations of existing diffusion-based cloud removal methods by better exploiting multi-temporal correlations and improving sampling efficiency, with publicly available code."}}
{"id": "2602.00542", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.00542", "abs": "https://arxiv.org/abs/2602.00542", "authors": ["Mohammad Saeid", "Amir Salarpour", "Pedram MohajerAnsari", "Mert D. Pes\u00e9"], "title": "NPNet: A Non-Parametric Network with Adaptive Gaussian-Fourier Positional Encoding for 3D Classification and Segmentation", "comment": "Accepted to the 2026 IEEE Intelligent Vehicles Symposium (IV 2026)", "summary": "We present NPNet, a fully non-parametric approach for 3D point-cloud classification and part segmentation. NPNet contains no learned weights; instead, it builds point features using deterministic operators such as farthest point sampling, k-nearest neighbors, and pooling. Our key idea is an adaptive Gaussian-Fourier positional encoding whose bandwidth and Gaussian-cosine mixing are chosen from the input geometry, helping the method remain stable across different scales and sampling densities. For segmentation, we additionally incorporate fixed-frequency Fourier features to provide global context alongside the adaptive encoding. Across ModelNet40/ModelNet-R, ScanObjectNN, and ShapeNetPart, NPNet achieves strong performance among non-parametric baselines, and it is particularly effective in few-shot settings on ModelNet40. NPNet also offers favorable memory use and inference time compared to prior non-parametric methods", "AI": {"tldr": "NPNet is a fully non-parametric 3D point-cloud method for classification and segmentation that uses deterministic operators and adaptive positional encoding without learned weights.", "motivation": "To create a 3D point-cloud analysis method that doesn't rely on learned weights, remains stable across different scales and sampling densities, and performs well in few-shot settings.", "method": "Uses deterministic operators (farthest point sampling, k-nearest neighbors, pooling) with adaptive Gaussian-Fourier positional encoding whose parameters are chosen from input geometry. For segmentation, adds fixed-frequency Fourier features for global context.", "result": "Achieves strong performance on ModelNet40/ModelNet-R, ScanObjectNN, and ShapeNetPart among non-parametric baselines, particularly effective in few-shot settings on ModelNet40, with favorable memory use and inference time.", "conclusion": "NPNet demonstrates that effective 3D point-cloud analysis can be achieved without learned weights through adaptive positional encoding and deterministic operators, offering advantages in few-shot learning and computational efficiency."}}
{"id": "2602.00559", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.00559", "abs": "https://arxiv.org/abs/2602.00559", "authors": ["Wenbin Xing", "Quanxing Zha", "Lizheng Zu", "Mengran Li", "Ming Li", "Junchi Yan"], "title": "Learning to Decode Against Compositional Hallucination in Video Multimodal Large Language Models", "comment": null, "summary": "Current research on video hallucination mitigation primarily focuses on isolated error types, leaving compositional hallucinations, arising from incorrect reasoning over multiple interacting spatial and temporal factors largely underexplored. We introduce OmniVCHall, a benchmark designed to systematically evaluate both isolated and compositional hallucinations in video multimodal large language models (VLLMs). OmniVCHall spans diverse video domains, introduces a novel camera-based hallucination type, and defines a fine-grained taxonomy, together with adversarial answer options (e.g., \"All are correct\" and \"None of the above\") to prevent shortcut reasoning. The evaluations of 39 representative VLLMs reveal that even advanced models (e.g., Qwen3-VL and GPT-5) exhibit substantial performance degradation. We propose TriCD, a contrastive decoding framework with a triple-pathway calibration mechanism. An adaptive perturbation controller dynamically selects distracting operations to construct negative video variants, while a saliency-guided enhancement module adaptively reinforces grounded token-wise visual evidences. These components are optimized via reinforcement learning to encourage precise decision-making under compositional hallucination settings. Experimental results show that TriCD consistently improves performance across two representative backbones, achieving an average accuracy improvement of over 10%. The data and code can be find at https://github.com/BMRETURN/OmniVCHall.", "AI": {"tldr": "OmniVCHall benchmark evaluates isolated and compositional hallucinations in video VLLMs, revealing significant performance gaps. TriCD framework with contrastive decoding and triple-pathway calibration improves accuracy by over 10%.", "motivation": "Current video hallucination research focuses on isolated errors, leaving compositional hallucinations (incorrect reasoning over multiple spatial-temporal factors) largely unexplored. There's a need for systematic evaluation of both isolated and compositional hallucinations in video multimodal large language models.", "method": "1) Introduces OmniVCHall benchmark spanning diverse video domains with novel camera-based hallucination type, fine-grained taxonomy, and adversarial answer options. 2) Proposes TriCD framework with triple-pathway calibration: adaptive perturbation controller constructs negative video variants, saliency-guided enhancement module reinforces grounded visual evidence, optimized via reinforcement learning for compositional hallucination settings.", "result": "Evaluation of 39 representative VLLMs shows even advanced models (Qwen3-VL, GPT-5) exhibit substantial performance degradation. TriCD consistently improves performance across two representative backbones, achieving average accuracy improvement of over 10%.", "conclusion": "OmniVCHall provides comprehensive benchmark for video hallucination evaluation. TriCD framework effectively addresses compositional hallucinations through contrastive decoding and adaptive calibration mechanisms, demonstrating significant performance improvements across diverse VLLM architectures."}}
{"id": "2602.00570", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.00570", "abs": "https://arxiv.org/abs/2602.00570", "authors": ["Xingyu Luo", "Yidong Cai", "Jie Liu", "Jie Tang", "Gangshan Wu", "Limin Wang"], "title": "GLAD: Generative Language-Assisted Visual Tracking for Low-Semantic Templates", "comment": null, "summary": "Vision-language tracking has gained increasing attention in many scenarios. This task simultaneously deals with visual and linguistic information to localize objects in videos. Despite its growing utility, the development of vision-language tracking methods remains in its early stage. Current vision-language trackers usually employ Transformer architectures for interactive integration of template, search, and text features. However, persistent challenges about low-semantic images including prevalent image blurriness, low resolution and so on, may compromise model performance through degraded cross-modal understanding. To solve this problem, language assistance is usually used to deal with the obstacles posed by low-semantic images. However, due to the existing gap between current textual and visual features, direct concatenation and fusion of these features may have limited effectiveness. To address these challenges, we introduce a pioneering Generative Language-AssisteD tracking model, GLAD, which utilizes diffusion models for the generative multi-modal fusion of text description and template image to bolster compatibility between language and image and enhance template image semantic information. Our approach demonstrates notable improvements over the existing fusion paradigms. Blurry and semantically ambiguous template images can be restored to improve multi-modal features in the generative fusion paradigm. Experiments show that our method establishes a new state-of-the-art on multiple benchmarks and achieves an impressive inference speed. The code and models will be released at: https://github.com/Confetti-lxy/GLAD", "AI": {"tldr": "GLAD introduces a generative language-assisted tracking model using diffusion models to fuse text descriptions with template images, improving compatibility between language and visual features for better performance on low-semantic images.", "motivation": "Current vision-language trackers struggle with low-semantic images (blurry, low resolution) that degrade cross-modal understanding. Direct concatenation of textual and visual features has limited effectiveness due to the gap between modalities.", "method": "Proposes GLAD, a generative language-assisted tracking model that uses diffusion models for generative multi-modal fusion of text descriptions and template images to enhance compatibility and semantic information.", "result": "Establishes new state-of-the-art on multiple benchmarks with impressive inference speed. Blurry/ambiguous template images can be restored to improve multi-modal features in the generative fusion paradigm.", "conclusion": "GLAD demonstrates notable improvements over existing fusion paradigms by using generative diffusion models to bridge the gap between language and visual features, particularly effective for low-semantic images."}}
{"id": "2602.00579", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.00579", "abs": "https://arxiv.org/abs/2602.00579", "authors": ["JiaKui Hu", "Zhengjian Yao", "Lujia Jin", "Yanye Lu"], "title": "Bridging Degradation Discrimination and Generation for Universal Image Restoration", "comment": "Accepted by ICLR 2026", "summary": "Universal image restoration is a critical task in low-level vision, requiring the model to remove various degradations from low-quality images to produce clean images with rich detail. The challenges lie in sampling the distribution of high-quality images and adjusting the outputs on the basis of the degradation. This paper presents a novel approach, Bridging Degradation discrimination and Generation (BDG), which aims to address these challenges concurrently. First, we propose the Multi-Angle and multi-Scale Gray Level Co-occurrence Matrix (MAS-GLCM) and demonstrate its effectiveness in performing fine-grained discrimination of degradation types and levels. Subsequently, we divide the diffusion training process into three distinct stages: generation, bridging, and restoration. The objective is to preserve the diffusion model's capability of restoring rich textures while simultaneously integrating the discriminative information from the MAS-GLCM into the restoration process. This enhances its proficiency in addressing multi-task and multi-degraded scenarios. Without changing the architecture, BDG achieves significant performance gains in all-in-one restoration and real-world super-resolution tasks, primarily evidenced by substantial improvements in fidelity without compromising perceptual quality. The code and pretrained models are provided in https://github.com/MILab-PKU/BDG.", "AI": {"tldr": "BDG is a universal image restoration method that combines degradation discrimination via MAS-GLCM with diffusion-based generation, achieving state-of-the-art performance in all-in-one restoration and real-world super-resolution tasks.", "motivation": "Universal image restoration faces challenges in sampling high-quality image distributions and adjusting outputs based on degradation types/levels. Existing methods struggle with multi-task, multi-degradation scenarios requiring both texture preservation and degradation-aware restoration.", "method": "Proposes BDG with two key components: 1) MAS-GLCM for fine-grained degradation discrimination, and 2) Three-stage diffusion training (generation, bridging, restoration) that integrates discriminative information into restoration while preserving texture generation capabilities.", "result": "Achieves significant performance gains in all-in-one restoration and real-world super-resolution without architectural changes. Shows substantial improvements in fidelity while maintaining perceptual quality.", "conclusion": "BDG successfully bridges degradation discrimination and generation, enhancing multi-task/multi-degradation restoration capabilities. The method demonstrates that integrating fine-grained degradation understanding with diffusion models leads to superior universal image restoration."}}
{"id": "2602.00583", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.00583", "abs": "https://arxiv.org/abs/2602.00583", "authors": ["Xiangdong Li", "Ye Lou", "Ao Gao", "Wei Zhang", "Siyang Song"], "title": "MAUGen: A Unified Diffusion Approach for Multi-Identity Facial Expression and AU Label Generation", "comment": null, "summary": "The lack of large-scale, demographically diverse face images with precise Action Unit (AU) occurrence and intensity annotations has long been recognized as a fundamental bottleneck in developing generalizable AU recognition systems. In this paper, we propose MAUGen, a diffusion-based multi-modal framework that jointly generates a large collection of photorealistic facial expressions and anatomically consistent AU labels, including both occurrence and intensity, conditioned on a single descriptive text prompt. Our MAUGen involves two key modules: (1) a Multi-modal Representation Learning (MRL) module that captures the relationships among the paired textual description, facial identity, expression image, and AU activations within a unified latent space; and (2) a Diffusion-based Image label Generator (DIG) that decodes the joint representation into aligned facial image-label pairs across diverse identities. Under this framework, we introduce Multi-Identity Facial Action (MIFA), a large-scale multimodal synthetic dataset featuring comprehensive AU annotations and identity variations. Extensive experiments demonstrate that MAUGen outperforms existing methods in synthesizing photorealistic, demographically diverse facial images along with semantically aligned AU labels.", "AI": {"tldr": "MAUGen is a diffusion-based framework that generates photorealistic facial expressions with anatomically consistent AU labels from text prompts, creating the MIFA dataset to address data scarcity in AU recognition.", "motivation": "The paper addresses the fundamental bottleneck in developing generalizable Action Unit (AU) recognition systems: the lack of large-scale, demographically diverse face images with precise AU occurrence and intensity annotations.", "method": "MAUGen uses a two-module framework: (1) Multi-modal Representation Learning (MRL) that captures relationships among text descriptions, facial identity, expression images, and AU activations in a unified latent space, and (2) Diffusion-based Image label Generator (DIG) that decodes joint representations into aligned facial image-label pairs across diverse identities.", "result": "The framework creates the Multi-Identity Facial Action (MIFA) dataset, a large-scale multimodal synthetic dataset with comprehensive AU annotations and identity variations. MAUGen outperforms existing methods in synthesizing photorealistic, demographically diverse facial images with semantically aligned AU labels.", "conclusion": "MAUGen successfully addresses the data scarcity problem in AU recognition by generating high-quality synthetic data with precise AU annotations, enabling the development of more generalizable AU recognition systems."}}
{"id": "2602.00593", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.00593", "abs": "https://arxiv.org/abs/2602.00593", "authors": ["Yifan Jiang", "Cong Zhang", "Bofei Zhang", "Yifan Yang", "Bingzhang Wang", "Yew-Soon Ong"], "title": "From Pixels to Facts (Pix2Fact): Benchmarking Multi-Hop Reasoning for Fine-Grained Visual Fact Checking", "comment": null, "summary": "Despite progress on general tasks, VLMs struggle with challenges demanding both detailed visual grounding and deliberate knowledge-based reasoning, a synergy not captured by existing benchmarks that evaluate these skills separately. To close this gap, we introduce Pix2Fact, a new visual question-answering benchmark designed to evaluate expert-level perception and knowledge-intensive multi-hop reasoning. Pix2Fact contains 1,000 high-resolution (4K+) images spanning 8 daily-life scenarios and situations, with questions and answers meticulously crafted by annotators holding PhDs from top global universities working in partnership with a professional data annotation firm. Each question requires detailed visual grounding, multi-hop reasoning, and the integration of external knowledge to answer. Our evaluation of 9 state-of-the-art VLMs, including proprietary models like Gemini-3-Pro and GPT-5, reveals the substantial challenge posed by Pix2Fact: the most advanced model achieves only 24.0% average accuracy, in stark contrast to human performance of 56%. This significant gap underscores the limitations of current models in replicating human-level visual comprehension. We believe Pix2Fact will serve as a critical benchmark to drive the development of next-generation multimodal agents that combine fine-grained perception with robust, knowledge-based reasoning.", "AI": {"tldr": "Pix2Fact is a new visual QA benchmark requiring expert-level perception and knowledge-intensive multi-hop reasoning, where current VLMs achieve only 24% accuracy vs human 56%.", "motivation": "Current VLMs struggle with tasks requiring both detailed visual grounding and deliberate knowledge-based reasoning, but existing benchmarks evaluate these skills separately rather than their synergy.", "method": "Created Pix2Fact benchmark with 1,000 high-resolution (4K+) images across 8 daily-life scenarios, with questions/answers crafted by PhD annotators from top universities working with professional data annotation firm. Each question requires detailed visual grounding, multi-hop reasoning, and external knowledge integration.", "result": "Evaluation of 9 state-of-the-art VLMs (including Gemini-3-Pro and GPT-5) shows substantial challenge: most advanced model achieves only 24.0% average accuracy, compared to human performance of 56%.", "conclusion": "The significant performance gap underscores limitations of current models in replicating human-level visual comprehension. Pix2Fact will serve as critical benchmark to drive development of next-generation multimodal agents combining fine-grained perception with robust knowledge-based reasoning."}}
{"id": "2602.00618", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.00618", "abs": "https://arxiv.org/abs/2602.00618", "authors": ["Yian Zhao", "Rushi Ye", "Ruochong Zheng", "Zesen Cheng", "Chaoran Feng", "Jiashu Yang", "Pengchong Qiao", "Chang Liu", "Jie Chen"], "title": "Tune-Your-Style: Intensity-tunable 3D Style Transfer with Gaussian Splatting", "comment": "ICCV 2025", "summary": "3D style transfer refers to the artistic stylization of 3D assets based on reference style images. Recently, 3DGS-based stylization methods have drawn considerable attention, primarily due to their markedly enhanced training and rendering speeds. However, a vital challenge for 3D style transfer is to strike a balance between the content and the patterns and colors of the style. Although the existing methods strive to achieve relatively balanced outcomes, the fixed-output paradigm struggles to adapt to the diverse content-style balance requirements from different users. In this work, we introduce a creative intensity-tunable 3D style transfer paradigm, dubbed \\textbf{Tune-Your-Style}, which allows users to flexibly adjust the style intensity injected into the scene to match their desired content-style balance, thus enhancing the customizability of 3D style transfer. To achieve this goal, we first introduce Gaussian neurons to explicitly model the style intensity and parameterize a learnable style tuner to achieve intensity-tunable style injection. To facilitate the learning of tunable stylization, we further propose the tunable stylization guidance, which obtains multi-view consistent stylized views from diffusion models through cross-view style alignment, and then employs a two-stage optimization strategy to provide stable and efficient guidance by modulating the balance between full-style guidance from the stylized views and zero-style guidance from the initial rendering. Extensive experiments demonstrate that our method not only delivers visually appealing results, but also exhibits flexible customizability for 3D style transfer. Project page is available at https://zhao-yian.github.io/TuneStyle.", "AI": {"tldr": "Tune-Your-Style is an intensity-tunable 3D style transfer method that allows users to flexibly adjust style intensity for personalized content-style balance, using Gaussian neurons and tunable stylization guidance.", "motivation": "Existing 3D style transfer methods use fixed-output paradigms that cannot adapt to diverse user preferences for content-style balance. Users need flexible control over style intensity to match their desired artistic preferences.", "method": "Introduces Gaussian neurons to explicitly model style intensity and parameterizes a learnable style tuner. Uses tunable stylization guidance with cross-view style alignment from diffusion models, employing a two-stage optimization strategy that modulates between full-style and zero-style guidance.", "result": "The method delivers visually appealing results with flexible customizability for 3D style transfer, allowing users to adjust style intensity to achieve their preferred content-style balance.", "conclusion": "Tune-Your-Style provides an effective intensity-tunable paradigm that enhances the customizability of 3D style transfer, addressing the limitation of fixed-output methods and enabling personalized artistic stylization."}}
{"id": "2602.00621", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.00621", "abs": "https://arxiv.org/abs/2602.00621", "authors": ["Guangtao Lyu", "Xinyi Cheng", "Qi Liu", "Chenghao Xu", "Jiexi Yan", "Muli Yang", "Fen Fang", "Cheng Deng"], "title": "Towards Interpretable Hallucination Analysis and Mitigation in LVLMs via Contrastive Neuron Steering", "comment": null, "summary": "LVLMs achieve remarkable multimodal understanding and generation but remain susceptible to hallucinations. Existing mitigation methods predominantly focus on output-level adjustments, leaving the internal mechanisms that give rise to these hallucinations largely unexplored. To gain a deeper understanding, we adopt a representation-level perspective by introducing sparse autoencoders (SAEs) to decompose dense visual embeddings into sparse, interpretable neurons. Through neuron-level analysis, we identify distinct neuron types, including always-on neurons and image-specific neurons. Our findings reveal that hallucinations often result from disruptions or spurious activations of image-specific neurons, while always-on neurons remain largely stable. Moreover, selectively enhancing or suppressing image-specific neurons enables controllable intervention in LVLM outputs, improving visual grounding and reducing hallucinations. Building on these insights, we propose Contrastive Neuron Steering (CNS), which identifies image-specific neurons via contrastive analysis between clean and noisy inputs. CNS selectively amplifies informative neurons while suppressing perturbation-induced activations, producing more robust and semantically grounded visual representations. This not only enhances visual understanding but also effectively mitigates hallucinations. By operating at the prefilling stage, CNS is fully compatible with existing decoding-stage methods. Extensive experiments on both hallucination-focused and general multimodal benchmarks demonstrate that CNS consistently reduces hallucinations while preserving overall multimodal understanding.", "AI": {"tldr": "The paper introduces Contrastive Neuron Steering (CNS), a method that reduces hallucinations in LVLMs by analyzing and manipulating sparse interpretable neurons in visual embeddings, improving visual grounding without compromising multimodal understanding.", "motivation": "Current LVLM hallucination mitigation methods focus on output-level adjustments without exploring internal mechanisms. The authors aim to understand the root causes of hallucinations at the representation level to develop more effective interventions.", "method": "Use sparse autoencoders to decompose visual embeddings into interpretable neurons. Identify image-specific and always-on neurons. Propose Contrastive Neuron Steering (CNS) that uses contrastive analysis between clean and noisy inputs to identify image-specific neurons, then selectively amplifies informative neurons while suppressing perturbation-induced activations.", "result": "CNS consistently reduces hallucinations while preserving overall multimodal understanding across hallucination-focused and general multimodal benchmarks. The method operates at the prefilling stage and is compatible with existing decoding-stage methods.", "conclusion": "Hallucinations in LVLMs stem from disruptions in image-specific neurons. By understanding and manipulating these neurons through CNS, we can achieve more robust visual representations and effectively mitigate hallucinations while maintaining multimodal capabilities."}}
{"id": "2602.00627", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.00627", "abs": "https://arxiv.org/abs/2602.00627", "authors": ["Benxiang Zhai", "Yifang Xu", "Guofeng Zhang", "Yang Li", "Sidan Du"], "title": "FaceSnap: Enhanced ID-fidelity Network for Tuning-free Portrait Customization", "comment": "Accept by ICANN 2025", "summary": "Benefiting from the significant advancements in text-to-image diffusion models, research in personalized image generation, particularly customized portrait generation, has also made great strides recently. However, existing methods either require time-consuming fine-tuning and lack generalizability or fail to achieve high fidelity in facial details. To address these issues, we propose FaceSnap, a novel method based on Stable Diffusion (SD) that requires only a single reference image and produces extremely consistent results in a single inference stage. This method is plug-and-play and can be easily extended to different SD models. Specifically, we design a new Facial Attribute Mixer that can extract comprehensive fused information from both low-level specific features and high-level abstract features, providing better guidance for image generation. We also introduce a Landmark Predictor that maintains reference identity across landmarks with different poses, providing diverse yet detailed spatial control conditions for image generation. Then we use an ID-preserving module to inject these into the UNet. Experimental results demonstrate that our approach performs remarkably in personalized and customized portrait generation, surpassing other state-of-the-art methods in this domain.", "AI": {"tldr": "FaceSnap: A plug-and-play Stable Diffusion method for personalized portrait generation using single reference image with high facial fidelity and identity consistency.", "motivation": "Existing personalized portrait generation methods either require time-consuming fine-tuning and lack generalizability, or fail to achieve high fidelity in facial details. There's a need for a method that can generate highly consistent results with just a single reference image in a single inference stage.", "method": "FaceSnap uses Stable Diffusion with three key components: 1) Facial Attribute Mixer that extracts comprehensive fused information from both low-level specific features and high-level abstract features, 2) Landmark Predictor that maintains reference identity across different poses for spatial control, and 3) ID-preserving module to inject these features into the UNet architecture. The method is plug-and-play and requires only a single reference image.", "result": "Experimental results show that FaceSnap performs remarkably in personalized and customized portrait generation, surpassing other state-of-the-art methods in this domain. It produces extremely consistent results in a single inference stage.", "conclusion": "FaceSnap addresses the limitations of existing methods by providing a plug-and-play solution that requires only a single reference image, achieves high facial fidelity, and maintains identity consistency across different poses, making it superior to current state-of-the-art approaches."}}
{"id": "2602.00635", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.00635", "abs": "https://arxiv.org/abs/2602.00635", "authors": ["Lingsong Wang", "Mancheng Meng", "Ziyan Wu", "Terrence Chen", "Fan Yang", "Dinggang Shen"], "title": "S$^3$POT: Contrast-Driven Face Occlusion Segmentation via Self-Supervised Prompt Learning", "comment": null, "summary": "Existing face parsing methods usually misclassify occlusions as facial components. This is because occlusion is a high-level concept, it does not refer to a concrete category of object. Thus, constructing a real-world face dataset covering all categories of occlusion object is almost impossible and accurate mask annotation is labor-intensive. To deal with the problems, we present S$^3$POT, a contrast-driven framework synergizing face generation with self-supervised spatial prompting, to achieve occlusion segmentation. The framework is inspired by the insights: 1) Modern face generators' ability to realistically reconstruct occluded regions, creating an image that preserve facial geometry while eliminating occlusion, and 2) Foundation segmentation models' (e.g., SAM) capacity to extract precise mask when provided with appropriate prompts. In particular, S$^3$POT consists of three modules: Reference Generation (RF), Feature enhancement (FE), and Prompt Selection (PS). First, a reference image is produced by RF using structural guidance from parsed mask. Second, FE performs contrast of tokens between raw and reference images to obtain an initial prompt, then modifies image features with the prompt by cross-attention. Third, based on the enhanced features, PS constructs a set of positive and negative prompts and screens them with a self-attention network for a mask decoder. The network is learned under the guidance of three novel and complementary objective functions without occlusion ground truth mask involved. Extensive experiments on a dedicatedly collected dataset demonstrate S$^3$POT's superior performance and the effectiveness of each module.", "AI": {"tldr": "S\u00b3POT is a contrast-driven framework for face occlusion segmentation that synergizes face generation with self-supervised spatial prompting, achieving accurate occlusion segmentation without requiring occlusion ground truth masks.", "motivation": "Existing face parsing methods misclassify occlusions as facial components because occlusion is a high-level concept that doesn't refer to concrete object categories. Constructing a real-world face dataset covering all occlusion types is impossible, and accurate mask annotation is labor-intensive.", "method": "S\u00b3POT consists of three modules: 1) Reference Generation (RF) produces occlusion-free reference images using structural guidance from parsed masks, 2) Feature Enhancement (FE) performs contrast between raw and reference images to obtain initial prompts and modifies features via cross-attention, 3) Prompt Selection (PS) constructs positive/negative prompts and screens them with a self-attention network for mask decoding. The framework leverages modern face generators' ability to reconstruct occluded regions and foundation segmentation models' capacity to extract precise masks with appropriate prompts.", "result": "Extensive experiments on a dedicatedly collected dataset demonstrate S\u00b3POT's superior performance and the effectiveness of each module in achieving accurate occlusion segmentation.", "conclusion": "S\u00b3POT successfully addresses the occlusion segmentation problem by combining face generation with self-supervised spatial prompting, eliminating the need for occlusion ground truth masks while achieving state-of-the-art performance through its contrast-driven framework."}}
{"id": "2602.00637", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.00637", "abs": "https://arxiv.org/abs/2602.00637", "authors": ["Vivek Madhavaram", "Vartika Sengar", "Arkadipta De", "Charu Sharma"], "title": "VIZOR: Viewpoint-Invariant Zero-Shot Scene Graph Generation for 3D Scene Reasoning", "comment": "WACV 2026, Project page: https://vivekmadhavaram.github.io/vizor/", "summary": "Scene understanding and reasoning has been a fundamental problem in 3D computer vision, requiring models to identify objects, their properties, and spatial or comparative relationships among the objects. Existing approaches enable this by creating scene graphs using multiple inputs such as 2D images, depth maps, object labels, and annotated relationships from specific reference view. However, these methods often struggle with generalization and produce inaccurate spatial relationships like \"left/right\", which become inconsistent across different viewpoints. To address these limitations, we propose Viewpoint-Invariant Zero-shot scene graph generation for 3D scene Reasoning (VIZOR). VIZOR is a training-free, end-to-end framework that constructs dense, viewpoint-invariant 3D scene graphs directly from raw 3D scenes. The generated scene graph is unambiguous, as spatial relationships are defined relative to each object's front-facing direction, making them consistent regardless of the reference view. Furthermore, it infers open-vocabulary relationships that describe spatial and proximity relationships among scene objects without requiring annotated training data. We conduct extensive quantitative and qualitative evaluations to assess the effectiveness of VIZOR in scene graph generation and downstream tasks, such as query-based object grounding. VIZOR outperforms state-of-the-art methods, showing clear improvements in scene graph generation and achieving 22% and 4.81% gains in zero-shot grounding accuracy on the Replica and Nr3D datasets, respectively.", "AI": {"tldr": "VIZOR is a training-free, end-to-end framework for viewpoint-invariant 3D scene graph generation that creates unambiguous spatial relationships relative to each object's front-facing direction, enabling zero-shot reasoning without annotated training data.", "motivation": "Existing scene graph generation methods struggle with generalization and produce inaccurate spatial relationships (like \"left/right\") that become inconsistent across different viewpoints, as they rely on specific reference views and multiple inputs (2D images, depth maps, annotations).", "method": "VIZOR constructs dense, viewpoint-invariant 3D scene graphs directly from raw 3D scenes without training. It defines spatial relationships relative to each object's front-facing direction, making them consistent regardless of reference view, and infers open-vocabulary relationships without requiring annotated training data.", "result": "VIZOR outperforms state-of-the-art methods in scene graph generation and achieves 22% and 4.81% gains in zero-shot grounding accuracy on Replica and Nr3D datasets respectively, demonstrating clear improvements in both scene graph generation and downstream tasks.", "conclusion": "VIZOR provides an effective training-free solution for viewpoint-invariant 3D scene graph generation that produces unambiguous spatial relationships and enables zero-shot reasoning, addressing key limitations of existing approaches while showing superior performance on benchmark datasets."}}
{"id": "2602.00639", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.00639", "abs": "https://arxiv.org/abs/2602.00639", "authors": ["Yifang Xu", "Benxiang Zhai", "Chenyu Zhang", "Ming Li", "Yang Li", "Sidan Du"], "title": "Diff-PC: Identity-preserving and 3D-aware Controllable Diffusion for Zero-shot Portrait Customization", "comment": "Accepted by Information Fusion 2025", "summary": "Portrait customization (PC) has recently garnered significant attention due to its potential applications. However, existing PC methods lack precise identity (ID) preservation and face control. To address these tissues, we propose Diff-PC, a diffusion-based framework for zero-shot PC, which generates realistic portraits with high ID fidelity, specified facial attributes, and diverse backgrounds. Specifically, our approach employs the 3D face predictor to reconstruct the 3D-aware facial priors encompassing the reference ID, target expressions, and poses. To capture fine-grained face details, we design ID-Encoder that fuses local and global facial features. Subsequently, we devise ID-Ctrl using the 3D face to guide the alignment of ID features. We further introduce ID-Injector to enhance ID fidelity and facial controllability. Finally, training on our collected ID-centric dataset improves face similarity and text-to-image (T2I) alignment. Extensive experiments demonstrate that Diff-PC surpasses state-of-the-art methods in ID preservation, facial control, and T2I consistency. Furthermore, our method is compatible with multi-style foundation models.", "AI": {"tldr": "Diff-PC is a diffusion-based framework for zero-shot portrait customization that generates realistic portraits with high identity fidelity, specified facial attributes, and diverse backgrounds using 3D facial priors and specialized ID modules.", "motivation": "Existing portrait customization methods lack precise identity preservation and facial control, limiting their practical applications. The authors aim to address these limitations by developing a method that can generate portraits with accurate identity representation and controllable facial attributes.", "method": "The approach uses a 3D face predictor to reconstruct 3D-aware facial priors (reference identity, target expressions, poses). It includes: 1) ID-Encoder for fine-grained face details by fusing local and global facial features, 2) ID-Ctrl using 3D face to guide ID feature alignment, 3) ID-Injector to enhance ID fidelity and facial controllability. The method is trained on a collected ID-centric dataset to improve face similarity and text-to-image alignment.", "result": "Extensive experiments show that Diff-PC surpasses state-of-the-art methods in identity preservation, facial control, and text-to-image consistency. The method is also compatible with multi-style foundation models.", "conclusion": "Diff-PC successfully addresses the limitations of existing portrait customization methods by providing a diffusion-based framework that achieves high identity fidelity, precise facial control, and maintains text-to-image consistency while being compatible with various foundation models."}}
{"id": "2602.00650", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.00650", "abs": "https://arxiv.org/abs/2602.00650", "authors": ["Mohammadreza Gholipour Shahraki", "Mehdi Rezaeian", "Mohammad Ghasemzadeh"], "title": "A Hybrid Mamba-SAM Architecture for Efficient 3D Medical Image Segmentation", "comment": null, "summary": "Accurate segmentation of 3D medical images such as MRI and CT is essential for clinical diagnosis and treatment planning. Foundation models like the Segment Anything Model (SAM) provide powerful general-purpose representations but struggle in medical imaging due to domain shift, their inherently 2D design, and the high computational cost of fine-tuning. To address these challenges, we propose Mamba-SAM, a novel and efficient hybrid architecture that combines a frozen SAM encoder with the linear-time efficiency and long-range modeling capabilities of Mamba-based State Space Models (SSMs). We investigate two parameter-efficient adaptation strategies. The first is a dual-branch architecture that explicitly fuses general features from a frozen SAM encoder with domain-specific representations learned by a trainable VMamba encoder using cross-attention. The second is an adapter-based approach that injects lightweight, 3D-aware Tri-Plane Mamba (TPMamba) modules into the frozen SAM ViT encoder to implicitly model volumetric context. Within this framework, we introduce Multi-Frequency Gated Convolution (MFGC), which enhances feature representation by jointly analyzing spatial and frequency-domain information via 3D discrete cosine transforms and adaptive gating. Extensive experiments on the ACDC cardiac MRI dataset demonstrate the effectiveness of the proposed methods. The dual-branch Mamba-SAM-Base model achieves a mean Dice score of 0.906, comparable to UNet++ (0.907), while outperforming all baselines on Myocardium (0.910) and Left Ventricle (0.971) segmentation. The adapter-based TP MFGC variant offers superior inference speed (4.77 FPS) with strong accuracy (0.880 Dice). These results show that hybridizing foundation models with efficient SSM-based architectures provides a practical and effective solution for 3D medical image segmentation.", "AI": {"tldr": "Mamba-SAM combines frozen SAM encoder with Mamba SSMs for efficient 3D medical image segmentation, using dual-branch or adapter approaches with frequency-domain enhancements.", "motivation": "Foundation models like SAM struggle with medical imaging due to domain shift, 2D design limitations, and high fine-tuning costs, requiring efficient adaptation strategies.", "method": "Two parameter-efficient strategies: 1) Dual-branch architecture fusing frozen SAM features with trainable VMamba encoder via cross-attention; 2) Adapter-based approach injecting lightweight Tri-Plane Mamba modules into frozen SAM ViT encoder. Includes Multi-Frequency Gated Convolution for spatial-frequency analysis.", "result": "On ACDC cardiac MRI: Dual-branch Mamba-SAM-Base achieves 0.906 mean Dice (comparable to UNet++'s 0.907), outperforms baselines on Myocardium (0.910) and Left Ventricle (0.971). Adapter-based TP MFGC variant offers 4.77 FPS with 0.880 Dice.", "conclusion": "Hybridizing foundation models with efficient SSM-based architectures provides practical and effective solution for 3D medical image segmentation, balancing accuracy and computational efficiency."}}
{"id": "2602.00653", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.00653", "abs": "https://arxiv.org/abs/2602.00653", "authors": ["Lukas Kuhn", "Giuseppe Serra", "Florian Buettner"], "title": "Non-Contrastive Vision-Language Learning with Predictive Embedding Alignment", "comment": null, "summary": "Vision-language models have transformed multimodal representation learning, yet dominant contrastive approaches like CLIP require large batch sizes, careful negative sampling, and extensive hyperparameter tuning. We introduce NOVA, a NOn-contrastive Vision-language Alignment framework based on joint embedding prediction with distributional regularization. NOVA aligns visual representations to a frozen, domain-specific text encoder by predicting text embeddings from augmented image views, while enforcing an isotropic Gaussian structure via Sketched Isotropic Gaussian Regularization (SIGReg). This eliminates the need for negative sampling, momentum encoders, or stop-gradients, reducing the training objective to a single hyperparameter. We evaluate NOVA on zeroshot chest X-ray classification using ClinicalBERT as the text encoder and Vision Transformers trained from scratch on MIMIC-CXR. On zero-shot classification across three benchmark datasets, NOVA outperforms multiple standard baselines while exhibiting substantially more consistent training runs. Our results demonstrate that non-contrastive vision-language pretraining offers a simpler, more stable, and more effective alternative to contrastive methods.", "AI": {"tldr": "NOVA is a non-contrastive vision-language alignment framework that predicts text embeddings from augmented image views with distributional regularization, eliminating the need for negative sampling and complex training setups.", "motivation": "Contrastive approaches like CLIP require large batch sizes, careful negative sampling, and extensive hyperparameter tuning, which are complex and unstable. The authors aim to develop a simpler, more stable alternative for vision-language alignment.", "method": "NOVA aligns visual representations to a frozen, domain-specific text encoder by predicting text embeddings from augmented image views. It uses Sketched Isotropic Gaussian Regularization (SIGReg) to enforce an isotropic Gaussian structure, eliminating the need for negative sampling, momentum encoders, or stop-gradients.", "result": "On zero-shot chest X-ray classification using ClinicalBERT as text encoder and Vision Transformers trained on MIMIC-CXR, NOVA outperforms multiple standard baselines across three benchmark datasets while exhibiting substantially more consistent training runs.", "conclusion": "Non-contrastive vision-language pretraining offers a simpler, more stable, and more effective alternative to contrastive methods, with NOVA demonstrating strong performance in medical imaging applications."}}
{"id": "2602.00661", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.00661", "abs": "https://arxiv.org/abs/2602.00661", "authors": ["Ahsan Raza Siyal", "Markus Haltmeier", "Ruth Steiger", "Elke Ruth Gizewski", "Astrid Ellen Grams"], "title": "Schr\u00f6dinger-Inspired Time-Evolution for 4D Deformation Forecasting", "comment": null, "summary": "Spatiotemporal forecasting of complex three-dimensional phenomena (4D: 3D + time) is fundamental to applications in medical imaging, fluid and material dynamics, and geophysics. In contrast to unconstrained neural forecasting models, we propose a Schr\u00f6dinger-inspired, physics-guided neural architecture that embeds an explicit time-evolution operator within a deep convolutional framework for 4D prediction. From observed volumetric sequences, the model learns voxelwise amplitude, phase, and potential fields that define a complex-valued wavefunction $\u03c8= A e^{i\u03c6}$, which is evolved forward in time using a differentiable, unrolled Schr\u00f6dinger time stepper. This physics-guided formulation yields several key advantages: (i) temporal stability arising from the structured evolution operator, which mitigates drift and error accumulation in long-horizon forecasting; (ii) an interpretable latent representation, where phase encodes transport dynamics, amplitude captures structural intensity, and the learned potential governs spatiotemporal interactions; and (iii) natural compatibility with deformation-based synthesis, which is critical for preserving anatomical fidelity in medical imaging applications. By integrating physical priors directly into the learning process, the proposed approach combines the expressivity of deep networks with the robustness and interpretability of physics-based modeling. We demonstrate accurate and stable prediction of future 4D states, including volumetric intensities and deformation fields, on synthetic benchmarks that emulate realistic shape deformations and topological changes. To our knowledge, this is the first end-to-end 4D neural forecasting framework to incorporate a Schr\u00f6dinger-type evolution operator, offering a principled pathway toward interpretable, stable, and anatomically consistent spatiotemporal prediction.", "AI": {"tldr": "A physics-guided neural architecture for 4D spatiotemporal forecasting that embeds a Schr\u00f6dinger-type evolution operator within deep convolutional networks to predict future volumetric states with improved stability and interpretability.", "motivation": "Spatiotemporal forecasting of complex 3D+time phenomena is crucial for medical imaging, fluid dynamics, and geophysics, but existing neural forecasting models lack physical constraints, leading to instability and poor interpretability in long-horizon predictions.", "method": "Proposes a Schr\u00f6dinger-inspired neural architecture that learns voxelwise amplitude, phase, and potential fields to define a complex-valued wavefunction \u03c8 = A e^{i\u03c6}. The model uses a differentiable, unrolled Schr\u00f6dinger time stepper to evolve the wavefunction forward in time, integrating physical priors directly into the learning process.", "result": "Demonstrates accurate and stable prediction of future 4D states (volumetric intensities and deformation fields) on synthetic benchmarks emulating realistic shape deformations and topological changes. The approach shows improved temporal stability, interpretable latent representations, and natural compatibility with deformation-based synthesis.", "conclusion": "This is the first end-to-end 4D neural forecasting framework incorporating a Schr\u00f6dinger-type evolution operator, offering a principled pathway toward interpretable, stable, and anatomically consistent spatiotemporal prediction by combining deep network expressivity with physics-based modeling robustness."}}
{"id": "2602.00669", "categories": ["cs.CV", "cs.AI", "physics.med-ph"], "pdf": "https://arxiv.org/pdf/2602.00669", "abs": "https://arxiv.org/abs/2602.00669", "authors": ["Marina Crespo Aguirre", "Jonathan Williams-Ramirez", "Dina Zemlyanker", "Xiaoling Hu", "Lucas J. Deden-Binder", "Rogeny Herisse", "Mark Montine", "Theresa R. Connors", "Christopher Mount", "Christine L. MacDonald", "C. Dirk Keene", "Caitlin S. Latimer", "Derek H. Oakley", "Bradley T. Hyman", "Ana Lawry Aguila", "Juan Eugenio Iglesias"], "title": "Improving Neuropathological Reconstruction Fidelity via AI Slice Imputation", "comment": "12 pages of main content, 5 pages of supplement", "summary": "Neuropathological analyses benefit from spatially precise volumetric reconstructions that enhance anatomical delineation and improve morphometric accuracy. Our prior work has shown the feasibility of reconstructing 3D brain volumes from 2D dissection photographs. However these outputs sometimes exhibit coarse, overly smooth reconstructions of structures, especially under high anisotropy (i.e., reconstructions from thick slabs). Here, we introduce a computationally efficient super-resolution step that imputes slices to generate anatomically consistent isotropic volumes from anisotropic 3D reconstructions of dissection photographs. By training on domain-randomized synthetic data, we ensure that our method generalizes across dissection protocols and remains robust to large slab thicknesses. The imputed volumes yield improved automated segmentations, achieving higher Dice scores, particularly in cortical and white matter regions. Validation on surface reconstruction and atlas registration tasks demonstrates more accurate cortical surfaces and MRI registration. By enhancing the resolution and anatomical fidelity of photograph-based reconstructions, our approach strengthens the bridge between neuropathology and neuroimaging. Our method is publicly available at https://surfer.nmr.mgh.harvard.edu/fswiki/mri_3d_photo_recon", "AI": {"tldr": "A super-resolution method that enhances 3D brain reconstructions from 2D dissection photos by imputing slices to create isotropic volumes, improving anatomical accuracy and segmentation performance.", "motivation": "Existing 3D reconstructions from 2D dissection photographs often produce coarse, overly smooth results, especially with thick tissue slabs (high anisotropy), limiting anatomical precision and morphometric accuracy in neuropathological analysis.", "method": "Introduces a computationally efficient super-resolution step that imputes additional slices to convert anisotropic 3D reconstructions into anatomically consistent isotropic volumes. The method is trained on domain-randomized synthetic data to ensure generalization across different dissection protocols and robustness to large slab thicknesses.", "result": "The imputed volumes yield improved automated segmentations with higher Dice scores, particularly in cortical and white matter regions. Validation shows more accurate cortical surface reconstructions and better MRI registration performance.", "conclusion": "The approach enhances resolution and anatomical fidelity of photograph-based brain reconstructions, bridging neuropathology and neuroimaging. The method is publicly available for research use."}}
{"id": "2602.00671", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.00671", "abs": "https://arxiv.org/abs/2602.00671", "authors": ["Yangzhi Ma", "Bojun Liu", "Wenting Liao", "Dong Liu", "Zhu Li", "Li Li"], "title": "HPC: Hierarchical Point-based Latent Representation for Streaming Dynamic Gaussian Splatting Compression", "comment": null, "summary": "While dynamic Gaussian Splatting has driven significant advances in free-viewpoint video, maintaining its rendering quality with a small memory footprint for efficient streaming transmission still presents an ongoing challenge. Existing streaming dynamic Gaussian Splatting compression methods typically leverage a latent representation to drive the neural network for predicting Gaussian residuals between frames. Their core latent representations can be categorized into structured grid-based and unstructured point-based paradigms. However, the former incurs significant parameter redundancy by inevitably modeling unoccupied space, while the latter suffers from limited compactness as it fails to exploit local correlations. To relieve these limitations, we propose HPC, a novel streaming dynamic Gaussian Splatting compression framework. It employs a hierarchical point-based latent representation that operates on a per-Gaussian basis to avoid parameter redundancy in unoccupied space. Guided by a tailored aggregation scheme, these latent points achieve high compactness with low spatial redundancy. To improve compression efficiency, we further undertake the first investigation to compress neural networks for streaming dynamic Gaussian Splatting through mining and exploiting the inter-frame correlation of parameters. Combined with latent compression, this forms a fully end-to-end compression framework. Comprehensive experimental evaluations demonstrate that HPC substantially outperforms state-of-the-art methods. It achieves a storage reduction of 67% against its baseline while maintaining high reconstruction fidelity.", "AI": {"tldr": "HPC is a streaming compression framework for dynamic Gaussian Splatting that uses hierarchical point-based latent representation and neural network compression to reduce storage by 67% while maintaining quality.", "motivation": "Existing streaming dynamic Gaussian Splatting compression methods have limitations: grid-based approaches have parameter redundancy in unoccupied space, while point-based methods lack compactness due to poor exploitation of local correlations. This creates challenges for efficient streaming transmission with small memory footprint.", "method": "Proposes HPC framework with: 1) Hierarchical point-based latent representation operating per-Gaussian to avoid unoccupied space redundancy, 2) Tailored aggregation scheme for high compactness with low spatial redundancy, 3) First investigation of neural network compression for streaming dynamic Gaussian Splatting by mining inter-frame parameter correlations, 4) End-to-end compression combining latent and neural network compression.", "result": "HPC substantially outperforms state-of-the-art methods, achieving 67% storage reduction against baseline while maintaining high reconstruction fidelity. Comprehensive experimental evaluations demonstrate superior performance.", "conclusion": "HPC successfully addresses limitations of existing compression methods by combining hierarchical point-based latent representation with neural network compression, enabling efficient streaming transmission of dynamic Gaussian Splatting with minimal storage requirements and preserved quality."}}
{"id": "2602.00683", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.00683", "abs": "https://arxiv.org/abs/2602.00683", "authors": ["Thong Thanh Nguyen"], "title": "Video Understanding: Through A Temporal Lens", "comment": "PhD Thesis, NUS, 2025", "summary": "This thesis explores the central question of how to leverage temporal relations among video elements to advance video understanding. Addressing the limitations of existing methods, the work presents a five-fold contribution: (1) an automatic annotation framework that utilizes large vision-language models and a noise-robust contrastive learning objective with a subtractive angular margin; (2) a parameter-efficient fine-tuning strategy using \"recurrent adapters\" to capture temporal dynamics in low-data regimes; (3) the integration of State Space Layers (SSL) for efficient long-form video modeling, supported by the introduction of two new long-term benchmarks for egocentric and feature-length content; (4) a novel contrastive learning framework designed to explicitly model fine-grained relations between motions and video moments; and (5) a comprehensive empirical study on Large Vision-Language Models (LVLMs) that identifies the visual-language interface as a bottleneck for temporal reasoning, leading to a new \"temporal-oriented recipe\" for upscaled video understanding. Collectively, these contributions demonstrate that explicit temporal modeling significantly enhances a model's ability to represent and reason about the fluid nature of video content.", "AI": {"tldr": "This thesis presents five contributions for improving video understanding through explicit temporal modeling, including automatic annotation, recurrent adapters, state space layers for long videos, motion-moment contrastive learning, and a temporal-oriented recipe for LVLMs.", "motivation": "Existing video understanding methods have limitations in leveraging temporal relations among video elements. The thesis aims to address these limitations by developing approaches that better capture and model temporal dynamics in video content.", "method": "Five-fold approach: (1) Automatic annotation using large vision-language models with noise-robust contrastive learning; (2) Parameter-efficient fine-tuning with \"recurrent adapters\"; (3) State Space Layers for long-form video modeling with new benchmarks; (4) Contrastive learning for fine-grained motion-moment relations; (5) Empirical study on LVLMs leading to temporal-oriented recipe.", "result": "The contributions demonstrate that explicit temporal modeling significantly enhances a model's ability to represent and reason about video content. The work introduces new techniques, benchmarks, and frameworks for improved video understanding.", "conclusion": "Explicit temporal modeling is crucial for advancing video understanding capabilities. The five contributions collectively show that leveraging temporal relations among video elements significantly improves model performance in representing and reasoning about video content."}}
{"id": "2602.00687", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.00687", "abs": "https://arxiv.org/abs/2602.00687", "authors": ["Yuankun Zeng", "Shaohui Li", "Zhi Li", "Shulan Ruan", "Yu Liu", "You He"], "title": "V2X-DSC: Multi-Agent Collaborative Perception with Distributed Source Coding Guided Communication", "comment": null, "summary": "Collaborative perception improves 3D understanding by fusing multi-agent observations, yet intermediate-feature sharing faces strict bandwidth constraints as dense BEV features saturate V2X links. We observe that collaborators view the same physical world, making their features strongly correlated; thus receivers only need innovation beyond their local context. Revisiting this from a distributed source coding perspective, we propose V2X-DSC, a framework with a Conditional Codec (DCC) for bandwidth-constrained fusion. The sender compresses BEV features into compact codes, while the receiver performs conditional reconstruction using its local features as side information, allocating bits to complementary cues rather than redundant content. This conditional structure regularizes learning, encouraging incremental representation and yielding lower-noise features. Experiments on DAIR-V2X, OPV2V, and V2X-Real demonstrate state-of-the-art accuracy-bandwidth trade-offs under KB-level communication, and generalizes as a plug-and-play communication layer across multiple fusion backbones.", "AI": {"tldr": "V2X-DSC: A distributed source coding framework for bandwidth-constrained collaborative perception that compresses BEV features by sending only innovative information beyond local context, using conditional reconstruction with local features as side information.", "motivation": "Collaborative perception via intermediate-feature sharing faces strict bandwidth constraints as dense BEV features saturate V2X links. Since collaborators view the same physical world with strongly correlated features, receivers only need innovation beyond their local context rather than redundant information.", "method": "Proposes V2X-DSC framework with Conditional Codec (DCC) for bandwidth-constrained fusion. Sender compresses BEV features into compact codes, while receiver performs conditional reconstruction using local features as side information. This allocates bits to complementary cues rather than redundant content, and the conditional structure regularizes learning to encourage incremental representation.", "result": "Experiments on DAIR-V2X, OPV2V, and V2X-Real demonstrate state-of-the-art accuracy-bandwidth trade-offs under KB-level communication. The framework generalizes as a plug-and-play communication layer across multiple fusion backbones.", "conclusion": "V2X-DSC effectively addresses bandwidth constraints in collaborative perception by leveraging distributed source coding principles, enabling efficient feature sharing through conditional reconstruction that focuses on complementary information rather than redundant features."}}
{"id": "2602.00702", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.00702", "abs": "https://arxiv.org/abs/2602.00702", "authors": ["Ruikui Wang", "Jinheng Feng", "Lang Tian", "Huaishao Luo", "Chaochao Li", "Liangbo Zhou", "Huan Zhang", "Youzheng Wu", "Xiaodong He"], "title": "JoyAvatar: Unlocking Highly Expressive Avatars via Harmonized Text-Audio Conditioning", "comment": null, "summary": "Existing video avatar models have demonstrated impressive capabilities in scenarios such as talking, public speaking, and singing. However, the majority of these methods exhibit limited alignment with respect to text instructions, particularly when the prompts involve complex elements including large full-body movement, dynamic camera trajectory, background transitions, or human-object interactions. To break out this limitation, we present JoyAvatar, a framework capable of generating long duration avatar videos, featuring two key technical innovations. Firstly, we introduce a twin-teacher enhanced training algorithm that enables the model to transfer inherent text-controllability from the foundation model while simultaneously learning audio-visual synchronization. Secondly, during training, we dynamically modulate the strength of multi-modal conditions (e.g., audio and text) based on the distinct denoising timestep, aiming to mitigate conflicts between the heterogeneous conditioning signals. These two key designs serve to substantially expand the avatar model's capacity to generate natural, temporally coherent full-body motions and dynamic camera movements as well as preserve the basic avatar capabilities, such as accurate lip-sync and identity consistency. GSB evaluation results demonstrate that our JoyAvatar model outperforms the state-of-the-art models such as Omnihuman-1.5 and KlingAvatar 2.0. Moreover, our approach enables complex applications including multi-person dialogues and non-human subjects role-playing. Some video samples are provided on https://joyavatar.github.io/.", "AI": {"tldr": "JoyAvatar is a framework for generating long-duration avatar videos with enhanced text controllability, addressing limitations in existing methods for complex prompts involving full-body movement, camera trajectories, and human-object interactions.", "motivation": "Existing video avatar models have limited alignment with text instructions, especially for complex prompts involving full-body movement, dynamic camera trajectories, background transitions, or human-object interactions.", "method": "Two key innovations: 1) Twin-teacher enhanced training algorithm to transfer text-controllability from foundation models while learning audio-visual synchronization, 2) Dynamic modulation of multi-modal condition strengths based on denoising timesteps to mitigate conflicts between heterogeneous conditioning signals.", "result": "Outperforms state-of-the-art models like Omnihuman-1.5 and KlingAvatar 2.0 in GSB evaluation, enables complex applications including multi-person dialogues and non-human subjects role-playing, and generates natural, temporally coherent full-body motions and dynamic camera movements.", "conclusion": "JoyAvatar substantially expands avatar model capacity for complex text-controlled video generation while preserving basic capabilities like accurate lip-sync and identity consistency."}}
{"id": "2602.00703", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.00703", "abs": "https://arxiv.org/abs/2602.00703", "authors": ["Zhongtian Huang", "Zhi Chen", "Zi Huang", "Xin Yu", "Daniel Smith", "Chaitanya Purushothama", "Erik Van Oosterom", "Alex Wu", "William Salter", "Yan Li", "Scott Chapman"], "title": "StomataSeg: Semi-Supervised Instance Segmentation for Sorghum Stomatal Components", "comment": null, "summary": "Sorghum is a globally important cereal grown widely in water-limited and stress-prone regions. Its strong drought tolerance makes it a priority crop for climate-resilient agriculture. Improving water-use efficiency in sorghum requires precise characterisation of stomatal traits, as stomata control of gas exchange, transpiration and photosynthesis have a major influence on crop performance. Automated analysis of sorghum stomata is difficult because the stomata are small (often less than 40 $\u03bc$m in length in grasses such as sorghum) and vary in shape across genotypes and leaf surfaces. Automated segmentation contributes to high-throughput stomatal phenotyping, yet current methods still face challenges related to nested small structures and annotation bottlenecks. In this paper, we propose a semi-supervised instance segmentation framework tailored for analysis of sorghum stomatal components. We collect and annotate a sorghum leaf imagery dataset containing 11,060 human-annotated patches, covering the three stomatal components (pore, guard cell and complex area) across multiple genotypes and leaf surfaces. To improve the detection of tiny structures, we split high-resolution microscopy images into overlapping small patches. We then apply a pseudo-labelling strategy to unannotated images, producing an additional 56,428 pseudo-labelled patches. Benchmarking across semantic and instance segmentation models shows substantial performance gains: for semantic models the top mIoU increases from 65.93% to 70.35%, whereas for instance models the top AP rises from 28.30% to 46.10%. These results demonstrate that combining patch-based preprocessing with semi-supervised learning significantly improves the segmentation of fine stomatal structures. The proposed framework supports scalable extraction of stomatal traits and facilitates broader adoption of AI-driven phenotyping in crop science.", "AI": {"tldr": "Semi-supervised instance segmentation framework improves automated analysis of tiny sorghum stomatal components using patch-based preprocessing and pseudo-labeling.", "motivation": "Sorghum is crucial for climate-resilient agriculture due to drought tolerance, but automated stomatal analysis is challenging due to small size (often <40\u03bcm) and shape variations across genotypes and leaf surfaces. Current methods struggle with nested small structures and annotation bottlenecks.", "method": "Proposed semi-supervised instance segmentation framework: 1) Collected and annotated 11,060 human-annotated patches covering three stomatal components (pore, guard cell, complex area) across multiple genotypes and leaf surfaces; 2) Split high-resolution microscopy images into overlapping small patches to improve detection of tiny structures; 3) Applied pseudo-labeling strategy to unannotated images, producing additional 56,428 pseudo-labeled patches.", "result": "Significant performance gains: semantic models' top mIoU increased from 65.93% to 70.35%; instance models' top AP rose from 28.30% to 46.10%. Combining patch-based preprocessing with semi-supervised learning significantly improves segmentation of fine stomatal structures.", "conclusion": "The framework enables scalable extraction of stomatal traits and facilitates broader adoption of AI-driven phenotyping in crop science, supporting improved water-use efficiency in sorghum through precise stomatal characterization."}}
{"id": "2602.00729", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.00729", "abs": "https://arxiv.org/abs/2602.00729", "authors": ["Qihe Pan", "Yiming Wu", "Xing Zhao", "Liang Xie", "Guodao Sun", "Ronghua Liang"], "title": "Supervised makeup transfer with a curated dataset: Decoupling identity and makeup features for enhanced transformation", "comment": "This paper has been accepted for publication in the proceedings of 2026 IEEE ICASSP Conference", "summary": "Diffusion models have recently shown strong progress in generative tasks, offering a more stable alternative to GAN-based approaches for makeup transfer. Existing methods often suffer from limited datasets, poor disentanglement between identity and makeup features, and weak controllability. To address these issues, we make three contributions. First, we construct a curated high-quality dataset using a train-generate-filter-retrain strategy that combines synthetic, realistic, and filtered samples to improve diversity and fidelity. Second, we design a diffusion-based framework that disentangles identity and makeup features, ensuring facial structure and skin tone are preserved while applying accurate and diverse cosmetic styles. Third, we propose a text-guided mechanism that allows fine-grained and region-specific control, enabling users to modify eyes, lips, or face makeup with natural language prompts. Experiments on benchmarks and real-world scenarios demonstrate improvements in fidelity, identity preservation, and flexibility. Examples of our dataset can be found at: https://makeup-adapter.github.io.", "AI": {"tldr": "A diffusion-based makeup transfer framework with curated dataset, disentangled identity/makeup features, and text-guided control for fine-grained region-specific makeup application.", "motivation": "Existing makeup transfer methods suffer from limited datasets, poor disentanglement between identity and makeup features, and weak controllability, which diffusion models can address with better stability than GAN-based approaches.", "method": "Three contributions: 1) curated high-quality dataset using train-generate-filter-retrain strategy with synthetic/realistic/filtered samples; 2) diffusion-based framework that disentangles identity and makeup features while preserving facial structure and skin tone; 3) text-guided mechanism for fine-grained region-specific control using natural language prompts.", "result": "Experiments on benchmarks and real-world scenarios demonstrate improvements in fidelity, identity preservation, and flexibility compared to existing methods.", "conclusion": "The proposed diffusion-based approach effectively addresses key challenges in makeup transfer through dataset curation, feature disentanglement, and text-guided control, offering a more stable and controllable alternative to GAN-based methods."}}
{"id": "2602.00739", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.00739", "abs": "https://arxiv.org/abs/2602.00739", "authors": ["Zhengyan Qin", "Liyuan Qiu"], "title": "Diffusion-Driven Inter-Outer Surface Separation for Point Clouds with Open Boundaries", "comment": null, "summary": "We propose a diffusion-based algorithm for separating the inter and outer layer surfaces from double-layered point clouds, particularly those exhibiting the \"double surface artifact\" caused by truncation in Truncated Signed Distance Function (TSDF) fusion during indoor or medical 3D reconstruction. This artifact arises from asymmetric truncation thresholds, leading to erroneous inter and outer shells in the fused volume, which our method addresses by extracting the true inter layer to mitigate challenges like overlapping surfaces and disordered normals. We focus on point clouds with \\emph{open boundaries} (i.e., sampled surfaces with topological openings/holes through which particles may escape), rather than point clouds with \\emph{missing surface regions} where no samples exist. Our approach enables robust processing of both watertight and open-boundary models, achieving extraction of the inter layer from 20,000 inter and 20,000 outer points in approximately 10 seconds. This solution is particularly effective for applications requiring accurate surface representations, such as indoor scene modeling and medical imaging, where double-layered point clouds are prevalent, and it accommodates both closed (watertight) and open-boundary surface geometries. Our goal is \\emph{post-hoc} inter/outer shell separation as a lightweight module after TSDF fusion; we do not aim to replace full variational or learning-based reconstruction pipelines.", "AI": {"tldr": "A diffusion-based algorithm for separating inter and outer layer surfaces from double-layered point clouds caused by TSDF truncation artifacts, handling both watertight and open-boundary models efficiently.", "motivation": "Addresses the \"double surface artifact\" problem in TSDF fusion where asymmetric truncation thresholds create erroneous inter and outer shells in 3D reconstruction, particularly problematic for indoor scene modeling and medical imaging applications.", "method": "Uses a diffusion-based algorithm to separate true inter layer surfaces from double-layered point clouds, specifically designed for point clouds with open boundaries (topological openings/holes) rather than missing surface regions.", "result": "Achieves robust processing of both watertight and open-boundary models, extracting inter layer from 20,000 inter and 20,000 outer points in approximately 10 seconds.", "conclusion": "Provides an effective post-hoc solution for inter/outer shell separation as a lightweight module after TSDF fusion, particularly valuable for applications requiring accurate surface representations where double-layered point clouds are common."}}
{"id": "2602.00749", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.00749", "abs": "https://arxiv.org/abs/2602.00749", "authors": ["Xiangming Wang", "Benteng Sun", "Yungeng Liu", "Haijin Zeng", "Yongyong Chen", "Jingyong Su", "Jie Liu"], "title": "HSI-VAR: Rethinking Hyperspectral Restoration through Spatial-Spectral Visual Autoregression", "comment": null, "summary": "Hyperspectral images (HSIs) capture richer spatial-spectral information beyond RGB, yet real-world HSIs often suffer from a composite mix of degradations, such as noise, blur, and missing bands. Existing generative approaches for HSI restoration like diffusion models require hundreds of iterative steps, making them computationally impractical for high-dimensional HSIs. While regression models tend to produce oversmoothed results, failing to preserve critical structural details. We break this impasse by introducing HSI-VAR, rethinking HSI restoration as an autoregressive generation problem, where spectral and spatial dependencies can be progressively modeled rather than globally reconstructed. HSI-VAR incorporates three key innovations: (1) Latent-condition alignment, which couples semantic consistency between latent priors and conditional embeddings for precise reconstruction; (2) Degradation-aware guidance, which uniquely encodes mixed degradations as linear combinations in the embedding space for automatic control, remarkably achieving a nearly $50\\%$ reduction in computational cost at inference; (3) A spatial-spectral adaptation module that refines details across both domains in the decoding phase. Extensive experiments on nine all-in-one HSI restoration benchmarks confirm HSI-VAR's state-of-the-art performance, achieving a 3.77 dB PSNR improvement on \\textbf{\\textit{ICVL}} and offering superior structure preservation with an inference speed-up of up to $95.5 \\times$ compared with diffusion-based methods, making it a highly practical solution for real-world HSI restoration.", "AI": {"tldr": "HSI-VAR introduces an autoregressive approach for hyperspectral image restoration that handles mixed degradations (noise, blur, missing bands) with 50% computational cost reduction and 95.5\u00d7 speed-up over diffusion models.", "motivation": "Real-world hyperspectral images suffer from composite degradations (noise, blur, missing bands). Existing approaches have limitations: diffusion models are computationally impractical for high-dimensional HSIs requiring hundreds of iterative steps, while regression models produce oversmoothed results that fail to preserve critical structural details.", "method": "HSI-VAR rethinks HSI restoration as an autoregressive generation problem, progressively modeling spectral and spatial dependencies rather than global reconstruction. Key innovations: (1) Latent-condition alignment for semantic consistency between latent priors and conditional embeddings; (2) Degradation-aware guidance encoding mixed degradations as linear combinations in embedding space for automatic control; (3) Spatial-spectral adaptation module refining details across both domains in decoding phase.", "result": "State-of-the-art performance on nine all-in-one HSI restoration benchmarks, achieving 3.77 dB PSNR improvement on ICVL dataset. Offers superior structure preservation with inference speed-up of up to 95.5\u00d7 compared with diffusion-based methods and nearly 50% reduction in computational cost at inference.", "conclusion": "HSI-VAR provides a highly practical solution for real-world HSI restoration by breaking the computational impasse of diffusion models while avoiding the oversmoothing issues of regression approaches, making it suitable for practical applications requiring both quality and efficiency."}}
{"id": "2602.00763", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.00763", "abs": "https://arxiv.org/abs/2602.00763", "authors": ["Dylan Yves", "Khush Agarwal", "Jonathan Hoyin Chan", "Patcharapit Promoppatum", "Aroonkamon Pattanasiricharoen"], "title": "Evaluating Deep Learning-Based Nerve Segmentation in Brachial Plexus Ultrasound Under Realistic Data Constraints", "comment": "9 pages, 6 figures", "summary": "Accurate nerve localization is critical for the success of ultrasound-guided regional anesthesia, yet manual identification remains challenging due to low image contrast, speckle noise, and inter-patient anatomical variability. This study evaluates deep learning-based nerve segmentation in ultrasound images of the brachial plexus using a U-Net architecture, with a focus on how dataset composition and annotation strategy influence segmentation performance. We find that training on combined data from multiple ultrasound machines (SIEMENS ACUSON NX3 Elite and Philips EPIQ5) provides regularization benefits for lower-performing acquisition sources, though it does not surpass single-source training when matched to the target domain. Extending the task from binary nerve segmentation to multi-class supervision (artery, vein, nerve, muscle) results in decreased nerve-specific Dice scores, with performance drops ranging from 9% to 61% depending on dataset, likely due to class imbalance and boundary ambiguity. Additionally, we observe a moderate positive correlation between nerve size and segmentation accuracy (Pearson r=0.587, p<0.001), indicating that smaller nerves remain a primary challenge. These findings provide methodological guidance for developing robust ultrasound nerve segmentation systems under realistic clinical data constraints.", "AI": {"tldr": "Deep learning-based nerve segmentation in ultrasound images using U-Net, evaluating dataset composition, annotation strategies, and their impact on performance for brachial plexus localization.", "motivation": "Manual nerve localization in ultrasound-guided regional anesthesia is challenging due to low image contrast, speckle noise, and anatomical variability, necessitating automated solutions.", "method": "U-Net architecture for nerve segmentation in brachial plexus ultrasound images, evaluating effects of dataset composition (single vs. multi-machine data) and annotation strategy (binary vs. multi-class segmentation).", "result": "Multi-machine training provides regularization but doesn't surpass single-source training; multi-class supervision reduces nerve-specific performance (9-61% drop); moderate correlation between nerve size and accuracy (r=0.587).", "conclusion": "Dataset composition and annotation strategy significantly impact segmentation performance, with smaller nerves remaining challenging; findings provide guidance for developing robust ultrasound nerve segmentation systems."}}
{"id": "2602.00795", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.00795", "abs": "https://arxiv.org/abs/2602.00795", "authors": ["Wenhao Li", "Xianjing Meng", "Qiangchang Wang", "Zhongyi Han", "Zhibin Wu", "Yilong Yin"], "title": "DVLA-RL: Dual-Level Vision-Language Alignment with Reinforcement Learning Gating for Few-Shot Learning", "comment": "Accepted by ICLR 2026", "summary": "Few-shot learning (FSL) aims to generalize to novel categories with only a few samples. Recent approaches incorporate large language models (LLMs) to enrich visual representations with semantic embeddings derived from class names. However, they overlook progressive and adaptive alignment between vision and language from low-level to high-level semantics, resulting in limited semantic gains. To address these challenges, we propose Dual-level Vision-Language Alignment with Reinforcement Learning gating (DVLA-RL), which consists of Dual-level Semantic Construction (DSC) and RL-gated Attention (RLA). Specifically, DSC conditions LLMs on both class names and support samples to generate discriminative attributes, progressively selects the most relevant ones, and then synthesizes them into coherent class descriptions. This process provides complementary low-level attributes and high-level descriptions, enabling both fine-grained grounding and holistic class understanding. To dynamically integrate dual-level semantics along with the visual network layers, RLA formulates cross-modal fusion as a sequential decision process. A lightweight policy trained with episodic REINFORCE adaptively adjusts the contributions of self-attention and cross-attention to integrate textual and visual tokens. As a result, shallow layers refine local attributes and deep layers emphasize global semantics, enabling more precise cross-modal alignment. This achieves class-specific discrimination and generalized representations with merely a few support samples. DVLA-RL achieves new state-of-the-art performance across nine benchmarks in three diverse FSL scenarios.", "AI": {"tldr": "DVLA-RL is a novel few-shot learning method that uses dual-level vision-language alignment with reinforcement learning gating to achieve better cross-modal integration and state-of-the-art performance across multiple benchmarks.", "motivation": "Existing few-shot learning approaches that incorporate LLMs overlook progressive and adaptive alignment between vision and language from low-level to high-level semantics, resulting in limited semantic gains. There's a need for more dynamic and hierarchical cross-modal integration.", "method": "DVLA-RL consists of two main components: 1) Dual-level Semantic Construction (DSC) that conditions LLMs on class names and support samples to generate discriminative attributes, progressively selects relevant ones, and synthesizes them into coherent class descriptions; 2) RL-gated Attention (RLA) that formulates cross-modal fusion as a sequential decision process using a lightweight policy trained with episodic REINFORCE to adaptively adjust contributions of self-attention and cross-attention.", "result": "DVLA-RL achieves new state-of-the-art performance across nine benchmarks in three diverse few-shot learning scenarios, demonstrating superior generalization with limited samples.", "conclusion": "The proposed dual-level vision-language alignment with reinforcement learning gating enables more precise cross-modal integration, allowing shallow layers to refine local attributes and deep layers to emphasize global semantics, achieving both class-specific discrimination and generalized representations with few support samples."}}
{"id": "2602.00807", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2602.00807", "abs": "https://arxiv.org/abs/2602.00807", "authors": ["Xianzhe Fan", "Shengliang Deng", "Xiaoyang Wu", "Yuxiang Lu", "Zhuoling Li", "Mi Yan", "Yujia Zhang", "Zhizheng Zhang", "He Wang", "Hengshuang Zhao"], "title": "Any3D-VLA: Enhancing VLA Robustness via Diverse Point Clouds", "comment": null, "summary": "Existing Vision-Language-Action (VLA) models typically take 2D images as visual input, which limits their spatial understanding in complex scenes. How can we incorporate 3D information to enhance VLA capabilities? We conduct a pilot study across different observation spaces and visual representations. The results show that explicitly lifting visual input into point clouds yields representations that better complement their corresponding 2D representations. To address the challenges of (1) scarce 3D data and (2) the domain gap induced by cross-environment differences and depth-scale biases, we propose Any3D-VLA. It unifies the simulator, sensor, and model-estimated point clouds within a training pipeline, constructs diverse inputs, and learns domain-agnostic 3D representations that are fused with the corresponding 2D representations. Simulation and real-world experiments demonstrate Any3D-VLA's advantages in improving performance and mitigating the domain gap. Our project homepage is available at https://xianzhefan.github.io/Any3D-VLA.github.io.", "AI": {"tldr": "Any3D-VLA enhances Vision-Language-Action models by incorporating 3D point cloud representations alongside 2D images to improve spatial understanding in complex scenes, addressing data scarcity and domain gap issues.", "motivation": "Existing VLA models rely on 2D images which limit spatial understanding in complex 3D environments. The paper aims to incorporate 3D information to enhance VLA capabilities and address challenges of scarce 3D data and domain gaps from cross-environment differences and depth-scale biases.", "method": "Proposes Any3D-VLA which: 1) unifies simulator, sensor, and model-estimated point clouds in training pipeline; 2) constructs diverse 3D inputs; 3) learns domain-agnostic 3D representations; 4) fuses these 3D representations with corresponding 2D representations.", "result": "Simulation and real-world experiments demonstrate Any3D-VLA's advantages in improving performance and mitigating domain gaps. The approach shows that explicitly lifting visual input into point clouds yields representations that better complement 2D representations.", "conclusion": "Incorporating 3D point cloud representations enhances VLA model capabilities for spatial understanding in complex scenes. The unified training pipeline with diverse 3D inputs and domain-agnostic learning effectively addresses data scarcity and domain gap challenges."}}
{"id": "2602.00810", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2602.00810", "abs": "https://arxiv.org/abs/2602.00810", "authors": ["Ze Huang", "Zhongyang Xiao", "Mingliang Song", "Longan Yang", "Hongyuan Yuan", "Li Sun"], "title": "VVLoc: Prior-free 3-DoF Vehicle Visual Localization", "comment": null, "summary": "Localization is a critical technology in autonomous driving, encompassing both topological localization, which identifies the most similar map keyframe to the current observation, and metric localization, which provides precise spatial coordinates. Conventional methods typically address these tasks independently, rely on single-camera setups, and often require additional 3D semantic or pose priors, while lacking mechanisms to quantify the confidence of localization results, making them less feasible for real industrial applications. In this paper, we propose VVLoc, a unified pipeline that employs a single neural network to concurrently achieve topological and metric vehicle localization using multi-camera system. VVLoc first evaluates the geo-proximity between visual observations, then estimates their relative metric poses using a matching strategy, while also providing a confidence measure. Additionally, the training process for VVLoc is highly efficient, requiring only pairs of visual data and corresponding ground-truth poses, eliminating the need for complex supplementary data. We evaluate VVLoc not only on the publicly available datasets, but also on a more challenging self-collected dataset, demonstrating its ability to deliver state-of-the-art localization accuracy across a wide range of localization tasks.", "AI": {"tldr": "VVLoc is a unified neural network pipeline for both topological and metric vehicle localization using multi-camera systems, providing confidence measures and requiring only visual data with ground-truth poses for training.", "motivation": "Current localization methods have limitations: they handle topological and metric localization separately, use single-camera setups, require additional 3D semantic or pose priors, and lack confidence quantification mechanisms, making them impractical for real industrial applications.", "method": "VVLoc uses a single neural network with multi-camera inputs to simultaneously achieve both localization tasks. It first evaluates geo-proximity between visual observations, then estimates relative metric poses using a matching strategy, while also providing confidence measures. Training requires only pairs of visual data and corresponding ground-truth poses.", "result": "VVLoc achieves state-of-the-art localization accuracy across a wide range of tasks, validated on both public datasets and a more challenging self-collected dataset.", "conclusion": "VVLoc provides a practical, unified solution for vehicle localization that addresses limitations of conventional methods, offering efficient training, confidence quantification, and strong performance across diverse localization tasks suitable for real industrial applications."}}
{"id": "2602.00813", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.00813", "abs": "https://arxiv.org/abs/2602.00813", "authors": ["Tong Wang", "Yunhan Zhao", "Shu Kong"], "title": "Generating a Paracosm for Training-Free Zero-Shot Composed Image Retrieval", "comment": null, "summary": "Composed Image Retrieval (CIR) is the task of retrieving a target image from a database using a multimodal query, which consists of a reference image and a modification text. The text specifies how to alter the reference image to form a ``mental image'', based on which CIR should find the target image in the database. The fundamental challenge of CIR is that this ``mental image'' is not physically available and is only implicitly defined by the query. The contemporary literature pursues zero-shot methods and uses a Large Multimodal Model (LMM) to generate a textual description for a given multimodal query, and then employs a Vision-Language Model (VLM) for textual-visual matching to search the target image. In contrast, we address CIR from first principles by directly generating the ``mental image'' for more accurate matching. Particularly, we prompt an LMM to generate a ``mental image'' for a given multimodal query and propose to use this ``mental image'' to search for the target image. As the ``mental image'' has a synthetic-to-real domain gap with real images, we also generate a synthetic counterpart for each real image in the database to facilitate matching. In this sense, our method uses LMM to construct a ``paracosm'', where it matches the multimodal query and database images. Hence, we call this method Paracosm. Notably, Paracosm is a training-free zero-shot CIR method. It significantly outperforms existing zero-shot methods on four challenging benchmarks, achieving state-of-the-art performance for zero-shot CIR.", "AI": {"tldr": "Paracosm is a training-free zero-shot CIR method that directly generates the \"mental image\" from multimodal queries using LMMs, then matches it with synthetic counterparts of database images to overcome domain gaps.", "motivation": "Current zero-shot CIR methods use LMMs to generate textual descriptions from multimodal queries, then match text to images. This indirect approach is suboptimal because the \"mental image\" is only implicitly defined and not physically available. The paper aims to address CIR from first principles by directly generating the mental image for more accurate matching.", "method": "Paracosm prompts an LMM to directly generate the \"mental image\" (visual representation) for a given multimodal query. To address the synthetic-to-real domain gap, it also generates synthetic counterparts for each real image in the database. This creates a \"paracosm\" where matching occurs between the generated mental image and synthetic versions of database images.", "result": "Paracosm significantly outperforms existing zero-shot methods on four challenging benchmarks, achieving state-of-the-art performance for zero-shot CIR.", "conclusion": "Directly generating the \"mental image\" from multimodal queries using LMMs and matching in a synthetic domain (paracosm) is an effective training-free approach for zero-shot CIR, overcoming limitations of current text-based matching methods."}}
{"id": "2602.00821", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.00821", "abs": "https://arxiv.org/abs/2602.00821", "authors": ["Konstantinos Moutselos", "Ilias Maglogiannis"], "title": "Edge-Native Generative De-identification: Inversion-Free Flow for Privacy-Preserving Federated Skin Image Analysis", "comment": "8 pages, 5 figures", "summary": "The deployment of Federated Learning (FL) for clinical dermatology is hindered by the competing requirements of protecting patient privacy and preserving diagnostic features. Traditional de-identification methods often degrade pathological fidelity, while standard generative editing techniques rely on computationally intensive inversion processes unsuitable for resource-constrained edge devices. We propose a framework for identity-agnostic pathology preservation that serves as a client-side privacy-preserving utility. By leveraging inversion-free Rectified Flow Transformers (FlowEdit), the system performs high-fidelity identity transformation in near real-time (less than 20s), facilitating local deployment on clinical nodes. We introduce a \"Segment-by-Synthesis\" mechanism that generates counterfactual healthy and pathological twin pairs locally. This enables the extraction of differential erythema masks that are decoupled from biometric markers and semantic artifacts (e.g. jewelry). Pilot validation on high-resolution clinical samples demonstrates an Intersection over Union (IoU) stability greater than 0.67 across synthetic identities. By generating privacy-compliant synthetic surrogates at the edge, this framework mitigates the risk of gradient leakage at the source, providing a secure pathway for high-precision skin image analysis in federated environments.", "AI": {"tldr": "A framework for privacy-preserving federated learning in dermatology that uses inversion-free generative editing to create identity-agnostic synthetic images while preserving pathological features, enabling secure edge deployment.", "motivation": "Federated learning for clinical dermatology faces privacy-diagnostic tradeoffs: traditional de-identification degrades pathological features, while standard generative methods are too computationally intensive for edge devices.", "method": "Uses inversion-free Rectified Flow Transformers (FlowEdit) for high-fidelity identity transformation in near real-time (<20s). Introduces \"Segment-by-Synthesis\" mechanism to generate counterfactual healthy/pathological twin pairs locally, extracting differential erythema masks decoupled from biometric markers.", "result": "Pilot validation shows Intersection over Union (IoU) stability >0.67 across synthetic identities. Framework generates privacy-compliant synthetic surrogates at the edge, mitigating gradient leakage risk.", "conclusion": "The framework provides a secure pathway for high-precision skin image analysis in federated environments by enabling privacy-preserving synthetic data generation at clinical edge nodes."}}
{"id": "2602.00839", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.00839", "abs": "https://arxiv.org/abs/2602.00839", "authors": ["Mingwei Li", "Hehe Fan", "Yi Yang"], "title": "TransNormal: Dense Visual Semantics for Diffusion-based Transparent Object Normal Estimation", "comment": "Project Page: https://longxiang-ai.github.io/TransNormal", "summary": "Monocular normal estimation for transparent objects is critical for laboratory automation, yet it remains challenging due to complex light refraction and reflection. These optical properties often lead to catastrophic failures in conventional depth and normal sensors, hindering the deployment of embodied AI in scientific environments. We propose TransNormal, a novel framework that adapts pre-trained diffusion priors for single-step normal regression. To handle the lack of texture in transparent surfaces, TransNormal integrates dense visual semantics from DINOv3 via a cross-attention mechanism, providing strong geometric cues. Furthermore, we employ a multi-task learning objective and wavelet-based regularization to ensure the preservation of fine-grained structural details. To support this task, we introduce TransNormal-Synthetic, a physics-based dataset with high-fidelity normal maps for transparent labware. Extensive experiments demonstrate that TransNormal significantly outperforms state-of-the-art methods: on the ClearGrasp benchmark, it reduces mean error by 24.4% and improves 11.25\u00b0 accuracy by 22.8%; on ClearPose, it achieves a 15.2% reduction in mean error. The code and dataset will be made publicly available at https://longxiang-ai.github.io/TransNormal.", "AI": {"tldr": "TransNormal is a diffusion-based framework for monocular normal estimation of transparent objects, using DINOv3 semantics and wavelet regularization to handle complex refraction/reflection, achieving state-of-the-art performance on transparent object benchmarks.", "motivation": "Transparent object normal estimation is critical for lab automation but challenging due to complex light refraction/reflection that causes failures in conventional sensors, hindering embodied AI deployment in scientific environments.", "method": "Adapts pre-trained diffusion priors for single-step normal regression, integrates DINOv3 dense visual semantics via cross-attention for geometric cues, uses multi-task learning and wavelet-based regularization to preserve fine structural details.", "result": "Significantly outperforms SOTA: on ClearGrasp, reduces mean error by 24.4% and improves 11.25\u00b0 accuracy by 22.8%; on ClearPose, achieves 15.2% reduction in mean error. Introduces TransNormal-Synthetic physics-based dataset.", "conclusion": "TransNormal effectively addresses transparent object normal estimation challenges through diffusion priors enhanced with semantic features and regularization, enabling better robotic manipulation in laboratory settings. Code and dataset will be publicly released."}}
{"id": "2602.00841", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.00841", "abs": "https://arxiv.org/abs/2602.00841", "authors": ["Jintao Cheng", "Weibin Li", "Zhijian He", "Jin Wu", "Chi Man Vong", "Wei Zhang"], "title": "Invariance on Manifolds: Understanding Robust Visual Representations for Place Recognition", "comment": "14pages, 5 figures", "summary": "Visual Place Recognition (VPR) demands representations robust to drastic environmental and viewpoint shifts. Current aggregation paradigms, however, either rely on data-hungry supervision or simplistic first-order statistics, often neglecting intrinsic structural correlations. In this work, we propose a Second-Order Geometric Statistics framework that inherently captures geometric stability without training. We conceptualize scenes as covariance descriptors on the Symmetric Positive Definite (SPD) manifold, where perturbations manifest as tractable congruence transformations. By leveraging geometry-aware Riemannian mappings, we project these descriptors into a linearized Euclidean embedding, effectively decoupling signal structure from noise. Our approach introduces a training-free framework built upon fixed, pre-trained backbones, achieving strong zero-shot generalization without parameter updates. Extensive experiments confirm that our method achieves highly competitive performance against state-of-the-art baselines, particularly excelling in challenging zero-shot scenarios.", "AI": {"tldr": "Training-free VPR using second-order geometric statistics on SPD manifold for robust zero-shot generalization.", "motivation": "Current VPR methods either need heavy supervision or use simplistic first-order statistics, missing intrinsic structural correlations needed for robustness to environmental/viewpoint changes.", "method": "Proposes second-order geometric statistics framework using covariance descriptors on SPD manifold, treating perturbations as congruence transformations. Uses Riemannian mappings to project descriptors into linear Euclidean embedding, decoupling signal from noise. Training-free approach built on fixed pre-trained backbones.", "result": "Achieves highly competitive performance against SOTA baselines, particularly excels in challenging zero-shot scenarios without parameter updates.", "conclusion": "Second-order geometric statistics on SPD manifold provides effective training-free solution for robust VPR with strong zero-shot generalization capabilities."}}
{"id": "2602.00865", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.00865", "abs": "https://arxiv.org/abs/2602.00865", "authors": ["Brandon Leblanc", "Charalambos Poullis"], "title": "Distill3R: A Pipeline for Democratizing 3D Foundation Models on Commodity Hardware", "comment": "Submitted to the Canadian Conference on Robotics and Vision (CRV). 10 pages, 5 figures", "summary": "While multi-view 3D reconstruction has shifted toward large-scale foundation models capable of inferring globally consistent geometry, their reliance on massive computational clusters for training has created a significant barrier to entry for most academic laboratories. To bridge this compute divide, we introduce Distill3R, a framework designed to distill the geometric reasoning of 3D foundation models into compact students fully trainable on a single workstation. Our methodology centers on two primary innovations: (1) an offline caching pipeline that decouples heavy teacher inference from the training loop through compressed supervision signals, and (2) a confidence-aware distillation loss that leverages teacher uncertainty to enable training on commodity hardware. We propose a 72M-parameter student model which achieves a 9x reduction in parameters and a 5x inference speedup compared to its 650M-parameter teacher. The student is fully trainable in under 3 days on a single workstation, whereas its teacher requires massive GPU clusters for up to a week. We demonstrate that the student preserves the structural consistency and qualitative geometric understanding required for functional 3D awareness. By providing a reproducible, single-workstation training recipe, Distill3R serves as an exploratory entry point for democratized 3D vision research and efficient edge deployment. This work is not intended to compete with state-of-the-art foundation models, but to provide an accessible research baseline for laboratories without access to large-scale compute to train and specialize models on their own domain-specific data at minimal cost.", "AI": {"tldr": "Distill3R is a framework that distills large 3D foundation models into compact student models trainable on a single workstation, enabling accessible 3D vision research without massive compute clusters.", "motivation": "Large-scale 3D foundation models require massive computational clusters for training, creating a significant barrier to entry for academic laboratories without access to such resources.", "method": "Two key innovations: (1) offline caching pipeline that decouples heavy teacher inference from training through compressed supervision signals, and (2) confidence-aware distillation loss that leverages teacher uncertainty for training on commodity hardware.", "result": "A 72M-parameter student model achieves 9x parameter reduction and 5x inference speedup compared to its 650M-parameter teacher. The student trains in under 3 days on a single workstation versus teacher requiring massive GPU clusters for up to a week.", "conclusion": "Distill3R provides a reproducible, single-workstation training recipe for democratized 3D vision research, serving as an accessible baseline for laboratories to train and specialize models on domain-specific data without large-scale compute."}}
{"id": "2602.00883", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.00883", "abs": "https://arxiv.org/abs/2602.00883", "authors": ["Alicja Polowczyk", "Agnieszka Polowczyk", "Piotr Borycki", "Joanna Waczy\u0144ska", "Jacek Tabor", "Przemys\u0142aw Spurek"], "title": "DIAMOND: Directed Inference for Artifact Mitigation in Flow Matching Models", "comment": null, "summary": "Despite impressive results from recent text-to-image models like FLUX, visual and anatomical artifacts remain a significant hurdle for practical and professional use. Existing methods for artifact reduction, typically work in a post-hoc manner, consequently failing to intervene effectively during the core image formation process. Notably, current techniques require problematic and invasive modifications to the model weights, or depend on a computationally expensive and time-consuming process of regional refinement. To address these limitations, we propose DIAMOND, a training-free method that applies trajectory correction to mitigate artifacts during inference. By reconstructing an estimate of the clean sample at every step of the generative trajectory, DIAMOND actively steers the generation process away from latent states that lead to artifacts. Furthermore, we extend the proposed method to standard Diffusion Models, demonstrating that DIAMOND provides a robust, zero-shot path to high-fidelity, artifact-free image synthesis without the need for additional training or weight modifications in modern generative architectures. Code is available at https://gmum.github.io/DIAMOND/", "AI": {"tldr": "DIAMOND is a training-free method that applies trajectory correction during inference to reduce visual and anatomical artifacts in text-to-image generation without modifying model weights or requiring additional training.", "motivation": "Current text-to-image models like FLUX still produce visual and anatomical artifacts that hinder practical professional use. Existing artifact reduction methods work post-hoc, fail to intervene during image formation, require invasive weight modifications, or rely on computationally expensive regional refinement.", "method": "DIAMOND uses trajectory correction during inference by reconstructing an estimate of the clean sample at every step of the generative trajectory. This actively steers the generation process away from latent states that lead to artifacts, without requiring training or weight modifications.", "result": "The method demonstrates robust, zero-shot artifact reduction and high-fidelity image synthesis. It's also extended to work with standard Diffusion Models, showing broad applicability across modern generative architectures.", "conclusion": "DIAMOND provides an effective training-free solution for artifact reduction in text-to-image generation that intervenes during the core image formation process, offering a practical path to high-quality results without computational overhead or model modifications."}}
{"id": "2602.00904", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.00904", "abs": "https://arxiv.org/abs/2602.00904", "authors": ["Kunal Mahatha", "Ali Bahri", "Pierre Marza", "Sahar Dastani", "Maria Vakalopoulou", "Stergios Christodoulidis", "Jose Dolz", "Christian Desrosiers"], "title": "OCTOPUS: Enhancing the Spatial-Awareness of Vision SSMs with Multi-Dimensional Scans and Traversal Selection", "comment": null, "summary": "State space models (SSMs) have recently emerged as an alternative to transformers due to their unique ability of modeling global relationships in text with linear complexity. However, their success in vision tasks has been limited due to their causal formulation, which is suitable for sequential text but detrimental in the spatial domain where causality breaks the inherent spatial relationships among pixels or patches. As a result, standard SSMs fail to capture local spatial coherence, often linking non-adjacent patches while ignoring neighboring ones that are visually correlated. To address these limitations, we introduce OCTOPUS , a novel architecture that preserves both global context and local spatial structure within images, while maintaining the linear complexity of SSMs. OCTOPUS performs discrete reoccurrence along eight principal orientations, going forward or backward in the horizontal, vertical, and diagonal directions, allowing effective information exchange across all spatially connected regions while maintaining independence among unrelated patches. This design enables multi-directional recurrence, capturing both global context and local spatial structure with SSM-level efficiency. In our classification and segmentation benchmarks, OCTOPUS demonstrates notable improvements in boundary preservation and region consistency, as evident from the segmentation results, while maintaining relatively better classification accuracy compared to existing V-SSM based models. These results suggest that OCTOPUS appears as a foundation method for multi-directional recurrence as a scalable and effective mechanism for building spatially aware and computationally efficient vision architectures.", "AI": {"tldr": "OCTOPUS is a novel vision architecture using state space models with 8-directional recurrence to capture both global context and local spatial structure while maintaining linear complexity.", "motivation": "Standard SSMs fail in vision tasks due to their causal formulation that breaks spatial relationships, linking non-adjacent patches while ignoring neighboring visually correlated ones.", "method": "OCTOPUS performs discrete recurrence along eight principal orientations (horizontal, vertical, and diagonal directions, both forward and backward) to enable effective information exchange across spatially connected regions while maintaining independence among unrelated patches.", "result": "OCTOPUS shows notable improvements in boundary preservation and region consistency in segmentation tasks, while maintaining relatively better classification accuracy compared to existing V-SSM models.", "conclusion": "OCTOPUS emerges as a foundation method for multi-directional recurrence, providing a scalable and effective mechanism for building spatially aware and computationally efficient vision architectures."}}
{"id": "2602.00946", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.00946", "abs": "https://arxiv.org/abs/2602.00946", "authors": ["Dhruv Parikh", "Haoyang Fan", "Rajgopal Kannan", "Viktor Prasanna"], "title": "ConsensusDrop: Fusing Visual and Cross-Modal Saliency for Efficient Vision Language Models", "comment": "Technical Report", "summary": "Vision-Language Models (VLMs) are expensive because the LLM processes hundreds of largely redundant visual tokens. Existing token reduction methods typically exploit \\textit{either} vision-encoder saliency (broad but query-agnostic) \\textit{or} LLM cross-attention (query-aware but sparse and costly). We show that neither signal alone is sufficient: fusing them consistently improves performance compared to unimodal visual token selection (ranking). However, making such fusion practical is non-trivial: cross-modal saliency is usually only available \\emph{inside} the LLM (too late for efficient pre-LLM pruning), and the two signals are inherently asymmetric, so naive fusion underutilizes their complementary strengths. We propose \\textbf{ConsensusDrop}, a training-free framework that derives a \\emph{consensus} ranking by reconciling vision encoder saliency with query-aware cross-attention, retaining the most informative tokens while compressing the remainder via encoder-guided token merging. Across LLaVA-1.5/NeXT, Video-LLaVA, and other open-source VLMs, ConsensusDrop consistently outperforms prior pruning methods under identical token budgets and delivers a stronger accuracy-efficiency Pareto frontier -- preserving near-baseline accuracy even at aggressive token reductions while reducing TTFT and KV cache footprint. Our code will be open-sourced.", "AI": {"tldr": "ConsensusDrop: Training-free framework that fuses vision encoder saliency with LLM cross-attention to reduce redundant visual tokens in VLMs, improving efficiency while maintaining accuracy.", "motivation": "VLMs are expensive due to LLMs processing hundreds of redundant visual tokens. Existing token reduction methods use either vision-encoder saliency (broad but query-agnostic) or LLM cross-attention (query-aware but sparse/costly), but neither alone is sufficient for optimal token selection.", "method": "ConsensusDrop reconciles vision encoder saliency with query-aware cross-attention to derive a consensus ranking, retaining the most informative tokens while compressing the remainder via encoder-guided token merging. The framework is training-free and works across various VLMs.", "result": "Outperforms prior pruning methods under identical token budgets, delivers stronger accuracy-efficiency Pareto frontier, preserves near-baseline accuracy even at aggressive token reductions, and reduces TTFT and KV cache footprint across LLaVA-1.5/NeXT, Video-LLaVA, and other open-source VLMs.", "conclusion": "Fusing vision encoder saliency with LLM cross-attention is crucial for effective visual token reduction in VLMs. ConsensusDrop provides a practical, training-free solution that significantly improves efficiency while maintaining accuracy, with code to be open-sourced."}}
{"id": "2602.00949", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.00949", "abs": "https://arxiv.org/abs/2602.00949", "authors": ["Xiang Zhang", "Boxuan Zhang", "Alireza Naghizadeh", "Mohab Mohamed", "Dongfang Liu", "Ruixiang Tang", "Dimitris Metaxas", "Dongfang Liu"], "title": "Data Augmentation for High-Fidelity Generation of CAR-T/NK Immunological Synapse Images", "comment": null, "summary": "Chimeric antigen receptor (CAR)-T and NK cell immunotherapies have transformed cancer treatment, and recent studies suggest that the quality of the CAR-T/NK cell immunological synapse (IS) may serve as a functional biomarker for predicting therapeutic efficacy. Accurate detection and segmentation of CAR-T/NK IS structures using artificial neural networks (ANNs) can greatly increase the speed and reliability of IS quantification. However, a persistent challenge is the limited size of annotated microscopy datasets, which restricts the ability of ANNs to generalize. To address this challenge, we integrate two complementary data-augmentation frameworks. First, we employ Instance Aware Automatic Augmentation (IAAA), an automated, instance-preserving augmentation method that generates synthetic CAR-T/NK IS images and corresponding segmentation masks by applying optimized augmentation policies to original IS data. IAAA supports multiple imaging modalities (e.g., fluorescence and brightfield) and can be applied directly to CAR-T/NK IS images derived from patient samples. In parallel, we introduce a Semantic-Aware AI Augmentation (SAAA) pipeline that combines a diffusion-based mask generator with a Pix2Pix conditional image synthesizer. This second method enables the creation of diverse, anatomically realistic segmentation masks and produces high-fidelity CAR-T/NK IS images aligned with those masks, further expanding the training corpus beyond what IAAA alone can provide. Together, these augmentation strategies generate synthetic images whose visual and structural properties closely match real IS data, significantly improving CAR-T/NK IS detection and segmentation performance. By enhancing the robustness and accuracy of IS quantification, this work supports the development of more reliable imaging-based biomarkers for predicting patient response to CAR-T/NK immunotherapy.", "AI": {"tldr": "The paper presents two complementary data augmentation frameworks (IAAA and SAAA) to generate synthetic CAR-T/NK immunological synapse images for improving detection and segmentation performance when annotated microscopy datasets are limited.", "motivation": "Limited size of annotated microscopy datasets restricts the ability of artificial neural networks to generalize for CAR-T/NK immunological synapse detection and segmentation, which is important for predicting therapeutic efficacy in cancer immunotherapy.", "method": "Two complementary data augmentation frameworks: 1) Instance Aware Automatic Augmentation (IAAA) - automated, instance-preserving augmentation method that applies optimized policies to original IS data; 2) Semantic-Aware AI Augmentation (SAAA) - combines diffusion-based mask generator with Pix2Pix conditional image synthesizer to create diverse, anatomically realistic segmentation masks and corresponding high-fidelity images.", "result": "The augmentation strategies generate synthetic images with visual and structural properties closely matching real IS data, significantly improving CAR-T/NK IS detection and segmentation performance.", "conclusion": "By enhancing robustness and accuracy of IS quantification, this work supports development of more reliable imaging-based biomarkers for predicting patient response to CAR-T/NK immunotherapy."}}
{"id": "2602.00956", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.00956", "abs": "https://arxiv.org/abs/2602.00956", "authors": ["Faisal Ahmed"], "title": "Hybrid Topological and Deep Feature Fusion for Accurate MRI-Based Alzheimer's Disease Severity Classification", "comment": "20 pages, 6 Figures", "summary": "Early and accurate diagnosis of Alzheimer's disease (AD) remains a critical challenge in neuroimaging-based clinical decision support systems. In this work, we propose a novel hybrid deep learning framework that integrates Topological Data Analysis (TDA) with a DenseNet121 backbone for four-class Alzheimer's disease classification using structural MRI data from the OASIS dataset. TDA is employed to capture complementary topological characteristics of brain structures that are often overlooked by conventional neural networks, while DenseNet121 efficiently learns hierarchical spatial features from MRI slices. The extracted deep and topological features are fused to enhance class separability across the four AD stages.\n  Extensive experiments conducted on the OASIS-1 Kaggle MRI dataset demonstrate that the proposed TDA+DenseNet121 model significantly outperforms existing state-of-the-art approaches. The model achieves an accuracy of 99.93% and an AUC of 100%, surpassing recently published CNN-based, transfer learning, ensemble, and multi-scale architectures. These results confirm the effectiveness of incorporating topological insights into deep learning pipelines and highlight the potential of the proposed framework as a robust and highly accurate tool for automated Alzheimer's disease diagnosis.", "AI": {"tldr": "A hybrid deep learning framework combining Topological Data Analysis (TDA) with DenseNet121 achieves state-of-the-art performance (99.93% accuracy, 100% AUC) for four-class Alzheimer's disease classification using structural MRI data.", "motivation": "Early and accurate diagnosis of Alzheimer's disease remains challenging in neuroimaging-based clinical systems. Conventional neural networks may overlook important topological characteristics of brain structures that could improve classification performance.", "method": "Proposes a hybrid framework integrating Topological Data Analysis (TDA) with DenseNet121 backbone. TDA captures complementary topological features of brain structures, while DenseNet121 learns hierarchical spatial features from MRI slices. The extracted deep and topological features are fused to enhance class separability across four AD stages.", "result": "The TDA+DenseNet121 model achieves 99.93% accuracy and 100% AUC on the OASIS-1 Kaggle MRI dataset, significantly outperforming existing state-of-the-art approaches including CNN-based, transfer learning, ensemble, and multi-scale architectures.", "conclusion": "The results confirm the effectiveness of incorporating topological insights into deep learning pipelines and highlight the potential of the proposed framework as a robust and highly accurate tool for automated Alzheimer's disease diagnosis."}}
{"id": "2602.00971", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.00971", "abs": "https://arxiv.org/abs/2602.00971", "authors": ["Meng Luo", "Bobo Li", "Shanqing Xu", "Shize Zhang", "Qiuchan Chen", "Menglu Han", "Wenhao Chen", "Yanxiang Huang", "Hao Fei", "Mong-Li Lee", "Wynne Hsu"], "title": "Unveiling the Cognitive Compass: Theory-of-Mind-Guided Multimodal Emotion Reasoning", "comment": "Accepted by ICLR 2026", "summary": "Despite rapid progress in multimodal large language models (MLLMs), their capability for deep emotional understanding remains limited. We argue that genuine affective intelligence requires explicit modeling of Theory of Mind (ToM), the cognitive substrate from which emotions arise. To this end, we introduce HitEmotion, a ToM-grounded hierarchical benchmark that diagnoses capability breakpoints across increasing levels of cognitive depth. Second, we propose a ToM-guided reasoning chain that tracks mental states and calibrates cross-modal evidence to achieve faithful emotional reasoning. We further introduce TMPO, a reinforcement learning method that uses intermediate mental states as process-level supervision to guide and strengthen model reasoning. Extensive experiments show that HitEmotion exposes deep emotional reasoning deficits in state-of-the-art models, especially on cognitively demanding tasks. In evaluation, the ToM-guided reasoning chain and TMPO improve end-task accuracy and yield more faithful, more coherent rationales. In conclusion, our work provides the research community with a practical toolkit for evaluating and enhancing the cognition-based emotional understanding capabilities of MLLMs. Our dataset and code are available at: https://HitEmotion.github.io/.", "AI": {"tldr": "This paper introduces HitEmotion, a Theory of Mind (ToM)-grounded benchmark for evaluating emotional understanding in multimodal LLMs, along with ToM-guided reasoning methods to improve affective intelligence.", "motivation": "Current multimodal large language models (MLLMs) lack deep emotional understanding capabilities. The authors argue that genuine affective intelligence requires explicit modeling of Theory of Mind (ToM), which is the cognitive foundation from which emotions arise.", "method": "1) HitEmotion benchmark: A ToM-grounded hierarchical benchmark that diagnoses capability breakpoints across increasing cognitive depth levels. 2) ToM-guided reasoning chain: Tracks mental states and calibrates cross-modal evidence for faithful emotional reasoning. 3) TMPO: A reinforcement learning method using intermediate mental states as process-level supervision to strengthen model reasoning.", "result": "HitEmotion exposes deep emotional reasoning deficits in state-of-the-art models, especially on cognitively demanding tasks. The ToM-guided reasoning chain and TMPO improve end-task accuracy and yield more faithful, coherent rationales.", "conclusion": "The work provides a practical toolkit for evaluating and enhancing cognition-based emotional understanding capabilities of MLLMs, with publicly available dataset and code."}}
{"id": "2602.00982", "categories": ["cs.CV", "cs.AI", "cs.NE", "cs.RO"], "pdf": "https://arxiv.org/pdf/2602.00982", "abs": "https://arxiv.org/abs/2602.00982", "authors": ["Phu-Hoa Pham", "Chi-Nguyen Tran", "Dao Sy Duy Minh", "Nguyen Lam Phu Quy", "Huynh Trung Kiet"], "title": "Navigating Simply, Aligning Deeply: Winning Solutions for Mouse vs. AI 2025", "comment": "15 pages, 8 tables. Technical Report for winning solutions (Track 1 & Track 2) at the NeurIPS 2025 Mouse vs. AI Challenge", "summary": "Visual robustness and neural alignment remain critical challenges in developing artificial agents that can match biological vision systems. We present the winning approaches from Team HCMUS_TheFangs for both tracks of the NeurIPS 2025 Mouse vs. AI: Robust Visual Foraging Competition. For Track 1 (Visual Robustness), we demonstrate that architectural simplicity combined with targeted components yields superior generalization, achieving 95.4% final score with a lightweight two-layer CNN enhanced by Gated Linear Units and observation normalization. For Track 2 (Neural Alignment), we develop a deep ResNet-like architecture with 16 convolutional layers and GLU-based gating that achieves top-1 neural prediction performance with 17.8 million parameters. Our systematic analysis of ten model checkpoints trained between 60K to 1.14M steps reveals that training duration exhibits a non-monotonic relationship with performance, with optimal results achieved around 200K steps. Through comprehensive ablation studies and failure case analysis, we provide insights into why simpler architectures excel at visual robustness while deeper models with increased capacity achieve better neural alignment. Our results challenge conventional assumptions about model complexity in visuomotor learning and offer practical guidance for developing robust, biologically-inspired visual agents.", "AI": {"tldr": "Team HCMUS_TheFangs won NeurIPS 2025 Mouse vs. AI competition with simple CNN for visual robustness (95.4% score) and deep ResNet for neural alignment (top-1 performance). Training duration shows non-monotonic relationship with optimal results at ~200K steps.", "motivation": "Visual robustness and neural alignment remain critical challenges in developing artificial agents that can match biological vision systems. The competition aims to address these challenges in visuomotor learning.", "method": "For Track 1 (Visual Robustness): lightweight two-layer CNN enhanced by Gated Linear Units (GLU) and observation normalization. For Track 2 (Neural Alignment): deep ResNet-like architecture with 16 convolutional layers and GLU-based gating (17.8M parameters). Systematic analysis of ten model checkpoints trained between 60K to 1.14M steps with comprehensive ablation studies and failure case analysis.", "result": "Track 1 achieved 95.4% final score with simple architecture. Track 2 achieved top-1 neural prediction performance. Training duration exhibits non-monotonic relationship with performance, with optimal results achieved around 200K steps. Simpler architectures excel at visual robustness while deeper models with increased capacity achieve better neural alignment.", "conclusion": "Results challenge conventional assumptions about model complexity in visuomotor learning and offer practical guidance for developing robust, biologically-inspired visual agents. Architectural simplicity combined with targeted components yields superior generalization for visual robustness, while deeper models with increased capacity are better for neural alignment."}}
{"id": "2602.00995", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.00995", "abs": "https://arxiv.org/abs/2602.00995", "authors": ["Nick DiSanto", "Ehsan Khodapanah Aghdam", "Han Liu", "Jacob Watson", "Yuankai K. Tao", "Hao Li", "Ipek Oguz"], "title": "VAMOS-OCTA: Vessel-Aware Multi-Axis Orthogonal Supervision for Inpainting Motion-Corrupted OCT Angiography Volumes", "comment": "Accepted to SPIE Medical Imaging 2026", "summary": "Handheld Optical Coherence Tomography Angiography (OCTA) enables noninvasive retinal imaging in uncooperative or pediatric subjects, but is highly susceptible to motion artifacts that severely degrade volumetric image quality. Sudden motion during 3D acquisition can lead to unsampled retinal regions across entire B-scans (cross-sectional slices), resulting in blank bands in en face projections. We propose VAMOS-OCTA, a deep learning framework for inpainting motion-corrupted B-scans using vessel-aware multi-axis supervision. We employ a 2.5D U-Net architecture that takes a stack of neighboring B-scans as input to reconstruct a corrupted center B-scan, guided by a novel Vessel-Aware Multi-Axis Orthogonal Supervision (VAMOS) loss. This loss combines vessel-weighted intensity reconstruction with axial and lateral projection consistency, encouraging vascular continuity in native B-scans and across orthogonal planes. Unlike prior work that focuses primarily on restoring the en face MIP, VAMOS-OCTA jointly enhances both cross-sectional B-scan sharpness and volumetric projection accuracy, even under severe motion corruptions. We trained our model on both synthetic and real-world corrupted volumes and evaluated its performance using both perceptual quality and pixel-wise accuracy metrics. VAMOS-OCTA consistently outperforms prior methods, producing reconstructions with sharp capillaries, restored vessel continuity, and clean en face projections. These results demonstrate that multi-axis supervision offers a powerful constraint for restoring motion-degraded 3D OCTA data. Our source code is available at https://github.com/MedICL-VU/VAMOS-OCTA.", "AI": {"tldr": "VAMOS-OCTA: A deep learning framework using vessel-aware multi-axis supervision to inpaint motion-corrupted B-scans in handheld OCTA imaging, restoring both cross-sectional and volumetric image quality.", "motivation": "Handheld OCTA enables retinal imaging in uncooperative patients but suffers from severe motion artifacts during 3D acquisition, leading to unsampled retinal regions and blank bands in en face projections that degrade diagnostic quality.", "method": "Uses a 2.5D U-Net that takes neighboring B-scans as input to reconstruct corrupted center B-scans, guided by a novel Vessel-Aware Multi-Axis Orthogonal Supervision (VAMOS) loss combining vessel-weighted intensity reconstruction with axial and lateral projection consistency.", "result": "Outperforms prior methods, producing reconstructions with sharp capillaries, restored vessel continuity, and clean en face projections under severe motion corruptions, as validated by both perceptual quality and pixel-wise accuracy metrics.", "conclusion": "Multi-axis supervision provides a powerful constraint for restoring motion-degraded 3D OCTA data, enabling joint enhancement of both cross-sectional B-scan sharpness and volumetric projection accuracy in handheld imaging scenarios."}}
{"id": "2602.01000", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.01000", "abs": "https://arxiv.org/abs/2602.01000", "authors": ["Vagish Kumar", "Souvik Chakraborty"], "title": "CortiNet: A Physics-Perception Hybrid Cortical-Inspired Dual-Stream Network for Gallbladder Disease Diagnosis from Ultrasound", "comment": null, "summary": "Ultrasound imaging is the primary diagnostic modality for detecting Gallbladder diseases due to its non-invasive nature, affordability, and wide accessibility. However, the low resolution and speckle noise inherent to ultrasound images hinder diagnostic reliability, prompting the use of large convolutional neural networks that are difficult to deploy in routine clinical settings. In this work, we propose CortiNet, a lightweight, cortical-inspired dual-stream neural architecture for gallbladder disease diagnosis that integrates physically interpretable multi-scale signal decomposition with perception-driven feature learning. Inspired by parallel processing pathways in the human visual cortex, CortiNet explicitly separates low-frequency structural information from high-frequency perceptual details and processes them through specialized encoding streams. By operating directly on structured, frequency-selective representations rather than raw pixel intensities, the architecture embeds strong physics-based inductive bias, enabling efficient feature learning with a significantly reduced parameter footprint. A late-stage cortical-style fusion mechanism integrates complementary structural and textural cues while preserving computational efficiency. Additionally, we propose a structure-aware explainability framework wherein gradient-weighted class activation mapping is only applied to the structural branch of the proposed CortiNet architecture. This choice allows the model to only focus on the structural features, making it robust against speckle noise. We evaluate CortiNet on 10,692 expert-annotated images spanning nine clinically relevant gallbladder disease categories. Experimental results demonstrate that CortiNet achieves high diagnostic accuracy (98.74%) with only a fraction of the parameters required by conventional deep convolutional models.", "AI": {"tldr": "CortiNet: A lightweight, cortical-inspired dual-stream neural network for gallbladder disease diagnosis from ultrasound images that separates structural and perceptual features for efficient, interpretable analysis.", "motivation": "Ultrasound imaging is primary for gallbladder disease diagnosis but suffers from low resolution and speckle noise. Existing deep learning models are too large and complex for routine clinical deployment, creating a need for lightweight, efficient alternatives.", "method": "CortiNet uses a cortical-inspired dual-stream architecture that separates low-frequency structural information from high-frequency perceptual details through specialized encoding streams. It employs multi-scale signal decomposition, operates on frequency-selective representations rather than raw pixels, and uses late-stage cortical-style fusion. Also includes a structure-aware explainability framework with gradient-weighted class activation mapping applied only to the structural branch.", "result": "Achieved 98.74% diagnostic accuracy on 10,692 expert-annotated images spanning nine gallbladder disease categories, with significantly fewer parameters than conventional deep convolutional models.", "conclusion": "CortiNet provides an efficient, lightweight solution for gallbladder disease diagnosis that combines physics-based inductive bias with perception-driven learning, making it suitable for routine clinical deployment while maintaining high accuracy and interpretability."}}
{"id": "2602.01004", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.01004", "abs": "https://arxiv.org/abs/2602.01004", "authors": ["Zihao Zhao", "Shengting Cao", "Muchao Ye"], "title": "SRVAU-R1: Enhancing Video Anomaly Understanding via Reflection-Aware Learning", "comment": null, "summary": "Multi-modal large language models (MLLMs) have demonstrated significant progress in reasoning capabilities and shown promising effectiveness in video anomaly understanding (VAU) tasks. However, existing MLLM-based approaches remain largely focused on surface-level descriptions of anomalies, lacking deep reasoning over abnormal behaviors like explicit self-reflection and self-correction. To address that, we propose Self-Reflection-Enhanced Reasoning for Video Anomaly Understanding (SRVAU-R1), a reflection-aware learning framework that incorporates reflection in MLLM reasoning. Specifically, SRVAU-R1 introduces the first reflection-oriented Chain-of-Thought dataset tailored for VAU, providing structured supervision with initial reasoning, self-reflection, and revised reasoning. Based on that, it includes a novel reflection-aware learning paradigm with supervised fine-tuning and reinforcement fine-tuning to enhance multi-modal reasoning for VAU. Extensive experiments on multiple video anomaly benchmarks demonstrate that SRVAU-R1 consistently outperforms existing methods, achieving significant improvements in both temporal anomaly localization accuracy and reasoning quality.", "AI": {"tldr": "SRVAU-R1 introduces a reflection-aware learning framework for video anomaly understanding that enhances MLLM reasoning through self-reflection and self-correction mechanisms.", "motivation": "Existing MLLM-based approaches for video anomaly understanding focus on surface-level descriptions but lack deep reasoning capabilities like explicit self-reflection and self-correction, limiting their effectiveness in understanding abnormal behaviors.", "method": "Proposes SRVAU-R1 framework with: 1) First reflection-oriented Chain-of-Thought dataset for VAU with structured supervision (initial reasoning, self-reflection, revised reasoning), 2) Reflection-aware learning paradigm combining supervised fine-tuning and reinforcement fine-tuning to enhance multi-modal reasoning.", "result": "Extensive experiments on multiple video anomaly benchmarks show SRVAU-R1 consistently outperforms existing methods with significant improvements in both temporal anomaly localization accuracy and reasoning quality.", "conclusion": "The proposed reflection-aware learning framework effectively enhances MLLM reasoning for video anomaly understanding by incorporating self-reflection mechanisms, leading to better performance in both localization and reasoning tasks."}}
{"id": "2602.01012", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.01012", "abs": "https://arxiv.org/abs/2602.01012", "authors": ["Yiyang Su", "Minchul Kim", "Jie Zhu", "Christopher Perry", "Feng Liu", "Anil Jain", "Xiaoming Liu"], "title": "LocalScore: Local Density-Aware Similarity Scoring for Biometrics", "comment": null, "summary": "Open-set biometrics faces challenges with probe subjects who may not be enrolled in the gallery, as traditional biometric systems struggle to detect these non-mated probes. Despite the growing prevalence of multi-sample galleries in real-world deployments, most existing methods collapse intra-subject variability into a single global representation, leading to suboptimal decision boundaries and poor open-set robustness. To address this issue, we propose LocalScore, a simple yet effective scoring algorithm that explicitly incorporates the local density of the gallery feature distribution using the k-th nearest neighbors. LocalScore is architecture-agnostic, loss-independent, and incurs negligible computational overhead, making it a plug-and-play solution for existing biometric systems. Extensive experiments across multiple modalities demonstrate that LocalScore consistently achieves substantial gains in open-set retrieval (FNIR@FPIR reduced from 53% to 40%) and verification (TAR@FAR improved from 51% to 74%). We further provide theoretical analysis and empirical validation explaining when and why the method achieves the most significant gains based on dataset characteristics.", "AI": {"tldr": "LocalScore improves open-set biometrics by using k-nearest neighbors to incorporate gallery feature distribution density, achieving significant performance gains with minimal computational overhead.", "motivation": "Traditional biometric systems struggle with open-set scenarios where probe subjects may not be enrolled in the gallery. Most existing methods collapse intra-subject variability into single global representations, leading to poor open-set robustness, especially problematic with multi-sample galleries common in real-world deployments.", "method": "Proposes LocalScore, a scoring algorithm that explicitly incorporates the local density of gallery feature distribution using k-th nearest neighbors. The method is architecture-agnostic, loss-independent, and adds negligible computational overhead, making it a plug-and-play solution for existing biometric systems.", "result": "Extensive experiments across multiple modalities show LocalScore consistently achieves substantial gains: open-set retrieval FNIR@FPIR reduced from 53% to 40%, and verification TAR@FAR improved from 51% to 74%. The paper includes theoretical analysis explaining when and why the method achieves most significant gains based on dataset characteristics.", "conclusion": "LocalScore effectively addresses open-set biometric challenges by leveraging local density information from gallery feature distributions, providing a simple yet powerful solution that significantly improves open-set robustness across various biometric modalities with minimal implementation cost."}}
{"id": "2602.01020", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.01020", "abs": "https://arxiv.org/abs/2602.01020", "authors": ["Jichen Yang", "Jikai Zhang", "Benjamin Wildman-Tobriner", "Maciej A. Mazurowski"], "title": "Effectiveness of Automatically Curated Dataset in Thyroid Nodules Classification Algorithms Using Deep Learning", "comment": "9 pages, 3 figures", "summary": "The diagnosis of thyroid nodule cancers commonly utilizes ultrasound images. Several studies showed that deep learning algorithms designed to classify benign and malignant thyroid nodules could match radiologists' performance. However, data availability for training deep learning models is often limited due to the significant effort required to curate such datasets. The previous study proposed a method to curate thyroid nodule datasets automatically. It was tested to have a 63% yield rate and 83% accuracy. However, the usefulness of the generated data for training deep learning models remains unknown. In this study, we conducted experiments to determine whether using a automatically-curated dataset improves deep learning algorithms' performance. We trained deep learning models on the manually annotated and automatically-curated datasets. We also trained with a smaller subset of the automatically-curated dataset that has higher accuracy to explore the optimum usage of such dataset. As a result, the deep learning model trained on the manually selected dataset has an AUC of 0.643 (95% confidence interval [CI]: 0.62, 0.66). It is significantly lower than the AUC of the 6automatically-curated dataset trained deep learning model, 0.694 (95% confidence interval [CI]: 0.67, 0.73, P < .001). The AUC of the accurate subset trained deep learning model is 0.689 (95% confidence interval [CI]: 0.66, 0.72, P > .43), which is insignificantly worse than the AUC of the full automatically-curated dataset. In conclusion, we showed that using a automatically-curated dataset can substantially increase the performance of deep learning algorithms, and it is suggested to use all the data rather than only using the accurate subset.", "AI": {"tldr": "Automatically-curated thyroid nodule datasets improve deep learning model performance compared to manually annotated datasets, with full automatically-curated data performing better than accurate subsets.", "motivation": "While deep learning can match radiologists in thyroid nodule classification, limited training data availability hinders model development. Previous automatic curation methods showed promise but their usefulness for training deep learning models was unknown.", "method": "Trained deep learning models on three datasets: manually annotated dataset, full automatically-curated dataset, and a smaller high-accuracy subset of the automatically-curated dataset. Compared model performance using AUC metrics with statistical significance testing.", "result": "Models trained on automatically-curated datasets outperformed manually annotated ones (AUC 0.694 vs 0.643, P<.001). The full automatically-curated dataset performed insignificantly better than the high-accuracy subset (AUC 0.694 vs 0.689, P>.43).", "conclusion": "Automatically-curated datasets substantially improve deep learning algorithm performance for thyroid nodule classification. Using the full automatically-curated dataset is recommended over selecting only high-accuracy subsets."}}
{"id": "2602.01033", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.01033", "abs": "https://arxiv.org/abs/2602.01033", "authors": ["Chentian Sun"], "title": "GMAC: Global Multi-View Constraint for Automatic Multi-Camera Extrinsic Calibration", "comment": "A 5-page paper with 1 figure, prepared for submission to the 2026 IEEE International Conference on Image Processing (ICIP)", "summary": "Automatic calibration of multi-camera systems, namely the accurate estimation of spatial extrinsic parameters, is fundamental for 3D reconstruction, panoramic perception, and multi-view data fusion. Existing methods typically rely on calibration targets, explicit geometric modeling, or task-specific neural networks. Such approaches often exhibit limited robustness and applicability in complex dynamic environments or online scenarios, making them difficult to deploy in practical applications. To address this, this paper proposes GMAC, a multi-camera extrinsic estimation framework based on the implicit geometric representations learned by multi-view reconstruction networks. GMAC models extrinsics as global variables constrained by the latent multi-view geometric structure and prunes and structurally reconfigures existing networks so that their latent features can directly support extrinsic prediction through a lightweight regression head, without requiring a completely new network design. Furthermore, GMAC jointly optimizes cross-view reprojection consistency and multi-view cycle consistency, ensuring geometric coherence across cameras while improving prediction accuracy and optimization stability. Experiments on both synthetic and real-world multi-camera datasets demonstrate that GMAC achieves accurate and stable extrinsic estimation without explicit 3D reconstruction or manual calibration, providing a new solution for efficient deployment and online calibration of multi-camera systems.", "AI": {"tldr": "GMAC is a novel framework for automatic multi-camera extrinsic calibration using implicit geometric representations from multi-view reconstruction networks, achieving accurate calibration without explicit 3D reconstruction or manual intervention.", "motivation": "Existing multi-camera calibration methods rely on calibration targets, explicit geometric modeling, or task-specific networks, which lack robustness and applicability in complex dynamic environments and online scenarios, making practical deployment difficult.", "method": "GMAC models extrinsics as global variables constrained by latent multi-view geometric structure, prunes and reconfigures existing multi-view reconstruction networks to support extrinsic prediction through lightweight regression heads, and jointly optimizes cross-view reprojection consistency and multi-view cycle consistency.", "result": "Experiments on synthetic and real-world multi-camera datasets show GMAC achieves accurate and stable extrinsic estimation without explicit 3D reconstruction or manual calibration, providing efficient deployment and online calibration capabilities.", "conclusion": "GMAC offers a new solution for automatic multi-camera extrinsic calibration that is robust, applicable in dynamic environments, and suitable for online deployment, overcoming limitations of traditional calibration methods."}}
{"id": "2602.01035", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.01035", "abs": "https://arxiv.org/abs/2602.01035", "authors": ["Chentian Sun"], "title": "FUSE-Flow: Scalable Real-Time Multi-View Point Cloud Reconstruction Using Confidence", "comment": "A 5-page paper, prepared for submission to the 2026 IEEE International Conference on Image Processing (ICIP)", "summary": "Real-time multi-view point cloud reconstruction is a core problem in 3D vision and immersive perception, with wide applications in VR, AR, robotic navigation, digital twins, and computer interaction. Despite advances in multi-camera systems and high-resolution depth sensors, fusing large-scale multi-view depth observations into high-quality point clouds under strict real-time constraints remains challenging. Existing methods relying on voxel-based fusion, temporal accumulation, or global optimization suffer from high computational complexity, excessive memory usage, and limited scalability, failing to simultaneously achieve real-time performance, reconstruction quality, and multi-camera extensibility. We propose FUSE-Flow, a frame-wise, stateless, and linearly scalable point cloud streaming reconstruction framework. Each frame independently generates point cloud fragments, fused via two weights, measurement confidence and 3D distance consistency to suppress noise while preserving geometric details. For large-scale multi-camera efficiency, we introduce an adaptive spatial hashing-based weighted aggregation method: 3D space is adaptively partitioned by local point cloud density, representative points are selected per cell, and weighted fusion is performed to handle both sparse and dense regions. With GPU parallelization, FUSE-Flow achieves high-throughput, low-latency point cloud generation and fusion with linear complexity. Experiments demonstrate that the framework improves reconstruction stability and geometric fidelity in overlapping, depth-discontinuous, and dynamic scenes, while maintaining real-time frame rates on modern GPUs, verifying its effectiveness, robustness, and scalability.", "AI": {"tldr": "FUSE-Flow is a real-time multi-view point cloud reconstruction framework that uses frame-wise, stateless processing with adaptive spatial hashing and weighted fusion to achieve linear scalability while maintaining high quality.", "motivation": "Real-time multi-view point cloud reconstruction is crucial for VR/AR, robotics, digital twins, and human-computer interaction, but existing methods suffer from high computational complexity, excessive memory usage, and limited scalability, failing to simultaneously achieve real-time performance, reconstruction quality, and multi-camera extensibility.", "method": "Frame-wise, stateless framework where each frame independently generates point cloud fragments, fused via two weights (measurement confidence and 3D distance consistency). Uses adaptive spatial hashing-based weighted aggregation: 3D space is adaptively partitioned by local point cloud density, representative points are selected per cell, and weighted fusion handles both sparse and dense regions. GPU parallelization enables high-throughput, low-latency processing with linear complexity.", "result": "The framework improves reconstruction stability and geometric fidelity in overlapping, depth-discontinuous, and dynamic scenes while maintaining real-time frame rates on modern GPUs. Demonstrates effectiveness, robustness, and scalability.", "conclusion": "FUSE-Flow successfully addresses the challenges of real-time multi-view point cloud reconstruction by providing a linearly scalable, high-quality solution that overcomes limitations of existing methods in computational complexity, memory usage, and scalability."}}
{"id": "2602.01037", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.01037", "abs": "https://arxiv.org/abs/2602.01037", "authors": ["Guangshuo Qin", "Zhiteng Li", "Zheng Chen", "Weihang Zhang", "Linghe Kong", "Yulun Zhang"], "title": "VEQ: Modality-Adaptive Quantization for MoE Vision-Language Models", "comment": null, "summary": "Mixture-of-Experts(MoE) Vision-Language Models (VLMs) offer remarkable performance but incur prohibitive memory and computational costs, making compression essential. Post-Training Quantization (PTQ) is an effective training-free technique to address the massive memory and computation overhead. Existing quantization paradigms fall short as they are oblivious to two critical forms of heterogeneity: the inherent discrepancy between vision and language tokens, and the non-uniform contribution of different experts. To bridge this gap, we propose Visual Expert Quantization (VEQ), a dual-aware quantization framework designed to simultaneously accommodate cross-modal differences and heterogeneity between experts. Specifically, VEQ incorporates 1)Modality-expert-aware Quantization, which utilizes expert activation frequency to prioritize error minimization for pivotal experts, and 2)Modality-affinity-aware Quantization, which constructs an enhanced Hessian matrix by integrating token-expert affinity with modality information to guide the calibration process. Extensive experiments across diverse benchmarks verify that VEQ consistently outperforms state-of-the-art baselines. Specifically, under the W3A16 configuration, our method achieves significant average accuracy gains of 2.04\\% on Kimi-VL and 3.09\\% on Qwen3-VL compared to the previous SOTA quantization methods, demonstrating superior robustness across various multimodal tasks. Our code will be available at https://github.com/guangshuoqin/VEQ.", "AI": {"tldr": "VEQ is a dual-aware quantization framework for MoE VLMs that addresses cross-modal differences and expert heterogeneity to reduce memory/computation costs while maintaining performance.", "motivation": "MoE VLMs offer great performance but have prohibitive memory/computational costs. Existing quantization methods fail to address two critical heterogeneities: differences between vision/language tokens and non-uniform contributions of different experts.", "method": "VEQ incorporates: 1) Modality-expert-aware Quantization using expert activation frequency to prioritize error minimization for pivotal experts, and 2) Modality-affinity-aware Quantization that integrates token-expert affinity with modality information into an enhanced Hessian matrix to guide calibration.", "result": "VEQ outperforms SOTA baselines across diverse benchmarks. Under W3A16 configuration, achieves average accuracy gains of 2.04% on Kimi-VL and 3.09% on Qwen3-VL compared to previous SOTA quantization methods.", "conclusion": "VEQ effectively addresses the dual heterogeneities in MoE VLMs through modality-expert-aware and modality-affinity-aware quantization, demonstrating superior robustness across various multimodal tasks while reducing computational overhead."}}
{"id": "2602.01038", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.01038", "abs": "https://arxiv.org/abs/2602.01038", "authors": ["Lavisha Aggarwal", "Vikas Bahirwani", "Andrea Colaco"], "title": "From Videos to Conversations: Egocentric Instructions for Task Assistance", "comment": null, "summary": "Many everyday tasks, ranging from appliance repair and cooking to car maintenance, require expert knowledge, particularly for complex, multi-step procedures. Despite growing interest in AI agents for augmented reality (AR) assistance, progress remains limited by the scarcity of large-scale multimodal conversational datasets grounded in real-world task execution, in part due to the cost and logistical complexity of human-assisted data collection. In this paper, we present a framework to automatically transform single person instructional videos into two-person multimodal task-guidance conversations. Our fully automatic pipeline, based on large language models, provides a scalable and cost efficient alternative to traditional data collection approaches. Using this framework, we introduce HowToDIV, a multimodal dataset comprising 507 conversations, 6,636 question answer pairs, and 24 hours of video spanning multiple domains. Each session consists of a multi-turn expert-novice interaction. Finally, we report baseline results using Gemma 3 and Qwen 2.5 on HowToDIV, providing an initial benchmark for multimodal procedural task assistance.", "AI": {"tldr": "Automatic framework converts single-person instructional videos into two-person multimodal conversations for scalable dataset creation in AR task assistance.", "motivation": "AI agents for AR assistance need large-scale multimodal conversational datasets grounded in real-world tasks, but current data collection is costly and logistically complex due to human involvement.", "method": "Developed fully automatic pipeline using large language models to transform single-person instructional videos into two-person multimodal task-guidance conversations, creating expert-novice interactions.", "result": "Created HowToDIV dataset with 507 conversations, 6,636 QA pairs, and 24 hours of video across multiple domains, plus baseline results using Gemma 3 and Qwen 2.5 models.", "conclusion": "The framework provides scalable, cost-efficient alternative to traditional data collection, enabling progress in multimodal procedural task assistance with established benchmarks."}}
{"id": "2602.01046", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.01046", "abs": "https://arxiv.org/abs/2602.01046", "authors": ["Jiawei Lin", "Shizhao Sun", "Danqing Huang", "Ting Liu", "Ji Li", "Jiang Bian"], "title": "ReLayout: Versatile and Structure-Preserving Design Layout Editing via Relation-Aware Design Reconstruction", "comment": null, "summary": "Automated redesign without manual adjustments marks a key step forward in the design workflow. In this work, we focus on a foundational redesign task termed design layout editing, which seeks to autonomously modify the geometric composition of a design based on user intents. To overcome the ambiguity of user needs expressed in natural language, we introduce four basic and important editing actions and standardize the format of editing operations. The underexplored task presents a unique challenge: satisfying specified editing operations while simultaneously preserving the layout structure of unedited elements. Besides, the scarcity of triplet (original design, editing operation, edited design) samples poses another formidable challenge. To this end, we present ReLayout, a novel framework for versatile and structure-preserving design layout editing that operates without triplet data. Specifically, ReLayout first introduces the relation graph, which contains the position and size relationships among unedited elements, as the constraint for layout structure preservation. Then, relation-aware design reconstruction (RADR) is proposed to bypass the data challenge. By learning to reconstruct a design from its elements, a relation graph, and a synthesized editing operation, RADR effectively emulates the editing process in a self-supervised manner. A multi-modal large language model serves as the backbone for RADR, unifying multiple editing actions within a single model and thus achieving versatile editing after fine-tuning. Qualitative, quantitative results and user studies show that ReLayout significantly outperforms the baseline models in terms of editing quality, accuracy, and layout structure preservation.", "AI": {"tldr": "ReLayout: A novel framework for automated design layout editing that preserves layout structure without requiring triplet training data, using relation graphs and self-supervised learning.", "motivation": "Automated redesign without manual adjustments is crucial for design workflows. Current challenges include ambiguity in user needs expressed in natural language, difficulty preserving layout structure of unedited elements, and scarcity of training data (original design, editing operation, edited design triplets).", "method": "Introduces four basic editing actions and standardized operation format. Uses relation graphs to capture position/size relationships among unedited elements as constraints. Proposes Relation-Aware Design Reconstruction (RADR) - a self-supervised approach that learns to reconstruct designs from elements, relation graphs, and synthesized editing operations. Uses multi-modal large language model as backbone to unify multiple editing actions in a single model.", "result": "ReLayout significantly outperforms baseline models in editing quality, accuracy, and layout structure preservation according to qualitative, quantitative results and user studies.", "conclusion": "ReLayout provides a versatile and structure-preserving solution for design layout editing that operates without triplet data, representing a key advancement in automated redesign workflows."}}
{"id": "2602.01047", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.01047", "abs": "https://arxiv.org/abs/2602.01047", "authors": ["Xinrong Chen", "Xu Chu", "Yingmin Qiu", "Hengyuan Zhang", "Jing Xiong", "Shiyu Tang", "Shuai Liu", "Shaokang Yang", "Cheng Yang", "Hayden Kwok-Hay So", "Ngai Wong"], "title": "Residual Decoding: Mitigating Hallucinations in Large Vision-Language Models via History-Aware Residual Guidance", "comment": null, "summary": "Large Vision-Language Models (LVLMs) can reason effectively from image-text inputs and perform well in various multimodal tasks. Despite this success, they are affected by language priors and often produce hallucinations. Hallucinations denote generated content that is grammatically and syntactically coherent, yet bears no match or direct relevance to actual visual input. To address this problem, we propose Residual Decoding (ResDec). It is a novel training-free method that uses historical information to aid decoding. The method relies on the internal implicit reasoning mechanism and token logits evolution mechanism of LVLMs to correct biases. Extensive experiments demonstrate that ResDec effectively suppresses hallucinations induced by language priors, significantly improves visual grounding, and reduces object hallucinations. In addition to mitigating hallucinations, ResDec also performs exceptionally well on comprehensive LVLM benchmarks, highlighting its broad applicability.", "AI": {"tldr": "ResDec is a training-free method that uses historical decoding information to suppress hallucinations in Large Vision-Language Models by correcting biases through internal reasoning mechanisms.", "motivation": "Large Vision-Language Models suffer from language priors and hallucinations - generating content that appears coherent but doesn't match visual input, despite their strong multimodal reasoning capabilities.", "method": "Residual Decoding (ResDec) leverages historical information during decoding, utilizing the model's internal implicit reasoning mechanism and token logits evolution to correct biases without requiring additional training.", "result": "ResDec effectively suppresses hallucinations from language priors, improves visual grounding, reduces object hallucinations, and performs well on comprehensive LVLM benchmarks.", "conclusion": "ResDec is a broadly applicable, training-free solution that addresses hallucination issues in LVLMs while maintaining strong performance across multimodal tasks."}}
{"id": "2602.01055", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.01055", "abs": "https://arxiv.org/abs/2602.01055", "authors": ["Bo Deng", "Yitong Tang", "Jiake Li", "Yuxin Huang", "Li Wang", "Yu Zhang", "Yufei Zhan", "Hua Lu", "Xiaoshen Zhang", "Jieyun Bai"], "title": "Baseline Method of the Foundation Model Challenge for Ultrasound Image Analysis", "comment": null, "summary": "Ultrasound (US) imaging exhibits substantial heterogeneity across anatomical structures and acquisition protocols, posing significant challenges to the development of generalizable analysis models. Most existing methods are task-specific, limiting their suitability as clinically deployable foundation models. To address this limitation, the Foundation Model Challenge for Ultrasound Image Analysis (FM\\_UIA~2026) introduces a large-scale multi-task benchmark comprising 27 subtasks across segmentation, classification, detection, and regression. In this paper, we present the official baseline for FM\\_UIA~2026 based on a unified Multi-Head Multi-Task Learning (MH-MTL) framework that supports all tasks within a single shared network. The model employs an ImageNet-pretrained EfficientNet--B4 backbone for robust feature extraction, combined with a Feature Pyramid Network (FPN) to capture multi-scale contextual information. A task-specific routing strategy enables global tasks to leverage high-level semantic features, while dense prediction tasks exploit spatially detailed FPN representations. Training incorporates a composite loss with task-adaptive learning rate scaling and a cosine annealing schedule. Validation results demonstrate the feasibility and robustness of this unified design, establishing a strong and extensible baseline for ultrasound foundation model research. The code and dataset are publicly available at \\href{https://github.com/lijiake2408/Foundation-Model-Challenge-for-Ultrasound-Image-Analysis}{GitHub}.", "AI": {"tldr": "The paper presents a unified multi-task learning baseline for ultrasound foundation model research, using a shared network architecture to handle 27 diverse ultrasound analysis tasks across segmentation, classification, detection, and regression.", "motivation": "Ultrasound imaging has substantial heterogeneity across anatomical structures and acquisition protocols, making it challenging to develop generalizable analysis models. Most existing methods are task-specific, limiting their suitability as clinically deployable foundation models.", "method": "A unified Multi-Head Multi-Task Learning (MH-MTL) framework with ImageNet-pretrained EfficientNet-B4 backbone, Feature Pyramid Network for multi-scale features, and task-specific routing strategy. Global tasks use high-level semantic features while dense prediction tasks use spatially detailed FPN representations. Training uses composite loss with task-adaptive learning rate scaling and cosine annealing.", "result": "Validation results demonstrate the feasibility and robustness of the unified design, establishing a strong and extensible baseline for ultrasound foundation model research.", "conclusion": "The proposed framework provides an effective baseline for the FM_UIA 2026 challenge, supporting all 27 ultrasound analysis tasks within a single shared network and enabling foundation model development for heterogeneous ultrasound imaging."}}
{"id": "2602.01057", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.01057", "abs": "https://arxiv.org/abs/2602.01057", "authors": ["Ling Chen", "Bao Yang"], "title": "Radioactive 3D Gaussian Ray Tracing for Tomographic Reconstruction", "comment": null, "summary": "3D Gaussian Splatting (3DGS) has recently emerged in computer vision as a promising rendering technique. By adapting the principles of Elliptical Weighted Average (EWA) splatting to a modern differentiable pipeline, 3DGS enables real-time, high-quality novel view synthesis. Building upon this, R2-Gaussian extended the 3DGS paradigm to tomographic reconstruction by rectifying integration bias, achieving state-of-the-art performance in computed tomography (CT). To enable differentiability, R2-Gaussian adopts a local affine approximation: each 3D Gaussian is locally mapped to a 2D Gaussian on the detector and composed via alpha blending to form projections. However, the affine approximation can degrade reconstruction quantitative accuracy and complicate the incorporation of nonlinear geometric corrections. To address these limitations, we propose a tomographic reconstruction framework based on 3D Gaussian ray tracing. Our approach provides two key advantages over splatting-based models: (i) it computes the line integral through 3D Gaussian primitives analytically, avoiding the local affine collapse and thus yielding a more physically consistent forward projection model; and (ii) the ray-tracing formulation gives explicit control over ray origins and directions, which facilitates the precise application of nonlinear geometric corrections, e.g., arc-correction used in positron emission tomography (PET). These properties extend the applicability of Gaussian-based reconstruction to a wider range of realistic tomography systems while improving projection accuracy.", "AI": {"tldr": "The paper proposes a tomographic reconstruction framework using 3D Gaussian ray tracing instead of splatting, offering more physically consistent projections and better handling of nonlinear geometric corrections.", "motivation": "Existing 3DGS-based tomographic reconstruction (R2-Gaussian) uses local affine approximations that degrade quantitative accuracy and complicate nonlinear geometric corrections. The authors aim to overcome these limitations.", "method": "Proposes a tomographic reconstruction framework based on 3D Gaussian ray tracing that computes line integrals through 3D Gaussian primitives analytically, avoiding local affine collapse and enabling precise control over ray origins/directions.", "result": "The ray-tracing approach provides more physically consistent forward projection models and facilitates precise application of nonlinear geometric corrections (like arc-correction in PET), extending Gaussian-based reconstruction to more realistic tomography systems.", "conclusion": "The proposed 3D Gaussian ray tracing framework improves projection accuracy over splatting-based models and enables better handling of complex geometric corrections, broadening the applicability of Gaussian-based reconstruction in tomography."}}
{"id": "2602.01059", "categories": ["cs.CV", "cs.MM"], "pdf": "https://arxiv.org/pdf/2602.01059", "abs": "https://arxiv.org/abs/2602.01059", "authors": ["Ying Shu", "Pujian Zhan", "Huiqi Yang", "Hehe Fan", "Youfang Lin", "Kai Lv"], "title": "DRFormer: A Dual-Regularized Bidirectional Transformer for Person Re-identification", "comment": null, "summary": "Both fine-grained discriminative details and global semantic features can contribute to solving person re-identification challenges, such as occlusion and pose variations. Vision foundation models (\\textit{e.g.}, DINO) excel at mining local textures, and vision-language models (\\textit{e.g.}, CLIP) capture strong global semantic difference. Existing methods predominantly rely on a single paradigm, neglecting the potential benefits of their integration. In this paper, we analyze the complementary roles of these two architectures and propose a framework to synergize their strengths by a \\textbf{D}ual-\\textbf{R}egularized Bidirectional \\textbf{Transformer} (\\textbf{DRFormer}). The dual-regularization mechanism ensures diverse feature extraction and achieves a better balance in the contributions of the two models. Extensive experiments on five benchmarks show that our method effectively harmonizes local and global representations, achieving competitive performance against state-of-the-art methods.", "AI": {"tldr": "DRFormer is a dual-regularized bidirectional transformer framework that synergizes DINO's local texture mining with CLIP's global semantic understanding for better person re-identification under occlusion and pose variations.", "motivation": "Current person re-identification methods rely on single paradigms (either local discriminative features or global semantics), missing the complementary benefits of combining both. Vision foundation models like DINO excel at local textures while vision-language models like CLIP capture global semantic differences.", "method": "Proposes DRFormer (Dual-Regularized Bidirectional Transformer) that integrates DINO and CLIP through a dual-regularization mechanism to ensure diverse feature extraction and balance contributions from both models.", "result": "Extensive experiments on five benchmarks show the method effectively harmonizes local and global representations, achieving competitive performance against state-of-the-art methods.", "conclusion": "The proposed framework successfully synergizes the complementary strengths of vision foundation models (DINO) and vision-language models (CLIP) through dual regularization, improving person re-identification performance by balancing local texture details with global semantic understanding."}}
{"id": "2602.01069", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.01069", "abs": "https://arxiv.org/abs/2602.01069", "authors": ["Seema K. Poudel", "Sunny K. Khadka"], "title": "PDE-Constrained Optimization for Neural Image Segmentation with Physics Priors", "comment": null, "summary": "Segmentation of microscopy images constitutes an ill-posed inverse problem due to measurement noise, weak object boundaries, and limited labeled data. Although deep neural networks provide flexible nonparametric estimators, unconstrained empirical risk minimization often leads to unstable solutions and poor generalization. In this work, image segmentation is formulated as a PDE-constrained optimization problem that integrates physically motivated priors into deep learning models through variational regularization. The proposed framework minimizes a composite objective function consisting of a data fidelity term and penalty terms derived from reaction-diffusion equations and phase-field interface energies, all implemented as differentiable residual losses. Experiments are conducted on the LIVECell dataset, a high-quality, manually annotated collection of phase-contrast microscopy images. Training is performed on two cell types, while evaluation is carried out on a distinct, unseen cell type to assess generalization. A UNet architecture is used as the unconstrained baseline model. Experimental results demonstrate consistent improvements in segmentation accuracy and boundary fidelity compared to unconstrained deep learning baselines. Moreover, the PDE-regularized models exhibit enhanced stability and improved generalization in low-sample regimes, highlighting the advantages of incorporating structured priors. The proposed approach illustrates how PDE-constrained optimization can strengthen data-driven learning frameworks, providing a principled bridge between variational methods, statistical learning, and scientific machine learning.", "AI": {"tldr": "PDE-constrained optimization framework for microscopy image segmentation that integrates physical priors through variational regularization to improve stability and generalization.", "motivation": "Microscopy image segmentation is ill-posed due to noise, weak boundaries, and limited labeled data. Unconstrained deep learning often leads to unstable solutions and poor generalization, especially with limited data.", "method": "Formulates segmentation as PDE-constrained optimization with composite objective: data fidelity term + penalty terms from reaction-diffusion equations and phase-field interface energies. Uses differentiable residual losses and evaluates on LIVECell dataset with UNet baseline.", "result": "Consistent improvements in segmentation accuracy and boundary fidelity compared to unconstrained baselines. Enhanced stability and better generalization in low-sample regimes, especially when evaluated on unseen cell types.", "conclusion": "PDE-constrained optimization provides principled bridge between variational methods and deep learning, improving segmentation by incorporating structured physical priors into data-driven frameworks."}}
{"id": "2602.01077", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.01077", "abs": "https://arxiv.org/abs/2602.01077", "authors": ["Haopeng Li", "Shitong Shao", "Wenliang Zhong", "Zikai Zhou", "Lichen Bai", "Hui Xiong", "Zeke Xie"], "title": "PISA: Piecewise Sparse Attention Is Wiser for Efficient Diffusion Transformers", "comment": "17 pages", "summary": "Diffusion Transformers are fundamental for video and image generation, but their efficiency is bottlenecked by the quadratic complexity of attention. While block sparse attention accelerates computation by attending only critical key-value blocks, it suffers from degradation at high sparsity by discarding context. In this work, we discover that attention scores of non-critical blocks exhibit distributional stability, allowing them to be approximated accurately and efficiently rather than discarded, which is essentially important for sparse attention design. Motivated by this key insight, we propose PISA, a training-free Piecewise Sparse Attention that covers the full attention span with sub-quadratic complexity. Unlike the conventional keep-or-drop paradigm that directly drop the non-critical block information, PISA introduces a novel exact-or-approximate strategy: it maintains exact computation for critical blocks while efficiently approximating the remainder through block-wise Taylor expansion. This design allows PISA to serve as a faithful proxy to full attention, effectively bridging the gap between speed and quality. Experimental results demonstrate that PISA achieves 1.91 times and 2.57 times speedups on Wan2.1-14B and Hunyuan-Video, respectively, while consistently maintaining the highest quality among sparse attention methods. Notably, even for image generation on FLUX, PISA achieves a 1.2 times acceleration without compromising visual quality. Code is available at: https://github.com/xie-lab-ml/piecewise-sparse-attention.", "AI": {"tldr": "PISA introduces a training-free piecewise sparse attention method that approximates non-critical attention blocks via Taylor expansion instead of discarding them, achieving significant speedups while maintaining quality.", "motivation": "Current block sparse attention methods suffer from quality degradation at high sparsity levels because they discard non-critical context. The authors discovered that attention scores of non-critical blocks have distributional stability, making them suitable for approximation rather than complete removal.", "method": "PISA uses a novel exact-or-approximate strategy: critical blocks are computed exactly, while non-critical blocks are efficiently approximated using block-wise Taylor expansion. This maintains the full attention span with sub-quadratic complexity without requiring training.", "result": "PISA achieves 1.91\u00d7 speedup on Wan2.1-14B and 2.57\u00d7 speedup on Hunyuan-Video while maintaining the highest quality among sparse attention methods. For image generation on FLUX, it achieves 1.2\u00d7 acceleration without compromising visual quality.", "conclusion": "PISA effectively bridges the gap between speed and quality in diffusion transformers by approximating rather than discarding non-critical attention blocks, offering a practical solution for efficient video and image generation."}}
{"id": "2602.01081", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.01081", "abs": "https://arxiv.org/abs/2602.01081", "authors": ["Haitao Zhang", "Yingying Wang", "Jiaxiang Wang", "Haote Xu", "Hongyang Zhang", "Yirong Chen", "Yue Huang", "Xinghao Ding"], "title": "MedAD-R1: Eliciting Consistent Reasoning in Interpretible Medical Anomaly Detection via Consistency-Reinforced Policy Optimization", "comment": "9 pages, 4 figures", "summary": "Medical Anomaly Detection (MedAD) presents a significant opportunity to enhance diagnostic accuracy using Large Multimodal Models (LMMs) to interpret and answer questions based on medical images. However, the reliance on Supervised Fine-Tuning (SFT) on simplistic and fragmented datasets has hindered the development of models capable of plausible reasoning and robust multimodal generalization. To overcome this, we introduce MedAD-38K, the first large-scale, multi-modal, and multi-center benchmark for MedAD featuring diagnostic Chain-of-Thought (CoT) annotations alongside structured Visual Question-Answering (VQA) pairs. On this foundation, we propose a two-stage training framework. The first stage, Cognitive Injection, uses SFT to instill foundational medical knowledge and align the model with a structured think-then-answer paradigm. Given that standard policy optimization can produce reasoning that is disconnected from the final answer, the second stage incorporates Consistency Group Relative Policy Optimization (Con-GRPO). This novel algorithm incorporates a crucial consistency reward to ensure the generated reasoning process is relevant and logically coherent with the final diagnosis. Our proposed model, MedAD-R1, achieves state-of-the-art (SOTA) performance on the MedAD-38K benchmark, outperforming strong baselines by more than 10\\%. This superior performance stems from its ability to generate transparent and logically consistent reasoning pathways, offering a promising approach to enhancing the trustworthiness and interpretability of AI for clinical decision support.", "AI": {"tldr": "MedAD-R1 model achieves SOTA on MedAD-38K benchmark with 10%+ improvement using two-stage training: Cognitive Injection SFT + Con-GRPO for consistent reasoning.", "motivation": "Current MedAD models rely on simplistic SFT datasets, lacking plausible reasoning and robust multimodal generalization needed for trustworthy clinical AI.", "method": "Two-stage framework: 1) Cognitive Injection via SFT for medical knowledge and think-then-answer alignment; 2) Con-GRPO with consistency reward to ensure reasoning coherence with diagnosis.", "result": "MedAD-R1 achieves state-of-the-art performance on MedAD-38K benchmark, outperforming strong baselines by more than 10%.", "conclusion": "The approach enables transparent, logically consistent reasoning pathways, enhancing trustworthiness and interpretability of AI for clinical decision support."}}
{"id": "2602.01089", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.01089", "abs": "https://arxiv.org/abs/2602.01089", "authors": ["Zhiqi Zhang", "Xinhao Zhong", "Yi Sun", "Shuoyang Sun", "Bin Chen", "Shu-Tao Xia", "Xuan Wang"], "title": "Differential Vector Erasure: Unified Training-Free Concept Erasure for Flow Matching Models", "comment": null, "summary": "Text-to-image diffusion models have demonstrated remarkable capabilities in generating high-quality images, yet their tendency to reproduce undesirable concepts, such as NSFW content, copyrighted styles, or specific objects, poses growing concerns for safe and controllable deployment. While existing concept erasure approaches primarily focus on DDPM-based diffusion models and rely on costly fine-tuning, the recent emergence of flow matching models introduces a fundamentally different generative paradigm for which prior methods are not directly applicable. In this paper, we propose Differential Vector Erasure (DVE), a training-free concept erasure method specifically designed for flow matching models. Our key insight is that semantic concepts are implicitly encoded in the directional structure of the velocity field governing the generative flow. Leveraging this observation, we construct a differential vector field that characterizes the directional discrepancy between a target concept and a carefully chosen anchor concept. During inference, DVE selectively removes concept-specific components by projecting the velocity field onto the differential direction, enabling precise concept suppression without affecting irrelevant semantics. Extensive experiments on FLUX demonstrate that DVE consistently outperforms existing baselines on a wide range of concept erasure tasks, including NSFW suppression, artistic style removal, and object erasure, while preserving image quality and diversity.", "AI": {"tldr": "Differential Vector Erasure (DVE) is a training-free method for removing unwanted concepts from flow matching diffusion models by projecting velocity fields onto differential directions between target and anchor concepts.", "motivation": "Text-to-image diffusion models often reproduce undesirable concepts (NSFW content, copyrighted styles, specific objects), raising concerns for safe deployment. Existing erasure methods focus on DDPM-based models and require costly fine-tuning, but don't work for the newer flow matching paradigm.", "method": "DVE constructs a differential vector field capturing directional differences between target and anchor concepts in the velocity field. During inference, it projects the velocity field onto this differential direction to remove concept-specific components without affecting other semantics.", "result": "Extensive experiments on FLUX show DVE consistently outperforms existing baselines across various concept erasure tasks including NSFW suppression, artistic style removal, and object erasure, while preserving image quality and diversity.", "conclusion": "DVE provides an effective training-free solution for concept erasure in flow matching models by leveraging the directional structure of velocity fields, enabling precise concept suppression without compromising other semantic content."}}
{"id": "2602.01095", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.01095", "abs": "https://arxiv.org/abs/2602.01095", "authors": ["Jinghong Zheng", "Changlong Jiang", "Yang Xiao", "Jiaqi Li", "Haohong Kuang", "Hang Xu", "Ran Wang", "Zhiguo Cao", "Min Du", "Joey Tianyi Zhou"], "title": "PandaPose: 3D Human Pose Lifting from a Single Image via Propagating 2D Pose Prior to 3D Anchor Space", "comment": "Accepted at NeurIPS 2025", "summary": "3D human pose lifting from a single RGB image is a challenging task in 3D vision. Existing methods typically establish a direct joint-to-joint mapping from 2D to 3D poses based on 2D features. This formulation suffers from two fundamental limitations: inevitable error propagation from input predicted 2D pose to 3D predictions and inherent difficulties in handling self-occlusion cases. In this paper, we propose PandaPose, a 3D human pose lifting approach via propagating 2D pose prior to 3D anchor space as the unified intermediate representation. Specifically, our 3D anchor space comprises: (1) Joint-wise 3D anchors in the canonical coordinate system, providing accurate and robust priors to mitigate 2D pose estimation inaccuracies. (2) Depth-aware joint-wise feature lifting that hierarchically integrates depth information to resolve self-occlusion ambiguities. (3) The anchor-feature interaction decoder that incorporates 3D anchors with lifted features to generate unified anchor queries encapsulating joint-wise 3D anchor set, visual cues and geometric depth information. The anchor queries are further employed to facilitate anchor-to-joint ensemble prediction. Experiments on three well-established benchmarks (i.e., Human3.6M, MPI-INF-3DHP and 3DPW) demonstrate the superiority of our proposition. The substantial reduction in error by $14.7\\%$ compared to SOTA methods on the challenging conditions of Human3.6M and qualitative comparisons further showcase the effectiveness and robustness of our approach.", "AI": {"tldr": "PandaPose: A 3D human pose lifting method that uses 3D anchor space as intermediate representation to address error propagation from 2D pose estimation and self-occlusion challenges.", "motivation": "Existing 3D pose lifting methods suffer from two fundamental limitations: (1) inevitable error propagation from input predicted 2D pose to 3D predictions, and (2) inherent difficulties in handling self-occlusion cases where direct joint-to-joint mapping fails.", "method": "Proposes PandaPose with three key components: (1) Joint-wise 3D anchors in canonical coordinate system for robust priors, (2) Depth-aware joint-wise feature lifting to resolve self-occlusion ambiguities, (3) Anchor-feature interaction decoder that combines 3D anchors with lifted features to generate unified anchor queries containing joint-wise 3D anchor set, visual cues, and geometric depth information.", "result": "Achieves 14.7% error reduction compared to SOTA methods on challenging Human3.6M conditions. Demonstrates superiority on three benchmarks: Human3.6M, MPI-INF-3DHP, and 3DPW through both quantitative and qualitative comparisons.", "conclusion": "PandaPose effectively addresses error propagation and self-occlusion challenges in 3D human pose lifting by introducing 3D anchor space as unified intermediate representation, resulting in more accurate and robust 3D pose estimation from single RGB images."}}
{"id": "2602.01101", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.01101", "abs": "https://arxiv.org/abs/2602.01101", "authors": ["Felix Breiteneder", "Mohammad Belal", "Muhammad Saad Saeed", "Shahed Masoudian", "Usman Naseem", "Kulshrestha Juhi", "Markus Schedl", "Shah Nawaz"], "title": "Robust Harmful Meme Detection under Missing Modalities via Shared Representation Learning", "comment": "Accepted at WWW2026", "summary": "Internet memes are powerful tools for communication, capable of spreading political, psychological, and sociocultural ideas. However, they can be harmful and can be used to disseminate hate toward targeted individuals or groups. Although previous studies have focused on designing new detection methods, these often rely on modal-complete data, such as text and images. In real-world settings, however, modalities like text may be missing due to issues like poor OCR quality, making existing methods sensitive to missing information and leading to performance deterioration. To address this gap, in this paper, we present the first-of-its-kind work to comprehensively investigate the behavior of harmful meme detection methods in the presence of modal-incomplete data. Specifically, we propose a new baseline method that learns a shared representation for multiple modalities by projecting them independently. These shared representations can then be leveraged when data is modal-incomplete. Experimental results on two benchmark datasets demonstrate that our method outperforms existing approaches when text is missing. Moreover, these results suggest that our method allows for better integration of visual features, reducing dependence on text and improving robustness in scenarios where textual information is missing. Our work represents a significant step forward in enabling the real-world application of harmful meme detection, particularly in situations where a modality is absent.", "AI": {"tldr": "Proposes a new baseline method for harmful meme detection that handles modal-incomplete data by learning shared representations across modalities, improving robustness when text is missing.", "motivation": "Internet memes can spread harmful content and hate, but existing detection methods rely on complete multimodal data (text + images). In real-world scenarios, text may be missing due to poor OCR quality, causing performance deterioration in current methods.", "method": "Proposes a new baseline method that learns shared representations for multiple modalities by projecting them independently. These shared representations can be leveraged when data is modal-incomplete, allowing better integration of visual features and reducing dependence on text.", "result": "Experimental results on two benchmark datasets show the method outperforms existing approaches when text is missing. The method enables better visual feature integration and improves robustness in scenarios with missing textual information.", "conclusion": "This work represents a significant step forward in enabling real-world application of harmful meme detection, particularly in situations where modalities are absent, addressing a critical gap in current multimodal detection systems."}}
{"id": "2602.01118", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.01118", "abs": "https://arxiv.org/abs/2602.01118", "authors": ["Jingjing Wang", "Qirui Hu", "Chong Bao", "Yuke Zhu", "Hujun Bao", "Zhaopeng Cui", "Guofeng Zhang"], "title": "LightCity: An Urban Dataset for Outdoor Inverse Rendering and Reconstruction under Multi-illumination Conditions", "comment": null, "summary": "Inverse rendering in urban scenes is pivotal for applications like autonomous driving and digital twins. Yet, it faces significant challenges due to complex illumination conditions, including multi-illumination and indirect light and shadow effects. However, the effects of these challenges on intrinsic decomposition and 3D reconstruction have not been explored due to the lack of appropriate datasets. In this paper, we present LightCity, a novel high-quality synthetic urban dataset featuring diverse illumination conditions with realistic indirect light and shadow effects. LightCity encompasses over 300 sky maps with highly controllable illumination, varying scales with street-level and aerial perspectives over 50K images, and rich properties such as depth, normal, material components, light and indirect light, etc. Besides, we leverage LightCity to benchmark three fundamental tasks in the urban environments and conduct a comprehensive analysis of these benchmarks, laying a robust foundation for advancing related research.", "AI": {"tldr": "LightCity is a new synthetic urban dataset for inverse rendering with diverse illumination conditions, addressing the lack of appropriate datasets for studying complex lighting effects in urban scenes.", "motivation": "Inverse rendering in urban scenes faces challenges due to complex illumination (multi-illumination, indirect light, shadows), but these effects haven't been properly studied due to lack of appropriate datasets with realistic lighting conditions.", "method": "Created LightCity - a high-quality synthetic urban dataset with over 300 sky maps featuring controllable illumination, 50K+ images from street-level and aerial perspectives, and rich properties including depth, normal, material components, light and indirect light.", "result": "LightCity provides comprehensive urban scene data with diverse illumination conditions, enabling benchmarking of three fundamental tasks in urban environments and comprehensive analysis of these benchmarks.", "conclusion": "LightCity lays a robust foundation for advancing inverse rendering research in urban scenes by providing the necessary dataset to study complex illumination effects on intrinsic decomposition and 3D reconstruction."}}
{"id": "2602.01127", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.01127", "abs": "https://arxiv.org/abs/2602.01127", "authors": ["Matej Suchanek", "Klara Janouskova", "Ondrej Vasatko", "Jiri Matas"], "title": "Koo-Fu CLIP: Closed-Form Adaptation of Vision-Language Models via Fukunaga-Koontz Linear Discriminant Analysis", "comment": null, "summary": "Visual-language models such as CLIP provide powerful general-purpose representations, but their raw embeddings are not optimized for supervised classification, often exhibiting limited class separation and excessive dimensionality. We propose Koo-Fu CLIP, a supervised CLIP adaptation method based on Fukunaga-Koontz Linear Discriminant Analysis, which operates in a whitened embedding space to suppress within-class variation and enhance between-class discrimination. The resulting closed-form linear projection reshapes the geometry of CLIP embeddings, improving class separability while performing effective dimensionality reduction, and provides a lightweight and efficient adaptation of CLIP representations.\n  Across large-scale ImageNet benchmarks, nearest visual prototype classification in the Koo-Fu CLIP space improves top-1 accuracy from 75.1% to 79.1% on ImageNet-1K, with consistent gains persisting as the label space expands to 14K and 21K classes. The method supports substantial compression by up to 10-12x with little or no loss in accuracy, enabling efficient large-scale classification and retrieval.", "AI": {"tldr": "Koo-Fu CLIP adapts CLIP embeddings using Fukunaga-Koontz Linear Discriminant Analysis to improve class separability and enable dimensionality reduction, achieving significant accuracy gains on ImageNet benchmarks.", "motivation": "Raw CLIP embeddings have limited class separation and excessive dimensionality, making them suboptimal for supervised classification tasks despite their powerful general-purpose representation capabilities.", "method": "Uses Fukunaga-Koontz Linear Discriminant Analysis in a whitened embedding space to suppress within-class variation and enhance between-class discrimination, creating a closed-form linear projection that reshapes CLIP embedding geometry.", "result": "Improves ImageNet-1K top-1 accuracy from 75.1% to 79.1%, with consistent gains across 14K and 21K class benchmarks. Enables 10-12x compression with minimal accuracy loss.", "conclusion": "Koo-Fu CLIP provides a lightweight, efficient adaptation method that transforms CLIP embeddings into more discriminative representations suitable for large-scale classification and retrieval tasks."}}
{"id": "2602.01158", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2602.01158", "abs": "https://arxiv.org/abs/2602.01158", "authors": ["Daniel Yezid Guarnizo Orjuela", "Leonardo Scappatura", "Veronica Di Gennaro", "Riccardo Andrea Izzo", "Gianluca Bardaro", "Matteo Matteucci"], "title": "Improving Robustness of Vision-Language-Action Models by Restoring Corrupted Visual Inputs", "comment": null, "summary": "Vision-Language-Action (VLA) models have emerged as a dominant paradigm for generalist robotic manipulation, unifying perception and control within a single end-to-end architecture. However, despite their success in controlled environments, reliable real-world deployment is severely hindered by their fragility to visual disturbances. While existing literature extensively addresses physical occlusions caused by scene geometry, a critical mode remains largely unexplored: image corruptions. These sensor-level artifacts, ranging from electronic noise and dead pixels to lens contaminants, directly compromise the integrity of the visual signal prior to interpretation. In this work, we quantify this vulnerability, demonstrating that state-of-the-art VLAs such as $\u03c0_{0.5}$ and SmolVLA, suffer catastrophic performance degradation, dropping from 90\\% success rates to as low as 2\\%, under common signal artifacts. To mitigate this, we introduce the Corruption Restoration Transformer (CRT), a plug-and-play and model-agnostic vision transformer designed to immunize VLA models against sensor disturbances. Leveraging an adversarial training objective, CRT restores clean observations from corrupted inputs without requiring computationally expensive fine-tuning of the underlying model. Extensive experiments across the LIBERO and Meta-World benchmarks demonstrate that CRT effectively recovers lost performance, enabling VLAs to maintain near-baseline success rates, even under severe visual corruption.", "AI": {"tldr": "The paper addresses VLA models' vulnerability to image corruptions, introduces CRT as a plug-and-play solution to restore performance without fine-tuning the underlying model.", "motivation": "VLA models are fragile to visual disturbances like image corruptions (noise, dead pixels, lens contaminants), which severely degrade performance in real-world deployment despite success in controlled environments.", "method": "Introduces Corruption Restoration Transformer (CRT), a plug-and-play, model-agnostic vision transformer that uses adversarial training to restore clean observations from corrupted inputs without fine-tuning the underlying VLA model.", "result": "State-of-the-art VLAs (\u03c0\u2080.\u2085 and SmolVLA) drop from 90% to 2% success under corruption; CRT effectively recovers lost performance, enabling near-baseline success rates even under severe visual corruption on LIBERO and Meta-World benchmarks.", "conclusion": "CRT provides an effective solution to VLA models' vulnerability to sensor-level image corruptions, enabling more reliable real-world deployment without computationally expensive fine-tuning."}}
{"id": "2602.01163", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.01163", "abs": "https://arxiv.org/abs/2602.01163", "authors": ["Chunliang Hua", "Zeyuan Yang", "Lei Zhang", "Jiayang Sun", "Fengwen Chen", "Chunlan Zeng", "Xiao Hu"], "title": "Semantically Aware UAV Landing Site Assessment from Remote Sensing Imagery via Multimodal Large Language Models", "comment": null, "summary": "Safe UAV emergency landing requires more than just identifying flat terrain; it demands understanding complex semantic risks (e.g., crowds, temporary structures) invisible to traditional geometric sensors. In this paper, we propose a novel framework leveraging Remote Sensing (RS) imagery and Multimodal Large Language Models (MLLMs) for global context-aware landing site assessment. Unlike local geometric methods, our approach employs a coarse-to-fine pipeline: first, a lightweight semantic segmentation module efficiently pre-screens candidate areas; second, a vision-language reasoning agent fuses visual features with Point-of-Interest (POI) data to detect subtle hazards. To validate this approach, we construct and release the Emergency Landing Site Selection (ELSS) benchmark. Experiments demonstrate that our framework significantly outperforms geometric baselines in risk identification accuracy. Furthermore, qualitative results confirm its ability to generate human-like, interpretable justifications, enhancing trust in automated decision-making. The benchmark dataset is publicly accessible at https://anonymous.4open.science/r/ELSS-dataset-43D7.", "AI": {"tldr": "A novel framework using Remote Sensing imagery and Multimodal LLMs for UAV emergency landing site assessment that goes beyond geometric analysis to understand semantic risks like crowds and temporary structures.", "motivation": "Traditional geometric sensors fail to detect complex semantic risks (crowds, temporary structures) crucial for safe UAV emergency landing, requiring a more comprehensive approach that understands global context.", "method": "Coarse-to-fine pipeline: 1) Lightweight semantic segmentation pre-screens candidate areas, 2) Vision-language reasoning agent fuses visual features with POI data to detect subtle hazards using MLLMs.", "result": "Framework significantly outperforms geometric baselines in risk identification accuracy; generates human-like, interpretable justifications; benchmark dataset (ELSS) released publicly.", "conclusion": "The RS+MLLM approach enables global context-aware landing site assessment with superior risk detection and interpretable reasoning, enhancing trust in automated UAV emergency landing decisions."}}
{"id": "2602.01173", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.01173", "abs": "https://arxiv.org/abs/2602.01173", "authors": ["Lancheng Gao", "Ziheng Jia", "Zixuan Xing", "Wei Sun", "Huiyu Duan", "Guangtao Zhai", "Xiongkuo Min"], "title": "EEmo-Logic: A Unified Dataset and Multi-Stage Framework for Comprehensive Image-Evoked Emotion Assessment", "comment": null, "summary": "Understanding the multi-dimensional attributes and intensity nuances of image-evoked emotions is pivotal for advancing machine empathy and empowering diverse human-computer interaction applications. However, existing models are still limited to coarse-grained emotion perception or deficient reasoning capabilities. To bridge this gap, we introduce EEmoDB, the largest image-evoked emotion understanding dataset to date. It features $5$ analysis dimensions spanning $5$ distinct task categories, facilitating comprehensive interpretation. Specifically, we compile $1.2M$ question-answering (QA) pairs (EEmoDB-QA) from $125k$ images via automated generation, alongside a $36k$ dataset (EEmoDB-Assess) curated from $25k$ images for fine-grained assessment. Furthermore, we propose EEmo-Logic, an all-in-one multimodal large language model (MLLM) developed via instruction fine-tuning and task-customized group relative preference optimization (GRPO) with novel reward design. Extensive experiments demonstrate that EEmo-Logic achieves robust performance in in-domain and cross-domain datasets, excelling in emotion QA and fine-grained assessment. The code is available at https://anonymous.4open.science/r/EEmoLogic.", "AI": {"tldr": "EEmoDB is the largest image-evoked emotion understanding dataset with 5 analysis dimensions and 5 task categories, featuring 1.2M QA pairs and 36k fine-grained assessments. EEmo-Logic is a multimodal LLM that achieves robust performance in emotion QA and assessment tasks.", "motivation": "Existing models for image-evoked emotion understanding are limited to coarse-grained perception or deficient reasoning capabilities, hindering machine empathy and human-computer interaction applications.", "method": "Created EEmoDB dataset with 5 analysis dimensions and 5 task categories, including 1.2M QA pairs (EEmoDB-QA) from 125k images and 36k fine-grained assessments (EEmoDB-Assess) from 25k images. Developed EEmo-Logic MLLM via instruction fine-tuning and task-customized group relative preference optimization (GRPO) with novel reward design.", "result": "EEmo-Logic demonstrates robust performance in both in-domain and cross-domain datasets, excelling in emotion question-answering and fine-grained assessment tasks.", "conclusion": "The proposed EEmoDB dataset and EEmo-Logic model bridge the gap in comprehensive image-evoked emotion understanding, advancing machine empathy and enabling diverse human-computer interaction applications."}}
{"id": "2602.01183", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.01183", "abs": "https://arxiv.org/abs/2602.01183", "authors": ["Chunming He", "Rihan Zhang", "Fengyang Xiao", "Dingming Zhang", "Zhiwen Cao", "Sina Farsiu"], "title": "Refining Context-Entangled Content Segmentation via Curriculum Selection and Anti-Curriculum Promotion", "comment": "8 figures, 11 tables", "summary": "Biological learning proceeds from easy to difficult tasks, gradually reinforcing perception and robustness. Inspired by this principle, we address Context-Entangled Content Segmentation (CECS), a challenging setting where objects share intrinsic visual patterns with their surroundings, as in camouflaged object detection. Conventional segmentation networks predominantly rely on architectural enhancements but often ignore the learning dynamics that govern robustness under entangled data distributions. We introduce CurriSeg, a dual-phase learning framework that unifies curriculum and anti-curriculum principles to improve representation reliability. In the Curriculum Selection phase, CurriSeg dynamically selects training data based on the temporal statistics of sample losses, distinguishing hard-but-informative samples from noisy or ambiguous ones, thus enabling stable capability enhancement. In the Anti-Curriculum Promotion phase, we design Spectral-Blindness Fine-Tuning, which suppresses high-frequency components to enforce dependence on low-frequency structural and contextual cues and thus strengthens generalization. Extensive experiments demonstrate that CurriSeg achieves consistent improvements across diverse CECS benchmarks without adding parameters or increasing total training time, offering a principled view of how progression and challenge interplay to foster robust and context-aware segmentation. Code will be released.", "AI": {"tldr": "CurriSeg is a dual-phase learning framework that combines curriculum and anti-curriculum learning for robust segmentation of context-entangled objects, achieving state-of-the-art performance without extra parameters or training time.", "motivation": "Biological learning progresses from easy to difficult tasks, gradually building perception and robustness. The paper addresses Context-Entangled Content Segmentation (CECS) where objects share visual patterns with surroundings (like camouflaged objects). Conventional segmentation networks focus on architectural improvements but ignore learning dynamics that affect robustness with entangled data distributions.", "method": "CurriSeg uses a dual-phase framework: 1) Curriculum Selection phase dynamically selects training data based on temporal loss statistics to distinguish hard-but-informative samples from noisy ones, enabling stable capability enhancement. 2) Anti-Curriculum Promotion phase uses Spectral-Blindness Fine-Tuning that suppresses high-frequency components to enforce dependence on low-frequency structural and contextual cues, strengthening generalization.", "result": "Extensive experiments show CurriSeg achieves consistent improvements across diverse CECS benchmarks without adding parameters or increasing total training time. The framework demonstrates how progression and challenge interplay fosters robust and context-aware segmentation.", "conclusion": "CurriSeg offers a principled view of how curriculum and anti-curriculum learning can be unified to improve representation reliability in challenging segmentation tasks. The biologically-inspired approach addresses the core challenge of context-entangled content segmentation through learning dynamics rather than architectural complexity."}}
{"id": "2602.01194", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.01194", "abs": "https://arxiv.org/abs/2602.01194", "authors": ["Hao Chen", "Tao Han", "Jie Zhang", "Song Guo", "Fenghua Ling", "Lei Bai"], "title": "EMFormer: Efficient Multi-Scale Transformer for Accumulative Context Weather Forecasting", "comment": null, "summary": "Long-term weather forecasting is critical for socioeconomic planning and disaster preparedness. While recent approaches employ finetuning to extend prediction horizons, they remain constrained by the issues of catastrophic forgetting, error accumulation, and high training overhead. To address these limitations, we present a novel pipeline across pretraining, finetuning and forecasting to enhance long-context modeling while reducing computational overhead. First, we introduce an Efficient Multi-scale Transformer (EMFormer) to extract multi-scale features through a single convolution in both training and inference. Based on the new architecture, we further employ an accumulative context finetuning to improve temporal consistency without degrading short-term accuracy. Additionally, we propose a composite loss that dynamically balances different terms via a sinusoidal weighting, thereby adaptively guiding the optimization trajectory throughout pretraining and finetuning. Experiments show that our approach achieves strong performance in weather forecasting and extreme event prediction, substantially improving long-term forecast accuracy. Moreover, EMFormer demonstrates strong generalization on vision benchmarks (ImageNet-1K and ADE20K) while delivering a 5.69x speedup over conventional multi-scale modules.", "AI": {"tldr": "Novel pipeline with Efficient Multi-scale Transformer (EMFormer) for long-term weather forecasting, addressing catastrophic forgetting, error accumulation, and high training overhead through multi-scale feature extraction, accumulative context finetuning, and composite loss.", "motivation": "Long-term weather forecasting is critical for socioeconomic planning and disaster preparedness, but current approaches suffer from catastrophic forgetting, error accumulation, and high training overhead when extending prediction horizons.", "method": "Three-part approach: 1) Efficient Multi-scale Transformer (EMFormer) extracts multi-scale features through single convolution; 2) Accumulative context finetuning improves temporal consistency; 3) Composite loss with sinusoidal weighting dynamically balances optimization terms.", "result": "Achieves strong performance in weather forecasting and extreme event prediction with substantial improvement in long-term forecast accuracy. EMFormer also shows strong generalization on vision benchmarks (ImageNet-1K and ADE20K) with 5.69x speedup over conventional multi-scale modules.", "conclusion": "The proposed pipeline effectively addresses key limitations in long-term weather forecasting while reducing computational overhead and improving both accuracy and generalization capabilities across domains."}}
{"id": "2602.01200", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.01200", "abs": "https://arxiv.org/abs/2602.01200", "authors": ["Haoran Lai", "Zihang Jiang", "Kun Zhang", "Qingsong Yao", "Rongsheng Wang", "Zhiyang He", "Xiaodong Tao", "Wei Wei", "Shaohua Kevin Zhou"], "title": "Med3D-R1: Incentivizing Clinical Reasoning in 3D Medical Vision-Language Models for Abnormality Diagnosis", "comment": null, "summary": "Developing 3D vision-language models with robust clinical reasoning remains a challenge due to the inherent complexity of volumetric medical imaging, the tendency of models to overfit superficial report patterns, and the lack of interpretability-aware reward designs. In this paper, we propose Med3D-R1, a reinforcement learning framework with a two-stage training process: Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL). During SFT stage, we introduce a residual alignment mechanism to bridge the gap between high-dimensional 3D features and textual embeddings, and an abnormality re-weighting strategy to emphasize clinically informative tokens and reduce structural bias in reports. In RL stage, we redesign the consistency reward to explicitly promote coherent, step-by-step diagnostic reasoning. We evaluate our method on medical multiple-choice visual question answering using two 3D diagnostic benchmarks, CT-RATE and RAD-ChestCT, where our model attains state-of-the-art accuracies of 41.92\\% on CT-RATE and 44.99\\% on RAD-ChestCT. These results indicate improved abnormality diagnosis and clinical reasoning and outperform prior methods on both benchmarks. Overall, our approach holds promise for enhancing real-world diagnostic workflows by enabling more reliable and transparent 3D medical vision-language systems.", "AI": {"tldr": "Med3D-R1: A reinforcement learning framework for 3D medical vision-language models that improves clinical reasoning through residual alignment, abnormality re-weighting, and redesigned consistency rewards.", "motivation": "Challenges in developing 3D vision-language models for medical imaging include: complexity of volumetric data, tendency to overfit superficial report patterns, and lack of interpretability-aware reward designs for robust clinical reasoning.", "method": "Two-stage training: 1) Supervised Fine-Tuning with residual alignment (bridges 3D features to text embeddings) and abnormality re-weighting (emphasizes clinical tokens); 2) Reinforcement Learning with redesigned consistency reward to promote step-by-step diagnostic reasoning.", "result": "State-of-the-art accuracies: 41.92% on CT-RATE and 44.99% on RAD-ChestCT benchmarks for medical multiple-choice visual question answering, outperforming prior methods and showing improved abnormality diagnosis and clinical reasoning.", "conclusion": "The approach enhances real-world diagnostic workflows by enabling more reliable and transparent 3D medical vision-language systems, with promise for clinical applications."}}
{"id": "2602.01257", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.01257", "abs": "https://arxiv.org/abs/2602.01257", "authors": ["Yunchuan Ma", "Laiyun Qing", "Guorong Li", "Yuqing Liu", "Yuankai Qi", "Qingming Huang"], "title": "Boosting Point-supervised Temporal Action Localization via Text Refinement and Alignment", "comment": null, "summary": "Recently, point-supervised temporal action localization has gained significant attention for its effective balance between labeling costs and localization accuracy. However, current methods only consider features from visual inputs, neglecting helpful semantic information from the text side. To address this issue, we propose a Text Refinement and Alignment (TRA) framework that effectively utilizes textual features from visual descriptions to complement the visual features as they are semantically rich. This is achieved by designing two new modules for the original point-supervised framework: a Point-based Text Refinement module (PTR) and a Point-based Multimodal Alignment module (PMA). Specifically, we first generate descriptions for video frames using a pre-trained multimodal model. Next, PTR refines the initial descriptions by leveraging point annotations together with multiple pre-trained models. PMA then projects all features into a unified semantic space and leverages a point-level multimodal feature contrastive learning to reduce the gap between visual and linguistic modalities. Last, the enhanced multi-modal features are fed into the action detector for precise localization. Extensive experimental results on five widely used benchmarks demonstrate the favorable performance of our proposed framework compared to several state-of-the-art methods. Moreover, our computational overhead analysis shows that the framework can run on a single 24 GB RTX 3090 GPU, indicating its practicality and scalability.", "AI": {"tldr": "A Text Refinement and Alignment (TRA) framework that enhances point-supervised temporal action localization by incorporating textual features from video descriptions, improving localization accuracy without significantly increasing computational cost.", "motivation": "Current point-supervised temporal action localization methods only use visual features, missing valuable semantic information from text. There's a need to leverage textual features from video descriptions to complement visual features for better localization performance.", "method": "Proposes TRA framework with two modules: Point-based Text Refinement (PTR) that refines initial video descriptions using point annotations and pre-trained models, and Point-based Multimodal Alignment (PMA) that projects all features into unified semantic space and uses point-level multimodal contrastive learning to align visual and linguistic modalities.", "result": "Extensive experiments on five benchmarks show favorable performance compared to state-of-the-art methods. The framework runs efficiently on a single 24 GB RTX 3090 GPU, demonstrating practicality and scalability.", "conclusion": "The TRA framework effectively leverages textual features to enhance point-supervised temporal action localization, achieving better accuracy while maintaining computational efficiency, making it a practical solution for real-world applications."}}
{"id": "2602.01268", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2602.01268", "abs": "https://arxiv.org/abs/2602.01268", "authors": ["Jaehyeon Cho", "Jhonghyun An"], "title": "OASIS-DC: Generalizable Depth Completion via Output-level Alignment of Sparse-Integrated Monocular Pseudo Depth", "comment": "Accepted to ICRA 2026", "summary": "Recent monocular foundation models excel at zero-shot depth estimation, yet their outputs are inherently relative rather than metric, limiting direct use in robotics and autonomous driving. We leverage the fact that relative depth preserves global layout and boundaries: by calibrating it with sparse range measurements, we transform it into a pseudo metric depth prior. Building on this prior, we design a refinement network that follows the prior where reliable and deviates where necessary, enabling accurate metric predictions from very few labeled samples. The resulting system is particularly effective when curated validation data are unavailable, sustaining stable scale and sharp edges across few-shot regimes. These findings suggest that coupling foundation priors with sparse anchors is a practical route to robust, deployment-ready depth completion under real-world label scarcity.", "AI": {"tldr": "This paper proposes a method to convert relative depth from foundation models into metric depth using sparse range measurements, enabling accurate metric depth prediction with minimal labeled data.", "motivation": "Monocular foundation models produce relative depth estimates, not metric depth, which limits their direct application in robotics and autonomous driving where metric measurements are essential.", "method": "The approach calibrates relative depth with sparse range measurements to create a pseudo-metric depth prior, then uses a refinement network that follows this prior where reliable and deviates where necessary.", "result": "The system achieves accurate metric depth predictions from very few labeled samples, maintaining stable scale and sharp edges even in few-shot regimes without curated validation data.", "conclusion": "Combining foundation model priors with sparse anchor measurements provides a practical solution for robust, deployment-ready depth completion under real-world label scarcity conditions."}}
{"id": "2602.01273", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.01273", "abs": "https://arxiv.org/abs/2602.01273", "authors": ["Xun Zhang", "Kaicheng Yang", "Hongliang Lu", "Haotong Qin", "Yong Guo", "Yulun Zhang"], "title": "Q-DiT4SR: Exploration of Detail-Preserving Diffusion Transformer Quantization for Real-World Image Super-Resolution", "comment": "Our code and models will be available at https://github.com/xunzhang1128/Q-DiT4SR", "summary": "Recently, Diffusion Transformers (DiTs) have emerged in Real-World Image Super-Resolution (Real-ISR) to generate high-quality textures, yet their heavy inference burden hinders real-world deployment. While Post-Training Quantization (PTQ) is a promising solution for acceleration, existing methods in super-resolution mostly focus on U-Net architectures, whereas generic DiT quantization is typically designed for text-to-image tasks. Directly applying these methods to DiT-based super-resolution models leads to severe degradation of local textures. Therefore, we propose Q-DiT4SR, the first PTQ framework specifically tailored for DiT-based Real-ISR. We propose H-SVD, a hierarchical SVD that integrates a global low-rank branch with a local block-wise rank-1 branch under a matched parameter budget. We further propose Variance-aware Spatio-Temporal Mixed Precision: VaSMP allocates cross-layer weight bit-widths in a data-free manner based on rate-distortion theory, while VaTMP schedules intra-layer activation precision across diffusion timesteps via dynamic programming (DP) with minimal calibration. Experiments on multiple real-world datasets demonstrate that our Q-DiT4SR achieves SOTA performance under both W4A6 and W4A4 settings. Notably, the W4A4 quantization configuration reduces model size by 5.8$\\times$ and computational operations by over 60$\\times$. Our code and models will be available at https://github.com/xunzhang1128/Q-DiT4SR.", "AI": {"tldr": "Q-DiT4SR: First PTQ framework for DiT-based Real-ISR using hierarchical SVD and variance-aware mixed precision to achieve efficient quantization with minimal performance loss.", "motivation": "Diffusion Transformers (DiTs) show promise for Real-World Image Super-Resolution but suffer from heavy inference burden. Existing quantization methods are either designed for U-Net architectures or text-to-image tasks, causing severe texture degradation when applied to DiT-based super-resolution models.", "method": "Proposes Q-DiT4SR with two key components: 1) H-SVD (hierarchical SVD) that integrates global low-rank branch with local block-wise rank-1 branch under matched parameter budget, and 2) Variance-aware Spatio-Temporal Mixed Precision (VaSMP for cross-layer weight bit-width allocation via rate-distortion theory, VaTMP for intra-layer activation precision scheduling across diffusion timesteps via dynamic programming).", "result": "Achieves SOTA performance on multiple real-world datasets under both W4A6 and W4A4 settings. W4A4 quantization reduces model size by 5.8\u00d7 and computational operations by over 60\u00d7 while maintaining quality.", "conclusion": "Q-DiT4SR effectively addresses the inference burden of DiT-based Real-ISR models through specialized quantization techniques, enabling practical deployment with significant efficiency gains and minimal performance degradation."}}
{"id": "2602.01277", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.01277", "abs": "https://arxiv.org/abs/2602.01277", "authors": ["Yihan Xie", "Han Xia", "Zhen Yang"], "title": "TF-Lane: Traffic Flow Module for Robust Lane Perception", "comment": "9 pages, 7 figures, 7 tables", "summary": "Autonomous driving systems require robust lane perception capabilities, yet existing vision-based detection methods suffer significant performance degradation when visual sensors provide insufficient cues, such as in occluded or lane-missing scenarios. While some approaches incorporate high-definition maps as supplementary information, these solutions face challenges of high subscription costs and limited real-time performance. To address these limitations, we explore an innovative information source: traffic flow, which offers real-time capabilities without additional costs. This paper proposes a TrafficFlow-aware Lane perception Module (TFM) that effectively extracts real-time traffic flow features and seamlessly integrates them with existing lane perception algorithms. This solution originated from real-world autonomous driving conditions and was subsequently validated on open-source algorithms and datasets. Extensive experiments on four mainstream models and two public datasets (Nuscenes and OpenLaneV2) using standard evaluation metrics show that TFM consistently improves performance, achieving up to +4.1% mAP gain on the Nuscenes dataset.", "AI": {"tldr": "TFM is a TrafficFlow-aware Lane perception Module that uses real-time traffic flow data to enhance lane detection in autonomous driving, especially in challenging scenarios like occlusions or missing lanes.", "motivation": "Existing vision-based lane detection methods degrade in occluded or lane-missing scenarios, while HD map solutions have high costs and limited real-time performance. Traffic flow offers a cost-free, real-time alternative information source.", "method": "Proposes TFM that extracts real-time traffic flow features and integrates them with existing lane perception algorithms. The solution was developed from real-world autonomous driving conditions and validated on open-source algorithms and datasets.", "result": "Extensive experiments on four mainstream models and two public datasets (Nuscenes and OpenLaneV2) show TFM consistently improves performance, achieving up to +4.1% mAP gain on Nuscenes dataset.", "conclusion": "TFM effectively leverages traffic flow as a novel information source to enhance lane perception in autonomous driving, providing real-time capabilities without additional costs and improving robustness in challenging scenarios."}}
{"id": "2602.01278", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.01278", "abs": "https://arxiv.org/abs/2602.01278", "authors": ["Zhengbo Zhang", "Yihe Tian", "Wanke Xia", "Lin Chen", "Yue Sun", "Kun Ding", "Ying Wang", "Bing Xu", "Shiming Xiang"], "title": "DSFC-Net: A Dual-Encoder Spatial and Frequency Co-Awareness Network for Rural Road Extraction", "comment": null, "summary": "Accurate extraction of rural roads from high-resolution remote sensing imagery is essential for infrastructure planning and sustainable development. However, this task presents unique challenges in rural settings due to several factors. These include high intra-class variability and low inter-class separability from diverse surface materials, frequent vegetation occlusions that disrupt spatial continuity, and narrow road widths that exacerbate detection difficulties. Existing methods, primarily optimized for structured urban environments, often underperform in these scenarios as they overlook such distinctive characteristics. To address these challenges, we propose DSFC-Net, a dual-encoder framework that synergistically fuses spatial and frequency-domain information. Specifically, a CNN branch is employed to capture fine-grained local road boundaries and short-range continuity, while a novel Spatial-Frequency Hybrid Transformer (SFT) is introduced to robustly model global topological dependencies against vegetation occlusions. Distinct from standard attention mechanisms that suffer from frequency bias, the SFT incorporates a Cross-Frequency Interaction Attention (CFIA) module that explicitly decouples high- and low-frequency information via a Laplacian Pyramid strategy. This design enables the dynamic interaction between spatial details and frequency-aware global contexts, effectively preserving the connectivity of narrow roads. Furthermore, a Channel Feature Fusion Module (CFFM) is proposed to bridge the two branches by adaptively recalibrating channel-wise feature responses, seamlessly integrating local textures with global semantics for accurate segmentation. Comprehensive experiments on the WHU-RuR+, DeepGlobe, and Massachusetts datasets validate the superiority of DSFC-Net over state-of-the-art approaches.", "AI": {"tldr": "DSFC-Net is a dual-encoder network that fuses spatial and frequency-domain information for accurate rural road extraction from high-resolution remote sensing imagery, addressing challenges like vegetation occlusions and narrow roads.", "motivation": "Rural road extraction faces unique challenges: high intra-class variability from diverse surface materials, frequent vegetation occlusions disrupting spatial continuity, and narrow road widths. Existing methods optimized for urban environments underperform in rural settings by overlooking these distinctive characteristics.", "method": "Proposes DSFC-Net with dual-encoder framework: 1) CNN branch captures fine-grained local road boundaries and short-range continuity, 2) Novel Spatial-Frequency Hybrid Transformer (SFT) with Cross-Frequency Interaction Attention (CFIA) module decouples high/low-frequency information via Laplacian Pyramid strategy to model global topological dependencies against occlusions, 3) Channel Feature Fusion Module (CFFM) adaptively recalibrates channel-wise features to integrate local textures with global semantics.", "result": "Comprehensive experiments on WHU-RuR+, DeepGlobe, and Massachusetts datasets validate DSFC-Net's superiority over state-of-the-art approaches for rural road extraction.", "conclusion": "DSFC-Net effectively addresses rural road extraction challenges by synergistically fusing spatial and frequency-domain information, preserving connectivity of narrow roads against vegetation occlusions through innovative frequency-aware attention mechanisms."}}
{"id": "2602.01283", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.01283", "abs": "https://arxiv.org/abs/2602.01283", "authors": ["Xianhui Zhang", "Chengyu Xie", "Linxia Zhu", "Yonghui Yang", "Weixiang Zhao", "Zifeng Cheng", "Cong Wang", "Fei Shen", "Tat-Seng Chua"], "title": "Who Transfers Safety? Identifying and Targeting Cross-Lingual Shared Safety Neurons", "comment": null, "summary": "Multilingual safety remains significantly imbalanced, leaving non-high-resource (NHR) languages vulnerable compared to robust high-resource (HR) ones. Moreover, the neural mechanisms driving safety alignment remain unclear despite observed cross-lingual representation transfer.\n  In this paper, we find that LLMs contain a set of cross-lingual shared safety neurons (SS-Neurons), a remarkably small yet critical neuronal subset that jointly regulates safety behavior across languages.\n  We first identify monolingual safety neurons (MS-Neurons) and validate their causal role in safety refusal behavior through targeted activation and suppression.\n  Our cross-lingual analyses then identify SS-Neurons as the subset of MS-Neurons shared between HR and NHR languages, serving as a bridge to transfer safety capabilities from HR to NHR domains.\n  We observe that suppressing these neurons causes concurrent safety drops across NHR languages, whereas reinforcing them improves cross-lingual defensive consistency.\n  Building on these insights, we propose a simple neuron-oriented training strategy that targets SS-Neurons based on language resource distribution and model architecture. Experiments demonstrate that fine-tuning this tiny neuronal subset outperforms state-of-the-art methods, significantly enhancing NHR safety while maintaining the model's general capabilities.\n  The code and dataset will be available athttps://github.com/1518630367/SS-Neuron-Expansion.", "AI": {"tldr": "LLMs contain cross-lingual shared safety neurons (SS-Neurons) - a small critical subset that regulates safety behavior across languages. Targeting these neurons improves non-high-resource language safety while maintaining general capabilities.", "motivation": "Multilingual safety is imbalanced, with non-high-resource (NHR) languages vulnerable compared to high-resource (HR) ones. Neural mechanisms behind safety alignment remain unclear despite observed cross-lingual representation transfer.", "method": "1) Identify monolingual safety neurons (MS-Neurons) and validate their causal role through activation/suppression. 2) Identify SS-Neurons as MS-Neurons shared between HR and NHR languages. 3) Propose neuron-oriented training targeting SS-Neurons based on language resource distribution and model architecture.", "result": "Suppressing SS-Neurons causes concurrent safety drops across NHR languages; reinforcing them improves cross-lingual defensive consistency. Fine-tuning this tiny neuronal subset outperforms state-of-the-art methods, significantly enhancing NHR safety while maintaining general capabilities.", "conclusion": "SS-Neurons serve as a bridge to transfer safety capabilities from HR to NHR domains. Targeted intervention on these neurons provides an effective approach to address multilingual safety imbalance with minimal computational cost."}}
{"id": "2602.01296", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.01296", "abs": "https://arxiv.org/abs/2602.01296", "authors": ["Zeran Ke", "Bin Tan", "Gui-Song Xia", "Yujun Shen", "Nan Xue"], "title": "Interacted Planes Reveal 3D Line Mapping", "comment": "submitted to TPAMI", "summary": "3D line mapping from multi-view RGB images provides a compact and structured visual representation of scenes. We study the problem from a physical and topological perspective: a 3D line most naturally emerges as the edge of a finite 3D planar patch. We present LiP-Map, a line-plane joint optimization framework that explicitly models learnable line and planar primitives. This coupling enables accurate and detailed 3D line mapping while maintaining strong efficiency (typically completing a reconstruction in 3 to 5 minutes per scene). LiP-Map pioneers the integration of planar topology into 3D line mapping, not by imposing pairwise coplanarity constraints but by explicitly constructing interactions between plane and line primitives, thus offering a principled route toward structured reconstruction in man-made environments. On more than 100 scenes from ScanNetV2, ScanNet++, Hypersim, 7Scenes, and Tanks\\&Temple, LiP-Map improves both accuracy and completeness over state-of-the-art methods. Beyond line mapping quality, LiP-Map significantly advances line-assisted visual localization, establishing strong performance on 7Scenes. Our code is released at https://github.com/calmke/LiPMAP for reproducible research.", "AI": {"tldr": "LiP-Map is a 3D line mapping framework that jointly optimizes line and planar primitives, achieving state-of-the-art accuracy and efficiency by modeling lines as edges of planar patches.", "motivation": "Traditional 3D line mapping lacks explicit modeling of the physical relationship between lines and planes. Lines naturally emerge as edges of planar patches in man-made environments, suggesting a need for joint line-plane optimization.", "method": "LiP-Map introduces a line-plane joint optimization framework that explicitly models learnable line and planar primitives. It constructs interactions between plane and line primitives rather than imposing pairwise coplanarity constraints, pioneering the integration of planar topology into 3D line mapping.", "result": "The method achieves state-of-the-art accuracy and completeness on 100+ scenes from ScanNetV2, ScanNet++, Hypersim, 7Scenes, and Tanks&Temple. It also significantly advances line-assisted visual localization on 7Scenes while maintaining strong efficiency (3-5 minutes per scene).", "conclusion": "Explicit modeling of line-plane interactions provides a principled approach for structured 3D reconstruction in man-made environments, offering both improved mapping quality and practical efficiency for real-world applications."}}
{"id": "2602.01298", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.01298", "abs": "https://arxiv.org/abs/2602.01298", "authors": ["Ching-Kai Huang", "Wen-Chieh Lin", "Yan-Cen Lee"], "title": "Interaction-Consistent Object Removal via MLLM-Based Reasoning", "comment": null, "summary": "Image-based object removal often erases only the named target, leaving behind interaction evidence that renders the result semantically inconsistent. We formalize this problem as Interaction-Consistent Object Removal (ICOR), which requires removing not only the target object but also associated interaction elements, such as lighting-dependent effects, physically connected objects, targetproduced elements, and contextually linked objects. To address this task, we propose Reasoning-Enhanced Object Removal with MLLM (REORM), a reasoningenhanced object removal framework that leverages multimodal large language models to infer which elements must be jointly removed. REORM features a modular design that integrates MLLM-driven analysis, mask-guided removal, and a self-correction mechanism, along with a local-deployment variant that supports accurate editing under limited resources. To support evaluation, we introduce ICOREval, a benchmark consisting of instruction-driven removals with rich interaction dependencies. On ICOREval, REORM outperforms state-of-the-art image editing systems, demonstrating its effectiveness in producing interactionconsistent results.", "AI": {"tldr": "REORM is a reasoning-enhanced object removal framework that uses MLLMs to identify and remove not just target objects but also associated interaction elements, achieving more semantically consistent image editing.", "motivation": "Current object removal methods often only erase the named target object, leaving behind interaction evidence (like lighting effects, physically connected objects, or contextually linked elements) that makes the result semantically inconsistent.", "method": "Proposes REORM (Reasoning-Enhanced Object Removal with MLLM), a modular framework that uses multimodal large language models to analyze which interaction elements must be jointly removed, features mask-guided removal, self-correction mechanism, and offers a local-deployment variant for resource-limited environments.", "result": "Outperforms state-of-the-art image editing systems on the ICOREval benchmark, demonstrating effectiveness in producing interaction-consistent removal results.", "conclusion": "The paper formalizes the ICOR problem and presents REORM as an effective solution that leverages MLLM reasoning to achieve semantically consistent object removal by identifying and removing associated interaction elements."}}
{"id": "2602.01303", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.01303", "abs": "https://arxiv.org/abs/2602.01303", "authors": ["Ayushman Sarkar", "Zhenyu Yu", "Chu Chen", "Wei Tang", "Kangning Cui", "Mohd Yamani Idna Idris"], "title": "ReDiStory: Region-Disentangled Diffusion for Consistent Visual Story Generation", "comment": null, "summary": "Generating coherent visual stories requires maintaining subject identity across multiple images while preserving frame-specific semantics. Recent training-free methods concatenate identity and frame prompts into a unified representation, but this often introduces inter-frame semantic interference that weakens identity preservation in complex stories. We propose ReDiStory, a training-free framework that improves multi-frame story generation via inference-time prompt embedding reorganization. ReDiStory explicitly decomposes text embeddings into identity-related and frame-specific components, then decorrelates frame embeddings by suppressing shared directions across frames. This reduces cross-frame interference without modifying diffusion parameters or requiring additional supervision. Under identical diffusion backbones and inference settings, ReDiStory improves identity consistency while maintaining prompt fidelity. Experiments on the ConsiStory+ benchmark show consistent gains over 1Prompt1Story on multiple identity consistency metrics. Code is available at: https://github.com/YuZhenyuLindy/ReDiStory", "AI": {"tldr": "ReDiStory improves multi-frame story generation by reorganizing prompt embeddings to separate identity and frame-specific components, reducing cross-frame interference without training.", "motivation": "Existing training-free methods for visual story generation concatenate identity and frame prompts, which causes inter-frame semantic interference that weakens identity preservation in complex stories.", "method": "ReDiStory decomposes text embeddings into identity-related and frame-specific components, then decorrelates frame embeddings by suppressing shared directions across frames during inference.", "result": "Experiments on ConsiStory+ benchmark show consistent gains over 1Prompt1Story on multiple identity consistency metrics while maintaining prompt fidelity.", "conclusion": "ReDiStory effectively improves identity consistency in multi-frame story generation through inference-time prompt embedding reorganization without modifying diffusion parameters or requiring additional supervision."}}
{"id": "2602.01305", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.01305", "abs": "https://arxiv.org/abs/2602.01305", "authors": ["Ayushman Sarkar", "Zhenyu Yu", "Wei Tang", "Chu Chen", "Kangning Cui", "Mohd Yamani Idna Idris"], "title": "StoryState: Agent-Based State Control for Consistent and Editable Storybooks", "comment": null, "summary": "Large multimodal models have enabled one-click storybook generation, where users provide a short description and receive a multi-page illustrated story. However, the underlying story state, such as characters, world settings, and page-level objects, remains implicit, making edits coarse-grained and often breaking visual consistency. We present StoryState, an agent-based orchestration layer that introduces an explicit and editable story state on top of training-free text-to-image generation. StoryState represents each story as a structured object composed of a character sheet, global settings, and per-page scene constraints, and employs a small set of LLM agents to maintain this state and derive 1Prompt1Story-style prompts for generation and editing. Operating purely through prompts, StoryState is model-agnostic and compatible with diverse generation backends. System-level experiments on multi-page editing tasks show that StoryState enables localized page edits, improves cross-page consistency, and reduces unintended changes, interaction turns, and editing time compared to 1Prompt1Story, while approaching the one-shot consistency of Gemini Storybook. Code is available at https://github.com/YuZhenyuLindy/StoryState", "AI": {"tldr": "StoryState introduces an explicit, editable story state layer for multimodal story generation, enabling localized edits and better visual consistency compared to existing methods.", "motivation": "Current multimodal story generation lacks explicit story state representation, making edits coarse-grained and breaking visual consistency across pages.", "method": "Agent-based orchestration layer with structured story state (character sheet, global settings, per-page constraints) and LLM agents to maintain state and generate prompts for training-free text-to-image generation.", "result": "Enables localized page edits, improves cross-page consistency, reduces unintended changes, interaction turns, and editing time compared to 1Prompt1Story, approaching Gemini Storybook's one-shot consistency.", "conclusion": "StoryState provides a model-agnostic, prompt-based solution for maintaining explicit story state in multimodal story generation, enabling more precise editing and better visual consistency."}}
{"id": "2602.01306", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.01306", "abs": "https://arxiv.org/abs/2602.01306", "authors": ["Ayushman Sarkar", "Zhenyu Yu", "Mohd Yamani Idna Idris"], "title": "DeCorStory: Gram-Schmidt Prompt Embedding Decorrelation for Consistent Storytelling", "comment": null, "summary": "Maintaining visual and semantic consistency across frames is a key challenge in text-to-image storytelling. Existing training-free methods, such as One-Prompt-One-Story, concatenate all prompts into a single sequence, which often induces strong embedding correlation and leads to color leakage, background blending, and identity drift. We propose DeCorStory, a training-free inference-time framework that explicitly reduces inter-frame semantic interference. DeCorStory applies Gram-Schmidt prompt embedding decorrelation to orthogonalize frame-level semantics, followed by singular value reweighting to strengthen prompt-specific information and identity-preserving cross-attention to stabilize character identity during diffusion. The method requires no model modification or fine-tuning and can be seamlessly integrated into existing diffusion pipelines. Experiments demonstrate consistent improvements in prompt-image alignment, identity consistency, and visual diversity, achieving state-of-the-art performance among training-free baselines. Code is available at: https://github.com/YuZhenyuLindy/DeCorStory", "AI": {"tldr": "DeCorStory is a training-free inference framework that reduces semantic interference in text-to-image storytelling by decorrelating prompt embeddings, strengthening prompt-specific information, and preserving character identity.", "motivation": "Existing training-free methods for text-to-image storytelling suffer from visual and semantic inconsistencies across frames due to strong embedding correlation, causing problems like color leakage, background blending, and identity drift.", "method": "DeCorStory uses Gram-Schmidt prompt embedding decorrelation to orthogonalize frame-level semantics, singular value reweighting to strengthen prompt-specific information, and identity-preserving cross-attention to stabilize character identity during diffusion inference.", "result": "The method achieves consistent improvements in prompt-image alignment, identity consistency, and visual diversity, reaching state-of-the-art performance among training-free baselines.", "conclusion": "DeCorStory provides an effective training-free solution for maintaining visual and semantic consistency in text-to-image storytelling without requiring model modifications or fine-tuning."}}
{"id": "2602.01329", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.01329", "abs": "https://arxiv.org/abs/2602.01329", "authors": ["Divya Jyoti Bajpai", "Shubham Agarwal", "Apoorv Saxena", "Kuldeep Kulkarni", "Subrata Mitra", "Manjesh Kumar Hanawal"], "title": "FlowCast: Trajectory Forecasting for Scalable Zero-Cost Speculative Flow Matching", "comment": "Accepted at International Conference on Learning Representations (ICLR 2026)", "summary": "Flow Matching (FM) has recently emerged as a powerful approach for high-quality visual generation. However, their prohibitively slow inference due to a large number of denoising steps limits their potential use in real-time or interactive applications. Existing acceleration methods, like distillation, truncation, or consistency training, either degrade quality, incur costly retraining, or lack generalization. We propose FlowCast, a training-free speculative generation framework that accelerates inference by exploiting the fact that FM models are trained to preserve constant velocity. FlowCast speculates future velocity by extrapolating current velocity without incurring additional time cost, and accepts it if it is within a mean-squared error threshold. This constant-velocity forecasting allows redundant steps in stable regions to be aggressively skipped while retaining precision in complex ones. FlowCast is a plug-and-play framework that integrates seamlessly with any FM model and requires no auxiliary networks. We also present a theoretical analysis and bound the worst-case deviation between speculative and full FM trajectories. Empirical evaluations demonstrate that FlowCast achieves $>2.5\\times$ speedup in image generation, video generation, and editing tasks, outperforming existing baselines with no quality loss as compared to standard full generation.", "AI": {"tldr": "FlowCast is a training-free speculative generation framework that accelerates Flow Matching models by exploiting their constant-velocity property to skip redundant denoising steps, achieving >2.5\u00d7 speedup without quality loss.", "motivation": "Flow Matching models produce high-quality visual generation but suffer from slow inference due to many denoising steps, limiting real-time applications. Existing acceleration methods degrade quality, require costly retraining, or lack generalization.", "method": "FlowCast speculates future velocity by extrapolating current velocity without additional time cost, accepting it if within a mean-squared error threshold. This allows skipping redundant steps in stable regions while maintaining precision in complex areas. It's plug-and-play with any FM model and requires no auxiliary networks.", "result": "FlowCast achieves >2.5\u00d7 speedup in image generation, video generation, and editing tasks, outperforming existing baselines with no quality loss compared to standard full generation. Theoretical analysis bounds worst-case deviation between speculative and full FM trajectories.", "conclusion": "FlowCast provides an effective training-free acceleration framework for Flow Matching models that maintains quality while significantly improving inference speed, making FM models more practical for real-time applications."}}
{"id": "2602.01334", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.01334", "abs": "https://arxiv.org/abs/2602.01334", "authors": ["Yan Ma", "Weiyu Zhang", "Tianle Li", "Linge Du", "Xuyang Shen", "Pengfei Liu"], "title": "What Does Vision Tool-Use Reinforcement Learning Really Learn? Disentangling Tool-Induced and Intrinsic Effects for Crop-and-Zoom", "comment": "code: https://github.com/GAIR-NLP/Med", "summary": "Vision tool-use reinforcement learning (RL) can equip vision-language models with visual operators such as crop-and-zoom and achieves strong performance gains, yet it remains unclear whether these gains are driven by improvements in tool use or evolving intrinsic capabilities.We introduce MED (Measure-Explain-Diagnose), a coarse-to-fine framework that disentangles intrinsic capability changes from tool-induced effects, decomposes the tool-induced performance difference into gain and harm terms, and probes the mechanisms driving their evolution. Across checkpoint-level analyses on two VLMs with different tool priors and six benchmarks, we find that improvements are dominated by intrinsic learning, while tool-use RL mainly reduces tool-induced harm (e.g., fewer call-induced errors and weaker tool schema interference) and yields limited progress in tool-based correction of intrinsic failures. Overall, current vision tool-use RL learns to coexist safely with tools rather than master them.", "AI": {"tldr": "Current vision tool-use RL mainly learns to coexist safely with tools rather than master them, with improvements dominated by intrinsic learning rather than tool mastery.", "motivation": "To understand whether performance gains in vision tool-use RL come from improved tool use or evolving intrinsic capabilities, and to disentangle these effects.", "method": "Introduces MED (Measure-Explain-Diagnose), a coarse-to-fine framework that: 1) disentangles intrinsic capability changes from tool-induced effects, 2) decomposes tool-induced performance difference into gain and harm terms, and 3) probes mechanisms driving their evolution.", "result": "Across two VLMs with different tool priors and six benchmarks, improvements are dominated by intrinsic learning. Tool-use RL mainly reduces tool-induced harm (fewer call-induced errors, weaker tool schema interference) and yields limited progress in tool-based correction of intrinsic failures.", "conclusion": "Current vision tool-use RL learns to coexist safely with tools rather than master them, suggesting a need for approaches that focus more on active tool mastery rather than just harm reduction."}}
{"id": "2602.01335", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.01335", "abs": "https://arxiv.org/abs/2602.01335", "authors": ["Yu Xu", "Yuxin Zhang", "Juan Cao", "Lin Gao", "Chunyu Wang", "Oliver Deussen", "Tong-Yee Lee", "Fan Tang"], "title": "Beyond Pixels: Visual Metaphor Transfer via Schema-Driven Agentic Reasoning", "comment": "11 pages, 10 figures", "summary": "A visual metaphor constitutes a high-order form of human creativity, employing cross-domain semantic fusion to transform abstract concepts into impactful visual rhetoric. Despite the remarkable progress of generative AI, existing models remain largely confined to pixel-level instruction alignment and surface-level appearance preservation, failing to capture the underlying abstract logic necessary for genuine metaphorical generation. To bridge this gap, we introduce the task of Visual Metaphor Transfer (VMT), which challenges models to autonomously decouple the \"creative essence\" from a reference image and re-materialize that abstract logic onto a user-specified target subject. We propose a cognitive-inspired, multi-agent framework that operationalizes Conceptual Blending Theory (CBT) through a novel Schema Grammar (\"G\"). This structured representation decouples relational invariants from specific visual entities, providing a rigorous foundation for cross-domain logic re-instantiation. Our pipeline executes VMT through a collaborative system of specialized agents: a perception agent that distills the reference into a schema, a transfer agent that maintains generic space invariance to discover apt carriers, a generation agent for high-fidelity synthesis and a hierarchical diagnostic agent that mimics a professional critic, performing closed-loop backtracking to identify and rectify errors across abstract logic, component selection, and prompt encoding. Extensive experiments and human evaluations demonstrate that our method significantly outperforms SOTA baselines in metaphor consistency, analogy appropriateness, and visual creativity, paving the way for automated high-impact creative applications in advertising and media. Source code will be made publicly available.", "AI": {"tldr": "The paper introduces Visual Metaphor Transfer (VMT) task and proposes a multi-agent framework based on Conceptual Blending Theory to generate visual metaphors by transferring abstract creative logic from reference images to target subjects.", "motivation": "Current generative AI models fail to capture the underlying abstract logic needed for genuine metaphorical generation, being limited to pixel-level alignment and surface appearance preservation. There's a gap in creating visual metaphors that require cross-domain semantic fusion.", "method": "Proposes a cognitive-inspired multi-agent framework using Conceptual Blending Theory with a novel Schema Grammar (\"G\") to decouple relational invariants from visual entities. The pipeline includes: perception agent (distills reference into schema), transfer agent (maintains generic space invariance), generation agent (high-fidelity synthesis), and hierarchical diagnostic agent (mimics professional critic with closed-loop backtracking).", "result": "Extensive experiments and human evaluations show the method significantly outperforms state-of-the-art baselines in metaphor consistency, analogy appropriateness, and visual creativity.", "conclusion": "The framework successfully bridges the gap in visual metaphor generation, enabling automated high-impact creative applications in advertising and media by transferring abstract creative logic across domains."}}
{"id": "2602.01340", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.01340", "abs": "https://arxiv.org/abs/2602.01340", "authors": ["Yubo Dong", "Linchao Zhu"], "title": "MTC-VAE: Multi-Level Temporal Compression with Content Awareness", "comment": null, "summary": "Latent Video Diffusion Models (LVDMs) rely on Variational Autoencoders (VAEs) to compress videos into compact latent representations. For continuous Variational Autoencoders (VAEs), achieving higher compression rates is desirable; yet, the efficiency notably declines when extra sampling layers are added without expanding the dimensions of hidden channels. In this paper, we present a technique to convert fixed compression rate VAEs into models that support multi-level temporal compression, providing a straightforward and minimal fine-tuning approach to counteract performance decline at elevated compression rates.Moreover, we examine how varying compression levels impact model performance over video segments with diverse characteristics, offering empirical evidence on the effectiveness of our proposed approach. We also investigate the integration of our multi-level temporal compression VAE with diffusion-based generative models, DiT, highlighting successful concurrent training and compatibility within these frameworks. This investigation illustrates the potential uses of multi-level temporal compression.", "AI": {"tldr": "A method to convert fixed compression rate VAEs into models supporting multi-level temporal compression for video diffusion models, with minimal fine-tuning to maintain performance at higher compression rates.", "motivation": "Current Latent Video Diffusion Models (LVDMs) use VAEs for video compression, but face performance decline when adding extra sampling layers without expanding hidden channel dimensions. There's a need for more efficient compression while maintaining model performance.", "method": "Proposed technique converts fixed compression rate VAEs into models supporting multi-level temporal compression through straightforward minimal fine-tuning. Also investigates integration with diffusion-based generative models (DiT) and examines how varying compression levels impact performance across different video segments.", "result": "The approach successfully counteracts performance decline at elevated compression rates, demonstrates compatibility with diffusion frameworks, and provides empirical evidence on effectiveness across diverse video characteristics.", "conclusion": "Multi-level temporal compression VAEs offer potential for more efficient video representation in diffusion models, with successful integration into existing frameworks and minimal training requirements."}}
{"id": "2602.01345", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.01345", "abs": "https://arxiv.org/abs/2602.01345", "authors": ["Yu Zhang", "Jingyi Liu", "Feng Liu", "Duoqian Miao", "Qi Zhang", "Kexue Fu", "Changwei Wang", "Longbing Cao"], "title": "Adaptive Visual Autoregressive Acceleration via Dual-Linkage Entropy Analysis", "comment": "11 pages, 8 figures", "summary": "Visual AutoRegressive modeling (VAR) suffers from substantial computational cost due to the massive token count involved. Failing to account for the continuous evolution of modeling dynamics, existing VAR token reduction methods face three key limitations: heuristic stage partition, non-adaptive schedules, and limited acceleration scope, thereby leaving significant acceleration potential untapped. Since entropy variation intrinsically reflects the transition of predictive uncertainty, it offers a principled measure to capture modeling dynamics evolution. Therefore, we propose NOVA, a training-free token reduction acceleration framework for VAR models via entropy analysis. NOVA adaptively determines the acceleration activation scale during inference by online identifying the inflection point of scale entropy growth. Through scale-linkage and layer-linkage ratio adjustment, NOVA dynamically computes distinct token reduction ratios for each scale and layer, pruning low-entropy tokens while reusing the cache derived from the residuals at the prior scale to accelerate inference and maintain generation quality. Extensive experiments and analyses validate NOVA as a simple yet effective training-free acceleration framework.", "AI": {"tldr": "NOVA is a training-free token reduction framework for Visual AutoRegressive models that uses entropy analysis to adaptively prune low-entropy tokens and accelerate inference while maintaining quality.", "motivation": "Existing VAR token reduction methods have three key limitations: heuristic stage partition, non-adaptive schedules, and limited acceleration scope, leaving significant acceleration potential untapped. Since entropy variation reflects predictive uncertainty evolution, it offers a principled measure to capture modeling dynamics.", "method": "NOVA adaptively determines acceleration activation scale during inference by online identifying the inflection point of scale entropy growth. Through scale-linkage and layer-linkage ratio adjustment, it dynamically computes distinct token reduction ratios for each scale and layer, pruning low-entropy tokens while reusing cache from prior scale residuals.", "result": "Extensive experiments and analyses validate NOVA as a simple yet effective training-free acceleration framework for VAR models.", "conclusion": "NOVA provides an effective training-free solution for accelerating VAR models through principled entropy-based token reduction, addressing limitations of existing methods while maintaining generation quality."}}
{"id": "2602.01352", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.01352", "abs": "https://arxiv.org/abs/2602.01352", "authors": ["Xingzu Zhan", "Chen Xie", "Honghang Chen", "Yixun Lin", "Xiaochun Mai"], "title": "T2M Mamba: Motion Periodicity-Saliency Coupling Approach for Stable Text-Driven Motion Generation", "comment": "8 pages,5 figures", "summary": "Text-to-motion generation, which converts motion language descriptions into coherent 3D human motion sequences, has attracted increasing attention in fields, such as avatar animation and humanoid robotic interaction. Though existing models have achieved significant fidelity, they still suffer from two core limitations: (i) They treat motion periodicity and keyframe saliency as independent factors, overlooking their coupling and causing generation drift in long sequences. (ii) They are fragile to semantically equivalent paraphrases, where minor synonym substitutions distort textual embeddings, propagating through the decoder and producing unstable or erroneous motions. In this work, we propose T2M Mamba to address these limitations by (i) proposing Periodicity-Saliency Aware Mamba, which utilizes novel algorithms for keyframe weight estimation via enhanced Density Peaks Clustering and motion periodicity estimation via FFT-accelerated autocorrelation to capture coupled dynamics with minimal computational overhead, and (ii) constructing a Periodic Differential Cross-modal Alignment Module (PDCAM) to enhance robust alignment of textual and motion embeddings. Extensive experiments on HumanML3D and KIT-ML datasets have been conducted, confirming the effectiveness of our approach, achieving an FID of 0.068 and consistent gains on all other metrics.", "AI": {"tldr": "T2M Mamba: A text-to-motion generation model that addresses periodicity-saliency coupling and paraphrase fragility using Mamba architecture with novel algorithms for keyframe weight estimation and motion periodicity detection.", "motivation": "Existing text-to-motion models suffer from two core limitations: (1) treating motion periodicity and keyframe saliency as independent factors, causing generation drift in long sequences, and (2) fragility to semantically equivalent paraphrases where minor synonym substitutions distort textual embeddings and produce unstable motions.", "method": "Proposes T2M Mamba with two key components: (1) Periodicity-Saliency Aware Mamba using novel algorithms for keyframe weight estimation via enhanced Density Peaks Clustering and motion periodicity estimation via FFT-accelerated autocorrelation, and (2) Periodic Differential Cross-modal Alignment Module (PDCAM) to enhance robust alignment of textual and motion embeddings.", "result": "Extensive experiments on HumanML3D and KIT-ML datasets show effectiveness, achieving an FID of 0.068 and consistent gains on all other metrics.", "conclusion": "T2M Mamba successfully addresses the limitations of existing text-to-motion models by capturing coupled periodicity-saliency dynamics and improving robustness to paraphrases through novel architectural components and alignment mechanisms."}}
{"id": "2602.01369", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.01369", "abs": "https://arxiv.org/abs/2602.01369", "authors": ["Songping Wang", "Qinglong Liu", "Yueming Lyu", "Ning Li", "Ziwen He", "Caifeng Shan"], "title": "Exposing and Defending the Achilles' Heel of Video Mixture-of-Experts", "comment": null, "summary": "Mixture-of-Experts (MoE) has demonstrated strong performance in video understanding tasks, yet its adversarial robustness remains underexplored. Existing attack methods often treat MoE as a unified architecture, overlooking the independent and collaborative weaknesses of key components such as routers and expert modules. To fill this gap, we propose Temporal Lipschitz-Guided Attacks (TLGA) to thoroughly investigate component-level vulnerabilities in video MoE models. We first design attacks on the router, revealing its independent weaknesses. Building on this, we introduce Joint Temporal Lipschitz-Guided Attacks (J-TLGA), which collaboratively perturb both routers and experts. This joint attack significantly amplifies adversarial effects and exposes the Achilles' Heel (collaborative weaknesses) of the MoE architecture. Based on these insights, we further propose Joint Temporal Lipschitz Adversarial Training (J-TLAT). J-TLAT performs joint training to further defend against collaborative weaknesses, enhancing component-wise robustness. Our framework is plug-and-play and reduces inference cost by more than 60% compared with dense models. It consistently enhances adversarial robustness across diverse datasets and architectures, effectively mitigating both the independent and collaborative weaknesses of MoE.", "AI": {"tldr": "TLGA framework exposes vulnerabilities in video MoE models through component-level attacks on routers and experts, then proposes J-TLAT defense that enhances robustness while reducing inference costs.", "motivation": "MoE models show strong video understanding performance but their adversarial robustness is underexplored. Existing attacks treat MoE as unified architecture, missing component-level vulnerabilities in routers and experts.", "method": "Proposes Temporal Lipschitz-Guided Attacks (TLGA) to investigate component vulnerabilities: 1) attacks on router to reveal independent weaknesses, 2) Joint TLGA (J-TLGA) that collaboratively perturbs both routers and experts to expose collaborative weaknesses, and 3) Joint Temporal Lipschitz Adversarial Training (J-TLAT) for defense.", "result": "J-TLGA significantly amplifies adversarial effects, exposing MoE's Achilles' Heel. J-TLAT enhances adversarial robustness across diverse datasets/architectures, reduces inference cost by >60% vs dense models, and mitigates both independent and collaborative weaknesses.", "conclusion": "The framework provides comprehensive analysis of MoE vulnerabilities at component level, offers effective defense through joint adversarial training, and maintains efficiency with plug-and-play design that significantly reduces computational costs."}}
{"id": "2602.01370", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.01370", "abs": "https://arxiv.org/abs/2602.01370", "authors": ["Leonardo Brusini", "Cristian Sbrolli", "Eugenio Lomurno", "Toshihiko Yamasaki", "Matteo Matteucci"], "title": "PolyGen: Fully Synthetic Vision-Language Training via Multi-Generator Ensembles", "comment": null, "summary": "Synthetic data offers a scalable solution for vision-language pre-training, yet current state-of-the-art methods typically rely on scaling up a single generative backbone, which introduces generator-specific spectral biases and limits feature diversity. In this work, we introduce PolyGen, a framework that redefines synthetic data construction by prioritizing manifold coverage and compositional rigor over simple dataset size. PolyGen employs a Polylithic approach to train on the intersection of architecturally distinct generators, effectively marginalizing out model-specific artifacts. Additionally, we introduce a Programmatic Hard Negative curriculum that enforces fine-grained syntactic understanding. By structurally reallocating the same data budget from unique captions to multi-source variations, PolyGen achieves a more robust feature space, outperforming the leading single-source baseline (SynthCLIP) by +19.0% on aggregate multi-task benchmarks and on the SugarCrepe++ compositionality benchmark (+9.1%). These results demonstrate that structural diversity is a more data-efficient scaling law than simply increasing the volume of single-source samples.", "AI": {"tldr": "PolyGen introduces a framework using multiple distinct generators for synthetic data creation, focusing on manifold coverage over dataset size, achieving better performance than single-source methods.", "motivation": "Current synthetic data methods rely on scaling up single generative backbones, which introduces generator-specific biases and limits feature diversity. There's a need for approaches that prioritize structural diversity over simple data volume.", "method": "PolyGen uses a Polylithic approach that trains on the intersection of architecturally distinct generators to marginalize out model-specific artifacts. It also employs a Programmatic Hard Negative curriculum to enforce fine-grained syntactic understanding, reallocating data budget from unique captions to multi-source variations.", "result": "PolyGen outperforms the leading single-source baseline (SynthCLIP) by +19.0% on aggregate multi-task benchmarks and +9.1% on the SugarCrepe++ compositionality benchmark, demonstrating superior performance with the same data budget.", "conclusion": "Structural diversity is a more data-efficient scaling law than simply increasing the volume of single-source samples. Prioritizing manifold coverage and compositional rigor over dataset size leads to more robust feature spaces."}}
{"id": "2602.01382", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.01382", "abs": "https://arxiv.org/abs/2602.01382", "authors": ["Fu-Yun Wang", "Han Zhang", "Michael Gharbi", "Hongsheng Li", "Taesung Park"], "title": "PromptRL: Prompt Matters in RL for Flow-Based Image Generation", "comment": null, "summary": "Flow matching models (FMs) have revolutionized text-to-image (T2I) generation, with reinforcement learning (RL) serving as a critical post-training strategy for alignment with reward objectives. In this research, we show that current RL pipelines for FMs suffer from two underappreciated yet important limitations: sample inefficiency due to insufficient generation diversity, and pronounced prompt overfitting, where models memorize specific training formulations and exhibit dramatic performance collapse when evaluated on semantically equivalent but stylistically varied prompts. We present PromptRL (Prompt Matters in RL for Flow-Based Image Generation), a framework that incorporates language models (LMs) as trainable prompt refinement agents directly within the flow-based RL optimization loop. This design yields two complementary benefits: rapid development of sophisticated prompt rewriting capabilities and, critically, a synergistic training regime that reshapes the optimization dynamics. PromptRL achieves state-of-the-art performance across multiple benchmarks, obtaining scores of 0.97 on GenEval, 0.98 on OCR accuracy, and 24.05 on PickScore.\n  Furthermore, we validate the effectiveness of our RL approach on large-scale image editing models, improving the EditReward of FLUX.1-Kontext from 1.19 to 1.43 with only 0.06 million rollouts, surpassing Gemini 2.5 Flash Image (also known as Nano Banana), which scores 1.37, and achieving comparable performance with ReasonNet (1.44), which relied on fine-grained data annotations along with a complex multi-stage training. Our extensive experiments empirically demonstrate that PromptRL consistently achieves higher performance ceilings while requiring over 2$\\times$ fewer rollouts compared to naive flow-only RL. Our code is available at https://github.com/G-U-N/UniRL.", "AI": {"tldr": "PromptRL improves RL for flow matching models by using language models as prompt refinement agents, addressing sample inefficiency and prompt overfitting issues, achieving SOTA performance with 2\u00d7 fewer rollouts.", "motivation": "Current RL pipelines for flow matching models suffer from two key limitations: sample inefficiency due to insufficient generation diversity, and pronounced prompt overfitting where models memorize specific training formulations and fail on semantically equivalent but stylistically varied prompts.", "method": "PromptRL incorporates language models as trainable prompt refinement agents directly within the flow-based RL optimization loop, enabling rapid development of sophisticated prompt rewriting capabilities and creating a synergistic training regime that reshapes optimization dynamics.", "result": "Achieves SOTA performance: 0.97 on GenEval, 0.98 on OCR accuracy, 24.05 on PickScore. Improves EditReward of FLUX.1-Kontext from 1.19 to 1.43 with only 0.06M rollouts, surpassing Gemini 2.5 Flash Image (1.37) and matching ReasonNet (1.44). Requires over 2\u00d7 fewer rollouts than naive flow-only RL.", "conclusion": "PromptRL demonstrates that incorporating language models as prompt refinement agents within RL optimization loops effectively addresses key limitations of current FM RL pipelines, achieving superior performance with significantly improved sample efficiency across multiple benchmarks and applications."}}
{"id": "2602.01391", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.01391", "abs": "https://arxiv.org/abs/2602.01391", "authors": ["Xiaoyan Xing", "Xiao Zhang", "Sezer Karaoglu", "Theo Gevers", "Anand Bhattad"], "title": "Stronger Semantic Encoders Can Harm Relighting Performance: Probing Visual Priors via Augmented Latent Intrinsics", "comment": "Project page: https:\\\\augmented-latent-intrinsics.github.io", "summary": "Image-to-image relighting requires representations that disentangle scene properties from illumination. Recent methods rely on latent intrinsic representations but remain under-constrained and often fail on challenging materials such as metal and glass. A natural hypothesis is that stronger pretrained visual priors should resolve these failures. We find the opposite: features from top-performing semantic encoders often degrade relighting quality, revealing a fundamental trade-off between semantic abstraction and photometric fidelity. We study this trade-off and introduce Augmented Latent Intrinsics (ALI), which balances semantic context and dense photometric structure by fusing features from a pixel-aligned visual encoder into a latent-intrinsic framework, together with a self-supervised refinement strategy to mitigate the scarcity of paired real-world data. Trained only on unlabeled real-world image pairs and paired with a dense, pixel-aligned visual prior, ALI achieves strong improvements in relighting, with the largest gains on complex, specular materials. Project page: https:\\\\augmented-latent-intrinsics.github.io", "AI": {"tldr": "ALI (Augmented Latent Intrinsics) improves image-to-image relighting by balancing semantic abstraction with photometric fidelity through fusion of pixel-aligned visual features and self-supervised refinement.", "motivation": "Current latent intrinsic methods for image relighting fail on challenging materials like metal and glass. Surprisingly, stronger semantic encoders degrade relighting quality, revealing a trade-off between semantic abstraction and photometric fidelity that needs to be addressed.", "method": "ALI fuses features from a pixel-aligned visual encoder into a latent-intrinsic framework and uses self-supervised refinement to handle scarce paired real-world data. It balances semantic context with dense photometric structure.", "result": "ALI achieves strong improvements in relighting quality, with the largest gains on complex, specular materials like metal and glass, outperforming previous methods.", "conclusion": "The paper demonstrates that balancing semantic abstraction with photometric fidelity through feature fusion and self-supervised refinement significantly improves image relighting, especially for challenging materials."}}
{"id": "2602.01418", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.01418", "abs": "https://arxiv.org/abs/2602.01418", "authors": ["Christoffer Koo \u00d8hrstr\u00f8m", "Rafael I. Cabral Muchacho", "Yifei Dong", "Filippos Moumtzidellis", "Ronja G\u00fcldenring", "Florian T. Pokorny", "Lazaros Nalpantidis"], "title": "Where to Attend: A Principled Vision-Centric Position Encoding with Parabolas", "comment": null, "summary": "We propose Parabolic Position Encoding (PaPE), a parabola-based position encoding for vision modalities in attention-based architectures. Given a set of vision tokens-such as images, point clouds, videos, or event camera streams-our objective is to encode their positions while accounting for the characteristics of vision modalities. Prior works have largely extended position encodings from 1D-sequences in language to nD-structures in vision, but only with partial account of vision characteristics. We address this gap by designing PaPE from principles distilled from prior work: translation invariance, rotation invariance (PaPE-RI), distance decay, directionality, and context awareness. We evaluate PaPE on 8 datasets that span 4 modalities. We find that either PaPE or PaPE-RI achieves the top performance on 7 out of 8 datasets. Extrapolation experiments on ImageNet-1K show that PaPE extrapolates remarkably well, improving in absolute terms by up to 10.5% over the next-best position encoding. Code is available at https://github.com/DTU-PAS/parabolic-position-encoding.", "AI": {"tldr": "Parabolic Position Encoding (PaPE) is a novel position encoding method for vision transformers that uses parabola-based encoding to better capture vision modality characteristics like translation/rotation invariance and distance decay.", "motivation": "Existing position encodings for vision transformers are largely extensions from 1D language models to nD vision structures, but they don't fully account for unique vision characteristics like translation invariance, rotation invariance, and spatial relationships.", "method": "PaPE is designed from five principles: translation invariance, rotation invariance (PaPE-RI variant), distance decay, directionality, and context awareness. It uses parabola-based encoding to capture these vision-specific properties across different modalities (images, point clouds, videos, event streams).", "result": "PaPE or its rotation-invariant variant PaPE-RI achieves top performance on 7 out of 8 datasets spanning 4 vision modalities. On ImageNet-1K extrapolation experiments, PaPE improves by up to 10.5% absolute over the next-best position encoding.", "conclusion": "PaPE effectively addresses the gap in vision-specific position encoding by incorporating key vision characteristics, demonstrating superior performance across multiple modalities and strong extrapolation capabilities."}}
{"id": "2602.01435", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.01435", "abs": "https://arxiv.org/abs/2602.01435", "authors": ["Soumyaroop Nandi", "Prem Natarajan"], "title": "BioTamperNet: Affinity-Guided State-Space Model Detecting Tampered Biomedical Images", "comment": null, "summary": "We propose BioTamperNet, a novel framework for detecting duplicated regions in tampered biomedical images, leveraging affinity-guided attention inspired by State Space Model (SSM) approximations. Existing forensic models, primarily trained on natural images, often underperform on biomedical data where subtle manipulations can compromise experimental validity. To address this, BioTamperNet introduces an affinity-guided self-attention module to capture intra-image similarities and an affinity-guided cross-attention module to model cross-image correspondences. Our design integrates lightweight SSM-inspired linear attention mechanisms to enable efficient, fine-grained localization. Trained end-to-end, BioTamperNet simultaneously identifies tampered regions and their source counterparts. Extensive experiments on the benchmark bio-forensic datasets demonstrate significant improvements over competitive baselines in accurately detecting duplicated regions. Code - https://github.com/SoumyaroopNandi/BioTamperNet", "AI": {"tldr": "BioTamperNet is a novel framework for detecting duplicated regions in tampered biomedical images using affinity-guided attention inspired by State Space Models, outperforming existing methods on biomedical data.", "motivation": "Existing forensic models trained on natural images often underperform on biomedical data where subtle manipulations can compromise experimental validity, creating a need for specialized biomedical image tampering detection.", "method": "Introduces affinity-guided self-attention to capture intra-image similarities and affinity-guided cross-attention to model cross-image correspondences, integrating lightweight SSM-inspired linear attention mechanisms for efficient, fine-grained localization.", "result": "Extensive experiments on benchmark bio-forensic datasets demonstrate significant improvements over competitive baselines in accurately detecting duplicated regions in biomedical images.", "conclusion": "BioTamperNet effectively addresses the limitations of existing forensic models on biomedical data through specialized attention mechanisms, enabling simultaneous identification of tampered regions and their source counterparts with high accuracy."}}
{"id": "2602.01452", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.01452", "abs": "https://arxiv.org/abs/2602.01452", "authors": ["Penghao Deng", "Jidong J. Yang", "Jiachen Bian"], "title": "Cross-Paradigm Evaluation of Gaze-Based Semantic Object Identification for Intelligent Vehicles", "comment": "21 pages, 15 figures, 3 tables", "summary": "Understanding where drivers direct their visual attention during driving, as characterized by gaze behavior, is critical for developing next-generation advanced driver-assistance systems and improving road safety. This paper tackles this challenge as a semantic identification task from the road scenes captured by a vehicle's front-view camera. Specifically, the collocation of gaze points with object semantics is investigated using three distinct vision-based approaches: direct object detection (YOLOv13), segmentation-assisted classification (SAM2 paired with EfficientNetV2 versus YOLOv13), and query-based Vision-Language Models, VLMs (Qwen2.5-VL-7b versus Qwen2.5-VL-32b). The results demonstrate that the direct object detection (YOLOv13) and Qwen2.5-VL-32b significantly outperform other approaches, achieving Macro F1-Scores over 0.84. The large VLM (Qwen2.5-VL-32b), in particular, exhibited superior robustness and performance for identifying small, safety-critical objects such as traffic lights, especially in adverse nighttime conditions. Conversely, the segmentation-assisted paradigm suffers from a \"part-versus-whole\" semantic gap that led to large failure in recall. The results reveal a fundamental trade-off between the real-time efficiency of traditional detectors and the richer contextual understanding and robustness offered by large VLMs. These findings provide critical insights and practical guidance for the design of future human-aware intelligent driver monitoring systems.", "AI": {"tldr": "This paper compares three vision-based approaches for identifying where drivers look during driving by analyzing gaze points with object semantics, finding that direct object detection (YOLOv13) and large VLMs (Qwen2.5-VL-32b) perform best, with VLMs showing superior robustness for safety-critical objects.", "motivation": "Understanding driver visual attention patterns is crucial for developing better driver-assistance systems and improving road safety. The paper addresses this as a semantic identification task from road scenes to determine what objects drivers are looking at.", "method": "Three distinct vision-based approaches were tested: 1) direct object detection using YOLOv13, 2) segmentation-assisted classification using SAM2 paired with EfficientNetV2 versus YOLOv13, and 3) query-based Vision-Language Models (VLMs) using Qwen2.5-VL-7b versus Qwen2.5-VL-32b.", "result": "Direct object detection (YOLOv13) and Qwen2.5-VL-32b significantly outperformed other approaches, achieving Macro F1-Scores over 0.84. The large VLM showed superior robustness for identifying small, safety-critical objects like traffic lights, especially in nighttime conditions. Segmentation-assisted approaches suffered from \"part-versus-whole\" semantic gaps leading to poor recall.", "conclusion": "There's a fundamental trade-off between real-time efficiency of traditional detectors and richer contextual understanding/robustness offered by large VLMs. These findings provide practical guidance for designing future human-aware intelligent driver monitoring systems."}}
{"id": "2602.01459", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.01459", "abs": "https://arxiv.org/abs/2602.01459", "authors": ["Joey Kuang", "Alexander Wong"], "title": "Understanding vision transformer robustness through the lens of out-of-distribution detection", "comment": "Accepted to JCVIS 2025", "summary": "Vision transformers have shown remarkable performance in vision tasks, but enabling them for accessible and real-time use is still challenging. Quantization reduces memory and inference costs at the risk of performance loss. Strides have been made to mitigate low precision issues mainly by understanding in-distribution (ID) task behaviour, but the attention mechanism may provide insight on quantization attributes by exploring out-of-distribution (OOD) situations. We investigate the behaviour of quantized small-variant popular vision transformers (DeiT, DeiT3, and ViT) on common OOD datasets. ID analyses show the initial instabilities of 4-bit models, particularly of those trained on the larger ImageNet-22k, as the strongest FP32 model, DeiT3, sharply drop 17% from quantization error to be one of the weakest 4-bit models. While ViT shows reasonable quantization robustness for ID calibration, OOD detection reveals more: ViT and DeiT3 pretrained on ImageNet-22k respectively experienced a 15.0% and 19.2% average quantization delta in AUPR-out between full precision to 4-bit while their ImageNet-1k-only counterparts experienced a 9.5% and 12.0% delta. Overall, our results suggest pretraining on large scale datasets may hinder low-bit quantization robustness in OOD detection and that data augmentation may be a more beneficial option.", "AI": {"tldr": "Vision transformers show quantization sensitivity, especially for OOD detection, with large-scale pretraining hurting 4-bit quantization robustness more than smaller datasets.", "motivation": "To understand how quantization affects vision transformers' performance, particularly in out-of-distribution (OOD) scenarios, since previous work mainly focused on in-distribution behavior and the attention mechanism might reveal quantization attributes through OOD analysis.", "method": "Investigating quantized small-variant vision transformers (DeiT, DeiT3, ViT) on common OOD datasets, comparing 4-bit quantization to full precision (FP32), analyzing both in-distribution and out-of-distribution detection performance.", "result": "Large-scale pretraining (ImageNet-22k) leads to worse quantization robustness: DeiT3 drops 17% in ID performance and shows 19.2% AUPR-out delta for OOD detection. Models trained on smaller datasets (ImageNet-1k) show better quantization tolerance (9.5-12.0% delta). ViT shows reasonable ID quantization robustness but reveals OOD sensitivity.", "conclusion": "Pretraining on large-scale datasets may hinder low-bit quantization robustness for OOD detection, suggesting data augmentation might be a better alternative than massive pretraining for quantization-friendly models."}}
{"id": "2602.01530", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.01530", "abs": "https://arxiv.org/abs/2602.01530", "authors": ["Parsa Esmaeilkhani", "Longin Jan Latecki"], "title": "Preserving Localized Patch Semantics in VLMs", "comment": null, "summary": "Logit Lens has been proposed for visualizing tokens that contribute most to LLM answers. Recently, Logit Lens was also shown to be applicable in autoregressive Vision-Language Models (VLMs), where it illustrates the conceptual content of image tokens in the form of heatmaps, e.g., which image tokens are likely to depict the concept of cat in a given image. However, the visual content of image tokens often gets diffused to language tokens, and consequently, the locality of visual information gets mostly destroyed, which renders Logit Lens visualization unusable for explainability. To address this issue, we introduce a complementary loss to next-token prediction (NTP) to prevent the visual tokens from losing the visual representation inherited from corresponding image patches. The proposed Logit Lens Loss (LLL) is designed to make visual token embeddings more semantically aligned with the textual concepts that describe their image regions (e.g., patches containing a cat with the word \"cat\"), without requiring any architectural modification or large-scale training. This way, LLL constrains the mixing of image and text tokens in the self-attention layers in order to prevent image tokens from losing their localized visual information. As our experiments show, LLL not only makes Logit Lens practically relevant by producing meaningful object confidence maps in images, but also improves performance on vision-centric tasks like segmentation without attaching any special heads.", "AI": {"tldr": "The paper introduces Logit Lens Loss (LLL) to improve Logit Lens visualization in Vision-Language Models by preventing visual tokens from losing localized visual information, enabling meaningful object confidence maps and better vision task performance.", "motivation": "Logit Lens visualization in VLMs suffers because visual content from image tokens gets diffused to language tokens, destroying locality of visual information and making Logit Lens unusable for explainability purposes.", "method": "Proposes Logit Lens Loss (LLL) - a complementary loss to next-token prediction that makes visual token embeddings semantically aligned with textual concepts describing their image regions, without architectural changes or large-scale training.", "result": "LLL makes Logit Lens practically relevant by producing meaningful object confidence maps in images and improves performance on vision-centric tasks like segmentation without special heads.", "conclusion": "The Logit Lens Loss effectively preserves localized visual information in image tokens, enabling better explainability through Logit Lens visualization and improving vision task performance in VLMs."}}
{"id": "2602.01533", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.01533", "abs": "https://arxiv.org/abs/2602.01533", "authors": ["Zhe Ling", "Sicheng Yu", "Danyu Yang"], "title": "Rotation-free Online Handwritten Character Recognition Using Linear Recurrent Units", "comment": null, "summary": "Online handwritten character recognition leverages stroke order and dynamic features, which generally provide higher accuracy and robustness compared with offline recognition. However, in practical applications, rotational deformations can disrupt the spatial layout of strokes, substantially reducing recognition accuracy. Extracting rotation-invariant features therefore remains a challenging open problem. In this work, we employ the Sliding Window Path Signature (SW-PS) to capture local structural features of characters, and introduce the lightweight Linear Recurrent Units (LRU) as the classifier. The LRU combine the fast incremental processing capability of recurrent neural networks (RNN) with the efficient parallel training of state space models (SSM), while reliably modelling dynamic stroke characteristics. We conducted recognition experiments with random rotation angle up to $\\pm 180^{\\circ}$ on three subsets of the CASIA-OLHWDB1.1 dataset: digits, English upper letters, and Chinese radicals. The accuracies achieved after ensemble learning were $99.62\\%$, $96.67\\%$, and $94.33\\%$, respectively. Experimental results demonstrate that the proposed SW-PS+LRU framework consistently surpasses competing models in both convergence speed and test accuracy.", "AI": {"tldr": "SW-PS+LRU framework achieves state-of-the-art rotation-invariant online handwritten character recognition using sliding window path signatures and linear recurrent units.", "motivation": "Online handwritten character recognition faces accuracy degradation due to rotational deformations that disrupt stroke spatial layout, making rotation-invariant feature extraction a challenging open problem.", "method": "Uses Sliding Window Path Signature (SW-PS) to capture local structural features and lightweight Linear Recurrent Units (LRU) as classifier, combining RNN's incremental processing with SSM's parallel training efficiency.", "result": "Achieved 99.62% accuracy on digits, 96.67% on English uppercase letters, and 94.33% on Chinese radicals with random rotation up to \u00b1180\u00b0 on CASIA-OLHWDB1.1 dataset.", "conclusion": "The SW-PS+LRU framework consistently surpasses competing models in both convergence speed and test accuracy for rotation-invariant online handwritten character recognition."}}
{"id": "2602.01538", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.01538", "abs": "https://arxiv.org/abs/2602.01538", "authors": ["Youliang Zhang", "Zhengguang Zhou", "Zhentao Yu", "Ziyao Huang", "Teng Hu", "Sen Liang", "Guozhen Zhang", "Ziqiao Peng", "Shunkai Li", "Yi Chen", "Zixiang Zhou", "Yuan Zhou", "Qinglin Lu", "Xiu Li"], "title": "Making Avatars Interact: Towards Text-Driven Human-Object Interaction for Controllable Talking Avatars", "comment": null, "summary": "Generating talking avatars is a fundamental task in video generation. Although existing methods can generate full-body talking avatars with simple human motion, extending this task to grounded human-object interaction (GHOI) remains an open challenge, requiring the avatar to perform text-aligned interactions with surrounding objects. This challenge stems from the need for environmental perception and the control-quality dilemma in GHOI generation. To address this, we propose a novel dual-stream framework, InteractAvatar, which decouples perception and planning from video synthesis for grounded human-object interaction. Leveraging detection to enhance environmental perception, we introduce a Perception and Interaction Module (PIM) to generate text-aligned interaction motions. Additionally, an Audio-Interaction Aware Generation Module (AIM) is proposed to synthesize vivid talking avatars performing object interactions. With a specially designed motion-to-video aligner, PIM and AIM share a similar network structure and enable parallel co-generation of motions and plausible videos, effectively mitigating the control-quality dilemma. Finally, we establish a benchmark, GroundedInter, for evaluating GHOI video generation. Extensive experiments and comparisons demonstrate the effectiveness of our method in generating grounded human-object interactions for talking avatars. Project page: https://interactavatar.github.io", "AI": {"tldr": "InteractAvatar: A dual-stream framework for generating talking avatars that perform text-aligned grounded human-object interactions, addressing environmental perception and control-quality challenges through parallel motion and video generation.", "motivation": "Existing talking avatar methods focus on simple human motion but cannot handle grounded human-object interactions (GHOI) which require environmental perception and text-aligned interactions with objects. The control-quality dilemma in GHOI generation needs to be addressed.", "method": "Proposes InteractAvatar with two main modules: 1) Perception and Interaction Module (PIM) for generating text-aligned interaction motions using detection-enhanced environmental perception, and 2) Audio-Interaction Aware Generation Module (AIM) for synthesizing vivid talking avatars performing object interactions. Uses a motion-to-video aligner to enable parallel co-generation of motions and videos.", "result": "Establishes GroundedInter benchmark for evaluating GHOI video generation. Extensive experiments demonstrate effectiveness in generating grounded human-object interactions for talking avatars.", "conclusion": "InteractAvatar successfully addresses the challenge of generating talking avatars with grounded human-object interactions by decoupling perception/planning from video synthesis, effectively mitigating the control-quality dilemma through parallel co-generation."}}
{"id": "2602.01540", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.01540", "abs": "https://arxiv.org/abs/2602.01540", "authors": ["Yuehai Chen"], "title": "FSCA-Net: Feature-Separated Cross-Attention Network for Robust Multi-Dataset Training", "comment": null, "summary": "Crowd counting plays a vital role in public safety, traffic regulation, and smart city management. However, despite the impressive progress achieved by CNN- and Transformer-based models, their performance often deteriorates when applied across diverse environments due to severe domain discrepancies. Direct joint training on multiple datasets, which intuitively should enhance generalization, instead results in negative transfer, as shared and domain-specific representations become entangled. To address this challenge, we propose the Feature Separation and Cross-Attention Network FSCA-Net, a unified framework that explicitly disentangles feature representations into domain-invariant and domain-specific components. A novel cross-attention fusion module adaptively models interactions between these components, ensuring effective knowledge transfer while preserving dataset-specific discriminability. Furthermore, a mutual information optimization objective is introduced to maximize consistency among domain-invariant features and minimize redundancy among domain-specific ones, promoting complementary shared-private representations. Extensive experiments on multiple crowd counting benchmarks demonstrate that FSCA-Net effectively mitigates negative transfer and achieves state-of-the-art cross-dataset generalization, providing a robust and scalable solution for real-world crowd analysis.", "AI": {"tldr": "FSCA-Net is a unified framework for crowd counting that disentangles domain-invariant and domain-specific features using cross-attention fusion and mutual information optimization to prevent negative transfer and improve cross-dataset generalization.", "motivation": "CNN- and Transformer-based crowd counting models suffer from performance degradation across diverse environments due to domain discrepancies. Joint training on multiple datasets causes negative transfer as shared and domain-specific representations become entangled, limiting generalization capability.", "method": "Proposes FSCA-Net with: 1) Feature separation into domain-invariant and domain-specific components, 2) Cross-attention fusion module to model interactions between these components, 3) Mutual information optimization objective to maximize consistency among domain-invariant features and minimize redundancy among domain-specific ones.", "result": "Extensive experiments on multiple crowd counting benchmarks demonstrate that FSCA-Net effectively mitigates negative transfer and achieves state-of-the-art cross-dataset generalization performance.", "conclusion": "FSCA-Net provides a robust and scalable solution for real-world crowd analysis by explicitly disentangling feature representations and enabling effective knowledge transfer while preserving dataset-specific discriminability."}}
{"id": "2602.01541", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.01541", "abs": "https://arxiv.org/abs/2602.01541", "authors": ["Boyi Li", "Yifan Shen", "Yuanzhe Liu", "Yifan Xu", "Jiateng Liu", "Xinzhuo Li", "Zhengyuan Li", "Jingyuan Zhu", "Yunhan Zhong", "Fangzhou Lan", "Jianguo Cao", "James M. Rehg", "Heng Ji", "Ismini Lourentzou", "Xu Cao"], "title": "Toward Cognitive Supersensing in Multimodal Large Language Model", "comment": null, "summary": "Multimodal Large Language Models (MLLMs) have achieved remarkable success in open-vocabulary perceptual tasks, yet their ability to solve complex cognitive problems remains limited, especially when visual details are abstract and require visual memory. Current approaches primarily scale Chain-of-Thought (CoT) reasoning in the text space, even when language alone is insufficient for clear and structured reasoning, and largely neglect visual reasoning mechanisms analogous to the human visuospatial sketchpad and visual imagery. To mitigate this deficiency, we introduce Cognitive Supersensing, a novel training paradigm that endows MLLMs with human-like visual imagery capabilities by integrating a Latent Visual Imagery Prediction (LVIP) head that jointly learns sequences of visual cognitive latent embeddings and aligns them with the answer, thereby forming vision-based internal reasoning chains. We further introduce a reinforcement learning stage that optimizes text reasoning paths based on this grounded visual latent. To evaluate the cognitive capabilities of MLLMs, we present CogSense-Bench, a comprehensive visual question answering (VQA) benchmark assessing five cognitive dimensions. Extensive experiments demonstrate that MLLMs trained with Cognitive Supersensing significantly outperform state-of-the-art baselines on CogSense-Bench and exhibit superior generalization on out-of-domain mathematics and science VQA benchmarks, suggesting that internal visual imagery is potentially key to bridging the gap between perceptual recognition and cognitive understanding. We will open-source the CogSense-Bench and our model weights.", "AI": {"tldr": "Cognitive Supersensing trains MLLMs with visual imagery capabilities using latent visual embeddings and reinforcement learning, achieving SOTA performance on cognitive VQA tasks.", "motivation": "Current MLLMs excel at perceptual tasks but struggle with complex cognitive problems requiring visual memory and reasoning. Existing approaches focus on text-based Chain-of-Thought reasoning even when visual details are abstract, neglecting human-like visual reasoning mechanisms.", "method": "Introduces Cognitive Supersensing with: 1) Latent Visual Imagery Prediction (LVIP) head that learns sequences of visual cognitive latent embeddings aligned with answers, forming vision-based internal reasoning chains; 2) Reinforcement learning stage that optimizes text reasoning paths based on grounded visual latent.", "result": "MLLMs trained with Cognitive Supersensing significantly outperform state-of-the-art baselines on CogSense-Bench (comprehensive VQA benchmark assessing five cognitive dimensions) and show superior generalization on out-of-domain mathematics and science VQA benchmarks.", "conclusion": "Internal visual imagery is key to bridging the gap between perceptual recognition and cognitive understanding in MLLMs. The approach demonstrates that human-like visual reasoning mechanisms can enhance cognitive capabilities beyond current text-based reasoning methods."}}
{"id": "2602.01559", "categories": ["cs.CV", "eess.IV"], "pdf": "https://arxiv.org/pdf/2602.01559", "abs": "https://arxiv.org/abs/2602.01559", "authors": ["Libo Zhu", "Zihan Zhou", "Zhiyi Zhou", "Yiyang Qu", "Weihang Zhang", "Keyu Shi", "Yifan Fu", "Yulun Zhang"], "title": "Combined Flicker-banding and Moire Removal for Screen-Captured Images", "comment": null, "summary": "Capturing display screens with mobile devices has become increasingly common, yet the resulting images often suffer from severe degradations caused by the coexistence of moir\u00e9 patterns and flicker-banding, leading to significant visual quality degradation. Due to the strong coupling of these two artifacts in real imaging processes, existing methods designed for single degradations fail to generalize to such compound scenarios. In this paper, we present the first systematic study on joint removal of moir\u00e9 patterns and flicker-banding in screen-captured images, and propose a unified restoration framework, named CLEAR. To support this task, we construct a large-scale dataset containing both moir\u00e9 patterns and flicker-banding, and introduce an ISP-based flicker simulation pipeline to stabilize model training and expand the degradation distribution. Furthermore, we design a frequency-domain decomposition and re-composition module together with a trajectory alignment loss to enhance the modeling of compound artifacts. Extensive experiments demonstrate that the proposed method consistently. outperforms existing image restoration approaches across multiple evaluation metrics, validating its effectiveness in complex real-world scenarios.", "AI": {"tldr": "CLEAR framework for joint removal of moir\u00e9 patterns and flicker-banding in screen-captured images, featuring frequency-domain decomposition and ISP-based simulation.", "motivation": "Mobile screen captures suffer from compound degradations of moir\u00e9 patterns and flicker-banding that existing single-degradation methods cannot handle effectively.", "method": "Unified restoration framework with frequency-domain decomposition/re-composition module, trajectory alignment loss, and ISP-based flicker simulation pipeline for training.", "result": "Consistently outperforms existing image restoration approaches across multiple evaluation metrics on a newly constructed large-scale dataset.", "conclusion": "CLEAR effectively addresses complex real-world compound artifacts in screen-captured images through systematic joint degradation modeling."}}
{"id": "2602.01561", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.01561", "abs": "https://arxiv.org/abs/2602.01561", "authors": ["Yejin Son", "Saejin Kim", "Dongjun Min", "Younjae Yu"], "title": "Multimodal UNcommonsense: From Odd to Ordinary and Ordinary to Odd", "comment": "24 pages", "summary": "Commonsense reasoning in multimodal contexts remains a foundational challenge in artificial intelligence. We introduce Multimodal UNcommonsense(MUN), a benchmark designed to evaluate models' ability to handle scenarios that deviate from typical visual or contextual expectations. MUN pairs visual scenes with surprising or unlikely outcomes described in natural language, prompting models to either rationalize seemingly odd images using everyday logic or uncover unexpected interpretations in ordinary scenes. To support this task, we propose a retrieval-based in-context learning (R-ICL) framework that transfers reasoning capabilities from larger models to smaller ones without additional training. Leveraging a novel Multimodal Ensemble Retriever (MER), our method identifies semantically relevant exemplars even when image and text pairs are deliberately discordant. Experiments show an average improvement of 8.3% over baseline ICL methods, highlighting the effectiveness of R-ICL in low-frequency, atypical settings. MUN opens new directions for evaluating and improving visual-language models' robustness and adaptability in real-world, culturally diverse, and non-prototypical scenarios.", "AI": {"tldr": "MUN benchmark evaluates multimodal commonsense reasoning on atypical scenarios using retrieval-based in-context learning with 8.3% improvement over baselines.", "motivation": "Commonsense reasoning in multimodal contexts remains challenging, especially for scenarios that deviate from typical visual or contextual expectations. Current models struggle with surprising or unlikely outcomes in visual-language scenarios.", "method": "Proposes Multimodal UNcommonsense (MUN) benchmark with surprising visual-language pairs, and a retrieval-based in-context learning (R-ICL) framework using Multimodal Ensemble Retriever (MER) to find relevant exemplars even for discordant image-text pairs.", "result": "R-ICL achieves average 8.3% improvement over baseline ICL methods, demonstrating effectiveness in low-frequency, atypical settings. MER successfully identifies semantically relevant exemplars for discordant multimodal inputs.", "conclusion": "MUN enables evaluation of visual-language models' robustness in non-prototypical scenarios, opening directions for improving adaptability in real-world, culturally diverse contexts."}}
{"id": "2602.01570", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.01570", "abs": "https://arxiv.org/abs/2602.01570", "authors": ["Yiwen Jia", "Hao Wei", "Yanhui Zhou", "Chenyang Ge"], "title": "One-Step Diffusion for Perceptual Image Compression", "comment": null, "summary": "Diffusion-based image compression methods have achieved notable progress, delivering high perceptual quality at low bitrates. However, their practical deployment is hindered by significant inference latency and heavy computational overhead, primarily due to the large number of denoising steps required during decoding. To address this problem, we propose a diffusion-based image compression method that requires only a single-step diffusion process, significantly improving inference speed. To enhance the perceptual quality of reconstructed images, we introduce a discriminator that operates on compact feature representations instead of raw pixels, leveraging the fact that features better capture high-level texture and structural details. Experimental results show that our method delivers comparable compression performance while offering a 46$\\times$ faster inference speed compared to recent diffusion-based approaches. The source code and models are available at https://github.com/cheesejiang/OSDiff.", "AI": {"tldr": "Single-step diffusion image compression method with 46\u00d7 faster inference while maintaining comparable performance to multi-step approaches.", "motivation": "Diffusion-based image compression methods achieve high perceptual quality at low bitrates but suffer from significant inference latency and computational overhead due to requiring many denoising steps during decoding.", "method": "Proposes diffusion-based image compression requiring only single-step diffusion process. Introduces discriminator operating on compact feature representations instead of raw pixels to enhance perceptual quality, leveraging features' better capture of high-level texture and structural details.", "result": "Method delivers comparable compression performance while offering 46\u00d7 faster inference speed compared to recent diffusion-based approaches.", "conclusion": "The proposed single-step diffusion approach significantly improves practical deployment viability by dramatically reducing inference latency while maintaining compression quality."}}
{"id": "2602.01574", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.01574", "abs": "https://arxiv.org/abs/2602.01574", "authors": ["Haobo Wang", "Weiqi Luo", "Xiaojun Jia", "Xiaochun Cao"], "title": "SGHA-Attack: Semantic-Guided Hierarchical Alignment for Transferable Targeted Attacks on Vision-Language Models", "comment": null, "summary": "Large vision-language models (VLMs) are vulnerable to transfer-based adversarial perturbations, enabling attackers to optimize on surrogate models and manipulate black-box VLM outputs. Prior targeted transfer attacks often overfit surrogate-specific embedding space by relying on a single reference and emphasizing final-layer alignment, which underutilizes intermediate semantics and degrades transfer across heterogeneous VLMs. To address this, we propose SGHA-Attack, a Semantic-Guided Hierarchical Alignment framework that adopts multiple target references and enforces intermediate-layer consistency. Concretely, we generate a visually grounded reference pool by sampling a frozen text-to-image model conditioned on the target prompt, and then carefully select the Top-K most semantically relevant anchors under the surrogate to form a weighted mixture for stable optimization guidance. Building on these anchors, SGHA-Attack injects target semantics throughout the feature hierarchy by aligning intermediate visual representations at both global and spatial granularities across multiple depths, and by synchronizing intermediate visual and textual features in a shared latent subspace to provide early cross-modal supervision before the final projection. Extensive experiments on open-source and commercial black-box VLMs show that SGHA-Attack achieves stronger targeted transferability than prior methods and remains robust under preprocessing and purification defenses.", "AI": {"tldr": "SGHA-Attack: A semantic-guided hierarchical alignment framework for targeted adversarial attacks on vision-language models that improves transferability across heterogeneous models by using multiple target references and enforcing intermediate-layer consistency.", "motivation": "Existing targeted transfer attacks on VLMs often overfit to surrogate-specific embedding spaces by relying on single references and focusing only on final-layer alignment, which underutilizes intermediate semantics and degrades transferability across different VLMs.", "method": "Proposes SGHA-Attack with two key components: 1) Generates multiple visually-grounded target references using a frozen text-to-image model and selects Top-K most semantically relevant anchors for weighted mixture guidance, 2) Enforces hierarchical alignment by matching intermediate visual representations at multiple depths (global and spatial granularities) and synchronizing intermediate visual-textual features in a shared latent subspace before final projection.", "result": "Extensive experiments on open-source and commercial black-box VLMs demonstrate that SGHA-Attack achieves stronger targeted transferability than prior methods and remains robust against preprocessing and purification defenses.", "conclusion": "SGHA-Attack effectively addresses the limitations of previous targeted transfer attacks by leveraging multiple semantic references and hierarchical feature alignment, resulting in improved cross-model transferability and defense robustness for adversarial attacks on vision-language models."}}
{"id": "2602.01586", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.01586", "abs": "https://arxiv.org/abs/2602.01586", "authors": ["Wencan Cheng", "Gim Hee Lee"], "title": "HandMCM: Multi-modal Point Cloud-based Correspondence State Space Model for 3D Hand Pose Estimation", "comment": "AAAI accepted", "summary": "3D hand pose estimation that involves accurate estimation of 3D human hand keypoint locations is crucial for many human-computer interaction applications such as augmented reality. However, this task poses significant challenges due to self-occlusion of the hands and occlusions caused by interactions with objects. In this paper, we propose HandMCM to address these challenges. Our HandMCM is a novel method based on the powerful state space model (Mamba). By incorporating modules for local information injection/filtering and correspondence modeling, the proposed correspondence Mamba effectively learns the highly dynamic kinematic topology of keypoints across various occlusion scenarios. Moreover, by integrating multi-modal image features, we enhance the robustness and representational capacity of the input, leading to more accurate hand pose estimation. Empirical evaluations on three benchmark datasets demonstrate that our model significantly outperforms current state-of-the-art methods, particularly in challenging scenarios involving severe occlusions. These results highlight the potential of our approach to advance the accuracy and reliability of 3D hand pose estimation in practical applications.", "AI": {"tldr": "HandMCM: A novel 3D hand pose estimation method using state space models (Mamba) with local information injection/filtering and correspondence modeling to handle occlusions, achieving state-of-the-art performance.", "motivation": "3D hand pose estimation is crucial for human-computer interaction applications like AR, but faces challenges from self-occlusion and object interactions causing occlusions.", "method": "Proposes HandMCM based on state space model (Mamba) with modules for local information injection/filtering and correspondence modeling to learn dynamic kinematic topology of keypoints across occlusion scenarios. Integrates multi-modal image features for enhanced robustness.", "result": "Significantly outperforms current state-of-the-art methods on three benchmark datasets, particularly in challenging scenarios with severe occlusions.", "conclusion": "Demonstrates potential to advance accuracy and reliability of 3D hand pose estimation in practical applications by effectively handling occlusion challenges."}}
{"id": "2602.01591", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.01591", "abs": "https://arxiv.org/abs/2602.01591", "authors": ["Zhixiong Yue", "Zixuan Ni", "Feiyang Ye", "Jinshan Zhang", "Sheng Shen", "Zhenpeng Mi"], "title": "Know Your Step: Faster and Better Alignment for Flow Matching Models via Step-aware Advantages", "comment": null, "summary": "Recent advances in flow matching models, particularly with reinforcement learning (RL), have significantly enhanced human preference alignment in few step text to image generators. However, existing RL based approaches for flow matching models typically rely on numerous denoising steps, while suffering from sparse and imprecise reward signals that often lead to suboptimal alignment. To address these limitations, we propose Temperature Annealed Few step Sampling with Group Relative Policy Optimization (TAFS GRPO), a novel framework for training flow matching text to image models into efficient few step generators well aligned with human preferences. Our method iteratively injects adaptive temporal noise onto the results of one step samples. By repeatedly annealing the model's sampled outputs, it introduces stochasticity into the sampling process while preserving the semantic integrity of each generated image. Moreover, its step aware advantage integration mechanism combines the GRPO to avoid the need for the differentiable of reward function and provide dense and step specific rewards for stable policy optimization. Extensive experiments demonstrate that TAFS GRPO achieves strong performance in few step text to image generation and significantly improves the alignment of generated images with human preferences. The code and models of this work will be available to facilitate further research.", "AI": {"tldr": "TAFS-GRPO is a novel RL framework that trains flow matching text-to-image models to generate high-quality images in few steps while better aligning with human preferences through temperature annealing and group relative policy optimization.", "motivation": "Existing RL approaches for flow matching models require many denoising steps and suffer from sparse, imprecise reward signals that lead to suboptimal human preference alignment in few-step text-to-image generation.", "method": "Proposes Temperature Annealed Few-step Sampling with Group Relative Policy Optimization (TAFS-GRPO): 1) Iteratively injects adaptive temporal noise onto one-step samples, 2) Uses temperature annealing to introduce stochasticity while preserving semantic integrity, 3) Employs step-aware advantage integration with GRPO to provide dense, step-specific rewards without requiring differentiable reward functions.", "result": "Extensive experiments show TAFS-GRPO achieves strong performance in few-step text-to-image generation and significantly improves alignment of generated images with human preferences.", "conclusion": "TAFS-GRPO effectively addresses limitations of existing RL approaches for flow matching models, enabling efficient few-step generation with better human preference alignment through temperature annealing and group relative policy optimization."}}
{"id": "2602.01593", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.01593", "abs": "https://arxiv.org/abs/2602.01593", "authors": ["Wenzhuo Zhao", "Keren Fu", "Jiahao He", "Xiaohong Liu", "Qijun Zhao", "Guangtao Zhai"], "title": "Samba+: General and Accurate Salient Object Detection via A More Unified Mamba-based Framework", "comment": null, "summary": "Existing salient object detection (SOD) models are generally constrained by the limited receptive fields of convolutional neural networks (CNNs) and quadratic computational complexity of Transformers. Recently, the emerging state-space model, namely Mamba, has shown great potential in balancing global receptive fields and computational efficiency. As a solution, we propose Saliency Mamba (Samba), a pure Mamba-based architecture that flexibly handles various distinct SOD tasks, including RGB/RGB-D/RGB-T SOD, video SOD (VSOD), RGB-D VSOD, and visible-depth-thermal SOD. Specifically, we rethink the scanning strategy of Mamba for SOD, and introduce a saliency-guided Mamba block (SGMB) that features a spatial neighborhood scanning (SNS) algorithm to preserve the spatial continuity of salient regions. A context-aware upsampling (CAU) method is also proposed to promote hierarchical feature alignment and aggregation by modeling contextual dependencies. As one step further, to avoid the \"task-specific\" problem as in previous SOD solutions, we develop Samba+, which is empowered by training Samba in a multi-task joint manner, leading to a more unified and versatile model. Two crucial components that collaboratively tackle challenges encountered in input of arbitrary modalities and continual adaptation are investigated. Specifically, a hub-and-spoke graph attention (HGA) module facilitates adaptive cross-modal interactive fusion, and a modality-anchored continual learning (MACL) strategy alleviates inter-modal conflicts together with catastrophic forgetting. Extensive experiments demonstrate that Samba individually outperforms existing methods across six SOD tasks on 22 datasets with lower computational cost, whereas Samba+ achieves even superior results on these tasks and datasets by using a single trained versatile model. Additional results further demonstrate the potential of our Samba framework.", "AI": {"tldr": "Samba is a pure Mamba-based architecture for salient object detection that achieves state-of-the-art performance across six SOD tasks with lower computational cost, while Samba+ extends this with multi-task training for a unified versatile model.", "motivation": "Existing SOD models are limited by CNNs' restricted receptive fields and Transformers' high computational complexity. Mamba offers a promising balance between global receptive fields and computational efficiency, but needs adaptation for SOD tasks.", "method": "Proposes Samba with saliency-guided Mamba blocks using spatial neighborhood scanning to preserve spatial continuity, and context-aware upsampling for hierarchical feature alignment. Samba+ adds hub-and-spoke graph attention for cross-modal fusion and modality-anchored continual learning to prevent catastrophic forgetting.", "result": "Samba outperforms existing methods across six SOD tasks on 22 datasets with lower computational cost. Samba+ achieves even better results using a single trained versatile model, demonstrating superior performance and versatility.", "conclusion": "The proposed Samba framework effectively addresses SOD limitations by leveraging Mamba's efficiency while introducing task-specific adaptations, and Samba+ provides a unified solution for multi-modal SOD tasks with strong performance across diverse datasets."}}
{"id": "2602.01594", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.01594", "abs": "https://arxiv.org/abs/2602.01594", "authors": ["Wenzhuo Liu", "Qiannan Guo", "Zhen Wang", "Wenshuo Wang", "Lei Yang", "Yicheng Qiao", "Lening Wang", "Zhiwei Li", "Chen Lv", "Shanghang Zhang", "Junqiang Xi", "Huaping Liu"], "title": "UV-M3TL: A Unified and Versatile Multimodal Multi-Task Learning Framework for Assistive Driving Perception", "comment": null, "summary": "Advanced Driver Assistance Systems (ADAS) need to understand human driver behavior while perceiving their navigation context, but jointly learning these heterogeneous tasks would cause inter-task negative transfer and impair system performance. Here, we propose a Unified and Versatile Multimodal Multi-Task Learning (UV-M3TL) framework to simultaneously recognize driver behavior, driver emotion, vehicle behavior, and traffic context, while mitigating inter-task negative transfer. Our framework incorporates two core components: dual-branch spatial channel multimodal embedding (DB-SCME) and adaptive feature-decoupled multi-task loss (AFD-Loss). DB-SCME enhances cross-task knowledge transfer while mitigating task conflicts by employing a dual-branch structure to explicitly model salient task-shared and task-specific features. AFD-Loss improves the stability of joint optimization while guiding the model to learn diverse multi-task representations by introducing an adaptive weighting mechanism based on learning dynamics and feature decoupling constraints. We evaluate our method on the AIDE dataset, and the experimental results demonstrate that UV-M3TL achieves state-of-the-art performance across all four tasks. To further prove the versatility, we evaluate UV-M3TL on additional public multi-task perception benchmarks (BDD100K, CityScapes, NYUD-v2, and PASCAL-Context), where it consistently delivers strong performance across diverse task combinations, attaining state-of-the-art results on most tasks.", "AI": {"tldr": "UV-M3TL is a multimodal multi-task learning framework that simultaneously recognizes driver behavior, emotion, vehicle behavior, and traffic context while mitigating inter-task negative transfer through dual-branch feature modeling and adaptive loss mechanisms.", "motivation": "ADAS systems need to understand both human driver behavior and navigation context, but jointly learning these heterogeneous tasks causes inter-task negative transfer that impairs system performance.", "method": "Proposes UV-M3TL with two core components: 1) DB-SCME (dual-branch spatial channel multimodal embedding) that explicitly models task-shared and task-specific features, and 2) AFD-Loss (adaptive feature-decoupled multi-task loss) with adaptive weighting based on learning dynamics and feature decoupling constraints.", "result": "Achieves state-of-the-art performance across all four tasks on AIDE dataset, and demonstrates versatility by delivering strong performance on additional benchmarks (BDD100K, CityScapes, NYUD-v2, PASCAL-Context) with state-of-the-art results on most tasks.", "conclusion": "UV-M3TL effectively mitigates inter-task negative transfer in multimodal multi-task learning for ADAS applications, achieving superior performance across diverse tasks and demonstrating strong generalization to other perception benchmarks."}}
{"id": "2602.01609", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.01609", "abs": "https://arxiv.org/abs/2602.01609", "authors": ["Junqing Lin", "Xingyu Zheng", "Pei Cheng", "Bin Fu", "Jingwei Sun", "Guangzhong Sun"], "title": "Token Pruning for In-Context Generation in Diffusion Transformers", "comment": "20 pages", "summary": "In-context generation significantly enhances Diffusion Transformers (DiTs) by enabling controllable image-to-image generation through reference examples. However, the resulting input concatenation drastically increases sequence length, creating a substantial computational bottleneck. Existing token reduction techniques, primarily tailored for text-to-image synthesis, fall short in this paradigm as they apply uniform reduction strategies, overlooking the inherent role asymmetry between reference contexts and target latents across spatial, temporal, and functional dimensions. To bridge this gap, we introduce ToPi, a training-free token pruning framework tailored for in-context generation in DiTs. Specifically, ToPi utilizes offline calibration-driven sensitivity analysis to identify pivotal attention layers, serving as a robust proxy for redundancy estimation. Leveraging these layers, we derive a novel influence metric to quantify the contribution of each context token for selective pruning, coupled with a temporal update strategy that adapts to the evolving diffusion trajectory. Empirical evaluations demonstrate that ToPi can achieve over 30\\% speedup in inference while maintaining structural fidelity and visual consistency across complex image generation tasks.", "AI": {"tldr": "ToPi is a training-free token pruning framework that reduces computational cost in Diffusion Transformers for in-context image generation by selectively pruning less important context tokens while maintaining quality.", "motivation": "In-context generation in Diffusion Transformers enables controllable image-to-image generation but creates computational bottlenecks due to increased sequence length from input concatenation. Existing token reduction techniques fail because they treat reference contexts and target latents uniformly, ignoring their functional asymmetry across spatial, temporal, and functional dimensions.", "method": "ToPi uses offline calibration-driven sensitivity analysis to identify pivotal attention layers as a proxy for redundancy estimation. It then derives an influence metric to quantify each context token's contribution for selective pruning, combined with a temporal update strategy that adapts to the evolving diffusion trajectory.", "result": "Empirical evaluations show ToPi achieves over 30% speedup in inference while maintaining structural fidelity and visual consistency across complex image generation tasks.", "conclusion": "ToPi effectively addresses the computational bottleneck in in-context generation for Diffusion Transformers through a tailored token pruning approach that preserves quality while significantly improving efficiency."}}
{"id": "2602.01623", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.01623", "abs": "https://arxiv.org/abs/2602.01623", "authors": ["Susan Liang", "Chao Huang", "Filippos Bellos", "Yolo Yunlong Tang", "Qianxiang Shen", "Jing Bi", "Luchuan Song", "Zeliang Zhang", "Jason Corso", "Chenliang Xu"], "title": "Omni-Judge: Can Omni-LLMs Serve as Human-Aligned Judges for Text-Conditioned Audio-Video Generation?", "comment": null, "summary": "State-of-the-art text-to-video generation models such as Sora 2 and Veo 3 can now produce high-fidelity videos with synchronized audio directly from a textual prompt, marking a new milestone in multi-modal generation. However, evaluating such tri-modal outputs remains an unsolved challenge. Human evaluation is reliable but costly and difficult to scale, while traditional automatic metrics, such as FVD, CLAP, and ViCLIP, focus on isolated modality pairs, struggle with complex prompts, and provide limited interpretability. Omni-modal large language models (omni-LLMs) present a promising alternative: they naturally process audio, video, and text, support rich reasoning, and offer interpretable chain-of-thought feedback. Driven by this, we introduce Omni-Judge, a study assessing whether omni-LLMs can serve as human-aligned judges for text-conditioned audio-video generation. Across nine perceptual and alignment metrics, Omni-Judge achieves correlation comparable to traditional metrics and excels on semantically demanding tasks such as audio-text alignment, video-text alignment, and audio-video-text coherence. It underperforms on high-FPS perceptual metrics, including video quality and audio-video synchronization, due to limited temporal resolution. Omni-Judge provides interpretable explanations that expose semantic or physical inconsistencies, enabling practical downstream uses such as feedback-based refinement. Our findings highlight both the potential and current limitations of omni-LLMs as unified evaluators for multi-modal generation.", "AI": {"tldr": "Omni-Judge uses omni-modal LLMs to evaluate text-to-video generation with audio, achieving human-aligned correlation on semantic tasks but limited on temporal metrics.", "motivation": "Current text-to-video models can generate high-quality videos with synchronized audio, but evaluating these tri-modal outputs is challenging. Human evaluation is expensive and doesn't scale, while traditional automatic metrics focus on isolated modalities, struggle with complex prompts, and lack interpretability.", "method": "The authors introduce Omni-Judge, which leverages omni-modal large language models (omni-LLMs) that can naturally process audio, video, and text. These models support rich reasoning and provide interpretable chain-of-thought feedback for evaluating text-conditioned audio-video generation across nine perceptual and alignment metrics.", "result": "Omni-Judge achieves correlation comparable to traditional metrics and excels on semantically demanding tasks like audio-text alignment, video-text alignment, and audio-video-text coherence. However, it underperforms on high-FPS perceptual metrics including video quality and audio-video synchronization due to limited temporal resolution.", "conclusion": "Omni-LLMs show promise as unified evaluators for multi-modal generation, providing interpretable explanations that expose inconsistencies and enabling practical downstream uses like feedback-based refinement. However, they have current limitations in temporal resolution for high-FPS perceptual metrics."}}
{"id": "2602.01624", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.01624", "abs": "https://arxiv.org/abs/2602.01624", "authors": ["Minh-Quan Le", "Gaurav Mittal", "Cheng Zhao", "David Gu", "Dimitris Samaras", "Mei Chen"], "title": "PISCES: Annotation-free Text-to-Video Post-Training via Optimal Transport-Aligned Rewards", "comment": null, "summary": "Text-to-video (T2V) generation aims to synthesize videos with high visual quality and temporal consistency that are semantically aligned with input text. Reward-based post-training has emerged as a promising direction to improve the quality and semantic alignment of generated videos. However, recent methods either rely on large-scale human preference annotations or operate on misaligned embeddings from pre-trained vision-language models, leading to limited scalability or suboptimal supervision. We present $\\texttt{PISCES}$, an annotation-free post-training algorithm that addresses these limitations via a novel Dual Optimal Transport (OT)-aligned Rewards module. To align reward signals with human judgment, $\\texttt{PISCES}$ uses OT to bridge text and video embeddings at both distributional and discrete token levels, enabling reward supervision to fulfill two objectives: (i) a Distributional OT-aligned Quality Reward that captures overall visual quality and temporal coherence; and (ii) a Discrete Token-level OT-aligned Semantic Reward that enforces semantic, spatio-temporal correspondence between text and video tokens. To our knowledge, $\\texttt{PISCES}$ is the first to improve annotation-free reward supervision in generative post-training through the lens of OT. Experiments on both short- and long-video generation show that $\\texttt{PISCES}$ outperforms both annotation-based and annotation-free methods on VBench across Quality and Semantic scores, with human preference studies further validating its effectiveness. We show that the Dual OT-aligned Rewards module is compatible with multiple optimization paradigms, including direct backpropagation and reinforcement learning fine-tuning.", "AI": {"tldr": "PISCES is an annotation-free post-training method for text-to-video generation that uses Dual Optimal Transport-aligned Rewards to improve video quality and semantic alignment without human preference annotations.", "motivation": "Current reward-based post-training methods for text-to-video generation either require large-scale human preference annotations (limiting scalability) or use misaligned embeddings from pre-trained vision-language models (providing suboptimal supervision). There's a need for an annotation-free approach that provides better reward signals aligned with human judgment.", "method": "PISCES introduces a Dual Optimal Transport (OT)-aligned Rewards module with two components: (1) Distributional OT-aligned Quality Reward that captures overall visual quality and temporal coherence, and (2) Discrete Token-level OT-aligned Semantic Reward that enforces semantic, spatio-temporal correspondence between text and video tokens. The method is annotation-free and uses OT to bridge text and video embeddings at both distributional and discrete token levels.", "result": "PISCES outperforms both annotation-based and annotation-free methods on VBench across Quality and Semantic scores for both short- and long-video generation. Human preference studies further validate its effectiveness. The Dual OT-aligned Rewards module is compatible with multiple optimization paradigms including direct backpropagation and reinforcement learning fine-tuning.", "conclusion": "PISCES successfully addresses the limitations of existing reward-based post-training methods by providing annotation-free, OT-aligned reward supervision that improves both visual quality and semantic alignment in text-to-video generation, demonstrating superior performance over existing approaches."}}
{"id": "2602.01630", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.01630", "abs": "https://arxiv.org/abs/2602.01630", "authors": ["Bohan Zeng", "Kaixin Zhu", "Daili Hua", "Bozhou Li", "Chengzhuo Tong", "Yuran Wang", "Xinyi Huang", "Yifan Dai", "Zixiang Zhang", "Yifan Yang", "Zhou Liu", "Hao Liang", "Xiaochen Ma", "Ruichuan An", "Tianyi Bai", "Hongcheng Gao", "Junbo Niu", "Yang Shi", "Xinlong Chen", "Yue Ding", "Minglei Shi", "Kai Zeng", "Yiwen Tang", "Yuanxing Zhang", "Pengfei Wan", "Xintao Wang", "Wentao Zhang"], "title": "Research on World Models Is Not Merely Injecting World Knowledge into Specific Tasks", "comment": "13 pages, 4 figures", "summary": "World models have emerged as a critical frontier in AI research, aiming to enhance large models by infusing them with physical dynamics and world knowledge. The core objective is to enable agents to understand, predict, and interact with complex environments. However, current research landscape remains fragmented, with approaches predominantly focused on injecting world knowledge into isolated tasks, such as visual prediction, 3D estimation, or symbol grounding, rather than establishing a unified definition or framework. While these task-specific integrations yield performance gains, they often lack the systematic coherence required for holistic world understanding. In this paper, we analyze the limitations of such fragmented approaches and propose a unified design specification for world models. We suggest that a robust world model should not be a loose collection of capabilities but a normative framework that integrally incorporates interaction, perception, symbolic reasoning, and spatial representation. This work aims to provide a structured perspective to guide future research toward more general, robust, and principled models of the world.", "AI": {"tldr": "The paper critiques fragmented approaches to world models and proposes a unified design specification for more coherent and principled world understanding.", "motivation": "Current world model research is fragmented with task-specific approaches (visual prediction, 3D estimation, symbol grounding) that lack systematic coherence for holistic world understanding. There's a need for a unified framework rather than isolated task improvements.", "method": "Analyzes limitations of fragmented approaches and proposes a unified design specification for world models. Suggests that robust world models should be normative frameworks integrating interaction, perception, symbolic reasoning, and spatial representation.", "result": "Provides a structured perspective and design specification to guide future research toward more general, robust, and principled world models.", "conclusion": "World models need a unified framework rather than fragmented task-specific approaches. A robust world model should integrate multiple capabilities systematically to achieve holistic world understanding and guide future AI research."}}
{"id": "2602.01633", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.01633", "abs": "https://arxiv.org/abs/2602.01633", "authors": ["Xinyuan Zhao", "Yihang Wu", "Ahmad Chaddad", "Tareef Daqqaq", "Reem Kateb"], "title": "Federated Vision Transformer with Adaptive Focal Loss for Medical Image Classification", "comment": "Accepted in Knowledge-Based Systems", "summary": "While deep learning models like Vision Transformer (ViT) have achieved significant advances, they typically require large datasets. With data privacy regulations, access to many original datasets is restricted, especially medical images. Federated learning (FL) addresses this challenge by enabling global model aggregation without data exchange. However, the heterogeneity of the data and the class imbalance that exist in local clients pose challenges for the generalization of the model. This study proposes a FL framework leveraging a dynamic adaptive focal loss (DAFL) and a client-aware aggregation strategy for local training. Specifically, we design a dynamic class imbalance coefficient that adjusts based on each client's sample distribution and class data distribution, ensuring minority classes receive sufficient attention and preventing sparse data from being ignored. To address client heterogeneity, a weighted aggregation strategy is adopted, which adapts to data size and characteristics to better capture inter-client variations. The classification results on three public datasets (ISIC, Ocular Disease and RSNA-ICH) show that the proposed framework outperforms DenseNet121, ResNet50, ViT-S/16, ViT-L/32, FedCLIP, Swin Transformer, CoAtNet, and MixNet in most cases, with accuracy improvements ranging from 0.98\\% to 41.69\\%. Ablation studies on the imbalanced ISIC dataset validate the effectiveness of the proposed loss function and aggregation strategy compared to traditional loss functions and other FL approaches. The codes can be found at: https://github.com/AIPMLab/ViT-FLDAF.", "AI": {"tldr": "Proposes a federated learning framework with dynamic adaptive focal loss and client-aware aggregation to handle data heterogeneity and class imbalance in medical image classification.", "motivation": "Deep learning models require large datasets, but medical images face privacy restrictions. Federated learning enables model training without data sharing, but suffers from data heterogeneity and class imbalance across clients.", "method": "Uses dynamic adaptive focal loss (DAFL) with class imbalance coefficient adjusted per client's sample distribution, plus weighted aggregation strategy adapting to data size and characteristics.", "result": "Outperforms DenseNet121, ResNet50, ViT variants, FedCLIP, Swin Transformer, CoAtNet, and MixNet on three medical datasets (ISIC, Ocular Disease, RSNA-ICH) with accuracy improvements from 0.98% to 41.69%.", "conclusion": "The proposed FL framework effectively addresses client heterogeneity and class imbalance in federated medical image classification, validated through ablation studies on imbalanced datasets."}}
{"id": "2602.01639", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.01639", "abs": "https://arxiv.org/abs/2602.01639", "authors": ["Tianyu Yang", "ChenWei He", "Xiangzhao Hao", "Tianyue Wang", "Jiarui Guo", "Haiyun Guo", "Leigang Qu", "Jinqiao Wang", "Tat-Seng Chua"], "title": "ReCALL: Recalibrating Capability Degradation for MLLM-based Composed Image Retrieval", "comment": null, "summary": "Composed Image Retrieval (CIR) aims to retrieve target images based on a hybrid query comprising a reference image and a modification text. Early dual-tower Vision-Language Models (VLMs) struggle with cross-modality compositional reasoning required for this task. Recently, adapting generative Multimodal Large Language Models (MLLMs) for retrieval offers a promising direction. However, we identify that this adaptation strategy overlooks a fundamental issue: adapting a generative MLLM into a single-embedding discriminative retriever triggers a paradigm conflict, which leads to Capability Degradation - the deterioration of native fine-grained reasoning after retrieval adaptation. To address this challenge, we propose ReCALL (Recalibrating Capability Degradation), a model-agnostic framework that follows a diagnose-generate-refine pipeline: Firstly, we diagnose cognitive blind spots of the retriever via self-guided informative instance mining. Next, we generate corrective instructions and triplets by CoT prompting the foundation MLLM and conduct quality control with VQA-based consistency filtering. Finally, we refine the retriever through continual training on these triplets with a grouped contrastive scheme, thereby internalizing fine-grained visual-semantic distinctions and realigning the discriminative embedding space of retriever with intrinsic compositional reasoning within the MLLM. Extensive experiments on CIRR and FashionIQ show that ReCALL consistently recalibrates degraded capabilities and achieves state-of-the-art performance. Code will be released soon.", "AI": {"tldr": "ReCALL addresses capability degradation in MLLM-based composed image retrieval by diagnosing cognitive blind spots, generating corrective training data via CoT prompting, and refining the retriever through grouped contrastive training.", "motivation": "Adapting generative Multimodal Large Language Models (MLLMs) into discriminative retrievers for composed image retrieval causes capability degradation - the deterioration of native fine-grained reasoning abilities due to paradigm conflict between generative and discriminative tasks.", "method": "ReCALL follows a diagnose-generate-refine pipeline: 1) Diagnose cognitive blind spots via self-guided informative instance mining, 2) Generate corrective instructions and triplets using CoT prompting on the foundation MLLM with VQA-based consistency filtering, 3) Refine the retriever through continual training on these triplets with grouped contrastive scheme.", "result": "Extensive experiments on CIRR and FashionIQ benchmarks show that ReCALL consistently recalibrates degraded capabilities and achieves state-of-the-art performance in composed image retrieval.", "conclusion": "ReCALL successfully addresses the capability degradation problem in MLLM-based retrieval by realigning the discriminative embedding space with the intrinsic compositional reasoning of the foundation MLLM, demonstrating a model-agnostic framework for improving composed image retrieval performance."}}
{"id": "2602.01649", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.01649", "abs": "https://arxiv.org/abs/2602.01649", "authors": ["Yinchao Ma", "Qiang Zhou", "Zhibin Wang", "Xianing Chen", "Hanqing Yang", "Jun Song", "Bo Zheng"], "title": "Contribution-aware Token Compression for Efficient Video Understanding via Reinforcement Learning", "comment": "This paper is accepted by AAAI2026", "summary": "Video large language models have demonstrated remarkable capabilities in video understanding tasks. However, the redundancy of video tokens introduces significant computational overhead during inference, limiting their practical deployment. Many compression algorithms are proposed to prioritize retaining features with the highest attention scores to minimize perturbations in attention computations. However, the correlation between attention scores and their actual contribution to correct answers remains ambiguous. To address the above limitation, we propose a novel \\textbf{C}ontribution-\\textbf{a}ware token \\textbf{Co}mpression algorithm for \\textbf{VID}eo understanding (\\textbf{CaCoVID}) that explicitly optimizes the token selection policy based on the contribution of tokens to correct predictions. First, we introduce a reinforcement learning-based framework that optimizes a policy network to select video token combinations with the greatest contribution to correct predictions. This paradigm shifts the focus from passive token preservation to active discovery of optimal compressed token combinations. Secondly, we propose a combinatorial policy optimization algorithm with online combination space sampling, which dramatically reduces the exploration space for video token combinations and accelerates the convergence speed of policy optimization. Extensive experiments on diverse video understanding benchmarks demonstrate the effectiveness of CaCoVID. Codes will be released.", "AI": {"tldr": "CaCoVID is a novel contribution-aware token compression algorithm for video understanding that uses reinforcement learning to optimize token selection based on their actual contribution to correct predictions, rather than just attention scores.", "motivation": "Current video LLMs suffer from computational overhead due to redundant video tokens. Existing compression methods prioritize tokens with high attention scores, but the correlation between attention scores and actual contribution to correct answers is unclear.", "method": "1) Reinforcement learning framework with policy network to select optimal token combinations based on contribution to correct predictions. 2) Combinatorial policy optimization with online combination space sampling to reduce exploration space and accelerate convergence.", "result": "Extensive experiments on diverse video understanding benchmarks demonstrate the effectiveness of CaCoVID. The method successfully reduces computational overhead while maintaining accuracy.", "conclusion": "CaCoVID provides a novel approach to video token compression that shifts from passive token preservation to active discovery of optimal compressed token combinations based on their actual contribution to correct predictions, enabling more efficient video understanding deployment."}}
{"id": "2602.01661", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.01661", "abs": "https://arxiv.org/abs/2602.01661", "authors": ["Xingyu Miao", "Junting Dong", "Qin Zhao", "Yuhang Yang", "Junhao Chen", "Yang Long"], "title": "From Frames to Sequences: Temporally Consistent Human-Centric Dense Prediction", "comment": null, "summary": "In this work, we focus on the challenge of temporally consistent human-centric dense prediction across video sequences. Existing models achieve strong per-frame accuracy but often flicker under motion, occlusion, and lighting changes, and they rarely have paired human video supervision for multiple dense tasks. We address this gap with a scalable synthetic data pipeline that generates photorealistic human frames and motion-aligned sequences with pixel-accurate depth, normals, and masks. Unlike prior static data synthetic pipelines, our pipeline provides both frame-level labels for spatial learning and sequence-level supervision for temporal learning. Building on this, we train a unified ViT-based dense predictor that (i) injects an explicit human geometric prior via CSE embeddings and (ii) improves geometry-feature reliability with a lightweight channel reweighting module after feature fusion. Our two-stage training strategy, combining static pretraining with dynamic sequence supervision, enables the model first to acquire robust spatial representations and then refine temporal consistency across motion-aligned sequences. Extensive experiments show that we achieve state-of-the-art performance on THuman2.1 and Hi4D and generalize effectively to in-the-wild videos.", "AI": {"tldr": "A unified ViT-based model for temporally consistent human-centric dense prediction across videos, trained on synthetic data with both frame-level and sequence-level supervision.", "motivation": "Existing human-centric dense prediction models suffer from temporal inconsistency (flickering) under motion, occlusion, and lighting changes, and lack paired video supervision for multiple dense tasks.", "method": "1) Scalable synthetic data pipeline generating photorealistic human frames with pixel-accurate depth, normals, and masks, providing both frame-level and sequence-level supervision. 2) Unified ViT-based dense predictor with explicit human geometric prior via CSE embeddings and lightweight channel reweighting module. 3) Two-stage training: static pretraining followed by dynamic sequence supervision.", "result": "Achieves state-of-the-art performance on THuman2.1 and Hi4D benchmarks and generalizes effectively to in-the-wild videos.", "conclusion": "The proposed synthetic data pipeline and unified model with explicit geometric priors and two-stage training effectively address temporal consistency in human-centric dense prediction, outperforming existing methods on standard benchmarks."}}
{"id": "2602.01666", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.01666", "abs": "https://arxiv.org/abs/2602.01666", "authors": ["Yan Wang", "Partho Hassan", "Samiha Sadeka", "Nada Soliman", "M M Sayeef Abdullah", "Sabit Hassan"], "title": "Moonworks Lunara Aesthetic II: An Image Variation Dataset", "comment": null, "summary": "We introduce Lunara Aesthetic II, a publicly released, ethically sourced image dataset designed to support controlled evaluation and learning of contextual consistency in modern image generation and editing systems. The dataset comprises 2,854 anchor-linked variation pairs derived from original art and photographs created by Moonworks. Each variation pair applies contextual transformations, such as illumination, weather, viewpoint, scene composition, color tone, or mood; while preserving a stable underlying identity. Lunara Aesthetic II operationalizes identity-preserving contextual variation as a supervision signal while also retaining Lunara's signature high aesthetic scores. Results show high identity stability, strong target attribute realization, and a robust aesthetic profile that exceeds large-scale web datasets. Released under the Apache 2.0 license, Lunara Aesthetic II is intended for benchmarking, fine-tuning, and analysis of contextual generalization, identity preservation, and edit robustness in image generation and image-to-image systems with interpretable, relational supervision. The dataset is publicly available at: https://huggingface.co/datasets/moonworks/lunara-aesthetic-image-variations.", "AI": {"tldr": "Lunara Aesthetic II is a publicly released image dataset with 2,854 anchor-linked variation pairs for evaluating contextual consistency in image generation/editing systems, featuring identity-preserving transformations and high aesthetic quality.", "motivation": "To support controlled evaluation and learning of contextual consistency in modern image generation and editing systems, addressing the need for interpretable supervision signals for identity preservation during contextual transformations.", "method": "Created a dataset of 2,854 anchor-linked variation pairs from original art and photographs, applying contextual transformations (illumination, weather, viewpoint, scene composition, color tone, mood) while preserving underlying identity.", "result": "The dataset shows high identity stability, strong target attribute realization, and robust aesthetic profile that exceeds large-scale web datasets, while being ethically sourced and released under Apache 2.0 license.", "conclusion": "Lunara Aesthetic II provides a valuable resource for benchmarking, fine-tuning, and analyzing contextual generalization, identity preservation, and edit robustness in image generation systems with interpretable relational supervision."}}
{"id": "2602.01673", "categories": ["cs.CV", "cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2602.01673", "abs": "https://arxiv.org/abs/2602.01673", "authors": ["Enguang Fan"], "title": "Real-Time Loop Closure Detection in Visual SLAM via NetVLAD and Faiss", "comment": null, "summary": "Loop closure detection (LCD) is a core component of simultaneous localization and mapping (SLAM): it identifies revisited places and enables pose-graph constraints that correct accumulated drift. Classic bag-of-words approaches such as DBoW are efficient but often degrade under appearance change and perceptual aliasing. In parallel, deep learning-based visual place recognition (VPR) descriptors (e.g., NetVLAD and Transformer-based models) offer stronger robustness, but their computational cost is often viewed as a barrier to real-time SLAM. In this paper, we empirically evaluate NetVLAD as an LCD module and compare it against DBoW on the KITTI dataset. We introduce a Fine-Grained Top-K precision-recall curve that better reflects LCD settings where a query may have zero or multiple valid matches. With Faiss-accelerated nearestneighbor search, NetVLAD achieves real-time query speed while improving accuracy and robustness over DBoW, making it a practical drop-in alternative for LCD in SLAM.", "AI": {"tldr": "NetVLAD outperforms DBoW for loop closure detection in SLAM, achieving real-time performance with Faiss acceleration while providing better accuracy and robustness against appearance changes.", "motivation": "Traditional bag-of-words approaches like DBoW degrade under appearance change and perceptual aliasing, while deep learning-based VPR descriptors (NetVLAD, Transformer models) offer stronger robustness but are considered computationally expensive for real-time SLAM applications.", "method": "Empirical evaluation of NetVLAD as an LCD module compared against DBoW on KITTI dataset, using Faiss-accelerated nearest-neighbor search for real-time performance. Introduces Fine-Grained Top-K precision-recall curve to better handle LCD scenarios where queries may have zero or multiple valid matches.", "result": "NetVLAD achieves real-time query speed with Faiss acceleration while improving accuracy and robustness over DBoW, making it a practical drop-in alternative for LCD in SLAM systems.", "conclusion": "NetVLAD with Faiss acceleration provides a viable, high-performance alternative to traditional bag-of-words methods for loop closure detection, offering better robustness to appearance changes while maintaining real-time performance requirements for SLAM applications."}}
{"id": "2602.01674", "categories": ["cs.CV", "cs.GR"], "pdf": "https://arxiv.org/pdf/2602.01674", "abs": "https://arxiv.org/abs/2602.01674", "authors": ["Hail Song", "Boram Yoon", "Seokhwan Yang", "Seoyoung Kang", "Hyunjeong Kim", "Henning Metzmacher", "Woontack Woo"], "title": "VRGaussianAvatar: Integrating 3D Gaussian Avatars into VR", "comment": "Accepted as an IEEE TVCG paper at IEEE VR 2026 (journal track)", "summary": "We present VRGaussianAvatar, an integrated system that enables real-time full-body 3D Gaussian Splatting (3DGS) avatars in virtual reality using only head-mounted display (HMD) tracking signals. The system adopts a parallel pipeline with a VR Frontend and a GA Backend. The VR Frontend uses inverse kinematics to estimate full-body pose and streams the resulting pose along with stereo camera parameters to the backend. The GA Backend stereoscopically renders a 3DGS avatar reconstructed from a single image. To improve stereo rendering efficiency, we introduce Binocular Batching, which jointly processes left and right eye views in a single batched pass to reduce redundant computation and support high-resolution VR displays. We evaluate VRGaussianAvatar with quantitative performance tests and a within-subject user study against image- and video-based mesh avatar baselines. Results show that VRGaussianAvatar sustains interactive VR performance and yields higher perceived appearance similarity, embodiment, and plausibility. Project page and source code are available at https://vrgaussianavatar.github.io.", "AI": {"tldr": "VRGaussianAvatar enables real-time full-body 3D Gaussian Splatting avatars in VR using only HMD tracking, achieving interactive performance with higher appearance similarity and embodiment than mesh-based baselines.", "motivation": "The paper aims to create real-time full-body 3D avatars for virtual reality using only head-mounted display tracking signals, addressing the need for realistic avatar representation without complex capture systems.", "method": "The system uses a parallel pipeline with VR Frontend (inverse kinematics for full-body pose estimation) and GA Backend (3D Gaussian Splatting avatar rendering). Introduces Binocular Batching to efficiently process stereo views in a single batched pass for VR displays.", "result": "VRGaussianAvatar sustains interactive VR performance and achieves higher perceived appearance similarity, embodiment, and plausibility compared to image- and video-based mesh avatar baselines in user studies.", "conclusion": "The proposed system successfully enables real-time full-body 3D Gaussian Splatting avatars in VR using only HMD tracking, offering improved rendering efficiency and user experience over existing mesh-based approaches."}}
{"id": "2602.01677", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.01677", "abs": "https://arxiv.org/abs/2602.01677", "authors": ["Yinchao Ma", "Dengqing Yang", "Zhangyu He", "Wenfei Yang", "Tianzhu Zhang"], "title": "SMTrack: State-Aware Mamba for Efficient Temporal Modeling in Visual Tracking", "comment": "This paper is accepted by IEEE TIP", "summary": "Visual tracking aims to automatically estimate the state of a target object in a video sequence, which is challenging especially in dynamic scenarios. Thus, numerous methods are proposed to introduce temporal cues to enhance tracking robustness. However, conventional CNN and Transformer architectures exhibit inherent limitations in modeling long-range temporal dependencies in visual tracking, often necessitating either complex customized modules or substantial computational costs to integrate temporal cues. Inspired by the success of the state space model, we propose a novel temporal modeling paradigm for visual tracking, termed State-aware Mamba Tracker (SMTrack), providing a neat pipeline for training and tracking without needing customized modules or substantial computational costs to build long-range temporal dependencies. It enjoys several merits. First, we propose a novel selective state-aware space model with state-wise parameters to capture more diverse temporal cues for robust tracking. Second, SMTrack facilitates long-range temporal interactions with linear computational complexity during training. Third, SMTrack enables each frame to interact with previously tracked frames via hidden state propagation and updating, which releases computational costs of handling temporal cues during tracking. Extensive experimental results demonstrate that SMTrack achieves promising performance with low computational costs.", "AI": {"tldr": "SMTrack is a novel visual tracking method using state space models (Mamba) for efficient long-range temporal modeling without complex custom modules or high computational costs.", "motivation": "Visual tracking in dynamic scenarios requires robust temporal modeling, but conventional CNN/Transformer architectures struggle with long-range temporal dependencies, requiring complex custom modules or high computational costs.", "method": "Proposes State-aware Mamba Tracker (SMTrack) with selective state-aware space model using state-wise parameters to capture diverse temporal cues. Uses hidden state propagation and updating for efficient temporal interactions during tracking with linear computational complexity.", "result": "Extensive experiments show SMTrack achieves promising performance with low computational costs.", "conclusion": "SMTrack provides an efficient temporal modeling paradigm for visual tracking that enables long-range temporal dependencies without complex custom modules or substantial computational overhead."}}
{"id": "2602.01683", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.01683", "abs": "https://arxiv.org/abs/2602.01683", "authors": ["Kangcong Li", "Peng Ye", "Lin Zhang", "Chao Wang", "Huafeng Qin", "Tao Chen"], "title": "FreshMem: Brain-Inspired Frequency-Space Hybrid Memory for Streaming Video Understanding", "comment": null, "summary": "Transitioning Multimodal Large Language Models (MLLMs) from offline to online streaming video understanding is essential for continuous perception. However, existing methods lack flexible adaptivity, leading to irreversible detail loss and context fragmentation. To resolve this, we propose FreshMem, a Frequency-Space Hybrid Memory network inspired by the brain's logarithmic perception and memory consolidation. FreshMem reconciles short-term fidelity with long-term coherence through two synergistic modules: Multi-scale Frequency Memory (MFM), which projects overflowing frames into representative frequency coefficients, complemented by residual details to reconstruct a global historical \"gist\"; and Space Thumbnail Memory (STM), which discretizes the continuous stream into episodic clusters by employing an adaptive compression strategy to distill them into high-density space thumbnails. Extensive experiments show that FreshMem significantly boosts the Qwen2-VL baseline, yielding gains of 5.20%, 4.52%, and 2.34% on StreamingBench, OV-Bench, and OVO-Bench, respectively. As a training-free solution, FreshMem outperforms several fully fine-tuned methods, offering a highly efficient paradigm for long-horizon streaming video understanding.", "AI": {"tldr": "FreshMem is a training-free memory network for streaming video understanding that uses frequency-space hybrid memory to maintain both short-term fidelity and long-term coherence, significantly boosting MLLM performance on streaming video benchmarks.", "motivation": "Existing methods for transitioning MLLMs to online streaming video understanding lack flexible adaptivity, causing irreversible detail loss and context fragmentation. The paper aims to address these limitations by developing a more adaptive memory system inspired by human cognitive processes.", "method": "FreshMem uses a Frequency-Space Hybrid Memory network with two synergistic modules: 1) Multi-scale Frequency Memory (MFM) projects overflowing frames into frequency coefficients with residual details to reconstruct a global historical \"gist\"; 2) Space Thumbnail Memory (STM) discretizes continuous streams into episodic clusters using adaptive compression to create high-density space thumbnails.", "result": "FreshMem significantly boosts the Qwen2-VL baseline with gains of 5.20% on StreamingBench, 4.52% on OV-Bench, and 2.34% on OVO-Bench. As a training-free solution, it outperforms several fully fine-tuned methods.", "conclusion": "FreshMem offers an efficient paradigm for long-horizon streaming video understanding by reconciling short-term fidelity with long-term coherence through brain-inspired memory mechanisms, providing a training-free solution that competes with fully fine-tuned approaches."}}
{"id": "2602.01696", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.01696", "abs": "https://arxiv.org/abs/2602.01696", "authors": ["Jiaming Cui", "Shuai Zhou", "Wenqiang Li", "Ruifeng Qin", "Feng Shen"], "title": "Cross-Modal Alignment and Fusion for RGB-D Transmission-Line Defect Detection", "comment": null, "summary": "Transmission line defect detection remains challenging for automated UAV inspection due to the dominance of small-scale defects, complex backgrounds, and illumination variations. Existing RGB-based detectors, despite recent progress, struggle to distinguish geometrically subtle defects from visually similar background structures under limited chromatic contrast. This paper proposes CMAFNet, a Cross-Modal Alignment and Fusion Network that integrates RGB appearance and depth geometry through a principled purify-then-fuse paradigm. CMAFNet consists of a Semantic Recomposition Module that performs dictionary-based feature purification via a learned codebook to suppress modality-specific noise while preserving defect-discriminative information, and a Contextual Semantic Integration Framework that captures global spatial dependencies using partial-channel attention to enhance structural semantic reasoning. Position-wise normalization within the purification stage enforces explicit reconstruction-driven cross-modal alignment, ensuring statistical compatibility between heterogeneous features prior to fusion. Extensive experiments on the TLRGBD benchmark, where 94.5% of instances are small objects, demonstrate that CMAFNet achieves 32.2% mAP@50 and 12.5% APs, outperforming the strongest baseline by 9.8 and 4.0 percentage points, respectively. A lightweight variant reaches 24.8% mAP50 at 228 FPS with only 4.9M parameters, surpassing all YOLO-based detectors while matching transformer-based methods at substantially lower computational cost.", "AI": {"tldr": "CMAFNet is a cross-modal fusion network for transmission line defect detection that integrates RGB and depth data using dictionary-based feature purification and attention-based fusion, achieving state-of-the-art performance on small-scale defects.", "motivation": "Transmission line defect detection is challenging due to small-scale defects, complex backgrounds, and illumination variations. RGB-based detectors struggle with geometrically subtle defects that have limited chromatic contrast with background structures.", "method": "CMAFNet uses a cross-modal alignment and fusion approach with: 1) Semantic Recomposition Module for dictionary-based feature purification via learned codebook to suppress noise while preserving defect information, 2) Contextual Semantic Integration Framework with partial-channel attention for global spatial dependencies, and 3) Position-wise normalization for explicit reconstruction-driven cross-modal alignment.", "result": "On the TLRGBD benchmark (94.5% small objects), CMAFNet achieves 32.2% mAP@50 and 12.5% APs, outperforming strongest baseline by 9.8 and 4.0 percentage points. Lightweight variant reaches 24.8% mAP50 at 228 FPS with only 4.9M parameters.", "conclusion": "The proposed cross-modal fusion approach effectively addresses transmission line defect detection challenges by integrating RGB appearance and depth geometry through principled purification and fusion, achieving superior performance with efficient computational cost."}}
{"id": "2602.01710", "categories": ["cs.CV", "cond-mat.mtrl-sci", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.01710", "abs": "https://arxiv.org/abs/2602.01710", "authors": ["Salma Zahran", "Zhou Ao", "Zhengyang Zhang", "Chen Chi", "Chenchen Yuan", "Yanming Wang"], "title": "Physics Informed Generative AI Enabling Labour Free Segmentation For Microscopy Analysis", "comment": null, "summary": "Semantic segmentation of microscopy images is a critical task for high-throughput materials characterisation, yet its automation is severely constrained by the prohibitive cost, subjectivity, and scarcity of expert-annotated data. While physics-based simulations offer a scalable alternative to manual labelling, models trained on such data historically fail to generalise due to a significant domain gap, lacking the complex textures, noise patterns, and imaging artefacts inherent to experimental data. This paper introduces a novel framework for labour-free segmentation that successfully bridges this simulation-to-reality gap. Our pipeline leverages phase-field simulations to generate an abundant source of microstructural morphologies with perfect, intrinsically-derived ground-truth masks. We then employ a Cycle-Consistent Generative Adversarial Network (CycleGAN) for unpaired image-to-image translation, transforming the clean simulations into a large-scale dataset of high-fidelity, realistic SEM images. A U-Net model, trained exclusively on this synthetic data, demonstrated remarkable generalisation when deployed on unseen experimental images, achieving a mean Boundary F1-Score of 0.90 and an Intersection over Union (IOU) of 0.88. Comprehensive validation using t-SNE feature-space projection and Shannon entropy analysis confirms that our synthetic images are statistically and featurally indistinguishable from the real data manifold. By completely decoupling model training from manual annotation, our generative framework transforms a data-scarce problem into one of data abundance, providing a robust and fully automated solution to accelerate materials discovery and analysis.", "AI": {"tldr": "A generative framework uses phase-field simulations and CycleGAN to create realistic synthetic SEM images for training segmentation models, achieving high performance on real experimental data without manual annotation.", "motivation": "Automating semantic segmentation of microscopy images is limited by the high cost, subjectivity, and scarcity of expert-annotated data. Physics-based simulations offer scalability but create a domain gap that prevents generalization to real experimental data with complex textures, noise, and artifacts.", "method": "1) Generate abundant microstructural morphologies with perfect ground-truth masks using phase-field simulations. 2) Transform clean simulations into realistic SEM images using CycleGAN for unpaired image-to-image translation. 3) Train a U-Net model exclusively on this synthetic data.", "result": "The U-Net model trained on synthetic data achieved remarkable generalization on unseen experimental images: mean Boundary F1-Score of 0.90 and Intersection over Union (IOU) of 0.88. Validation via t-SNE feature-space projection and Shannon entropy analysis confirmed synthetic images are statistically and featurally indistinguishable from real data.", "conclusion": "The framework successfully bridges the simulation-to-reality gap, transforming data-scarce segmentation problems into data-abundant ones. It completely decouples model training from manual annotation, providing a robust, fully automated solution to accelerate materials discovery and analysis."}}
{"id": "2602.01723", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.01723", "abs": "https://arxiv.org/abs/2602.01723", "authors": ["Yikun Ma", "Yiqing Li", "Jingwen Ye", "Zhongkai Wu", "Weidong Zhang", "Lin Gao", "Zhi Jin"], "title": "FastPhysGS: Accelerating Physics-based Dynamic 3DGS Simulation via Interior Completion and Adaptive Optimization", "comment": null, "summary": "Extending 3D Gaussian Splatting (3DGS) to 4D physical simulation remains challenging. Based on the Material Point Method (MPM), existing methods either rely on manual parameter tuning or distill dynamics from video diffusion models, limiting the generalization and optimization efficiency. Recent attempts using LLMs/VLMs suffer from a text/image-to-3D perceptual gap, yielding unstable physics behavior. In addition, they often ignore the surface structure of 3DGS, leading to implausible motion. We propose FastPhysGS, a fast and robust framework for physics-based dynamic 3DGS simulation:(1) Instance-aware Particle Filling (IPF) with Monte Carlo Importance Sampling (MCIS) to efficiently populate interior particles while preserving geometric fidelity; (2) Bidirectional Graph Decoupling Optimization (BGDO), an adaptive strategy that rapidly optimizes material parameters predicted from a VLM. Experiments show FastPhysGS achieves high-fidelity physical simulation in 1 minute using only 7 GB runtime memory, outperforming prior works with broad potential applications.", "AI": {"tldr": "FastPhysGS enables fast physics-based 4D Gaussian Splatting simulation using instance-aware particle filling and bidirectional graph decoupling optimization, achieving high-fidelity results in 1 minute with 7GB memory.", "motivation": "Existing methods for extending 3D Gaussian Splatting to 4D physical simulation have limitations: they require manual parameter tuning, rely on video diffusion models with limited generalization, or use LLMs/VLMs that suffer from perceptual gaps leading to unstable physics behavior and ignore surface structure.", "method": "Two key components: (1) Instance-aware Particle Filling (IPF) with Monte Carlo Importance Sampling (MCIS) to efficiently populate interior particles while preserving geometric fidelity; (2) Bidirectional Graph Decoupling Optimization (BGDO), an adaptive strategy that rapidly optimizes material parameters predicted from a VLM.", "result": "FastPhysGS achieves high-fidelity physical simulation in 1 minute using only 7 GB runtime memory, outperforming prior works with broad potential applications.", "conclusion": "The proposed FastPhysGS framework provides a fast and robust solution for physics-based dynamic 3D Gaussian Splatting simulation, addressing limitations of existing methods through efficient particle filling and adaptive optimization strategies."}}
{"id": "2602.01724", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.01724", "abs": "https://arxiv.org/abs/2602.01724", "authors": ["Tushar Anand", "Maheswar Bora", "Antitza Dantcheva", "Abhijit Das"], "title": "DenVisCoM: Dense Vision Correspondence Mamba for Efficient and Real-time Optical Flow and Stereo Estimation", "comment": "IEEE International Conference on Robotics and Automation 2026", "summary": "In this work, we propose a novel Mamba block DenVisCoM, as well as a novel hybrid architecture specifically tailored for accurate and real-time estimation of optical flow and disparity estimation. Given that such multi-view geometry and motion tasks are fundamentally related, we propose a unified architecture to tackle them jointly. Specifically, the proposed hybrid architecture is based on DenVisCoM and a Transformer-based attention block that efficiently addresses real-time inference, memory footprint, and accuracy at the same time for joint estimation of motion and 3D dense perception tasks. We extensively analyze the benchmark trade-off of accuracy and real-time processing on a large number of datasets. Our experimental results and related analysis suggest that our proposed model can accurately estimate optical flow and disparity estimation in real time. All models and associated code are available at https://github.com/vimstereo/DenVisCoM.", "AI": {"tldr": "DenVisCoM proposes a novel Mamba-based hybrid architecture for real-time joint estimation of optical flow and disparity, achieving accurate results with efficient inference.", "motivation": "Optical flow and disparity estimation are fundamentally related multi-view geometry and motion tasks that should be tackled jointly. Current approaches need to balance accuracy, real-time inference, and memory efficiency simultaneously.", "method": "Proposes DenVisCoM, a novel Mamba block combined with Transformer-based attention blocks in a hybrid architecture specifically designed for joint optical flow and disparity estimation. The architecture efficiently addresses real-time inference, memory footprint, and accuracy trade-offs.", "result": "Extensive analysis on multiple datasets shows the model can accurately estimate optical flow and disparity in real time, achieving a good benchmark trade-off between accuracy and processing speed.", "conclusion": "The proposed DenVisCoM hybrid architecture successfully enables accurate real-time joint estimation of optical flow and disparity, demonstrating that Mamba blocks combined with Transformer attention can effectively handle multi-view geometry and motion tasks simultaneously."}}
{"id": "2602.01738", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.01738", "abs": "https://arxiv.org/abs/2602.01738", "authors": ["Yue Zhou", "Xinan He", "Kaiqing Lin", "Bing Fan", "Feng Ding", "Bin Li"], "title": "Simplicity Prevails: The Emergence of Generalizable AIGI Detection in Visual Foundation Models", "comment": null, "summary": "While specialized detectors for AI-Generated Images (AIGI) achieve near-perfect accuracy on curated benchmarks, they suffer from a dramatic performance collapse in realistic, in-the-wild scenarios. In this work, we demonstrate that simplicity prevails over complex architectural designs. A simple linear classifier trained on the frozen features of modern Vision Foundation Models , including Perception Encoder, MetaCLIP 2, and DINOv3, establishes a new state-of-the-art. Through a comprehensive evaluation spanning traditional benchmarks, unseen generators, and challenging in-the-wild distributions, we show that this baseline not only matches specialized detectors on standard benchmarks but also decisively outperforms them on in-the-wild datasets, boosting accuracy by striking margins of over 30\\%. We posit that this superior capability is an emergent property driven by the massive scale of pre-training data containing synthetic content. We trace the source of this capability to two distinct manifestations of data exposure: Vision-Language Models internalize an explicit semantic concept of forgery, while Self-Supervised Learning models implicitly acquire discriminative forensic features from the pretraining data. However, we also reveal persistent limitations: these models suffer from performance degradation under recapture and transmission, remain blind to VAE reconstruction and localized editing. We conclude by advocating for a paradigm shift in AI forensics, moving from overfitting on static benchmarks to harnessing the evolving world knowledge of foundation models for real-world reliability.", "AI": {"tldr": "Simple linear classifier on frozen Vision Foundation Model features outperforms specialized AIGI detectors in real-world scenarios by 30+%, showing emergent detection capabilities from pre-training data exposure.", "motivation": "Specialized AI-Generated Image detectors achieve near-perfect accuracy on curated benchmarks but fail dramatically in realistic, in-the-wild scenarios, revealing a critical gap between benchmark performance and real-world reliability.", "method": "Train a simple linear classifier on frozen features from modern Vision Foundation Models (Perception Encoder, MetaCLIP 2, DINOv3) without complex architectural designs, then evaluate across traditional benchmarks, unseen generators, and challenging in-the-wild distributions.", "result": "The simple baseline matches specialized detectors on standard benchmarks and decisively outperforms them on in-the-wild datasets by over 30% accuracy margin. Vision-Language Models internalize explicit semantic concepts of forgery, while Self-Supervised Learning models acquire implicit forensic features from pre-training data.", "conclusion": "Foundation models' emergent detection capabilities from massive pre-training data exposure suggest a paradigm shift from overfitting on static benchmarks to leveraging evolving world knowledge for real-world AI forensics reliability, though limitations remain with recapture, transmission, VAE reconstruction, and localized editing."}}
{"id": "2602.01741", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.01741", "abs": "https://arxiv.org/abs/2602.01741", "authors": ["Sicheng Pan", "Chen Tang", "Shuzhao Xie", "Ke Yang", "Weixiang Zhang", "Jiawei Li", "Bin Chen", "Shu-Tao Xia", "Zhi Wang"], "title": "Tail-Aware Post-Training Quantization for 3D Geometry Models", "comment": null, "summary": "The burgeoning complexity and scale of 3D geometry models pose significant challenges for deployment on resource-constrained platforms. While Post-Training Quantization (PTQ) enables efficient inference without retraining, conventional methods, primarily optimized for 2D Vision Transformers, fail to transfer effectively to 3D models due to intricate feature distributions and prohibitive calibration overhead. To address these challenges, we propose TAPTQ, a Tail-Aware Post-Training Quantization pipeline specifically engineered for 3D geometric learning. Our contribution is threefold: (1) To overcome the data-scale bottleneck in 3D datasets, we develop a progressive coarse-to-fine calibration construction strategy that constructs a highly compact subset to achieve both statistical purity and geometric representativeness. (2) We reformulate the quantization interval search as an optimization problem and introduce a ternary-search-based solver, reducing the computational complexity from $\\mathcal{O}(N)$ to $\\mathcal{O}(\\log N)$ for accelerated deployment. (3) To mitigate quantization error accumulation, we propose TRE-Guided Module-wise Compensation, which utilizes a Tail Relative Error (TRE) metric to adaptively identify and rectify distortions in modules sensitive to long-tailed activation outliers. Extensive experiments on the VGGT and Pi3 benchmarks demonstrate that TAPTQ consistently outperforms state-of-the-art PTQ methods in accuracy while significantly reducing calibration time. The code will be released soon.", "AI": {"tldr": "TAPTQ is a Tail-Aware Post-Training Quantization pipeline for 3D geometric learning that addresses challenges of conventional PTQ methods by using progressive calibration, ternary-search optimization, and TRE-guided compensation.", "motivation": "3D geometry models are complex and large, making deployment on resource-constrained platforms challenging. Existing PTQ methods optimized for 2D Vision Transformers don't work well for 3D models due to intricate feature distributions and high calibration overhead.", "method": "Three key innovations: (1) Progressive coarse-to-fine calibration construction for compact subset selection, (2) Ternary-search-based solver to reduce quantization interval search complexity from O(N) to O(log N), (3) TRE-Guided Module-wise Compensation using Tail Relative Error metric to fix distortions from activation outliers.", "result": "Extensive experiments on VGGT and Pi3 benchmarks show TAPTQ consistently outperforms state-of-the-art PTQ methods in accuracy while significantly reducing calibration time.", "conclusion": "TAPTQ provides an effective PTQ solution for 3D geometric learning that addresses the unique challenges of 3D models, offering both accuracy improvements and reduced computational overhead for deployment on resource-constrained platforms."}}
{"id": "2602.01753", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.01753", "abs": "https://arxiv.org/abs/2602.01753", "authors": ["Shenghao Fu", "Yukun Su", "Fengyun Rao", "Jing Lyu", "Xiaohua Xie", "Wei-Shi Zheng"], "title": "ObjEmbed: Towards Universal Multimodal Object Embeddings", "comment": null, "summary": "Aligning objects with corresponding textual descriptions is a fundamental challenge and a realistic requirement in vision-language understanding. While recent multimodal embedding models excel at global image-text alignment, they often struggle with fine-grained alignment between image regions and specific phrases. In this work, we present ObjEmbed, a novel MLLM embedding model that decomposes the input image into multiple regional embeddings, each corresponding to an individual object, along with global embeddings. It supports a wide range of visual understanding tasks like visual grounding, local image retrieval, and global image retrieval. ObjEmbed enjoys three key properties: (1) Object-Oriented Representation: It captures both semantic and spatial aspects of objects by generating two complementary embeddings for each region: an object embedding for semantic matching and an IoU embedding that predicts localization quality. The final object matching score combines semantic similarity with the predicted IoU, enabling more accurate retrieval. (2) Versatility: It seamlessly handles both region-level and image-level tasks. (3) Efficient Encoding: All objects in an image, along with the full image, are encoded in a single forward pass for high efficiency. Superior performance on 18 diverse benchmarks demonstrates its strong semantic discrimination.", "AI": {"tldr": "ObjEmbed is a multimodal embedding model that decomposes images into object-level regional embeddings with both semantic and spatial representations, enabling fine-grained alignment between image regions and text phrases for various visual understanding tasks.", "motivation": "Current multimodal embedding models excel at global image-text alignment but struggle with fine-grained alignment between specific image regions and textual phrases, creating a need for better object-level alignment capabilities.", "method": "ObjEmbed decomposes images into multiple regional embeddings (one per object) plus global embeddings. For each region, it generates two complementary embeddings: object embedding for semantic matching and IoU embedding for localization quality prediction. The final matching score combines semantic similarity with predicted IoU.", "result": "Superior performance on 18 diverse benchmarks demonstrates strong semantic discrimination. The model efficiently encodes all objects in an image in a single forward pass.", "conclusion": "ObjEmbed provides an effective solution for fine-grained vision-language alignment with object-oriented representations, versatility across region-level and image-level tasks, and efficient encoding capabilities."}}
{"id": "2602.01754", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.01754", "abs": "https://arxiv.org/abs/2602.01754", "authors": ["Gustavo P. C. P. da Luz", "Alvaro M. Aspilcueta Narvaez", "Tiago Godoi Bannwart", "Gabriel Massuyoshi Sato", "Luis Fernando Gomez Gonzalez", "Juliana Freitag Borin"], "title": "Spot-Wise Smart Parking: An Edge-Enabled Architecture with YOLOv11 and Digital Twin Integration", "comment": "Submitted to Journal of Internet Services and Applications, 27 pages, 20 figures, 3 tables", "summary": "Smart parking systems help reduce congestion and minimize users' search time, thereby contributing to smart city adoption and enhancing urban mobility. In previous works, we presented a system developed on a university campus to monitor parking availability by estimating the number of free spaces from vehicle counts within a region of interest. Although this approach achieved good accuracy, it restricted the system's ability to provide spot-level insights and support more advanced applications. To overcome this limitation, we extend the system with a spot-wise monitoring strategy based on a distance-aware matching method with spatial tolerance, enhanced through an Adaptive Bounding Box Partitioning method for challenging spaces. The proposed approach achieves a balanced accuracy of 98.80% while maintaining an inference time of 8 seconds on a resource-constrained edge device, enhancing the capabilities of YOLOv11m, a model that has a size of 40.5 MB. In addition, two new components were introduced: (i) a Digital Shadow that visually represents parking lot entities as a base to evolve to a full Digital Twin, and (ii) an application support server based on a repurposed TV box. The latter not only enables scalable communication among cloud services, the parking totem, and a bot that provides detailed spot occupancy statistics, but also promotes hardware reuse as a step towards greater sustainability.", "AI": {"tldr": "The paper extends a smart parking system from region-level to spot-level monitoring using distance-aware matching with spatial tolerance and Adaptive Bounding Box Partitioning, achieving 98.80% accuracy on edge devices while adding Digital Shadow and application support server components.", "motivation": "Previous region-based parking monitoring systems lacked spot-level insights and couldn't support advanced applications. The authors wanted to overcome the limitation of only estimating free spaces from vehicle counts within regions of interest.", "method": "Extended the system with spot-wise monitoring using distance-aware matching method with spatial tolerance, enhanced by Adaptive Bounding Box Partitioning for challenging spaces. Uses YOLOv11m model (40.5 MB) on edge devices. Added Digital Shadow component and application support server based on repurposed TV box.", "result": "Achieved 98.80% balanced accuracy with 8-second inference time on resource-constrained edge device. Successfully implemented scalable communication between cloud services, parking totem, and occupancy statistics bot.", "conclusion": "The extended system provides spot-level monitoring capabilities while maintaining high accuracy and efficiency on edge hardware. The Digital Shadow and repurposed TV box server promote sustainability and enable evolution toward a full Digital Twin system."}}
{"id": "2602.01756", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.01756", "abs": "https://arxiv.org/abs/2602.01756", "authors": ["Jun He", "Junyan Ye", "Zilong Huang", "Dongzhi Jiang", "Chenjue Zhang", "Leqi Zhu", "Renrui Zhang", "Xiang Zhang", "Weijia Li"], "title": "Mind-Brush: Integrating Agentic Cognitive Search and Reasoning into Image Generation", "comment": "36 pages, 24 figures", "summary": "While text-to-image generation has achieved unprecedented fidelity, the vast majority of existing models function fundamentally as static text-to-pixel decoders. Consequently, they often fail to grasp implicit user intentions. Although emerging unified understanding-generation models have improved intent comprehension, they still struggle to accomplish tasks involving complex knowledge reasoning within a single model. Moreover, constrained by static internal priors, these models remain unable to adapt to the evolving dynamics of the real world. To bridge these gaps, we introduce Mind-Brush, a unified agentic framework that transforms generation into a dynamic, knowledge-driven workflow. Simulating a human-like 'think-research-create' paradigm, Mind-Brush actively retrieves multimodal evidence to ground out-of-distribution concepts and employs reasoning tools to resolve implicit visual constraints. To rigorously evaluate these capabilities, we propose Mind-Bench, a comprehensive benchmark comprising 500 distinct samples spanning real-time news, emerging concepts, and domains such as mathematical and Geo-Reasoning. Extensive experiments demonstrate that Mind-Brush significantly enhances the capabilities of unified models, realizing a zero-to-one capability leap for the Qwen-Image baseline on Mind-Bench, while achieving superior results on established benchmarks like WISE and RISE.", "AI": {"tldr": "Mind-Brush is a unified agentic framework that transforms text-to-image generation into a dynamic, knowledge-driven workflow using a 'think-research-create' paradigm to handle complex knowledge reasoning and adapt to real-world dynamics.", "motivation": "Existing text-to-image models fail to grasp implicit user intentions and struggle with complex knowledge reasoning. They also cannot adapt to evolving real-world dynamics due to static internal priors.", "method": "Mind-Brush transforms generation into a dynamic workflow simulating human-like 'think-research-create' paradigm. It actively retrieves multimodal evidence to ground out-of-distribution concepts and employs reasoning tools to resolve implicit visual constraints.", "result": "Mind-Brush significantly enhances unified model capabilities, achieving a zero-to-one capability leap for Qwen-Image baseline on Mind-Bench, while achieving superior results on established benchmarks like WISE and RISE.", "conclusion": "The proposed agentic framework successfully addresses limitations of current text-to-image models by incorporating dynamic knowledge retrieval and reasoning, enabling better handling of complex, real-world generation tasks."}}
{"id": "2602.01760", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.01760", "abs": "https://arxiv.org/abs/2602.01760", "authors": ["Hao Zhang", "Yanping Zha", "Zizhuo Li", "Meiqi Gong", "Jiayi Ma"], "title": "MagicFuse: Single Image Fusion for Visual and Semantic Reinforcement", "comment": null, "summary": "This paper focuses on a highly practical scenario: how to continue benefiting from the advantages of multi-modal image fusion under harsh conditions when only visible imaging sensors are available. To achieve this goal, we propose a novel concept of single-image fusion, which extends conventional data-level fusion to the knowledge level. Specifically, we develop MagicFuse, a novel single image fusion framework capable of deriving a comprehensive cross-spectral scene representation from a single low-quality visible image. MagicFuse first introduces an intra-spectral knowledge reinforcement branch and a cross-spectral knowledge generation branch based on the diffusion models. They mine scene information obscured in the visible spectrum and learn thermal radiation distribution patterns transferred to the infrared spectrum, respectively. Building on them, we design a multi-domain knowledge fusion branch that integrates the probabilistic noise from the diffusion streams of these two branches, from which a cross-spectral scene representation can be obtained through successive sampling. Then, we impose both visual and semantic constraints to ensure that this scene representation can satisfy human observation while supporting downstream semantic decision-making. Extensive experiments show that our MagicFuse achieves visual and semantic representation performance comparable to or even better than state-of-the-art fusion methods with multi-modal inputs, despite relying solely on a single degraded visible image.", "AI": {"tldr": "MagicFuse enables multi-modal image fusion capabilities using only a single degraded visible image by extending fusion from data-level to knowledge-level through diffusion models.", "motivation": "Address the practical challenge of maintaining multi-modal image fusion advantages when only visible imaging sensors are available under harsh conditions, overcoming limitations of conventional multi-modal fusion that requires multiple sensor inputs.", "method": "Proposes MagicFuse framework with three branches: 1) intra-spectral knowledge reinforcement branch using diffusion models to mine obscured visible scene information, 2) cross-spectral knowledge generation branch to learn thermal radiation patterns for infrared spectrum, and 3) multi-domain knowledge fusion branch that integrates probabilistic noise from both diffusion streams to obtain cross-spectral scene representation through successive sampling, with visual and semantic constraints.", "result": "Extensive experiments show MagicFuse achieves visual and semantic representation performance comparable to or better than state-of-the-art fusion methods with multi-modal inputs, despite using only a single degraded visible image.", "conclusion": "The proposed single-image fusion concept successfully extends conventional data-level fusion to knowledge level, enabling comprehensive cross-spectral scene representation from single visible images, demonstrating practical viability for harsh condition applications."}}
{"id": "2602.01764", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.01764", "abs": "https://arxiv.org/abs/2602.01764", "authors": ["Dennis Basile", "Dennis Sprute", "Helene D\u00f6rksen", "Holger Flatt"], "title": "GDPR-Compliant Person Recognition in Industrial Environments Using MEMS-LiDAR and Hybrid Data", "comment": "Accepted at 19th CIRP Conference on Intelligent Computation in Manufacturing Engineering", "summary": "The reliable detection of unauthorized individuals in safety-critical industrial indoor spaces is crucial to avoid plant shutdowns, property damage, and personal hazards. Conventional vision-based methods that use deep-learning approaches for person recognition provide image information but are sensitive to lighting and visibility conditions and often violate privacy regulations, such as the General Data Protection Regulation (GDPR) in the European Union. Typically, detection systems based on deep learning require annotated data for training. Collecting and annotating such data, however, is highly time-consuming and due to manual treatments not necessarily error free. Therefore, this paper presents a privacy-compliant approach based on Micro-Electro-Mechanical Systems LiDAR (MEMS-LiDAR), which exclusively captures anonymized 3D point clouds and avoids personal identification features. To compensate for the large amount of time required to record real LiDAR data and for post-processing and annotation, real recordings are augmented with synthetically generated scenes from the CARLA simulation framework. The results demonstrate that the hybrid data improves the average precision by 44 percentage points compared to a model trained exclusively with real data while reducing the manual annotation effort by 50 %. Thus, the proposed approach provides a scalable, cost-efficient alternative to purely real-data-based methods and systematically shows how synthetic LiDAR data can combine high performance in person detection with GDPR compliance in an industrial environment.", "AI": {"tldr": "Privacy-compliant person detection using MEMS-LiDAR with hybrid real+synthetic data improves performance while reducing annotation effort.", "motivation": "Need for reliable unauthorized person detection in industrial spaces while addressing privacy concerns (GDPR compliance) and reducing costly manual data annotation.", "method": "Uses MEMS-LiDAR for privacy-compliant 3D point clouds, combines real LiDAR recordings with synthetic scenes from CARLA simulation framework to create hybrid training data.", "result": "Hybrid data improves average precision by 44 percentage points compared to real-data-only models while reducing manual annotation effort by 50%.", "conclusion": "Proposed approach provides scalable, cost-efficient alternative that combines high person detection performance with GDPR compliance in industrial environments."}}
{"id": "2602.01780", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2602.01780", "abs": "https://arxiv.org/abs/2602.01780", "authors": ["Shicheng Yin", "Kaixuan Yin", "Weixing Chen", "Yang Liu", "Guanbin Li", "Liang Lin"], "title": "DDP-WM: Disentangled Dynamics Prediction for Efficient World Models", "comment": "Codes will be available at https://github.com/HCPLabSYSU/DDP-WM", "summary": "World models are essential for autonomous robotic planning. However, the substantial computational overhead of existing dense Transformerbased models significantly hinders real-time deployment. To address this efficiency-performance bottleneck, we introduce DDP-WM, a novel world model centered on the principle of Disentangled Dynamics Prediction (DDP). We hypothesize that latent state evolution in observed scenes is heterogeneous and can be decomposed into sparse primary dynamics driven by physical interactions and secondary context-driven background updates. DDP-WM realizes this decomposition through an architecture that integrates efficient historical processing with dynamic localization to isolate primary dynamics. By employing a crossattention mechanism for background updates, the framework optimizes resource allocation and provides a smooth optimization landscape for planners. Extensive experiments demonstrate that DDP-WM achieves significant efficiency and performance across diverse tasks, including navigation, precise tabletop manipulation, and complex deformable or multi-body interactions. Specifically, on the challenging Push-T task, DDP-WM achieves an approximately 9 times inference speedup and improves the MPC success rate from 90% to98% compared to state-of-the-art dense models. The results establish a promising path for developing efficient, high-fidelity world models. Codes will be available at https://github.com/HCPLabSYSU/DDP-WM.", "AI": {"tldr": "DDP-WM introduces Disentangled Dynamics Prediction to separate sparse primary dynamics from background updates, achieving 9x speedup and 98% success rate on Push-T task.", "motivation": "Existing dense Transformer-based world models have substantial computational overhead that hinders real-time deployment for autonomous robotic planning, creating an efficiency-performance bottleneck.", "method": "DDP-WM decomposes latent state evolution into sparse primary dynamics (physical interactions) and secondary context-driven background updates. Uses efficient historical processing with dynamic localization to isolate primary dynamics, and cross-attention for background updates.", "result": "Achieves significant efficiency and performance across diverse tasks including navigation, tabletop manipulation, and complex deformable/multi-body interactions. On Push-T task: ~9x inference speedup and improves MPC success rate from 90% to 98% compared to SOTA dense models.", "conclusion": "DDP-WM establishes a promising path for developing efficient, high-fidelity world models by disentangling dynamics, optimizing resource allocation, and providing smooth optimization for planners."}}
{"id": "2602.01783", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.01783", "abs": "https://arxiv.org/abs/2602.01783", "authors": ["Dibyayan Patra", "Pasindu Ranasinghe", "Bikram Banerjee", "Simit Raval"], "title": "Automated Discontinuity Set Characterisation in Enclosed Rock Face Point Clouds Using Single-Shot Filtering and Cyclic Orientation Transformation", "comment": null, "summary": "Characterisation of structural discontinuity sets in exposed rock faces of underground mine cavities is essential for assessing rock-mass stability, excavation safety, and operational efficiency. UAV and other mobile laser-scanning techniques provide efficient means of collecting point clouds from rock faces. However, the development of a robust and efficient approach for automatic characterisation of discontinuity sets in real-world scenarios, like fully enclosed rock faces in cavities, remains an open research problem. In this study, a new approach is proposed for automatic discontinuity set characterisation that uses a single-shot filtering strategy, an innovative cyclic orientation transformation scheme and a hierarchical clustering technique. The single-shot filtering step isolates planar regions while robustly suppressing noise and high-curvature artefacts in one pass using a signal-processing technique. To address the limitations of Cartesian clustering on polar orientation data, a cyclic orientation transformation scheme is developed, enabling accurate representation of dip angle and dip direction in Cartesian space. The transformed orientations are then characterised into sets using a hierarchical clustering technique, which handles varying density distributions and identifies clusters without requiring user-defined set numbers. The accuracy of the method is validated on real-world mine stope and against ground truth obtained using manually handpicked discontinuity planes identified with the Virtual Compass tool, as well as widely used automated structure mapping techniques. The proposed approach outperforms the other techniques by exhibiting the lowest mean absolute error in estimating discontinuity set orientations in real-world stope data with errors of 1.95\u00b0 and 2.20\u00b0 in nominal dip angle and dip direction, respectively, and dispersion errors lying below 3\u00b0.", "AI": {"tldr": "New automated method for discontinuity set characterization in underground mine rock faces using single-shot filtering, cyclic orientation transformation, and hierarchical clustering, achieving high accuracy with errors below 3\u00b0.", "motivation": "Characterizing structural discontinuity sets in underground mine cavities is crucial for rock-mass stability assessment and excavation safety, but existing automated methods struggle with real-world scenarios like fully enclosed rock faces.", "method": "Three-step approach: 1) Single-shot filtering using signal processing to isolate planar regions while suppressing noise and high-curvature artifacts; 2) Cyclic orientation transformation scheme to accurately represent dip angle and dip direction in Cartesian space; 3) Hierarchical clustering technique to characterize orientation sets without requiring user-defined cluster numbers.", "result": "Method outperforms existing techniques with lowest mean absolute error: 1.95\u00b0 in nominal dip angle and 2.20\u00b0 in dip direction, with dispersion errors below 3\u00b0. Validated on real-world mine stope data against ground truth from manual discontinuity plane picking.", "conclusion": "The proposed approach provides a robust and efficient solution for automatic discontinuity set characterization in challenging underground mining environments, addressing limitations of existing methods and achieving high accuracy in real-world applications."}}
{"id": "2602.01799", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.01799", "abs": "https://arxiv.org/abs/2602.01799", "authors": ["Ido Faran", "Nathan S. Netanyahu", "Maxim Shoshany"], "title": "Spatio-Temporal Transformers for Long-Term NDVI Forecasting", "comment": null, "summary": "Long-term satellite image time series (SITS) analysis in heterogeneous landscapes faces significant challenges, particularly in Mediterranean regions where complex spatial patterns, seasonal variations, and multi-decade environmental changes interact across different scales. This paper presents the Spatio-Temporal Transformer for Long Term Forecasting (STT-LTF ), an extended framework that advances beyond purely temporal analysis to integrate spatial context modeling with temporal sequence prediction. STT-LTF processes multi-scale spatial patches alongside temporal sequences (up to 20 years) through a unified transformer architecture, capturing both local neighborhood relationships and regional climate influences. The framework employs comprehensive self-supervised learning with spatial masking, temporal masking, and horizon sampling strategies, enabling robust model training from 40 years of unlabeled Landsat imagery. Unlike autoregressive approaches, STT-LTF directly predicts arbitrary future time points without error accumulation, incorporating spatial patch embeddings, cyclical temporal encoding, and geographic coordinates to learn complex dependencies across heterogeneous Mediterranean ecosystems. Experimental evaluation on Landsat data (1984-2024) demonstrates that STT-LTF achieves a Mean Absolute Error (MAE) of 0.0328 and R^2 of 0.8412 for next-year predictions, outperforming traditional statistical methods, CNN-based approaches, LSTM networks, and standard transformers. The framework's ability to handle irregular temporal sampling and variable prediction horizons makes it particularly suitable for analysis of heterogeneous landscapes experiencing rapid ecological transitions.", "AI": {"tldr": "STT-LTF is a transformer-based framework that integrates spatial context modeling with temporal sequence prediction for long-term satellite image analysis, outperforming existing methods on Mediterranean Landsat data.", "motivation": "Long-term satellite image analysis in heterogeneous landscapes like Mediterranean regions is challenging due to complex spatial patterns, seasonal variations, and multi-decade environmental changes interacting across different scales.", "method": "STT-LTF processes multi-scale spatial patches alongside temporal sequences (up to 20 years) through a unified transformer architecture with self-supervised learning using spatial masking, temporal masking, and horizon sampling strategies.", "result": "Achieves MAE of 0.0328 and R\u00b2 of 0.8412 for next-year predictions on Landsat data (1984-2024), outperforming traditional statistical methods, CNN-based approaches, LSTM networks, and standard transformers.", "conclusion": "The framework effectively handles irregular temporal sampling and variable prediction horizons, making it suitable for analyzing heterogeneous landscapes experiencing rapid ecological transitions."}}
{"id": "2602.01801", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.01801", "abs": "https://arxiv.org/abs/2602.01801", "authors": ["Dvir Samuel", "Issar Tzachor", "Matan Levy", "Micahel Green", "Gal Chechik", "Rami Ben-Ari"], "title": "Fast Autoregressive Video Diffusion and World Models with Temporal Cache Compression and Sparse Attention", "comment": "Project Page: https://dvirsamuel.github.io/fast-auto-regressive-video/", "summary": "Autoregressive video diffusion models enable streaming generation, opening the door to long-form synthesis, video world models, and interactive neural game engines. However, their core attention layers become a major bottleneck at inference time: as generation progresses, the KV cache grows, causing both increasing latency and escalating GPU memory, which in turn restricts usable temporal context and harms long-range consistency. In this work, we study redundancy in autoregressive video diffusion and identify three persistent sources: near-duplicate cached keys across frames, slowly evolving (largely semantic) queries/keys that make many attention computations redundant, and cross-attention over long prompts where only a small subset of tokens matters per frame. Building on these observations, we propose a unified, training-free attention framework for autoregressive diffusion: TempCache compresses the KV cache via temporal correspondence to bound cache growth; AnnCA accelerates cross-attention by selecting frame-relevant prompt tokens using fast approximate nearest neighbor (ANN) matching; and AnnSA sparsifies self-attention by restricting each query to semantically matched keys, also using a lightweight ANN. Together, these modules reduce attention, compute, and memory and are compatible with existing autoregressive diffusion backbones and world models. Experiments demonstrate up to x5--x10 end-to-end speedups while preserving near-identical visual quality and, crucially, maintaining stable throughput and nearly constant peak GPU memory usage over long rollouts, where prior methods progressively slow down and suffer from increasing memory usage.", "AI": {"tldr": "Training-free attention optimization framework for autoregressive video diffusion models that reduces KV cache growth and accelerates attention via temporal compression and ANN-based token selection.", "motivation": "Autoregressive video diffusion models suffer from growing KV cache during inference, causing increasing latency, escalating GPU memory usage, and restricting temporal context, which harms long-range consistency in generated videos.", "method": "Three training-free modules: 1) TempCache compresses KV cache via temporal correspondence to bound cache growth; 2) AnnCA accelerates cross-attention by selecting frame-relevant prompt tokens using approximate nearest neighbor matching; 3) AnnSA sparsifies self-attention by restricting queries to semantically matched keys using lightweight ANN.", "result": "Achieves 5-10x end-to-end speedups while preserving visual quality, maintaining stable throughput and nearly constant peak GPU memory usage over long rollouts, unlike prior methods that progressively slow down with increasing memory usage.", "conclusion": "The proposed attention optimization framework effectively addresses KV cache bottlenecks in autoregressive video diffusion, enabling efficient long-form video generation with consistent performance and memory usage."}}
{"id": "2602.01805", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.01805", "abs": "https://arxiv.org/abs/2602.01805", "authors": ["Menglin Han", "Zhangkai Ni"], "title": "FlowBypass: Rectified Flow Trajectory Bypass for Training-Free Image Editing", "comment": null, "summary": "Training-free image editing has attracted increasing attention for its efficiency and independence from training data. However, existing approaches predominantly rely on inversion-reconstruction trajectories, which impose an inherent trade-off: longer trajectories accumulate errors and compromise fidelity, while shorter ones fail to ensure sufficient alignment with the edit prompt. Previous attempts to address this issue typically employ backbone-specific feature manipulations, limiting general applicability. To address these challenges, we propose FlowBypass, a novel and analytical framework grounded in Rectified Flow that constructs a bypass directly connecting inversion and reconstruction trajectories, thereby mitigating error accumulation without relying on feature manipulations. We provide a formal derivation of two trajectories, from which we obtain an approximate bypass formulation and its numerical solution, enabling seamless trajectory transitions. Extensive experiments demonstrate that FlowBypass consistently outperforms state-of-the-art image editing methods, achieving stronger prompt alignment while preserving high-fidelity details in irrelevant regions.", "AI": {"tldr": "FlowBypass: A training-free image editing framework using Rectified Flow to create bypass trajectories between inversion and reconstruction, avoiding error accumulation and feature manipulation limitations.", "motivation": "Existing training-free image editing methods rely on inversion-reconstruction trajectories that face a trade-off: longer trajectories accumulate errors and reduce fidelity, while shorter ones fail to align properly with edit prompts. Previous solutions use backbone-specific feature manipulations, limiting general applicability.", "method": "FlowBypass uses Rectified Flow to analytically construct a bypass directly connecting inversion and reconstruction trajectories. The authors formally derive two trajectories, obtain an approximate bypass formulation and its numerical solution, enabling seamless trajectory transitions without feature manipulations.", "result": "Extensive experiments show FlowBypass consistently outperforms state-of-the-art image editing methods, achieving stronger prompt alignment while preserving high-fidelity details in irrelevant regions.", "conclusion": "FlowBypass provides a novel analytical framework that overcomes the limitations of existing training-free image editing approaches by creating bypass trajectories that mitigate error accumulation without relying on feature manipulations, offering better performance and broader applicability."}}
{"id": "2602.01812", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.01812", "abs": "https://arxiv.org/abs/2602.01812", "authors": ["Cheng Wang", "Qiyu Gao", "Fandong Zhang", "Shu Zhang", "Yizhou Yu"], "title": "LDRNet: Large Deformation Registration Model for Chest CT Registration", "comment": null, "summary": "Most of the deep learning based medical image registration algorithms focus on brain image registration tasks.Compared with brain registration, the chest CT registration has larger deformation, more complex background and region over-lap. In this paper, we propose a fast unsupervised deep learning method, LDRNet, for large deformation image registration of chest CT images. We first predict a coarse resolution registration field, then refine it from coarse to fine. We propose two innovative technical components: 1) a refine block that is used to refine the registration field in different resolutions, 2) a rigid block that is used to learn transformation matrix from high-level features. We train and evaluate our model on the private dataset and public dataset SegTHOR. We compare our performance with state-of-the-art traditional registration methods as well as deep learning registration models VoxelMorph, RCN, and LapIRN. The results demonstrate that our model achieves state-of-the-art performance for large deformation images registration and is much faster.", "AI": {"tldr": "LDRNet is a fast unsupervised deep learning method for large deformation chest CT registration that outperforms existing methods in both accuracy and speed.", "motivation": "Chest CT registration presents greater challenges than brain registration due to larger deformations, more complex backgrounds, and region overlap, requiring specialized solutions.", "method": "LDRNet uses a coarse-to-fine approach with two key components: a refine block for multi-resolution registration field refinement and a rigid block that learns transformation matrices from high-level features.", "result": "The model achieves state-of-the-art performance on large deformation image registration tasks and is significantly faster than both traditional methods and other deep learning approaches like VoxelMorph, RCN, and LapIRN.", "conclusion": "LDRNet provides an effective and efficient solution for challenging chest CT registration problems, addressing the limitations of existing methods for large deformation scenarios."}}
{"id": "2602.01814", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.01814", "abs": "https://arxiv.org/abs/2602.01814", "authors": ["Xiao Liang", "Yunzhu Zhang", "Linchao Zhu"], "title": "GPD: Guided Progressive Distillation for Fast and High-Quality Video Generation", "comment": null, "summary": "Diffusion models have achieved remarkable success in video generation; however, the high computational cost of the denoising process remains a major bottleneck. Existing approaches have shown promise in reducing the number of diffusion steps, but they often suffer from significant quality degradation when applied to video generation. We propose Guided Progressive Distillation (GPD), a framework that accelerates the diffusion process for fast and high-quality video generation. GPD introduces a novel training strategy in which a teacher model progressively guides a student model to operate with larger step sizes. The framework consists of two key components: (1) an online-generated training target that reduces optimization difficulty while improving computational efficiency, and (2) frequency-domain constraints in the latent space that promote the preservation of fine-grained details and temporal dynamics. Applied to the Wan2.1 model, GPD reduces the number of sampling steps from 48 to 6 while maintaining competitive visual quality on VBench. Compared with existing distillation methods, GPD demonstrates clear advantages in both pipeline simplicity and quality preservation.", "AI": {"tldr": "GPD accelerates video diffusion models by progressive distillation, reducing steps from 48 to 6 while maintaining quality.", "motivation": "Diffusion models for video generation suffer from high computational cost due to many denoising steps. Existing step-reduction methods cause significant quality degradation in video generation.", "method": "Guided Progressive Distillation (GPD) uses teacher-student progressive guidance with larger step sizes. Key components: 1) online-generated training targets for easier optimization and efficiency, 2) frequency-domain constraints in latent space to preserve details and temporal dynamics.", "result": "Applied to Wan2.1 model, GPD reduces sampling steps from 48 to 6 while maintaining competitive visual quality on VBench. Outperforms existing distillation methods in pipeline simplicity and quality preservation.", "conclusion": "GPD provides an effective framework for accelerating video diffusion models with minimal quality loss, addressing the computational bottleneck while preserving fine-grained details and temporal dynamics."}}
{"id": "2602.01816", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.01816", "abs": "https://arxiv.org/abs/2602.01816", "authors": ["Wenjin Hou", "Wei Liu", "Han Hu", "Xiaoxiao Sun", "Serena Yeung-Levy", "Hehe Fan"], "title": "Seeing Is Believing? A Benchmark for Multimodal Large Language Models on Visual Illusions and Anomalies", "comment": null, "summary": "Multimodal Large Language Models (MLLMs) have shown remarkable proficiency on general-purpose vision-language benchmarks, reaching or even exceeding human-level performance. However, these evaluations typically rely on standard in-distribution data, leaving the robustness of MLLMs largely unexamined when faced with scenarios that defy common-sense priors. To address this gap, we introduce VIA-Bench, a challenging benchmark designed to probe model performance on visual illusions and anomalies. It includes six core categories: color illusions, motion illusions, gestalt illusions, geometric and spatial illusions, general visual illusions, and visual anomalies. Through careful human-in-the-loop review, we construct over 1K high-quality question-answer pairs that require nuanced visual reasoning. Extensive evaluation of over 20 state-of-the-art MLLMs, including proprietary, open-source, and reasoning-enhanced models, uncovers significant vulnerabilities. Notably, we find that Chain-of-Thought (CoT) reasoning offers negligible robustness, often yielding ``brittle mirages'' where the model's logic collapses under illusory stimuli. Our findings reveal a fundamental divergence between machine and human perception, suggesting that resolving such perceptual bottlenecks is critical for the advancement of artificial general intelligence. The benchmark data and code will be released.", "AI": {"tldr": "VIA-Bench is a challenging benchmark for testing MLLMs on visual illusions and anomalies, revealing significant vulnerabilities in state-of-the-art models despite their strong performance on standard benchmarks.", "motivation": "Current MLLM evaluations focus on standard in-distribution data, leaving robustness to visual illusions and anomalies largely unexamined. There's a need to test models on scenarios that defy common-sense priors to understand their true perceptual capabilities.", "method": "Created VIA-Bench with six categories of visual illusions/anomalies (color, motion, gestalt, geometric/spatial, general visual illusions, and visual anomalies). Used human-in-the-loop review to construct over 1K high-quality QA pairs requiring nuanced visual reasoning. Evaluated over 20 state-of-the-art MLLMs including proprietary, open-source, and reasoning-enhanced models.", "result": "MLLMs show significant vulnerabilities on visual illusions and anomalies. Chain-of-Thought reasoning offers negligible robustness, often producing \"brittle mirages\" where model logic collapses under illusory stimuli. Reveals fundamental divergence between machine and human perception.", "conclusion": "Resolving perceptual bottlenecks in MLLMs is critical for advancing artificial general intelligence. The benchmark exposes limitations not captured by standard evaluations and highlights the need for more robust visual reasoning capabilities."}}
{"id": "2602.01836", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.01836", "abs": "https://arxiv.org/abs/2602.01836", "authors": ["Yin Wu", "Daniel Slieter", "Carl Esselborn", "Ahmed Abouelazm", "Tsung Yuan Tseng", "J. Marius Z\u00f6llner"], "title": "Efficient Cross-Country Data Acquisition Strategy for ADAS via Street-View Imagery", "comment": null, "summary": "Deploying ADAS and ADS across countries remains challenging due to differences in legislation, traffic infrastructure, and visual conventions, which introduce domain shifts that degrade perception performance. Traditional cross-country data collection relies on extensive on-road driving, making it costly and inefficient to identify representative locations. To address this, we propose a street-view-guided data acquisition strategy that leverages publicly available imagery to identify places of interest (POI). Two POI scoring methods are introduced: a KNN-based feature distance approach using a vision foundation model, and a visual-attribution approach using a vision-language model. To enable repeatable evaluation, we adopt a collect-detect protocol and construct a co-located dataset by pairing the Zenseact Open Dataset with Mapillary street-view images. Experiments on traffic sign detection, a task particularly sensitive to cross-country variations in sign appearance, show that our approach achieves performance comparable to random sampling while using only half of the target-domain data. We further provide cost estimations for full-country analysis, demonstrating that large-scale street-view processing remains economically feasible. These results highlight the potential of street-view-guided data acquisition for efficient and cost-effective cross-country model adaptation.", "AI": {"tldr": "Street-view-guided data acquisition uses public imagery to identify representative locations for cross-country ADAS/ADS adaptation, achieving comparable performance to random sampling with half the data.", "motivation": "Cross-country deployment of ADAS/ADS faces challenges due to domain shifts from differences in legislation, traffic infrastructure, and visual conventions. Traditional data collection through extensive on-road driving is costly and inefficient for identifying representative locations.", "method": "Proposes street-view-guided data acquisition using publicly available imagery to identify places of interest (POI). Introduces two POI scoring methods: 1) KNN-based feature distance using a vision foundation model, and 2) visual-attribution approach using a vision-language model. Uses collect-detect protocol and constructs co-located dataset pairing Zenseact Open Dataset with Mapillary street-view images.", "result": "Experiments on traffic sign detection show the approach achieves performance comparable to random sampling while using only half of the target-domain data. Cost estimations for full-country analysis demonstrate that large-scale street-view processing remains economically feasible.", "conclusion": "Street-view-guided data acquisition offers efficient and cost-effective solution for cross-country model adaptation in ADAS/ADS deployment, addressing domain shift challenges through smarter data collection strategies."}}
{"id": "2602.01843", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.01843", "abs": "https://arxiv.org/abs/2602.01843", "authors": ["Qian Xu", "Xi Li", "Fei Gao", "Jie Guo", "Haojuan Yuan", "Shuaipeng Fan", "Mingjin Zhang"], "title": "SPIRIT: Adapting Vision Foundation Models for Unified Single- and Multi-Frame Infrared Small Target Detection", "comment": null, "summary": "Infrared small target detection (IRSTD) is crucial for surveillance and early-warning, with deployments spanning both single-frame analysis and video-mode tracking. A practical solution should leverage vision foundation models (VFMs) to mitigate infrared data scarcity, while adopting a memory-attention-based temporal propagation framework that unifies single- and multi-frame inference. However, infrared small targets exhibit weak radiometric signals and limited semantic cues, which differ markedly from visible-spectrum imagery. This modality gap makes direct use of semantics-oriented VFMs and appearance-driven cross-frame association unreliable for IRSTD: hierarchical feature aggregation can submerge localized target peaks, and appearance-only memory attention becomes ambiguous, leading to spurious clutter associations. To address these challenges, we propose SPIRIT, a unified and VFM-compatible framework that adapts VFMs to IRSTD via lightweight physics-informed plug-ins. Spatially, PIFR refines features by approximating rank-sparsity decomposition to suppress structured background components and enhance sparse target-like signals. Temporally, PGMA injects history-derived soft spatial priors into memory cross-attention to constrain cross-frame association, enabling robust video detection while naturally reverting to single-frame inference when temporal context is absent. Experiments on multiple IRSTD benchmarks show consistent gains over VFM-based baselines and SOTA performance.", "AI": {"tldr": "SPIRIT is a unified framework that adapts vision foundation models to infrared small target detection using physics-informed plug-ins for spatial feature refinement and temporal memory attention with spatial priors.", "motivation": "Infrared small target detection suffers from data scarcity and modality gaps between infrared and visible-spectrum imagery. Direct use of vision foundation models fails because infrared targets have weak signals, limited semantic cues, and hierarchical feature aggregation submerges target peaks while appearance-only memory attention leads to spurious associations.", "method": "SPIRIT framework with two key components: 1) PIFR (spatial feature refinement via rank-sparsity decomposition to suppress background and enhance target signals), 2) PGMA (temporal memory attention with history-derived soft spatial priors to constrain cross-frame association). The framework unifies single- and multi-frame inference.", "result": "Experiments on multiple IRSTD benchmarks show consistent gains over VFM-based baselines and achieve state-of-the-art performance.", "conclusion": "SPIRIT successfully bridges the modality gap for infrared small target detection by adapting vision foundation models with lightweight physics-informed plug-ins, enabling robust video detection while maintaining single-frame inference capability."}}
{"id": "2602.01844", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.01844", "abs": "https://arxiv.org/abs/2602.01844", "authors": ["Yuliang Zhan", "Jian Li", "Wenbing Huang", "Wenbing Huang", "Yang Liu", "Hao Sun"], "title": "CloDS: Visual-Only Unsupervised Cloth Dynamics Learning in Unknown Conditions", "comment": "ICLR 2026", "summary": "Deep learning has demonstrated remarkable capabilities in simulating complex dynamic systems. However, existing methods require known physical properties as supervision or inputs, limiting their applicability under unknown conditions. To explore this challenge, we introduce Cloth Dynamics Grounding (CDG), a novel scenario for unsupervised learning of cloth dynamics from multi-view visual observations. We further propose Cloth Dynamics Splatting (CloDS), an unsupervised dynamic learning framework designed for CDG. CloDS adopts a three-stage pipeline that first performs video-to-geometry grounding and then trains a dynamics model on the grounded meshes. To cope with large non-linear deformations and severe self-occlusions during grounding, we introduce a dual-position opacity modulation that supports bidirectional mapping between 2D observations and 3D geometry via mesh-based Gaussian splatting in video-to-geometry grounding stage. It jointly considers the absolute and relative position of Gaussian components. Comprehensive experimental evaluations demonstrate that CloDS effectively learns cloth dynamics from visual data while maintaining strong generalization capabilities for unseen configurations. Our code is available at https://github.com/whynot-zyl/CloDS. Visualization results are available at https://github.com/whynot-zyl/CloDS_video}.%\\footnote{As in this example.", "AI": {"tldr": "CloDS is an unsupervised framework that learns cloth dynamics from multi-view videos without physical supervision, using mesh-based Gaussian splatting with dual-position opacity modulation to handle large deformations and occlusions.", "motivation": "Existing deep learning methods for simulating dynamic systems require known physical properties as supervision, limiting their applicability under unknown conditions where physical properties are unavailable.", "method": "Three-stage pipeline: 1) Video-to-geometry grounding using mesh-based Gaussian splatting with dual-position opacity modulation (considers both absolute and relative positions of Gaussian components), 2) Training dynamics model on grounded meshes, 3) Enables bidirectional mapping between 2D observations and 3D geometry.", "result": "Comprehensive experiments show CloDS effectively learns cloth dynamics from visual data while maintaining strong generalization capabilities for unseen configurations.", "conclusion": "CloDS successfully addresses the CDG challenge by enabling unsupervised learning of cloth dynamics from multi-view visual observations without requiring physical supervision, with code and visualizations publicly available."}}
{"id": "2602.01850", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.01850", "abs": "https://arxiv.org/abs/2602.01850", "authors": ["Pei Li", "Jiaxi Yin", "Lei Ouyang", "Shihan Pan", "Ge Wang", "Han Ding", "Fei Wang"], "title": "WS-IMUBench: Can Weakly Supervised Methods from Audio, Image, and Video Be Adapted for IMU-based Temporal Action Localization?", "comment": "Under Review. 28 pages, 9 figures, 6 tables", "summary": "IMU-based Human Activity Recognition (HAR) has enabled a wide range of ubiquitous computing applications, yet its dominant clip classification paradigm cannot capture the rich temporal structure of real-world behaviors. This motivates a shift toward IMU Temporal Action Localization (IMU-TAL), which predicts both action categories and their start/end times in continuous streams. However, current progress is strongly bottlenecked by the need for dense, frame-level boundary annotations, which are costly and difficult to scale. To address this bottleneck, we introduce WS-IMUBench, a systematic benchmark study of weakly supervised IMU-TAL (WS-IMU-TAL) under only sequence-level labels. Rather than proposing a new localization algorithm, we evaluate how well established weakly supervised localization paradigms from audio, image, and video transfer to IMU-TAL under only sequence-level labels. We benchmark seven representative weakly supervised methods on seven public IMU datasets, resulting in over 3,540 model training runs and 7,080 inference evaluations. Guided by three research questions on transferability, effectiveness, and insights, our findings show that (i) transfer is modality-dependent, with temporal-domain methods generally more stable than image-derived proposal-based approaches; (ii) weak supervision can be competitive on favorable datasets (e.g., with longer actions and higher-dimensional sensing); and (iii) dominant failure modes arise from short actions, temporal ambiguity, and proposal quality. Finally, we outline concrete directions for advancing WS-IMU-TAL (e.g., IMU-specific proposal generation, boundary-aware objectives, and stronger temporal reasoning). Beyond individual results, WS-IMUBench establishes a reproducible benchmarking template, datasets, protocols, and analyses, to accelerate community-wide progress toward scalable WS-IMU-TAL.", "AI": {"tldr": "WS-IMUBench is a benchmark study evaluating weakly supervised temporal action localization for IMU data using only sequence-level labels, comparing 7 methods across 7 datasets with over 3,540 training runs.", "motivation": "Current IMU Temporal Action Localization (IMU-TAL) requires costly frame-level boundary annotations, creating a scalability bottleneck. Weak supervision using only sequence-level labels could enable more scalable IMU-TAL applications.", "method": "Systematically benchmarked 7 representative weakly supervised localization methods from audio, image, and video domains on 7 public IMU datasets. Conducted 3,540 model training runs and 7,080 inference evaluations to study transferability, effectiveness, and insights across three research questions.", "result": "Temporal-domain methods transfer better than image-derived approaches; weak supervision can be competitive on datasets with longer actions and higher-dimensional sensing; main failure modes are short actions, temporal ambiguity, and poor proposal quality.", "conclusion": "WS-IMUBench establishes a reproducible benchmarking framework and identifies concrete directions for advancing WS-IMU-TAL, including IMU-specific proposal generation, boundary-aware objectives, and stronger temporal reasoning."}}
{"id": "2602.01851", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.01851", "abs": "https://arxiv.org/abs/2602.01851", "authors": ["Huanyu Zhang", "Xuehai Bai", "Chengzu Li", "Chen Liang", "Haochen Tian", "Haodong Li", "Ruichuan An", "Yifan Zhang", "Anna Korhonen", "Zhang Zhang", "Liang Wang", "Tieniu Tan"], "title": "How Well Do Models Follow Visual Instructions? VIBE: A Systematic Benchmark for Visual Instruction-Driven Image Editing", "comment": "https://vibe-benchmark.github.io/", "summary": "Recent generative models have achieved remarkable progress in image editing. However, existing systems and benchmarks remain largely text-guided. In contrast, human communication is inherently multimodal, where visual instructions such as sketches efficiently convey spatial and structural intent. To address this gap, we introduce VIBE, the Visual Instruction Benchmark for Image Editing with a three-level interaction hierarchy that captures deictic grounding, morphological manipulation, and causal reasoning. Across these levels, we curate high-quality and diverse test cases that reflect progressively increasing complexity in visual instruction following. We further propose a robust LMM-as-a-judge evaluation framework with task-specific metrics to enable scalable and fine-grained assessment. Through a comprehensive evaluation of 17 representative open-source and proprietary image editing models, we find that proprietary models exhibit early-stage visual instruction-following capabilities and consistently outperform open-source models. However, performance degrades markedly with increasing task difficulty even for the strongest systems, highlighting promising directions for future research.", "AI": {"tldr": "VIBE is a visual instruction benchmark for image editing that introduces a three-level hierarchy to evaluate models' ability to follow visual instructions like sketches, with comprehensive evaluation showing proprietary models outperform open-source ones but all struggle with complex tasks.", "motivation": "Current image editing systems and benchmarks are primarily text-guided, while human communication is multimodal with visual instructions (like sketches) being more efficient for conveying spatial and structural intent. There's a gap in evaluating models' ability to follow visual instructions.", "method": "Introduces VIBE (Visual Instruction Benchmark for Image Editing) with a three-level interaction hierarchy: deictic grounding, morphological manipulation, and causal reasoning. Curates high-quality diverse test cases with increasing complexity. Proposes an LMM-as-a-judge evaluation framework with task-specific metrics for scalable, fine-grained assessment.", "result": "Comprehensive evaluation of 17 open-source and proprietary models shows proprietary models exhibit early-stage visual instruction-following capabilities and consistently outperform open-source models. Performance degrades markedly with increasing task difficulty even for the strongest systems.", "conclusion": "Visual instruction following remains challenging, especially for complex tasks, highlighting promising research directions. The benchmark and evaluation framework provide tools for advancing multimodal image editing systems beyond text-only guidance."}}
{"id": "2602.01854", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.01854", "abs": "https://arxiv.org/abs/2602.01854", "authors": ["A S M Sharifuzzaman Sagar", "Mohammed Bennamoun", "Farid Boussaid", "Naeha Sharif", "Lian Xu", "Shaaban Sahmoud", "Ali Kishk"], "title": "Fact or Fake? Assessing the Role of Deepfake Detectors in Multimodal Misinformation Detection", "comment": null, "summary": "In multimodal misinformation, deception usually arises not just from pixel-level manipulations in an image, but from the semantic and contextual claim jointly expressed by the image-text pair. Yet most deepfake detectors, engineered to detect pixel-level forgeries, do not account for claim-level meaning, despite their growing integration in automated fact-checking (AFC) pipelines. This raises a central scientific and practical question: Do pixel-level detectors contribute useful signal for verifying image-text claims, or do they instead introduce misleading authenticity priors that undermine evidence-based reasoning? We provide the first systematic analysis of deepfake detectors in the context of multimodal misinformation detection. Using two complementary benchmarks, MMFakeBench and DGM4, we evaluate: (1) state-of-the-art image-only deepfake detectors, (2) an evidence-driven fact-checking system that performs tool-guided retrieval via Monte Carlo Tree Search (MCTS) and engages in deliberative inference through Multi-Agent Debate (MAD), and (3) a hybrid fact-checking system that injects detector outputs as auxiliary evidence. Results across both benchmark datasets show that deepfake detectors offer limited standalone value, achieving F1 scores in the range of 0.26-0.53 on MMFakeBench and 0.33-0.49 on DGM4, and that incorporating their predictions into fact-checking pipelines consistently reduces performance by 0.04-0.08 F1 due to non-causal authenticity assumptions. In contrast, the evidence-centric fact-checking system achieves the highest performance, reaching F1 scores of approximately 0.81 on MMFakeBench and 0.55 on DGM4. Overall, our findings demonstrate that multimodal claim verification is driven primarily by semantic understanding and external evidence, and that pixel-level artifact signals do not reliably enhance reasoning over real-world image-text misinformation.", "AI": {"tldr": "Pixel-level deepfake detectors provide limited value for multimodal misinformation detection and actually harm fact-checking performance when integrated, while evidence-based systems perform best.", "motivation": "Current deepfake detectors focus on pixel-level manipulations but ignore semantic meaning in image-text claims, raising questions about their usefulness in automated fact-checking pipelines for multimodal misinformation.", "method": "Systematic evaluation using MMFakeBench and DGM4 benchmarks to compare: (1) image-only deepfake detectors, (2) evidence-driven fact-checking system with MCTS retrieval and Multi-Agent Debate, and (3) hybrid system combining detector outputs with evidence-based approach.", "result": "Deepfake detectors achieved poor F1 scores (0.26-0.53 on MMFakeBench, 0.33-0.49 on DGM4). Incorporating them into fact-checking pipelines reduced performance by 0.04-0.08 F1. Evidence-centric system achieved highest performance (0.81 F1 on MMFakeBench, 0.55 on DGM4).", "conclusion": "Multimodal claim verification relies primarily on semantic understanding and external evidence, not pixel-level artifact signals. Deepfake detectors introduce misleading authenticity priors that undermine evidence-based reasoning in misinformation detection."}}
{"id": "2602.01864", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.01864", "abs": "https://arxiv.org/abs/2602.01864", "authors": ["Yuan Wang", "Yuhao Wan", "Siming Zheng", "Bo Li", "Qibin Hou", "Peng-Tao Jiang"], "title": "Trust but Verify: Adaptive Conditioning for Reference-Based Diffusion Super-Resolution via Implicit Reference Correlation Modeling", "comment": "26 pages, 19 figures. Accepted to ICLR 2026", "summary": "Recent works have explored reference-based super-resolution (RefSR) to mitigate hallucinations in diffusion-based image restoration. A key challenge is that real-world degradations make correspondences between low-quality (LQ) inputs and reference (Ref) images unreliable, requiring adaptive control of reference usage. Existing methods either ignore LQ-Ref correlations or rely on brittle explicit matching, leading to over-reliance on misleading references or under-utilization of valuable cues. To address this, we propose Ada-RefSR, a single-step diffusion framework guided by a \"Trust but Verify\" principle: reference information is leveraged when reliable and suppressed otherwise. Its core component, Adaptive Implicit Correlation Gating (AICG), employs learnable summary tokens to distill dominant reference patterns and capture implicit correlations with LQ features. Integrated into the attention backbone, AICG provides lightweight, adaptive regulation of reference guidance, serving as a built-in safeguard against erroneous fusion. Experiments on multiple datasets demonstrate that Ada-RefSR achieves a strong balance of fidelity, naturalness, and efficiency, while remaining robust under varying reference alignment.", "AI": {"tldr": "Ada-RefSR: A single-step diffusion framework for reference-based super-resolution that adaptively controls reference usage via learnable tokens and implicit correlation gating, balancing fidelity and naturalness while being robust to unreliable references.", "motivation": "Real-world degradations make correspondences between low-quality inputs and reference images unreliable in reference-based super-resolution. Existing methods either ignore LQ-Ref correlations or rely on brittle explicit matching, leading to over-reliance on misleading references or under-utilization of valuable cues.", "method": "Proposes Ada-RefSR with a \"Trust but Verify\" principle and Adaptive Implicit Correlation Gating (AICG). AICG uses learnable summary tokens to distill dominant reference patterns and capture implicit correlations with LQ features, integrated into attention backbone for lightweight adaptive regulation of reference guidance.", "result": "Experiments on multiple datasets demonstrate that Ada-RefSR achieves strong balance of fidelity, naturalness, and efficiency while remaining robust under varying reference alignment.", "conclusion": "Ada-RefSR provides an effective solution for adaptive reference usage in super-resolution, with built-in safeguards against erroneous fusion through its implicit correlation gating mechanism."}}
{"id": "2602.01881", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.01881", "abs": "https://arxiv.org/abs/2602.01881", "authors": ["Ye Chen", "Yupeng Zhu", "Xiongzhen Zhang", "Zhewen Wan", "Yingzhe Li", "Wenjun Zhang", "Bingbing Ni"], "title": "ProxyImg: Towards Highly-Controllable Image Representation via Hierarchical Disentangled Proxy Embedding", "comment": null, "summary": "Prevailing image representation methods, including explicit representations such as raster images and Gaussian primitives, as well as implicit representations such as latent images, either suffer from representation redundancy that leads to heavy manual editing effort, or lack a direct mapping from latent variables to semantic instances or parts, making fine-grained manipulation difficult. These limitations hinder efficient and controllable image and video editing. To address these issues, we propose a hierarchical proxy-based parametric image representation that disentangles semantic, geometric, and textural attributes into independent and manipulable parameter spaces. Based on a semantic-aware decomposition of the input image, our representation constructs hierarchical proxy geometries through adaptive Bezier fitting and iterative internal region subdivision and meshing. Multi-scale implicit texture parameters are embedded into the resulting geometry-aware distributed proxy nodes, enabling continuous high-fidelity reconstruction in the pixel domain and instance- or part-independent semantic editing. In addition, we introduce a locality-adaptive feature indexing mechanism to ensure spatial texture coherence, which further supports high-quality background completion without relying on generative models. Extensive experiments on image reconstruction and editing benchmarks, including ImageNet, OIR-Bench, and HumanEdit, demonstrate that our method achieves state-of-the-art rendering fidelity with significantly fewer parameters, while enabling intuitive, interactive, and physically plausible manipulation. Moreover, by integrating proxy nodes with Position-Based Dynamics, our framework supports real-time physics-driven animation using lightweight implicit rendering, achieving superior temporal consistency and visual realism compared with generative approaches.", "AI": {"tldr": "Proposes hierarchical proxy-based parametric image representation that disentangles semantic, geometric, and textural attributes for efficient and controllable image/video editing.", "motivation": "Existing image representations (raster images, Gaussian primitives, latent images) suffer from representation redundancy or lack direct mapping from latent variables to semantic instances/parts, hindering efficient and controllable editing.", "method": "Hierarchical proxy-based parametric representation with semantic-aware decomposition, adaptive Bezier fitting, iterative region subdivision/meshing, multi-scale implicit texture parameters embedded in geometry-aware proxy nodes, and locality-adaptive feature indexing for spatial coherence.", "result": "Achieves state-of-the-art rendering fidelity with significantly fewer parameters on ImageNet, OIR-Bench, and HumanEdit benchmarks, enables intuitive interactive editing, and supports real-time physics-driven animation with superior temporal consistency.", "conclusion": "The proposed representation addresses limitations of existing methods by enabling disentangled, manipulable parameter spaces for semantic, geometric, and textural attributes, supporting efficient high-fidelity reconstruction and controllable editing."}}
{"id": "2602.01901", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.01901", "abs": "https://arxiv.org/abs/2602.01901", "authors": ["Jiedong Zhuang", "Lu Lu", "Ming Dai", "Rui Hu", "Jian Chen", "Qiang Liu", "Haoji Hu"], "title": "Q Cache: Visual Attention is Valuable in Less than Half of Decode Layers for Multimodal Large Language Model", "comment": "Accepted by AAAI26", "summary": "Multimodal large language models (MLLMs) are plagued by exorbitant inference costs attributable to the profusion of visual tokens within the vision encoder. The redundant visual tokens engenders a substantial computational load and key-value (KV) cache footprint bottleneck. Existing approaches focus on token-wise optimization, leveraging diverse intricate token pruning techniques to eliminate non-crucial visual tokens. Nevertheless, these methods often unavoidably undermine the integrity of the KV cache, resulting in failures in long-text generation tasks. To this end, we conduct an in-depth investigation towards the attention mechanism of the model from a new perspective, and discern that attention within more than half of all decode layers are semantic similar. Upon this finding, we contend that the attention in certain layers can be streamlined by inheriting the attention from their preceding layers. Consequently, we propose Lazy Attention, an efficient attention mechanism that enables cross-layer sharing of similar attention patterns. It ingeniously reduces layer-wise redundant computation in attention. In Lazy Attention, we develop a novel layer-shared cache, Q Cache, tailored for MLLMs, which facilitates the reuse of queries across adjacent layers. In particular, Q Cache is lightweight and fully compatible with existing inference frameworks, including Flash Attention and KV cache. Additionally, our method is highly flexible as it is orthogonal to existing token-wise techniques and can be deployed independently or combined with token pruning approaches. Empirical evaluations on multiple benchmarks demonstrate that our method can reduce KV cache usage by over 35% and achieve 1.5x throughput improvement, while sacrificing only approximately 1% of performance on various MLLMs. Compared with SOTA token-wise methods, our technique achieves superior accuracy preservation.", "AI": {"tldr": "Lazy Attention reduces MLLM inference costs by sharing attention patterns across layers via Q Cache, cutting KV cache by 35% and boosting throughput 1.5x with minimal accuracy loss.", "motivation": "Multimodal LLMs suffer from high inference costs due to excessive visual tokens that create computational load and KV cache bottlenecks. Existing token pruning methods damage KV cache integrity and fail in long-text generation tasks.", "method": "Proposes Lazy Attention mechanism that enables cross-layer sharing of similar attention patterns. Develops Q Cache for MLLMs that reuses queries across adjacent layers. The method is lightweight, compatible with existing frameworks (Flash Attention, KV cache), and orthogonal to token pruning techniques.", "result": "Reduces KV cache usage by over 35%, achieves 1.5x throughput improvement, with only ~1% performance degradation on various MLLMs. Outperforms state-of-the-art token-wise methods in accuracy preservation.", "conclusion": "Lazy Attention provides an efficient attention mechanism that addresses KV cache bottlenecks in MLLMs through cross-layer attention sharing, offering significant efficiency gains while maintaining high accuracy."}}
{"id": "2602.01905", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.01905", "abs": "https://arxiv.org/abs/2602.01905", "authors": ["Theodore Zhengde Zhao", "Sid Kiblawi", "Jianwei Yang", "Naoto Usuyama", "Reuben Tan", "Noel C Codella", "Tristan Naumann", "Hoifung Poon", "Mu Wei"], "title": "Learning Sparse Visual Representations via Spatial-Semantic Factorization", "comment": null, "summary": "Self-supervised learning (SSL) faces a fundamental conflict between semantic understanding and image reconstruction. High-level semantic SSL (e.g., DINO) relies on global tokens that are forced to be location-invariant for augmentation alignment, a process that inherently discards the spatial coordinates required for reconstruction. Conversely, generative SSL (e.g., MAE) preserves dense feature grids for reconstruction but fails to produce high-level abstractions. We introduce STELLAR, a framework that resolves this tension by factorizing visual features into a low-rank product of semantic concepts and their spatial distributions. This disentanglement allows us to perform DINO-style augmentation alignment on the semantic tokens while maintaining the precise spatial mapping in the localization matrix necessary for pixel-level reconstruction. We demonstrate that as few as 16 sparse tokens under this factorized form are sufficient to simultaneously support high-quality reconstruction (2.60 FID) and match the semantic performance of dense backbones (79.10% ImageNet accuracy). Our results highlight STELLAR as a versatile sparse representation that bridges the gap between discriminative and generative vision by strategically separating semantic identity from spatial geometry. Code available at https://aka.ms/stellar.", "AI": {"tldr": "STELLAR resolves the conflict between semantic SSL (like DINO) and generative SSL (like MAE) by factorizing visual features into semantic concepts and their spatial distributions, enabling both high-quality reconstruction and strong semantic performance with sparse tokens.", "motivation": "There's a fundamental conflict in self-supervised learning: semantic SSL methods like DINO discard spatial information needed for reconstruction, while generative SSL methods like MAE preserve spatial details but lack high-level abstractions. The paper aims to bridge this gap between discriminative and generative vision.", "method": "STELLAR factorizes visual features into a low-rank product of semantic concepts (tokens) and their spatial distributions (localization matrix). This disentanglement allows DINO-style augmentation alignment on semantic tokens while maintaining precise spatial mapping for pixel-level reconstruction. The framework uses as few as 16 sparse tokens.", "result": "STELLAR achieves high-quality reconstruction (2.60 FID) while matching the semantic performance of dense backbones (79.10% ImageNet accuracy). The sparse representation simultaneously supports both discriminative and generative tasks, bridging the gap between these two approaches.", "conclusion": "STELLAR provides a versatile sparse representation that strategically separates semantic identity from spatial geometry, resolving the tension between semantic understanding and image reconstruction in self-supervised learning."}}
{"id": "2602.01906", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.01906", "abs": "https://arxiv.org/abs/2602.01906", "authors": ["Farhan Ullah", "Irfan Ullah", "Khalil Khan", "Giovanni Pau", "JaKeoung Koo"], "title": "DSXFormer: Dual-Pooling Spectral Squeeze-Expansion and Dynamic Context Attention Transformer for Hyperspectral Image Classification", "comment": null, "summary": "Hyperspectral image classification (HSIC) is a challenging task due to high spectral dimensionality, complex spectral-spatial correlations, and limited labeled training samples. Although transformer-based models have shown strong potential for HSIC, existing approaches often struggle to achieve sufficient spectral discriminability while maintaining computational efficiency. To address these limitations, we propose a novel DSXFormer, a novel dual-pooling spectral squeeze-expansion transformer with Dynamic Context Attention for HSIC. The proposed DSXFormer introduces a Dual-Pooling Spectral Squeeze-Expansion (DSX) block, which exploits complementary global average and max pooling to adaptively recalibrate spectral feature channels, thereby enhancing spectral discriminability and inter-band dependency modeling. In addition, DSXFormer incorporates a Dynamic Context Attention (DCA) mechanism within a window-based transformer architecture to dynamically capture local spectral-spatial relationships while significantly reducing computational overhead. The joint integration of spectral dual-pooling squeeze-expansion and DCA enables DSXFormer to achieve an effective balance between spectral emphasis and spatial contextual representation. Furthermore, patch extraction, embedding, and patch merging strategies are employed to facilitate efficient multi-scale feature learning. Extensive experiments conducted on four widely used hyperspectral benchmark datasets, including Salinas (SA), Indian Pines (IP), Pavia University (PU), and Kennedy Space Center (KSC), demonstrate that DSXFormer consistently outperforms state-of-the-art methods, achieving classification accuracies of 99.95%, 98.91%, 99.85%, and 98.52%, respectively.", "AI": {"tldr": "DSXFormer is a novel transformer-based model for hyperspectral image classification that uses dual-pooling spectral squeeze-expansion and dynamic context attention to improve spectral discriminability while maintaining computational efficiency.", "motivation": "Hyperspectral image classification faces challenges due to high spectral dimensionality, complex spectral-spatial correlations, and limited labeled training samples. Existing transformer-based models struggle to achieve sufficient spectral discriminability while maintaining computational efficiency.", "method": "Proposes DSXFormer with two key components: 1) Dual-Pooling Spectral Squeeze-Expansion (DSX) block that uses complementary global average and max pooling to adaptively recalibrate spectral feature channels, and 2) Dynamic Context Attention (DCA) mechanism within a window-based transformer architecture to dynamically capture local spectral-spatial relationships with reduced computational overhead. Also employs patch extraction, embedding, and patch merging for multi-scale feature learning.", "result": "Extensive experiments on four benchmark datasets (Salinas, Indian Pines, Pavia University, Kennedy Space Center) show DSXFormer consistently outperforms state-of-the-art methods with classification accuracies of 99.95%, 98.91%, 99.85%, and 98.52% respectively.", "conclusion": "DSXFormer effectively balances spectral emphasis and spatial contextual representation, achieving superior performance in hyperspectral image classification while maintaining computational efficiency through its innovative dual-pooling spectral squeeze-expansion and dynamic context attention mechanisms."}}
{"id": "2602.01951", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.01951", "abs": "https://arxiv.org/abs/2602.01951", "authors": ["Shuyang Wu", "Yifu Qiu", "Ines P. Nearchou", "Sandrine Prost", "Jonathan A Fallowfield", "Hakan Bilen", "Timothy J Kendall"], "title": "Enabling Progressive Whole-slide Image Analysis with Multi-scale Pyramidal Network", "comment": null, "summary": "Multiple-instance Learning (MIL) is commonly used to undertake computational pathology (CPath) tasks, and the use of multi-scale patches allows diverse features across scales to be learned. Previous studies using multi-scale features in clinical applications rely on multiple inputs across magnifications with late feature fusion, which does not retain the link between features across scales while the inputs are dependent on arbitrary, manufacturer-defined magnifications, being inflexible and computationally expensive. In this paper, we propose the Multi-scale Pyramidal Network (MSPN), which is plug-and-play over attention-based MIL that introduces progressive multi-scale analysis on WSI. Our MSPN consists of (1) grid-based remapping that uses high magnification features to derive coarse features and (2) the coarse guidance network (CGN) that learns coarse contexts. We benchmark MSPN as an add-on module to 4 attention-based frameworks using 4 clinically relevant tasks across 3 types of foundation model, as well as the pre-trained MIL framework. We show that MSPN consistently improves MIL across the compared configurations and tasks, while being lightweight and easy-to-use.", "AI": {"tldr": "MSPN is a plug-and-play multi-scale pyramidal network that improves attention-based MIL for computational pathology by enabling progressive multi-scale analysis without relying on arbitrary manufacturer-defined magnifications.", "motivation": "Current multi-scale approaches in computational pathology use late feature fusion with multiple inputs at arbitrary magnifications, which loses cross-scale feature relationships, is inflexible, and computationally expensive.", "method": "Proposes MSPN with two components: (1) grid-based remapping that uses high magnification features to derive coarse features, and (2) coarse guidance network (CGN) that learns coarse contexts. It's plug-and-play over attention-based MIL frameworks.", "result": "MSPN consistently improves MIL performance across 4 attention-based frameworks, 4 clinically relevant tasks, 3 foundation model types, and pre-trained MIL framework, while being lightweight and easy-to-use.", "conclusion": "MSPN provides an effective, flexible solution for multi-scale analysis in computational pathology that overcomes limitations of previous approaches and consistently enhances MIL performance."}}
{"id": "2602.01954", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.01954", "abs": "https://arxiv.org/abs/2602.01954", "authors": ["Shuai Yang", "Ziyue Huang", "Jiaxin Chen", "Qingjie Liu", "Yunhong Wang"], "title": "Beyond Open Vocabulary: Multimodal Prompting for Object Detection in Remote Sensing Images", "comment": null, "summary": "Open-vocabulary object detection in remote sensing commonly relies on text-only prompting to specify target categories, implicitly assuming that inference-time category queries can be reliably grounded through pretraining-induced text-visual alignment. In practice, this assumption often breaks down in remote sensing scenarios due to task- and application-specific category semantics, resulting in unstable category specification under open-vocabulary settings. To address this limitation, we propose RS-MPOD, a multimodal open-vocabulary detection framework that reformulates category specification beyond text-only prompting by incorporating instance-grounded visual prompts, textual prompts, and their multimodal integration. RS-MPOD introduces a visual prompt encoder to extract appearance-based category cues from exemplar instances, enabling text-free category specification, and a multimodal fusion module to integrate visual and textual information when both modalities are available. Extensive experiments on standard, cross-dataset, and fine-grained remote sensing benchmarks show that visual prompting yields more reliable category specification under semantic ambiguity and distribution shifts, while multimodal prompting provides a flexible alternative that remains competitive when textual semantics are well aligned.", "AI": {"tldr": "RS-MPOD is a multimodal open-vocabulary object detection framework for remote sensing that uses both visual and textual prompts to address semantic ambiguity issues in text-only approaches.", "motivation": "Text-only prompting for open-vocabulary object detection in remote sensing often fails due to task-specific category semantics and distribution shifts, leading to unstable category specification.", "method": "Proposes RS-MPOD with visual prompt encoder for appearance-based category cues from exemplar instances, enabling text-free specification, and multimodal fusion module to integrate visual and textual information.", "result": "Visual prompting provides more reliable category specification under semantic ambiguity and distribution shifts, while multimodal prompting remains competitive when textual semantics are well aligned.", "conclusion": "Multimodal prompting (visual + textual) addresses limitations of text-only approaches in remote sensing open-vocabulary detection, offering more robust and flexible category specification."}}
{"id": "2602.01973", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.01973", "abs": "https://arxiv.org/abs/2602.01973", "authors": ["Muli Yang", "Gabriel James Goenawan", "Henan Wang", "Huaiyuan Qin", "Chenghao Xu", "Yanhua Yang", "Fen Fang", "Ying Sun", "Joo-Hwee Lim", "Hongyuan Zhu"], "title": "Your AI-Generated Image Detector Can Secretly Achieve SOTA Accuracy, If Calibrated", "comment": "AAAI 2026. Code: https://github.com/muliyangm/AIGI-Det-Calib", "summary": "Despite being trained on balanced datasets, existing AI-generated image detectors often exhibit systematic bias at test time, frequently misclassifying fake images as real. We hypothesize that this behavior stems from distributional shift in fake samples and implicit priors learned during training. Specifically, models tend to overfit to superficial artifacts that do not generalize well across different generation methods, leading to a misaligned decision threshold when faced with test-time distribution shift. To address this, we propose a theoretically grounded post-hoc calibration framework based on Bayesian decision theory. In particular, we introduce a learnable scalar correction to the model's logits, optimized on a small validation set from the target distribution while keeping the backbone frozen. This parametric adjustment compensates for distributional shift in model output, realigning the decision boundary even without requiring ground-truth labels. Experiments on challenging benchmarks show that our approach significantly improves robustness without retraining, offering a lightweight and principled solution for reliable and adaptive AI-generated image detection in the open world. Code is available at https://github.com/muliyangm/AIGI-Det-Calib.", "AI": {"tldr": "A post-hoc calibration framework based on Bayesian decision theory to fix systematic bias in AI-generated image detectors that misclassify fake images as real due to distributional shift.", "motivation": "Existing AI-generated image detectors trained on balanced datasets show systematic bias at test time, frequently misclassifying fake images as real due to distributional shift in fake samples and implicit priors learned during training.", "method": "Propose a theoretically grounded post-hoc calibration framework based on Bayesian decision theory. Introduce a learnable scalar correction to model's logits, optimized on a small validation set from target distribution while keeping backbone frozen.", "result": "Experiments on challenging benchmarks show the approach significantly improves robustness without retraining, offering a lightweight and principled solution for reliable and adaptive AI-generated image detection.", "conclusion": "The proposed calibration framework effectively addresses systematic bias in AI-generated image detectors caused by distributional shift, providing a practical solution for open-world detection without requiring full retraining."}}
{"id": "2602.01984", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.01984", "abs": "https://arxiv.org/abs/2602.01984", "authors": ["Minyoung Lee", "Yeji Park", "Dongjun Hwang", "Yejin Kim", "Seong Joon Oh", "Junsuk Choe"], "title": "Enhancing Multi-Image Understanding through Delimiter Token Scaling", "comment": "Accepted at ICLR 2026", "summary": "Large Vision-Language Models (LVLMs) achieve strong performance on single-image tasks, but their performance declines when multiple images are provided as input. One major reason is the cross-image information leakage, where the model struggles to distinguish information across different images. Existing LVLMs already employ delimiter tokens to mark the start and end of each image, yet our analysis reveals that these tokens fail to effectively block cross-image information leakage. To enhance their effectiveness, we propose a method that scales the hidden states of delimiter tokens. This enhances the model's ability to preserve image-specific information by reinforcing intra-image interaction and limiting undesired cross-image interactions. Consequently, the model is better able to distinguish between images and reason over them more accurately. Experiments show performance gains on multi-image benchmarks such as Mantis, MuirBench, MIRB, and QBench2. We further evaluate our method on text-only tasks that require clear distinction. The method improves performance on multi-document and multi-table understanding benchmarks, including TQABench, MultiNews, and WCEP-10. Notably, our method requires no additional training or inference cost.", "AI": {"tldr": "Proposes scaling delimiter token hidden states to prevent cross-image information leakage in LVLMs, improving multi-image reasoning without extra training/inference cost.", "motivation": "LVLMs perform well on single-image tasks but decline with multiple images due to cross-image information leakage, where models struggle to distinguish information across different images despite using delimiter tokens.", "method": "Scales the hidden states of delimiter tokens to enhance their effectiveness in blocking cross-image information leakage, reinforcing intra-image interaction while limiting undesired cross-image interactions.", "result": "Shows performance gains on multi-image benchmarks (Mantis, MuirBench, MIRB, QBench2) and improves multi-document/multi-table understanding benchmarks (TQABench, MultiNews, WCEP-10) without additional training or inference cost.", "conclusion": "Simple scaling of delimiter token hidden states effectively addresses cross-image information leakage in LVLMs, improving multi-image reasoning and cross-document understanding while maintaining computational efficiency."}}
{"id": "2602.01991", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.01991", "abs": "https://arxiv.org/abs/2602.01991", "authors": ["Pablo Domingo-Gregorio", "Javier Ruiz-Hidalgo"], "title": "Leveraging Latent Vector Prediction for Localized Control in Image Generation via Diffusion Models", "comment": null, "summary": "Diffusion models emerged as a leading approach in text-to-image generation, producing high-quality images from textual descriptions. However, attempting to achieve detailed control to get a desired image solely through text remains a laborious trial-and-error endeavor. Recent methods have introduced image-level controls alongside with text prompts, using prior images to extract conditional information such as edges, segmentation and depth maps. While effective, these methods apply conditions uniformly across the entire image, limiting localized control. In this paper, we propose a novel methodology to enable precise local control over user-defined regions of an image, while leaving to the diffusion model the task of autonomously generating the remaining areas according to the original prompt. Our approach introduces a new training framework that incorporates masking features and an additional loss term, which leverages the prediction of the initial latent vector at any diffusion step to enhance the correspondence between the current step and the final sample in the latent space. Extensive experiments demonstrate that our method effectively synthesizes high-quality images with controlled local conditions.", "AI": {"tldr": "A novel diffusion model training framework enables precise local control over user-defined image regions while letting the model autonomously generate remaining areas according to text prompts.", "motivation": "Existing text-to-image diffusion models lack precise localized control - current methods apply conditions uniformly across entire images, making detailed control through text alone a trial-and-error process. There's a need for methods that allow users to control specific regions while letting the model handle the rest.", "method": "Proposes a new training framework with masking features and an additional loss term that leverages prediction of the initial latent vector at any diffusion step to enhance correspondence between current steps and final samples in latent space.", "result": "Extensive experiments demonstrate the method effectively synthesizes high-quality images with controlled local conditions, enabling precise regional control while maintaining overall image quality.", "conclusion": "The proposed approach successfully addresses the limitation of uniform conditioning in existing methods by enabling localized control over user-defined image regions while preserving the diffusion model's ability to generate coherent remaining areas according to text prompts."}}
{"id": "2602.02000", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.02000", "abs": "https://arxiv.org/abs/2602.02000", "authors": ["Bing He", "Jingnan Gao", "Yunuo Chen", "Ning Cao", "Gang Chen", "Zhengxue Cheng", "Li Song", "Wenjun Zhang"], "title": "SurfSplat: Conquering Feedforward 2D Gaussian Splatting with Surface Continuity Priors", "comment": "ICLR 2026", "summary": "Reconstructing 3D scenes from sparse images remains a challenging task due to the difficulty of recovering accurate geometry and texture without optimization. Recent approaches leverage generalizable models to generate 3D scenes using 3D Gaussian Splatting (3DGS) primitive. However, they often fail to produce continuous surfaces and instead yield discrete, color-biased point clouds that appear plausible at normal resolution but reveal severe artifacts under close-up views. To address this issue, we present SurfSplat, a feedforward framework based on 2D Gaussian Splatting (2DGS) primitive, which provides stronger anisotropy and higher geometric precision. By incorporating a surface continuity prior and a forced alpha blending strategy, SurfSplat reconstructs coherent geometry together with faithful textures. Furthermore, we introduce High-Resolution Rendering Consistency (HRRC), a new evaluation metric designed to evaluate high-resolution reconstruction quality. Extensive experiments on RealEstate10K, DL3DV, and ScanNet demonstrate that SurfSplat consistently outperforms prior methods on both standard metrics and HRRC, establishing a robust solution for high-fidelity 3D reconstruction from sparse inputs. Project page: https://hebing-sjtu.github.io/SurfSplat-website/", "AI": {"tldr": "SurfSplat: A feedforward framework using 2D Gaussian Splatting for high-fidelity 3D reconstruction from sparse images, addressing surface continuity and texture fidelity issues in previous methods.", "motivation": "Existing 3D reconstruction methods using 3D Gaussian Splatting often produce discrete, color-biased point clouds with poor surface continuity and severe artifacts under close-up views, failing to achieve accurate geometry and texture reconstruction from sparse images.", "method": "SurfSplat uses 2D Gaussian Splatting primitives for stronger anisotropy and higher geometric precision, incorporates surface continuity prior and forced alpha blending strategy for coherent geometry and faithful textures, and introduces High-Resolution Rendering Consistency (HRRC) evaluation metric.", "result": "Extensive experiments on RealEstate10K, DL3DV, and ScanNet show SurfSplat consistently outperforms prior methods on both standard metrics and the new HRRC metric, establishing robust high-fidelity 3D reconstruction from sparse inputs.", "conclusion": "SurfSplat provides a robust solution for high-fidelity 3D reconstruction from sparse images by addressing surface continuity and texture fidelity issues through 2D Gaussian Splatting with surface priors and forced alpha blending, validated by comprehensive experiments and a new evaluation metric."}}
{"id": "2602.02002", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.02002", "abs": "https://arxiv.org/abs/2602.02002", "authors": ["Guosheng Zhao", "Yaozeng Wang", "Xiaofeng Wang", "Zheng Zhu", "Tingdong Yu", "Guan Huang", "Yongchen Zai", "Ji Jiao", "Changliang Xue", "Xiaole Wang", "Zhen Yang", "Futang Zhu", "Xingang Wang"], "title": "UniDriveDreamer: A Single-Stage Multimodal World Model for Autonomous Driving", "comment": "16 pages, 7 figures", "summary": "World models have demonstrated significant promise for data synthesis in autonomous driving. However, existing methods predominantly concentrate on single-modality generation, typically focusing on either multi-camera video or LiDAR sequence synthesis. In this paper, we propose UniDriveDreamer, a single-stage unified multimodal world model for autonomous driving, which directly generates multimodal future observations without relying on intermediate representations or cascaded modules. Our framework introduces a LiDAR-specific variational autoencoder (VAE) designed to encode input LiDAR sequences, alongside a video VAE for multi-camera images. To ensure cross-modal compatibility and training stability, we propose Unified Latent Anchoring (ULA), which explicitly aligns the latent distributions of the two modalities. The aligned features are fused and processed by a diffusion transformer that jointly models their geometric correspondence and temporal evolution. Additionally, structured scene layout information is projected per modality as a conditioning signal to guide the synthesis. Extensive experiments demonstrate that UniDriveDreamer outperforms previous state-of-the-art methods in both video and LiDAR generation, while also yielding measurable improvements in downstream", "AI": {"tldr": "UniDriveDreamer: A single-stage unified multimodal world model for autonomous driving that jointly generates future multi-camera videos and LiDAR sequences without intermediate representations.", "motivation": "Existing world models for autonomous driving focus on single-modality generation (either video or LiDAR), lacking unified multimodal synthesis capabilities needed for comprehensive scene understanding.", "method": "Proposes a unified framework with: 1) LiDAR-specific VAE and video VAE for encoding inputs, 2) Unified Latent Anchoring (ULA) to align cross-modal latent distributions, 3) Diffusion transformer for joint modeling of geometric correspondence and temporal evolution, 4) Structured scene layout conditioning per modality.", "result": "Outperforms previous state-of-the-art methods in both video and LiDAR generation, and yields measurable improvements in downstream tasks.", "conclusion": "UniDriveDreamer demonstrates effective unified multimodal world modeling for autonomous driving, enabling joint synthesis of visual and geometric sensor data in a single-stage framework."}}
{"id": "2602.02004", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.02004", "abs": "https://arxiv.org/abs/2602.02004", "authors": ["Gongli Xi", "Kun Wang", "Zeming Gao", "Huahui Yi", "Haolang Lu", "Ye Tian", "Wendong Wang"], "title": "ClueTracer: Question-to-Vision Clue Tracing for Training-Free Hallucination Suppression in Multimodal Reasoning", "comment": "20 pages, 7 figures", "summary": "Large multimodal reasoning models solve challenging visual problems via explicit long-chain inference: they gather visual clues from images and decode clues into textual tokens. Yet this capability also increases hallucinations, where the model generates content that is not supported by the input image or the question. To understand this failure mode, we identify \\emph{reasoning drift}: during clue gathering, the model over-focuses on question-irrelevant entities, diluting focus on task-relevant cues and gradually decoupling the reasoning trace from visual grounding. As a consequence, many inference-time localization or intervention methods developed for non-reasoning models fail to pinpoint the true clues in reasoning settings. Motivated by these insights, we introduce ClueRecall, a metric for assessing visual clue retrieval, and present ClueTracer, a training-free, parameter-free, and architecture-agnostic plugin for hallucination suppression. ClueTracer starts from the question and traces how key clues propagate along the model's reasoning pathway (question $\\rightarrow$ outputs $\\rightarrow$ visual tokens), thereby localizing task-relevant patches while suppressing spurious attention to irrelevant regions. Remarkably, \\textbf{without any additional training}, ClueTracer improves all \\textbf{reasoning} architectures (including \\texttt{R1-OneVision}, \\texttt{Ocean-R1}, \\texttt{MM-Eureka}, \\emph{etc}.) by $\\mathbf{1.21\\times}$ on reasoning benchmarks. When transferred to \\textbf{non-reasoning} settings, it yields a $\\mathbf{1.14\\times}$ gain.", "AI": {"tldr": "The paper identifies \"reasoning drift\" as a key cause of hallucinations in multimodal reasoning models, where models focus on irrelevant entities during clue gathering. It introduces ClueRecall metric and ClueTracer plugin to suppress hallucinations by tracing clue propagation without additional training.", "motivation": "Large multimodal reasoning models suffer from hallucinations despite their strong reasoning capabilities. The authors identify \"reasoning drift\" - where models over-focus on question-irrelevant entities during clue gathering, diluting task-relevant cues and decoupling reasoning from visual grounding.", "method": "1) Introduce ClueRecall metric to assess visual clue retrieval. 2) Develop ClueTracer, a training-free, parameter-free, architecture-agnostic plugin that traces how key clues propagate along the reasoning pathway (question \u2192 outputs \u2192 visual tokens) to localize task-relevant patches while suppressing attention to irrelevant regions.", "result": "ClueTracer improves all reasoning architectures by 1.21\u00d7 on reasoning benchmarks without any additional training. When transferred to non-reasoning settings, it yields a 1.14\u00d7 gain.", "conclusion": "The paper successfully addresses hallucination in multimodal reasoning models through the identification of reasoning drift and development of ClueTracer, which effectively suppresses hallucinations by tracing clue propagation without requiring model retraining."}}
{"id": "2602.02014", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.02014", "abs": "https://arxiv.org/abs/2602.02014", "authors": ["Hongxin Xiang", "Pengsen Ma", "Yunkang Cao", "Di Yu", "Haowen Chen", "Xinyu Yang", "Xiangxiang Zeng"], "title": "Rethinking Genomic Modeling Through Optical Character Recognition", "comment": null, "summary": "Recent genomic foundation models largely adopt large language model architectures that treat DNA as a one-dimensional token sequence. However, exhaustive sequential reading is structurally misaligned with sparse and discontinuous genomic semantics, leading to wasted computation on low-information background and preventing understanding-driven compression for long contexts. Here, we present OpticalDNA, a vision-based framework that reframes genomic modeling as Optical Character Recognition (OCR)-style document understanding. OpticalDNA renders DNA into structured visual layouts and trains an OCR-capable vision--language model with a \\emph{visual DNA encoder} and a \\emph{document decoder}, where the encoder produces compact, reconstructible visual tokens for high-fidelity compression. Building on this representation, OpticalDNA defines prompt-conditioned objectives over core genomic primitives-reading, region grounding, subsequence retrieval, and masked span completion-thereby learning layout-aware DNA representations that retain fine-grained genomic information under a reduced effective token budget. Across diverse genomic benchmarks, OpticalDNA consistently outperforms recent baselines; on sequences up to 450k bases, it achieves the best overall performance with nearly $20\\times$ fewer effective tokens, and surpasses models with up to $985\\times$ more activated parameters while tuning only 256k \\emph{trainable} parameters.", "AI": {"tldr": "OpticalDNA reframes genomic modeling as OCR-style document understanding, using visual layouts to achieve 20x token efficiency and outperform larger models on long DNA sequences.", "motivation": "Current genomic foundation models treat DNA as 1D token sequences, which is structurally misaligned with sparse genomic semantics, wasting computation on low-information background and preventing efficient compression for long contexts.", "method": "OpticalDNA renders DNA into structured visual layouts and trains an OCR-capable vision-language model with a visual DNA encoder and document decoder. It defines prompt-conditioned objectives over genomic primitives (reading, region grounding, subsequence retrieval, masked span completion) to learn layout-aware DNA representations.", "result": "OpticalDNA consistently outperforms recent baselines across diverse genomic benchmarks; on sequences up to 450k bases, it achieves best overall performance with nearly 20x fewer effective tokens, and surpasses models with up to 985x more activated parameters while tuning only 256k trainable parameters.", "conclusion": "Vision-based OCR-style document understanding provides a more efficient and effective approach to genomic modeling, enabling high-fidelity compression and better performance with significantly reduced computational resources."}}
{"id": "2602.02033", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.02033", "abs": "https://arxiv.org/abs/2602.02033", "authors": ["Shuo Lu", "Haohan Wang", "Wei Feng", "Weizhen Wang", "Shen Zhang", "Yaoyu Li", "Ao Ma", "Zheng Zhang", "Jingjing Lv", "Junjie Shen", "Ching Law", "Bing Zhan", "Yuan Xu", "Huizai Yao", "Yongcan Yu", "Chenyang Si", "Jian Liang"], "title": "One Size, Many Fits: Aligning Diverse Group-Wise Click Preferences in Large-Scale Advertising Image Generation", "comment": null, "summary": "Advertising image generation has increasingly focused on online metrics like Click-Through Rate (CTR), yet existing approaches adopt a ``one-size-fits-all\" strategy that optimizes for overall CTR while neglecting preference diversity among user groups. This leads to suboptimal performance for specific groups, limiting targeted marketing effectiveness. To bridge this gap, we present \\textit{One Size, Many Fits} (OSMF), a unified framework that aligns diverse group-wise click preferences in large-scale advertising image generation. OSMF begins with product-aware adaptive grouping, which dynamically organizes users based on their attributes and product characteristics, representing each group with rich collective preference features. Building on these groups, preference-conditioned image generation employs a Group-aware Multimodal Large Language Model (G-MLLM) to generate tailored images for each group. The G-MLLM is pre-trained to simultaneously comprehend group features and generate advertising images. Subsequently, we fine-tune the G-MLLM using our proposed Group-DPO for group-wise preference alignment, which effectively enhances each group's CTR on the generated images. To further advance this field, we introduce the Grouped Advertising Image Preference Dataset (GAIP), the first large-scale public dataset of group-wise image preferences, including around 600K groups built from 40M users. Extensive experiments demonstrate that our framework achieves the state-of-the-art performance in both offline and online settings. Our code and datasets will be released at https://github.com/JD-GenX/OSMF.", "AI": {"tldr": "OSMF is a unified framework for advertising image generation that optimizes for diverse user group preferences rather than overall CTR, using adaptive grouping and group-aware multimodal LLM with preference alignment.", "motivation": "Existing advertising image generation approaches use a \"one-size-fits-all\" strategy that optimizes for overall CTR but neglects preference diversity among user groups, leading to suboptimal performance for specific groups and limiting targeted marketing effectiveness.", "method": "1) Product-aware adaptive grouping that dynamically organizes users based on attributes and product characteristics; 2) Preference-conditioned image generation using Group-aware Multimodal Large Language Model (G-MLLM); 3) Fine-tuning G-MLLM with Group-DPO for group-wise preference alignment; 4) Introduction of GAIP dataset with 600K groups from 40M users.", "result": "Extensive experiments demonstrate state-of-the-art performance in both offline and online settings, with enhanced CTR for each user group on generated images.", "conclusion": "OSMF effectively bridges the gap in personalized advertising by aligning diverse group-wise click preferences, advancing the field with a novel framework and the first large-scale public dataset for group-wise image preferences."}}
{"id": "2602.02043", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.02043", "abs": "https://arxiv.org/abs/2602.02043", "authors": ["Cristian Sbrolli", "Matteo Matteucci", "Toshihiko Yamasaki"], "title": "Auto-Comp: An Automated Pipeline for Scalable Compositional Probing of Contrastive Vision-Language Models", "comment": null, "summary": "Modern Vision-Language Models (VLMs) exhibit a critical flaw in compositional reasoning, often confusing \"a red cube and a blue sphere\" with \"a blue cube and a red sphere\". Disentangling the visual and linguistic roots of these failures is a fundamental challenge for robust evaluation. To enable fine-grained, controllable analysis, we introduce Auto-Comp, a fully automated and synthetic pipeline for generating scalable benchmarks. Its controllable nature is key to dissecting and isolating different reasoning skills. Auto-Comp generates paired images from Minimal (e.g., \"a monitor to the left of a bicycle on a white background\") and LLM-generated Contextual captions (e.g., \"In a brightly lit photography studio, a monitor is positioned to the left of a bicycle\"), allowing a controlled A/B test to disentangle core binding ability from visio-linguistic complexity. Our evaluation of 20 VLMs on novel benchmarks for color binding and spatial relations reveals universal compositional failures in both CLIP and SigLIP model families. Crucially, our novel \"Confusion Benchmark\" reveals a deeper flaw beyond simple attribute swaps: models are highly susceptible to low-entropy distractors (e.g., repeated objects or colors), demonstrating their compositional failures extend beyond known bag-of-words limitations. we uncover a surprising trade-off: visio-linguistic context, which provides global scene cues, aids spatial reasoning but simultaneously hinders local attribute binding by introducing visual clutter. We release the Auto-Comp pipeline to facilitate future benchmark creation, alongside all our generated benchmarks (https://huggingface.co/AutoComp).", "AI": {"tldr": "Auto-Comp introduces an automated synthetic pipeline for generating scalable benchmarks to analyze compositional reasoning failures in Vision-Language Models, revealing universal failures in attribute binding and spatial relations.", "motivation": "VLMs exhibit critical flaws in compositional reasoning (confusing \"a red cube and a blue sphere\" with \"a blue cube and a red sphere\"), but disentangling visual vs. linguistic roots is challenging. Need fine-grained, controllable analysis tools.", "method": "Auto-Comp: fully automated synthetic pipeline generating paired images from Minimal captions (simple descriptions) and LLM-generated Contextual captions (detailed scenes). Enables controlled A/B testing to isolate core binding ability from visio-linguistic complexity.", "result": "Evaluation of 20 VLMs reveals universal compositional failures in CLIP and SigLIP families. Confusion Benchmark shows models susceptible to low-entropy distractors (repeated objects/colors). Surprising trade-off: context aids spatial reasoning but hinders local attribute binding.", "conclusion": "Auto-Comp enables fine-grained analysis of VLM compositional reasoning failures. Models fail beyond known limitations, with context creating trade-offs. Pipeline and benchmarks released to facilitate future research."}}
{"id": "2602.02067", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.02067", "abs": "https://arxiv.org/abs/2602.02067", "authors": ["Nikola Cenikj", "\u00d6zg\u00fcn Turgut", "Alexander M\u00fcller", "Alexander Steger", "Jan Kehrer", "Marcus Brugger", "Daniel Rueckert", "Eimo Martens", "Philip M\u00fcller"], "title": "Multi-View Stenosis Classification Leveraging Transformer-Based Multiple-Instance Learning Using Real-World Clinical Data", "comment": null, "summary": "Coronary artery stenosis is a leading cause of cardiovascular disease, diagnosed by analyzing the coronary arteries from multiple angiography views. Although numerous deep-learning models have been proposed for stenosis detection from a single angiography view, their performance heavily relies on expensive view-level annotations, which are often not readily available in hospital systems. Moreover, these models fail to capture the temporal dynamics and dependencies among multiple views, which are crucial for clinical diagnosis. To address this, we propose SegmentMIL, a transformer-based multi-view multiple-instance learning framework for patient-level stenosis classification. Trained on a real-world clinical dataset, using patient-level supervision and without any view-level annotations, SegmentMIL jointly predicts the presence of stenosis and localizes the affected anatomical region, distinguishing between the right and left coronary arteries and their respective segments. SegmentMIL obtains high performance on internal and external evaluations and outperforms both view-level models and classical MIL baselines, underscoring its potential as a clinically viable and scalable solution for coronary stenosis diagnosis. Our code is available at https://github.com/NikolaCenic/mil-stenosis.", "AI": {"tldr": "SegmentMIL is a transformer-based multi-view MIL framework for patient-level coronary stenosis classification using only patient-level labels, without view-level annotations.", "motivation": "Existing deep learning models for stenosis detection require expensive view-level annotations and fail to capture temporal dynamics and dependencies among multiple angiography views, which are crucial for clinical diagnosis.", "method": "SegmentMIL uses a transformer-based multi-view multiple-instance learning framework that processes multiple angiography views jointly, predicts stenosis presence, and localizes affected anatomical regions (right/left coronary arteries and segments) using only patient-level supervision.", "result": "SegmentMIL achieves high performance on internal and external evaluations, outperforming both view-level models and classical MIL baselines.", "conclusion": "SegmentMIL demonstrates potential as a clinically viable and scalable solution for coronary stenosis diagnosis by eliminating the need for expensive view-level annotations while capturing multi-view dependencies."}}
{"id": "2602.02089", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.02089", "abs": "https://arxiv.org/abs/2602.02089", "authors": ["Changbai Li", "Haodong Zhu", "Hanlin Chen", "Xiuping Liang", "Tongfei Chen", "Shuwei Shao", "Linlin Yang", "Huobin Tan", "Baochang Zhang"], "title": "UrbanGS: A Scalable and Efficient Architecture for Geometrically Accurate Large-Scene Reconstruction", "comment": "ICLR 2026", "summary": "While 3D Gaussian Splatting (3DGS) enables high-quality, real-time rendering for bounded scenes, its extension to large-scale urban environments gives rise to critical challenges in terms of geometric consistency, memory efficiency, and computational scalability. To address these issues, we present UrbanGS, a scalable reconstruction framework that effectively tackles these challenges for city-scale applications. First, we propose a Depth-Consistent D-Normal Regularization module. Unlike existing approaches that rely solely on monocular normal estimators, which can effectively update rotation parameters yet struggle to update position parameters, our method integrates D-Normal constraints with external depth supervision. This allows for comprehensive updates of all geometric parameters. By further incorporating an adaptive confidence weighting mechanism based on gradient consistency and inverse depth deviation, our approach significantly enhances multi-view depth alignment and geometric coherence, which effectively resolves the issue of geometric accuracy in complex large-scale scenes. To improve scalability, we introduce a Spatially Adaptive Gaussian Pruning (SAGP) strategy, which dynamically adjusts Gaussian density based on local geometric complexity and visibility to reduce redundancy. Additionally, a unified partitioning and view assignment scheme is designed to eliminate boundary artifacts and optimize computational load. Extensive experiments on multiple urban datasets demonstrate that UrbanGS achieves superior performance in rendering quality, geometric accuracy, and memory efficiency, providing a systematic solution for high-fidelity large-scale scene reconstruction.", "AI": {"tldr": "UrbanGS is a scalable 3D Gaussian Splatting framework for large-scale urban environments that improves geometric consistency, memory efficiency, and computational scalability through depth-consistent regularization, adaptive pruning, and unified partitioning.", "motivation": "3D Gaussian Splatting works well for bounded scenes but faces critical challenges when extended to large-scale urban environments: geometric inconsistency, memory inefficiency, and computational scalability limitations.", "method": "1) Depth-Consistent D-Normal Regularization module that integrates D-Normal constraints with external depth supervision and adaptive confidence weighting. 2) Spatially Adaptive Gaussian Pruning (SAGP) that dynamically adjusts Gaussian density based on local geometric complexity and visibility. 3) Unified partitioning and view assignment scheme to eliminate boundary artifacts and optimize computational load.", "result": "Extensive experiments on multiple urban datasets demonstrate superior performance in rendering quality, geometric accuracy, and memory efficiency compared to existing approaches.", "conclusion": "UrbanGS provides a systematic solution for high-fidelity large-scale scene reconstruction, effectively addressing the key challenges of geometric consistency, memory efficiency, and computational scalability in city-scale applications."}}
{"id": "2602.02092", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.02092", "abs": "https://arxiv.org/abs/2602.02092", "authors": ["FSVideo Team", "Qingyu Chen", "Zhiyuan Fang", "Haibin Huang", "Xinwei Huang", "Tong Jin", "Minxuan Lin", "Bo Liu", "Celong Liu", "Chongyang Ma", "Xing Mei", "Xiaohui Shen", "Yaojie Shen", "Fuwen Tan", "Angtian Wang", "Xiao Yang", "Yiding Yang", "Jiamin Yuan", "Lingxi Zhang", "Yuxin Zhang"], "title": "FSVideo: Fast Speed Video Diffusion Model in a Highly-Compressed Latent Space", "comment": "Project Page: https://kingofprank.github.io/fsvideo/", "summary": "We introduce FSVideo, a fast speed transformer-based image-to-video (I2V) diffusion framework. We build our framework on the following key components: 1.) a new video autoencoder with highly-compressed latent space ($64\\times64\\times4$ spatial-temporal downsampling ratio), achieving competitive reconstruction quality; 2.) a diffusion transformer (DIT) architecture with a new layer memory design to enhance inter-layer information flow and context reuse within DIT, and 3.) a multi-resolution generation strategy via a few-step DIT upsampler to increase video fidelity. Our final model, which contains a 14B DIT base model and a 14B DIT upsampler, achieves competitive performance against other popular open-source models, while being an order of magnitude faster. We discuss our model design as well as training strategies in this report.", "AI": {"tldr": "FSVideo is a fast transformer-based image-to-video diffusion framework that achieves competitive performance while being 10x faster than other open-source models through highly-compressed latent space, enhanced diffusion transformer architecture, and multi-resolution generation.", "motivation": "To create an efficient image-to-video generation framework that maintains high quality while significantly improving generation speed compared to existing open-source models.", "method": "Three key components: 1) New video autoencoder with highly-compressed latent space (64\u00d764\u00d74 spatial-temporal downsampling), 2) Diffusion transformer (DIT) with layer memory design for better inter-layer information flow, and 3) Multi-resolution generation via few-step DIT upsampler.", "result": "The final model (14B DIT base + 14B DIT upsampler) achieves competitive performance against other open-source models while being an order of magnitude faster.", "conclusion": "FSVideo demonstrates that efficient transformer-based architectures with optimized latent space compression and memory designs can achieve high-quality video generation with dramatically improved speed."}}
{"id": "2602.02107", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.02107", "abs": "https://arxiv.org/abs/2602.02107", "authors": ["Yu Wang", "Chuanguang Yang", "Zhulin An", "Weilun Feng", "Jiarui Zhao", "Chengqing Yu", "Libo Huang", "Boyu Diao", "Yongjun Xu"], "title": "Teacher-Guided Student Self-Knowledge Distillation Using Diffusion Model", "comment": null, "summary": "Existing Knowledge Distillation (KD) methods often align feature information between teacher and student by exploring meaningful feature processing and loss functions. However, due to the difference in feature distributions between the teacher and student, the student model may learn incompatible information from the teacher. To address this problem, we propose teacher-guided student Diffusion Self-KD, dubbed as DSKD. Instead of the direct teacher-student alignment, we leverage the teacher classifier to guide the sampling process of denoising student features through a light-weight diffusion model. We then propose a novel locality-sensitive hashing (LSH)-guided feature distillation method between the original and denoised student features. The denoised student features encapsulate teacher knowledge and could be regarded as a teacher role. In this way, our DSKD method could eliminate discrepancies in mapping manners and feature distributions between the teacher and student, while learning meaningful knowledge from the teacher. Experiments on visual recognition tasks demonstrate that DSKD significantly outperforms existing KD methods across various models and datasets. Our code is attached in supplementary material.", "AI": {"tldr": "DSKD proposes a teacher-guided student diffusion self-KD method that uses teacher classifier to guide denoising of student features via diffusion model, then performs LSH-guided distillation between original and denoised features to eliminate teacher-student distribution discrepancies.", "motivation": "Existing KD methods suffer from incompatible information transfer due to differences in feature distributions between teacher and student models, leading to suboptimal knowledge transfer.", "method": "1) Use teacher classifier to guide sampling process of denoising student features through lightweight diffusion model. 2) Propose LSH-guided feature distillation between original and denoised student features. 3) Treat denoised features (encapsulating teacher knowledge) as teacher role for self-distillation.", "result": "DSKD significantly outperforms existing KD methods across various models and datasets on visual recognition tasks, eliminating discrepancies in mapping manners and feature distributions while learning meaningful knowledge.", "conclusion": "The proposed diffusion self-KD approach effectively addresses teacher-student distribution mismatch by using teacher-guided denoising and LSH-based feature alignment, enabling more compatible knowledge transfer."}}
{"id": "2602.02114", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.02114", "abs": "https://arxiv.org/abs/2602.02114", "authors": ["Xin Ding", "Yun Chen", "Sen Zhang", "Kao Zhang", "Nenglun Chen", "Peibei Cao", "Yongwei Wang", "Fei Wu"], "title": "Enhancing Diffusion-Based Quantitatively Controllable Image Generation via Matrix-Form EDM and Adaptive Vicinal Training", "comment": null, "summary": "Continuous Conditional Diffusion Model (CCDM) is a diffusion-based framework designed to generate high-quality images conditioned on continuous regression labels. Although CCDM has demonstrated clear advantages over prior approaches across a range of datasets, it still exhibits notable limitations and has recently been surpassed by a GAN-based method, namely CcGAN-AVAR. These limitations mainly arise from its reliance on an outdated diffusion framework and its low sampling efficiency due to long sampling trajectories. To address these issues, we propose an improved CCDM framework, termed iCCDM, which incorporates the more advanced \\textit{Elucidated Diffusion Model} (EDM) framework with substantial modifications to improve both generation quality and sampling efficiency. Specifically, iCCDM introduces a novel matrix-form EDM formulation together with an adaptive vicinal training strategy. Extensive experiments on four benchmark datasets, spanning image resolutions from $64\\times64$ to $256\\times256$, demonstrate that iCCDM consistently outperforms existing methods, including state-of-the-art large-scale text-to-image diffusion models (e.g., Stable Diffusion 3, FLUX.1, and Qwen-Image), achieving higher generation quality while significantly reducing sampling cost.", "AI": {"tldr": "The paper proposes iCCDM, an improved version of Continuous Conditional Diffusion Model (CCDM) that addresses limitations of the original CCDM by incorporating the Elucidated Diffusion Model (EDM) framework with novel matrix-form formulation and adaptive vicinal training strategy.", "motivation": "CCDM has limitations including reliance on outdated diffusion framework and low sampling efficiency due to long sampling trajectories, and has been surpassed by GAN-based methods like CcGAN-AVAR. The authors aim to improve both generation quality and sampling efficiency.", "method": "iCCDM incorporates the Elucidated Diffusion Model (EDM) framework with substantial modifications: 1) novel matrix-form EDM formulation, and 2) adaptive vicinal training strategy. This improves both generation quality and sampling efficiency.", "result": "Extensive experiments on four benchmark datasets (image resolutions from 64\u00d764 to 256\u00d7256) show iCCDM consistently outperforms existing methods, including state-of-the-art large-scale text-to-image diffusion models (Stable Diffusion 3, FLUX.1, Qwen-Image), achieving higher generation quality while significantly reducing sampling cost.", "conclusion": "iCCDM successfully addresses the limitations of CCDM by incorporating advanced EDM framework with novel modifications, achieving superior performance in both quality and efficiency compared to existing methods including cutting-edge text-to-image diffusion models."}}
{"id": "2602.02123", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.02123", "abs": "https://arxiv.org/abs/2602.02123", "authors": ["Yangyi Cao", "Yuanhang Li", "Lan Chen", "Qi Mao"], "title": "MLV-Edit: Towards Consistent and Highly Efficient Editing for Minute-Level Videos", "comment": null, "summary": "We propose MLV-Edit, a training-free, flow-based framework that address the unique challenges of minute-level video editing. While existing techniques excel in short-form video manipulation, scaling them to long-duration videos remains challenging due to prohibitive computational overhead and the difficulty of maintaining global temporal consistency across thousands of frames. To address this, MLV-Edit employs a divide-and-conquer strategy for segment-wise editing, facilitated by two core modules: Velocity Blend rectifies motion inconsistencies at segment boundaries by aligning the flow fields of adjacent chunks, eliminating flickering and boundary artifacts commonly observed in fragmented video processing; and Attention Sink anchors local segment features to global reference frames, effectively suppressing cumulative structural drift. Extensive quantitative and qualitative experiments demonstrate that MLV-Edit consistently outperforms state-of-the-art methods in terms of temporal stability and semantic fidelity.", "AI": {"tldr": "MLV-Edit is a training-free, flow-based framework for minute-level video editing that addresses computational overhead and temporal consistency challenges through segment-wise editing with Velocity Blend and Attention Sink modules.", "motivation": "Existing video editing techniques work well for short videos but struggle with long-duration videos due to prohibitive computational overhead and difficulty maintaining global temporal consistency across thousands of frames.", "method": "MLV-Edit uses a divide-and-conquer strategy for segment-wise editing with two core modules: Velocity Blend aligns flow fields of adjacent chunks to rectify motion inconsistencies at boundaries, and Attention Sink anchors local segment features to global reference frames to suppress cumulative structural drift.", "result": "Extensive experiments show MLV-Edit consistently outperforms state-of-the-art methods in terms of temporal stability and semantic fidelity for minute-level video editing.", "conclusion": "MLV-Edit provides an effective training-free solution for long-duration video editing that overcomes computational and consistency challenges through innovative flow-based techniques."}}
{"id": "2602.02124", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.02124", "abs": "https://arxiv.org/abs/2602.02124", "authors": ["Olga Graf", "Dhrupal Patel", "Peter Gro\u00df", "Charlotte Lempp", "Matthias Hein", "Fabian Heinemann"], "title": "Toxicity Assessment in Preclinical Histopathology via Class-Aware Mahalanobis Distance for Known and Novel Anomalies", "comment": null, "summary": "Drug-induced toxicity remains a leading cause of failure in preclinical development and early clinical trials. Detecting adverse effects at an early stage is critical to reduce attrition and accelerate the development of safe medicines. Histopathological evaluation remains the gold standard for toxicity assessment, but it relies heavily on expert pathologists, creating a bottleneck for large-scale screening. To address this challenge, we introduce an AI-based anomaly detection framework for histopathological whole-slide images (WSIs) in rodent livers from toxicology studies. The system identifies healthy tissue and known pathologies (anomalies) for which training data is available. In addition, it can detect rare pathologies without training data as out-of-distribution (OOD) findings. We generate a novel dataset of pixelwise annotations of healthy tissue and known pathologies and use this data to fine-tune a pre-trained Vision Transformer (DINOv2) via Low-Rank Adaptation (LoRA) in order to do tissue segmentation. Finally, we extract features for OOD detection using the Mahalanobis distance. To better account for class-dependent variability in histological data, we propose the use of class-specific thresholds. We optimize the thresholds using the mean of the false negative and false positive rates, resulting in only 0.16\\% of pathological tissue classified as healthy and 0.35\\% of healthy tissue classified as pathological. Applied to mouse liver WSIs with known toxicological findings, the framework accurately detects anomalies, including rare OOD morphologies. This work demonstrates the potential of AI-driven histopathology to support preclinical workflows, reduce late-stage failures, and improve efficiency in drug development.", "AI": {"tldr": "AI framework for detecting drug-induced liver toxicity in histopathology images using anomaly detection with Vision Transformers and Mahalanobis distance for out-of-distribution pathology identification.", "motivation": "Drug-induced toxicity causes high failure rates in preclinical development. Histopathology evaluation is gold standard but bottlenecked by expert pathologists. Need automated large-scale screening to detect adverse effects early and reduce drug development attrition.", "method": "Fine-tuned pre-trained Vision Transformer (DINOv2) with Low-Rank Adaptation (LoRA) for tissue segmentation using pixelwise annotated dataset of healthy and known pathologies. Extracted features for out-of-distribution detection using Mahalanobis distance with class-specific thresholds optimized via mean of false negative and false positive rates.", "result": "Achieved high accuracy: only 0.16% pathological tissue misclassified as healthy and 0.35% healthy tissue misclassified as pathological. Framework successfully detected anomalies including rare out-of-distribution morphologies in mouse liver whole-slide images with known toxicological findings.", "conclusion": "AI-driven histopathology has strong potential to support preclinical workflows, reduce late-stage drug development failures, and improve efficiency by automating toxicity detection at scale."}}
{"id": "2602.02130", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.02130", "abs": "https://arxiv.org/abs/2602.02130", "authors": ["Lukas Zimmermann", "Michael Rauter", "Maximilian Schmid", "Dietmar Georg", "Barbara Kn\u00e4usl"], "title": "Eliminating Registration Bias in Synthetic CT Generation: A Physics-Based Simulation Framework", "comment": null, "summary": "Supervised synthetic CT generation from CBCT requires registered training pairs, yet perfect registration between separately acquired scans remains unattainable. This registration bias propagates into trained models and corrupts standard evaluation metrics. This may suggest that superior benchmark performance indicates better reproduction of registration artifacts rather than anatomical fidelity. We propose physics-based CBCT simulation to provide geometrically aligned training pairs by construction, combined with evaluation using geometric alignment metrics against input CBCT rather than biased ground truth. On two independent pelvic datasets, models trained on synthetic data achieved superior geometric alignment (Normalized Mutual Information: 0.31 vs 0.22) despite lower conventional intensity scores. Intensity metrics showed inverted correlations with clinical assessment for deformably registered data, while Normalized Mutual Information consistently predicted observer preference across registration methodologies (rho = 0.31, p < 0.001). Clinical observers preferred synthetic-trained outputs in 87% of cases, demonstrating that geometric fidelity, not intensity agreement with biased ground truth, aligns with clinical requirements.", "AI": {"tldr": "Synthetic CT generation from CBCT using physics-based simulation for geometrically aligned training pairs improves clinical relevance over conventional intensity-based methods that reproduce registration artifacts.", "motivation": "Supervised CT generation requires registered training pairs, but perfect registration is unattainable, causing registration bias to propagate into models and corrupt evaluation metrics. This means superior benchmark performance may indicate better reproduction of registration artifacts rather than anatomical fidelity.", "method": "Proposed physics-based CBCT simulation to provide geometrically aligned training pairs by construction, combined with evaluation using geometric alignment metrics (Normalized Mutual Information) against input CBCT rather than biased ground truth.", "result": "Models trained on synthetic data achieved superior geometric alignment (NMI: 0.31 vs 0.22) despite lower conventional intensity scores. NMI consistently predicted observer preference across registration methodologies (rho = 0.31, p < 0.001). Clinical observers preferred synthetic-trained outputs in 87% of cases.", "conclusion": "Geometric fidelity, not intensity agreement with biased ground truth, aligns with clinical requirements. Physics-based simulation with geometric alignment metrics provides more clinically relevant CT generation than conventional intensity-based methods."}}
{"id": "2602.02154", "categories": ["cs.CV", "cs.IR"], "pdf": "https://arxiv.org/pdf/2602.02154", "abs": "https://arxiv.org/abs/2602.02154", "authors": ["Sidi Wu", "Yizi Chen", "Maurizio Gribaudi", "Konrad Schindler", "Cl\u00e9ment Mallet", "Julien Perret", "Lorenz Hurni"], "title": "Deep learning enables urban change profiling through alignment of historical maps", "comment": "40 pages", "summary": "Prior to modern Earth observation technologies, historical maps provide a unique record of long-term urban transformation and offer a lens on the evolving identity of cities. However, extracting consistent and fine-grained change information from historical map series remains challenging due to spatial misalignment, cartographic variation, and degrading document quality, limiting most analyses to small-scale or qualitative approaches. We propose a fully automated, deep learning-based framework for fine-grained urban change analysis from large collections of historical maps, built on a modular design that integrates dense map alignment, multi-temporal object detection, and change profiling. This framework shifts the analysis of historical maps from ad hoc visual comparison toward systematic, quantitative characterization of urban change. Experiments demonstrate the robust performance of the proposed alignment and object detection methods. Applied to Paris between 1868 and 1937, the framework reveals the spatial and temporal heterogeneity in urban transformation, highlighting its relevance for research in the social sciences and humanities. The modular design of our framework further supports adaptation to diverse cartographic contexts and downstream applications.", "AI": {"tldr": "A deep learning framework for automated, fine-grained urban change analysis from historical map collections, addressing challenges of spatial misalignment and cartographic variation.", "motivation": "Historical maps provide unique long-term urban transformation records but extracting consistent change information is challenging due to spatial misalignment, cartographic variation, and document quality degradation, limiting most analyses to small-scale or qualitative approaches.", "method": "A fully automated, deep learning-based framework with modular design integrating dense map alignment, multi-temporal object detection, and change profiling to enable systematic, quantitative characterization of urban change from historical map series.", "result": "The framework demonstrates robust performance in alignment and object detection, and when applied to Paris (1868-1937), reveals spatial and temporal heterogeneity in urban transformation, highlighting relevance for social sciences and humanities research.", "conclusion": "The proposed framework shifts historical map analysis from ad hoc visual comparison to systematic quantitative characterization, with modular design supporting adaptation to diverse cartographic contexts and downstream applications."}}
{"id": "2602.02156", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.02156", "abs": "https://arxiv.org/abs/2602.02156", "authors": ["Wen-Jie Shu", "Xuerui Qiu", "Rui-Jie Zhu", "Harold Haodong Chen", "Yexin Liu", "Harry Yang"], "title": "LoopViT: Scaling Visual ARC with Looped Transformers", "comment": "8 pages, 11 figures", "summary": "Recent advances in visual reasoning have leveraged vision transformers to tackle the ARC-AGI benchmark. However, we argue that the feed-forward architecture, where computational depth is strictly bound to parameter size, falls short of capturing the iterative, algorithmic nature of human induction. In this work, we propose a recursive architecture called Loop-ViT, which decouples reasoning depth from model capacity through weight-tied recurrence. Loop-ViT iterates a weight-tied Hybrid Block, combining local convolutions and global attention, to form a latent chain of thought. Crucially, we introduce a parameter-free Dynamic Exit mechanism based on predictive entropy: the model halts inference when its internal state ``crystallizes\" into a low-uncertainty attractor. Empirical results on the ARC-AGI-1 benchmark validate this perspective: our 18M model achieves 65.8% accuracy, outperforming massive 73M-parameter ensembles. These findings demonstrate that adaptive iterative computation offers a far more efficient scaling axis for visual reasoning than simply increasing network width. The code is available at https://github.com/WenjieShu/LoopViT.", "AI": {"tldr": "Loop-ViT: A recursive vision transformer with weight-tied recurrence and dynamic exit mechanism that achieves state-of-the-art performance on ARC-AGI benchmark with significantly fewer parameters.", "motivation": "Standard feed-forward vision transformers have computational depth bound to parameter size, which fails to capture the iterative, algorithmic nature of human inductive reasoning needed for visual reasoning tasks like ARC-AGI.", "method": "Proposes Loop-ViT with weight-tied recurrence that decouples reasoning depth from model capacity. Uses a Hybrid Block combining local convolutions and global attention, and introduces a parameter-free Dynamic Exit mechanism based on predictive entropy to halt inference when internal state reaches low uncertainty.", "result": "18M parameter Loop-ViT achieves 65.8% accuracy on ARC-AGI-1 benchmark, outperforming massive 73M-parameter ensembles, demonstrating superior efficiency and performance.", "conclusion": "Adaptive iterative computation is a more efficient scaling axis for visual reasoning than simply increasing network width, and recursive architectures with dynamic halting better capture algorithmic reasoning processes."}}
{"id": "2602.02163", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.02163", "abs": "https://arxiv.org/abs/2602.02163", "authors": ["Julian Wyatt", "Ronald Clark", "Irina Voiculescu"], "title": "Reg4Pru: Regularisation Through Random Token Routing for Token Pruning", "comment": "11 pages, 7 figures", "summary": "Transformers are widely adopted in modern vision models due to their strong ability to scale with dataset size and generalisability. However, this comes with a major drawback: computation scales quadratically to the total number of tokens. Numerous methods have been proposed to mitigate this. For example, we consider token pruning with reactivating tokens from preserved representations, but the increased computational efficiency of this method results in decreased stability from the preserved representations, leading to poorer dense prediction performance at deeper layers. In this work, we introduce Reg4Pru, a training regularisation technique that mitigates token-pruning performance loss for segmentation. We compare our models on the FIVES blood vessel segmentation dataset and find that Reg4Pru improves average precision by an absolute 46% compared to the same model trained without routing. This increase is observed using a configuration that achieves a 29% relative speedup in wall-clock time compared to the non-pruned baseline. These findings indicate that Reg4Pru is a valuable regulariser for token reduction strategies.", "AI": {"tldr": "Reg4Pru is a training regularization technique that improves segmentation performance for token-pruned vision transformers, achieving 46% AP improvement with 29% speedup.", "motivation": "Vision transformers suffer from quadratic computational scaling with token count. Token pruning methods improve efficiency but degrade performance in deeper layers due to instability from preserved representations, particularly for dense prediction tasks like segmentation.", "method": "Reg4Pru (Regularization for Pruning) - a training regularization technique designed to mitigate performance loss from token pruning in segmentation tasks. It addresses the stability issues of preserved representations in deeper layers.", "result": "On the FIVES blood vessel segmentation dataset, Reg4Pru improves average precision by 46% absolute compared to same model trained without routing. Achieves 29% relative speedup in wall-clock time compared to non-pruned baseline.", "conclusion": "Reg4Pru is an effective regularizer for token reduction strategies in vision transformers, enabling significant performance improvements while maintaining computational efficiency gains for segmentation tasks."}}
{"id": "2602.02171", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.02171", "abs": "https://arxiv.org/abs/2602.02171", "authors": ["Lu Cao", "Xiquan He", "Junying Zeng", "Chaoyun Mai", "Min Luo"], "title": "Lung Nodule Image Synthesis Driven by Two-Stage Generative Adversarial Networks", "comment": null, "summary": "The limited sample size and insufficient diversity of lung nodule CT datasets severely restrict the performance and generalization ability of detection models. Existing methods generate images with insufficient diversity and controllability, suffering from issues such as monotonous texture features and distorted anatomical structures. Therefore, we propose a two-stage generative adversarial network (TSGAN) to enhance the diversity and spatial controllability of synthetic data by decoupling the morphological structure and texture features of lung nodules. In the first stage, StyleGAN is used to generate semantic segmentation mask images, encoding lung nodules and tissue backgrounds to control the anatomical structure of lung nodule images; The second stage uses the DL-Pix2Pix model to translate the mask map into CT images, employing local importance attention to capture local features, while utilizing dynamic weight multi-head window attention to enhance the modeling capability of lung nodule texture and background. Compared to the original dataset, the accuracy improved by 4.6% and mAP by 4% on the LUNA16 dataset. Experimental results demonstrate that TSGAN can enhance the quality of synthetic images and the performance of detection models.", "AI": {"tldr": "TSGAN: A two-stage GAN that decouples morphological structure and texture features to generate diverse, controllable synthetic lung nodule CT images, improving detection model performance.", "motivation": "Limited sample size and insufficient diversity in lung nodule CT datasets restrict detection model performance and generalization. Existing methods generate images with insufficient diversity and controllability, suffering from monotonous textures and distorted anatomical structures.", "method": "Two-stage GAN: 1) StyleGAN generates semantic segmentation masks to control anatomical structure; 2) DL-Pix2Pix translates masks to CT images using local importance attention and dynamic weight multi-head window attention to enhance texture modeling.", "result": "On LUNA16 dataset: accuracy improved by 4.6% and mAP by 4% compared to original dataset. TSGAN enhances synthetic image quality and detection model performance.", "conclusion": "TSGAN effectively enhances diversity and spatial controllability of synthetic lung nodule CT data by decoupling structure and texture features, leading to improved detection model performance."}}
{"id": "2602.02175", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.02175", "abs": "https://arxiv.org/abs/2602.02175", "authors": ["Xinquan Yu", "Wei Lu", "Xiangyang Luo"], "title": "CIEC: Coupling Implicit and Explicit Cues for Multimodal Weakly Supervised Manipulation Localization", "comment": null, "summary": "To mitigate the threat of misinformation, multimodal manipulation localization has garnered growing attention. Consider that current methods rely on costly and time-consuming fine-grained annotations, such as patch/token-level annotations. This paper proposes a novel framework named Coupling Implicit and Explicit Cues (CIEC), which aims to achieve multimodal weakly-supervised manipulation localization for image-text pairs utilizing only coarse-grained image/sentence-level annotations. It comprises two branches, image-based and text-based weakly-supervised localization. For the former, we devise the Textual-guidance Refine Patch Selection (TRPS) module. It integrates forgery cues from both visual and textual perspectives to lock onto suspicious regions aided by spatial priors. Followed by the background silencing and spatial contrast constraints to suppress interference from irrelevant areas. For the latter, we devise the Visual-deviation Calibrated Token Grounding (VCTG) module. It focuses on meaningful content words and leverages relative visual bias to assist token localization. Followed by the asymmetric sparse and semantic consistency constraints to mitigate label noise and ensure reliability. Extensive experiments demonstrate the effectiveness of our CIEC, yielding results comparable to fully supervised methods on several evaluation metrics.", "AI": {"tldr": "CIEC is a weakly-supervised framework for multimodal manipulation localization that uses only coarse-grained image/sentence-level annotations instead of expensive fine-grained patch/token labels.", "motivation": "Current multimodal manipulation localization methods require costly fine-grained annotations (patch/token-level), which are time-consuming to obtain. The authors aim to develop a more practical approach using only coarse-grained annotations.", "method": "CIEC has two branches: 1) Image-based weakly-supervised localization with Textual-guidance Refine Patch Selection (TRPS) that integrates visual and textual forgery cues using spatial priors, plus background silencing and spatial contrast constraints. 2) Text-based weakly-supervised localization with Visual-deviation Calibrated Token Grounding (VCTG) that focuses on content words using visual bias, plus asymmetric sparse and semantic consistency constraints.", "result": "Extensive experiments show CIEC achieves results comparable to fully supervised methods on several evaluation metrics, demonstrating effectiveness despite using only coarse-grained annotations.", "conclusion": "CIEC successfully enables multimodal manipulation localization with weak supervision, reducing annotation costs while maintaining performance comparable to fully supervised approaches."}}
{"id": "2602.02185", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.02185", "abs": "https://arxiv.org/abs/2602.02185", "authors": ["Yu Zeng", "Wenxuan Huang", "Zhen Fang", "Shuang Chen", "Yufan Shen", "Yishuo Cai", "Xiaoman Wang", "Zhenfei Yin", "Lin Chen", "Zehui Chen", "Shiting Huang", "Yiming Zhao", "Yao Hu", "Philip Torr", "Wanli Ouyang", "Shaosheng Cao"], "title": "Vision-DeepResearch Benchmark: Rethinking Visual and Textual Search for Multimodal Large Language Models", "comment": null, "summary": "Multimodal Large Language Models (MLLMs) have advanced VQA and now support Vision-DeepResearch systems that use search engines for complex visual-textual fact-finding. However, evaluating these visual and textual search abilities is still difficult, and existing benchmarks have two major limitations. First, existing benchmarks are not visual search-centric: answers that should require visual search are often leaked through cross-textual cues in the text questions or can be inferred from the prior world knowledge in current MLLMs. Second, overly idealized evaluation scenario: On the image-search side, the required information can often be obtained via near-exact matching against the full image, while the text-search side is overly direct and insufficiently challenging. To address these issues, we construct the Vision-DeepResearch benchmark (VDR-Bench) comprising 2,000 VQA instances. All questions are created via a careful, multi-stage curation pipeline and rigorous expert review, designed to assess the behavior of Vision-DeepResearch systems under realistic real-world conditions. Moreover, to address the insufficient visual retrieval capabilities of current MLLMs, we propose a simple multi-round cropped-search workflow. This strategy is shown to effectively improve model performance in realistic visual retrieval scenarios. Overall, our results provide practical guidance for the design of future multimodal deep-research systems. The code will be released in https://github.com/Osilly/Vision-DeepResearch.", "AI": {"tldr": "VDR-Bench is a new benchmark for evaluating multimodal deep-research systems, addressing limitations of existing benchmarks through 2,000 carefully curated VQA instances and proposing a multi-round cropped-search workflow to improve visual retrieval.", "motivation": "Existing benchmarks for evaluating multimodal deep-research systems have two major limitations: 1) they are not visual search-centric (answers can be leaked through textual cues or inferred from prior knowledge), and 2) they use overly idealized evaluation scenarios where visual search relies on near-exact image matching and text search is insufficiently challenging.", "method": "The authors construct VDR-Bench with 2,000 VQA instances created through a multi-stage curation pipeline and expert review. They also propose a simple multi-round cropped-search workflow to address insufficient visual retrieval capabilities in current MLLMs.", "result": "The benchmark provides realistic evaluation conditions for vision-deepresearch systems, and the proposed multi-round cropped-search workflow effectively improves model performance in realistic visual retrieval scenarios.", "conclusion": "VDR-Bench addresses critical limitations in existing multimodal deep-research evaluation, and the proposed workflow offers practical guidance for designing future multimodal deep-research systems that can better handle real-world visual-textual fact-finding tasks."}}
{"id": "2602.02186", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.02186", "abs": "https://arxiv.org/abs/2602.02186", "authors": ["Ziqiao Weng", "Jiancheng Yang", "Kangxian Xie", "Bo Zhou", "Weidong Cai"], "title": "Learning Topology-Aware Implicit Field for Unified Pulmonary Tree Modeling with Incomplete Topological Supervision", "comment": "18 pages, 7 figures", "summary": "Pulmonary trees extracted from CT images frequently exhibit topological incompleteness, such as missing or disconnected branches, which substantially degrades downstream anatomical analysis and limits the applicability of existing pulmonary tree modeling pipelines. Current approaches typically rely on dense volumetric processing or explicit graph reasoning, leading to limited efficiency and reduced robustness under realistic structural corruption. We propose TopoField, a topology-aware implicit modeling framework that treats topology repair as a first-class modeling problem and enables unified multi-task inference for pulmonary tree analysis. TopoField represents pulmonary anatomy using sparse surface and skeleton point clouds and learns a continuous implicit field that supports topology repair without relying on complete or explicit disconnection annotations, by training on synthetically introduced structural disruptions over \\textit{already} incomplete trees. Building upon the repaired implicit representation, anatomical labeling and lung segment reconstruction are jointly inferred through task-specific implicit functions within a single forward pass.Extensive experiments on the Lung3D+ dataset demonstrate that TopoField consistently improves topological completeness and achieves accurate anatomical labeling and lung segment reconstruction under challenging incomplete scenarios. Owing to its implicit formulation, TopoField attains high computational efficiency, completing all tasks in just over one second per case, highlighting its practicality for large-scale and time-sensitive clinical applications. Code and data will be available at https://github.com/HINTLab/TopoField.", "AI": {"tldr": "TopoField is a topology-aware implicit modeling framework that repairs incomplete pulmonary trees from CT scans, enabling unified multi-task inference for anatomical analysis with high efficiency.", "motivation": "Pulmonary trees extracted from CT images often have topological incompleteness (missing/disconnected branches), which degrades anatomical analysis and limits existing modeling pipelines. Current approaches are inefficient and lack robustness under structural corruption.", "method": "TopoField uses sparse surface and skeleton point clouds to learn a continuous implicit field that supports topology repair without needing complete annotations. It trains on synthetically introduced structural disruptions over already incomplete trees. The repaired implicit representation enables joint anatomical labeling and lung segment reconstruction through task-specific implicit functions in a single forward pass.", "result": "Extensive experiments on Lung3D+ dataset show TopoField consistently improves topological completeness and achieves accurate anatomical labeling and lung segment reconstruction under challenging incomplete scenarios. It completes all tasks in just over one second per case due to its implicit formulation.", "conclusion": "TopoField provides an efficient, practical solution for large-scale clinical applications by treating topology repair as a first-class modeling problem and enabling unified multi-task inference for pulmonary tree analysis with high computational efficiency."}}
{"id": "2602.02193", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.02193", "abs": "https://arxiv.org/abs/2602.02193", "authors": ["Chen Min", "Enze Jiang", "Jishen Peng", "Zheng Ma"], "title": "SSI-DM: Singularity Skipping Inversion of Diffusion Models", "comment": null, "summary": "Inverting real images into the noise space is essential for editing tasks using diffusion models, yet existing methods produce non-Gaussian noise with poor editability due to the inaccuracy in early noising steps. We identify the root cause: a mathematical singularity that renders inversion fundamentally ill-posed. We propose Singularity Skipping Inversion of Diffusion Models (SSI-DM), which bypasses this singular region by adding small noise before standard inversion. This simple approach produces inverted noise with natural Gaussian properties while maintaining reconstruction fidelity. As a plug-and-play technique compatible with general diffusion models, our method achieves superior performance on public image datasets for reconstruction and interpolation tasks, providing a principled and efficient solution to diffusion model inversion.", "AI": {"tldr": "SSI-DM is a plug-and-play diffusion model inversion method that bypasses mathematical singularities by adding small noise before standard inversion, producing Gaussian noise with better editability while maintaining reconstruction fidelity.", "motivation": "Existing diffusion model inversion methods produce non-Gaussian noise with poor editability due to inaccuracies in early noising steps, caused by a mathematical singularity that makes inversion fundamentally ill-posed.", "method": "Singularity Skipping Inversion (SSI-DM) bypasses the singular region by adding small noise before standard inversion, producing inverted noise with natural Gaussian properties while maintaining reconstruction fidelity.", "result": "SSI-DM achieves superior performance on public image datasets for reconstruction and interpolation tasks, providing a principled and efficient solution to diffusion model inversion.", "conclusion": "SSI-DM offers a simple yet effective plug-and-play technique compatible with general diffusion models, solving the fundamental ill-posedness of inversion while preserving editability through Gaussian noise properties."}}
{"id": "2602.02212", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.02212", "abs": "https://arxiv.org/abs/2602.02212", "authors": ["Zheyuan Zhou", "Liang Du", "Zixun Sun", "Xiaoyu Zhou", "Ruimin Ye", "Qihao Chen", "Yinda Chen", "Lemiao Qiu"], "title": "MAIN-VLA: Modeling Abstraction of Intention and eNvironment for Vision-Language-Action Models", "comment": null, "summary": "Despite significant progress in Visual-Language-Action (VLA), in highly complex and dynamic environments that involve real-time unpredictable interactions (such as 3D open worlds and large-scale PvP games), existing approaches remain inefficient at extracting action-critical signals from redundant sensor streams. To tackle this, we introduce MAIN-VLA, a framework that explicitly Models the Abstraction of Intention and eNvironment to ground decision-making in deep semantic alignment rather than superficial pattern matching. Specifically, our Intention Abstraction (IA) extracts verbose linguistic instructions and their associated reasoning into compact, explicit semantic primitives, while the Environment Semantics Abstraction (ESA) projects overwhelming visual streams into a structured, topological affordance representation. Furthermore, aligning these two abstract modalities induces an emergent attention-concentration effect, enabling a parameter-free token-pruning strategy that filters out perceptual redundancy without degrading performance. Extensive experiments in open-world Minecraft and large-scale PvP environments (Game for Peace and Valorant) demonstrate that MAIN-VLA sets a new state-of-the-art, which achieves superior decision quality, stronger generalization, and cutting-edge inference efficiency.", "AI": {"tldr": "MAIN-VLA is a Visual-Language-Action framework that models intention and environment abstractions to improve decision-making in complex dynamic environments like 3D open worlds and PvP games, achieving state-of-the-art performance with better efficiency.", "motivation": "Existing VLA approaches are inefficient at extracting action-critical signals from redundant sensor streams in highly complex, dynamic environments with real-time unpredictable interactions (3D open worlds, large-scale PvP games).", "method": "Two key components: 1) Intention Abstraction (IA) extracts verbose linguistic instructions into compact semantic primitives; 2) Environment Semantics Abstraction (ESA) projects visual streams into structured topological affordance representations. Alignment induces attention-concentration effect enabling parameter-free token pruning.", "result": "Extensive experiments in Minecraft, Game for Peace, and Valorant show MAIN-VLA achieves state-of-the-art performance with superior decision quality, stronger generalization, and cutting-edge inference efficiency.", "conclusion": "MAIN-VLA successfully addresses inefficiencies in existing VLA approaches by modeling intention and environment abstractions, enabling deep semantic alignment for decision-making in complex dynamic environments."}}
{"id": "2602.02214", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.02214", "abs": "https://arxiv.org/abs/2602.02214", "authors": ["Hongzhou Zhu", "Min Zhao", "Guande He", "Hang Su", "Chongxuan Li", "Jun Zhu"], "title": "Causal Forcing: Autoregressive Diffusion Distillation Done Right for High-Quality Real-Time Interactive Video Generation", "comment": "Project page and the code: \\href{https://thu-ml.github.io/CausalForcing.github.io/}{https://thu-ml.github.io/CausalForcing.github.io/}", "summary": "To achieve real-time interactive video generation, current methods distill pretrained bidirectional video diffusion models into few-step autoregressive (AR) models, facing an architectural gap when full attention is replaced by causal attention. However, existing approaches do not bridge this gap theoretically. They initialize the AR student via ODE distillation, which requires frame-level injectivity, where each noisy frame must map to a unique clean frame under the PF-ODE of an AR teacher. Distilling an AR student from a bidirectional teacher violates this condition, preventing recovery of the teacher's flow map and instead inducing a conditional-expectation solution, which degrades performance. To address this issue, we propose Causal Forcing that uses an AR teacher for ODE initialization, thereby bridging the architectural gap. Empirical results show that our method outperforms all baselines across all metrics, surpassing the SOTA Self Forcing by 19.3\\% in Dynamic Degree, 8.7\\% in VisionReward, and 16.7\\% in Instruction Following. Project page and the code: \\href{https://thu-ml.github.io/CausalForcing.github.io/}{https://thu-ml.github.io/CausalForcing.github.io/}", "AI": {"tldr": "Causal Forcing bridges the architectural gap in video diffusion distillation by using an AR teacher for ODE initialization instead of bidirectional teacher, solving the frame-level injectivity violation problem.", "motivation": "Current methods distill bidirectional video diffusion models into autoregressive models but face an architectural gap when replacing full attention with causal attention. Existing approaches don't bridge this gap theoretically and violate frame-level injectivity conditions during ODE distillation, leading to performance degradation.", "method": "Proposes Causal Forcing that uses an autoregressive teacher for ODE initialization instead of a bidirectional teacher, ensuring frame-level injectivity is maintained and properly bridging the architectural gap between bidirectional and autoregressive models.", "result": "Outperforms all baselines across all metrics, surpassing state-of-the-art Self Forcing by 19.3% in Dynamic Degree, 8.7% in VisionReward, and 16.7% in Instruction Following.", "conclusion": "Causal Forcing effectively bridges the architectural gap in video diffusion distillation by using AR teacher initialization, solving the theoretical limitations of previous methods and achieving superior performance in real-time interactive video generation."}}
{"id": "2602.02220", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2602.02220", "abs": "https://arxiv.org/abs/2602.02220", "authors": ["Bo Miao", "Weijia Liu", "Jun Luo", "Lachlan Shinnick", "Jian Liu", "Thomas Hamilton-Smith", "Yuhe Yang", "Zijie Wu", "Vanja Videnovic", "Feras Dayoub", "Anton van den Hengel"], "title": "LangMap: A Hierarchical Benchmark for Open-Vocabulary Goal Navigation", "comment": null, "summary": "The relationships between objects and language are fundamental to meaningful communication between humans and AI, and to practically useful embodied intelligence. We introduce HieraNav, a multi-granularity, open-vocabulary goal navigation task where agents interpret natural language instructions to reach targets at four semantic levels: scene, room, region, and instance. To this end, we present Language as a Map (LangMap), a large-scale benchmark built on real-world 3D indoor scans with comprehensive human-verified annotations and tasks spanning these levels. LangMap provides region labels, discriminative region descriptions, discriminative instance descriptions covering 414 object categories, and over 18K navigation tasks. Each target features both concise and detailed descriptions, enabling evaluation across different instruction styles. LangMap achieves superior annotation quality, outperforming GOAT-Bench by 23.8% in discriminative accuracy using four times fewer words. Comprehensive evaluations of zero-shot and supervised models on LangMap reveal that richer context and memory improve success, while long-tailed, small, context-dependent, and distant goals, as well as multi-goal completion, remain challenging. HieraNav and LangMap establish a rigorous testbed for advancing language-driven embodied navigation. Project: https://bo-miao.github.io/LangMap", "AI": {"tldr": "HieraNav introduces a multi-granularity navigation task with four semantic levels, and LangMap provides a large-scale benchmark with high-quality annotations for evaluating language-driven embodied navigation.", "motivation": "The relationships between objects and language are fundamental for meaningful human-AI communication and practically useful embodied intelligence. Current benchmarks lack comprehensive multi-granularity evaluation of language-driven navigation.", "method": "Introduces HieraNav task with four semantic levels (scene, room, region, instance) and LangMap benchmark built on real-world 3D indoor scans with human-verified annotations, including region labels, discriminative descriptions, and over 18K navigation tasks with both concise and detailed descriptions.", "result": "LangMap achieves 23.8% higher discriminative accuracy than GOAT-Bench using four times fewer words. Evaluations show richer context and memory improve success, while long-tailed, small, context-dependent, and distant goals, as well as multi-goal completion remain challenging.", "conclusion": "HieraNav and LangMap establish a rigorous testbed for advancing language-driven embodied navigation, addressing fundamental challenges in object-language relationships for embodied AI systems."}}
{"id": "2602.02222", "categories": ["cs.CV", "cs.CR"], "pdf": "https://arxiv.org/pdf/2602.02222", "abs": "https://arxiv.org/abs/2602.02222", "authors": ["Ruiqi Liu", "Manni Cui", "Ziheng Qin", "Zhiyuan Yan", "Ruoxin Chen", "Yi Han", "Zhiheng Li", "Junkai Chen", "ZhiJin Chen", "Kaiqing Lin", "Jialiang Shen", "Lubin Weng", "Jing Dong", "Yan Wang", "Shu Wu"], "title": "MIRROR: Manifold Ideal Reference ReconstructOR for Generalizable AI-Generated Image Detection", "comment": null, "summary": "High-fidelity generative models have narrowed the perceptual gap between synthetic and real images, posing serious threats to media security. Most existing AI-generated image (AIGI) detectors rely on artifact-based classification and struggle to generalize to evolving generative traces. In contrast, human judgment relies on stable real-world regularities, with deviations from the human cognitive manifold serving as a more generalizable signal of forgery. Motivated by this insight, we reformulate AIGI detection as a Reference-Comparison problem that verifies consistency with the real-image manifold rather than fitting specific forgery cues. We propose MIRROR (Manifold Ideal Reference ReconstructOR), a framework that explicitly encodes reality priors using a learnable discrete memory bank. MIRROR projects an input into a manifold-consistent ideal reference via sparse linear combination, and uses the resulting residuals as robust detection signals. To evaluate whether detectors reach the \"superhuman crossover\" required to replace human experts, we introduce the Human-AIGI benchmark, featuring a psychophysically curated human-imperceptible subset. Across 14 benchmarks, MIRROR consistently outperforms prior methods, achieving gains of 2.1% on six standard benchmarks and 8.1% on seven in-the-wild benchmarks. On Human-AIGI, MIRROR reaches 89.6% accuracy across 27 generators, surpassing both lay users and visual experts, and further approaching the human perceptual limit as pretrained backbones scale. The code is publicly available at: https://github.com/349793927/MIRROR", "AI": {"tldr": "MIRROR reformulates AI-generated image detection as a reference-comparison problem using reality priors encoded in a learnable memory bank, achieving state-of-the-art performance and surpassing human experts on challenging benchmarks.", "motivation": "Existing AI-generated image detectors rely on artifact-based classification and struggle to generalize to evolving generative traces, while human judgment uses stable real-world regularities. The authors aim to create a more generalizable detection approach that mimics human cognitive processes.", "method": "Proposes MIRROR framework that encodes reality priors using a learnable discrete memory bank. Projects inputs into manifold-consistent ideal references via sparse linear combination and uses residuals as detection signals. Introduces Human-AIGI benchmark with psychophysically curated human-imperceptible subset.", "result": "Outperforms prior methods across 14 benchmarks: 2.1% gain on six standard benchmarks, 8.1% gain on seven in-the-wild benchmarks. Achieves 89.6% accuracy across 27 generators on Human-AIGI, surpassing both lay users and visual experts. Performance improves with pretrained backbone scaling.", "conclusion": "MIRROR successfully reformulates AIGI detection as reference-comparison with reality priors, achieving superhuman performance and approaching human perceptual limits. The approach demonstrates superior generalization to evolving generative models compared to artifact-based methods."}}
{"id": "2602.02223", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.02223", "abs": "https://arxiv.org/abs/2602.02223", "authors": ["Junchi Feng", "Nikhil Ballem", "Mahya Beheshti", "Giles Hamilton-Fletcher", "Todd Hudson", "Maurizio Porfiri", "William H. Seiple", "John-Ross Rizzo"], "title": "Evaluating OCR Performance for Assistive Technology: Effects of Walking Speed, Camera Placement, and Camera Type", "comment": null, "summary": "Optical character recognition (OCR), which converts printed or handwritten text into machine-readable form, is widely used in assistive technology for people with blindness and low vision. Yet, most evaluations rely on static datasets that do not reflect the challenges of mobile use. In this study, we systematically evaluated OCR performance under both static and dynamic conditions. Static tests measured detection range across distances of 1-7 meters and viewing angles of 0-75 degrees horizontally. Dynamic tests examined the impact of motion by varying walking speed from slow (0.8 m/s) to very fast (1.8 m/s) and comparing three camera mounting positions: head-mounted, shoulder-mounted, and hand-held. We evaluated both a smartphone and smart glasses, using the phone's main and ultra-wide cameras. Four OCR engines were benchmarked to assess accuracy at different distances and viewing angles: Google Vision, PaddleOCR 3.0, EasyOCR, and Tesseract. PaddleOCR 3.0 was then used to evaluate accuracy at different walking speeds. Accuracy was computed at the character level using the Levenshtein ratio against manually defined ground truth. Results showed that recognition accuracy declined with increased walking speed and wider viewing angles. Google Vision achieved the highest overall accuracy, with PaddleOCR close behind as the strongest open-source alternative. Across devices, the phone's main camera achieved the highest accuracy, and a shoulder-mounted placement yielded the highest average among body positions; however, differences among shoulder, head, and hand were not statistically significant.", "AI": {"tldr": "Systematic evaluation of OCR performance for assistive technology under static (distance/angle) and dynamic (walking speed/camera position) conditions shows accuracy declines with speed and wider angles, with Google Vision performing best and shoulder-mounted cameras yielding highest average accuracy.", "motivation": "Most OCR evaluations use static datasets that don't reflect real-world mobile use challenges for people with blindness and low vision, creating a gap in understanding how OCR performs under dynamic conditions.", "method": "Conducted static tests measuring detection range across distances (1-7m) and viewing angles (0-75\u00b0 horizontally), and dynamic tests varying walking speed (0.8-1.8 m/s) with three camera positions (head, shoulder, hand-held). Evaluated smartphone and smart glasses with four OCR engines (Google Vision, PaddleOCR 3.0, EasyOCR, Tesseract), using character-level Levenshtein ratio accuracy.", "result": "Recognition accuracy declined with increased walking speed and wider viewing angles. Google Vision achieved highest overall accuracy, with PaddleOCR as strongest open-source alternative. Phone's main camera performed best, and shoulder-mounted placement yielded highest average accuracy among body positions, though differences weren't statistically significant.", "conclusion": "Dynamic conditions significantly impact OCR performance for assistive technology, highlighting the need for real-world testing beyond static datasets. Google Vision leads performance, but open-source alternatives like PaddleOCR are viable, and shoulder-mounted cameras show promise for mobile OCR applications."}}
{"id": "2602.02227", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.02227", "abs": "https://arxiv.org/abs/2602.02227", "authors": ["Harold Haodong Chen", "Xinxiang Yin", "Wen-Jie Shu", "Hongfei Zhang", "Zixin Zhang", "Chenfei Liao", "Litao Guo", "Qifeng Chen", "Ying-Cong Chen"], "title": "Show, Don't Tell: Morphing Latent Reasoning into Image Generation", "comment": "Code: https://github.com/EnVision-Research/LatentMorph", "summary": "Text-to-image (T2I) generation has achieved remarkable progress, yet existing methods often lack the ability to dynamically reason and refine during generation--a hallmark of human creativity. Current reasoning-augmented paradigms most rely on explicit thought processes, where intermediate reasoning is decoded into discrete text at fixed steps with frequent image decoding and re-encoding, leading to inefficiencies, information loss, and cognitive mismatches. To bridge this gap, we introduce LatentMorph, a novel framework that seamlessly integrates implicit latent reasoning into the T2I generation process. At its core, LatentMorph introduces four lightweight components: (i) a condenser for summarizing intermediate generation states into compact visual memory, (ii) a translator for converting latent thoughts into actionable guidance, (iii) a shaper for dynamically steering next image token predictions, and (iv) an RL-trained invoker for adaptively determining when to invoke reasoning. By performing reasoning entirely in continuous latent spaces, LatentMorph avoids the bottlenecks of explicit reasoning and enables more adaptive self-refinement. Extensive experiments demonstrate that LatentMorph (I) enhances the base model Janus-Pro by $16\\%$ on GenEval and $25\\%$ on T2I-CompBench; (II) outperforms explicit paradigms (e.g., TwiG) by $15\\%$ and $11\\%$ on abstract reasoning tasks like WISE and IPV-Txt, (III) while reducing inference time by $44\\%$ and token consumption by $51\\%$; and (IV) exhibits $71\\%$ cognitive alignment with human intuition on reasoning invocation.", "AI": {"tldr": "LatentMorph introduces implicit latent reasoning for text-to-image generation, avoiding explicit text-based reasoning to improve efficiency and cognitive alignment.", "motivation": "Current T2I methods lack dynamic reasoning and refinement capabilities, and explicit reasoning paradigms suffer from inefficiencies, information loss, and cognitive mismatches due to frequent image decoding/re-encoding.", "method": "LatentMorph integrates implicit latent reasoning with four lightweight components: condenser for visual memory, translator for latent-to-guidance conversion, shaper for steering predictions, and RL-trained invoker for adaptive reasoning invocation.", "result": "LatentMorph improves Janus-Pro by 16% on GenEval and 25% on T2I-CompBench; outperforms explicit paradigms by 15-11% on abstract reasoning tasks; reduces inference time by 44% and token consumption by 51%; achieves 71% cognitive alignment with human intuition.", "conclusion": "LatentMorph demonstrates that implicit latent reasoning enables more efficient, adaptive, and cognitively-aligned text-to-image generation compared to explicit reasoning paradigms."}}
{"id": "2602.02232", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.02232", "abs": "https://arxiv.org/abs/2602.02232", "authors": ["Andrea Matteazzi", "Dietmar Tutsch"], "title": "LiFlow: Flow Matching for 3D LiDAR Scene Completion", "comment": null, "summary": "In autonomous driving scenarios, the collected LiDAR point clouds can be challenged by occlusion and long-range sparsity, limiting the perception of autonomous driving systems. Scene completion methods can infer the missing parts of incomplete 3D LiDAR scenes. Recent methods adopt local point-level denoising diffusion probabilistic models, which require predicting Gaussian noise, leading to a mismatch between training and inference initial distributions. This paper introduces the first flow matching framework for 3D LiDAR scene completion, improving upon diffusion-based methods by ensuring consistent initial distributions between training and inference. The model employs a nearest neighbor flow matching loss and a Chamfer distance loss to enhance both local structure and global coverage in the alignment of point clouds. LiFlow achieves state-of-the-art performance across multiple metrics. Code: https://github.com/matteandre/LiFlow.", "AI": {"tldr": "LiFlow: First flow matching framework for 3D LiDAR scene completion that improves upon diffusion methods by ensuring consistent training/inference distributions, achieving SOTA performance.", "motivation": "LiDAR point clouds in autonomous driving suffer from occlusion and long-range sparsity, limiting perception. Existing diffusion-based scene completion methods have a mismatch between training and inference initial distributions.", "method": "Proposes flow matching framework for 3D LiDAR scene completion with nearest neighbor flow matching loss and Chamfer distance loss to enhance local structure and global coverage alignment.", "result": "LiFlow achieves state-of-the-art performance across multiple metrics for 3D LiDAR scene completion.", "conclusion": "Flow matching provides better distribution consistency than diffusion methods for LiDAR scene completion, enabling improved perception for autonomous driving systems."}}
{"id": "2602.02318", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.02318", "abs": "https://arxiv.org/abs/2602.02318", "authors": ["Xiang Li", "Yupeng Zheng", "Pengfei Li", "Yilun Chen", "Ya-Qin Zhang", "Wenchao Ding"], "title": "Enhancing Indoor Occupancy Prediction via Sparse Query-Based Multi-Level Consistent Knowledge Distillation", "comment": "Accepted by RA-L", "summary": "Occupancy prediction provides critical geometric and semantic understanding for robotics but faces efficiency-accuracy trade-offs. Current dense methods suffer computational waste on empty voxels, while sparse query-based approaches lack robustness in diverse and complex indoor scenes. In this paper, we propose DiScene, a novel sparse query-based framework that leverages multi-level distillation to achieve efficient and robust occupancy prediction. In particular, our method incorporates two key innovations: (1) a Multi-level Consistent Knowledge Distillation strategy, which transfers hierarchical representations from large teacher models to lightweight students through coordinated alignment across four levels, including encoder-level feature alignment, query-level feature matching, prior-level spatial guidance, and anchor-level high-confidence knowledge transfer and (2) a Teacher-Guided Initialization policy, employing optimized parameter warm-up to accelerate model convergence. Validated on the Occ-Scannet benchmark, DiScene achieves 23.2 FPS without depth priors while outperforming our baseline method, OPUS, by 36.1% and even better than the depth-enhanced version, OPUS\u2020. With depth integration, DiScene\u2020 attains new SOTA performance, surpassing EmbodiedOcc by 3.7% with 1.62$\\times$ faster inference speed. Furthermore, experiments on the Occ3D-nuScenes benchmark and in-the-wild scenarios demonstrate the versatility of our approach in various environments. Code and models can be accessed at https://github.com/getterupper/DiScene.", "AI": {"tldr": "DiScene: A sparse query-based occupancy prediction framework using multi-level knowledge distillation for efficient and robust performance in indoor scenes.", "motivation": "Current occupancy prediction methods face efficiency-accuracy trade-offs: dense methods waste computation on empty voxels, while sparse query-based approaches lack robustness in diverse complex indoor scenes.", "method": "Proposes DiScene with two key innovations: 1) Multi-level Consistent Knowledge Distillation transferring hierarchical representations from teacher to student models through four coordinated alignment levels (encoder, query, prior, anchor), and 2) Teacher-Guided Initialization policy for optimized parameter warm-up to accelerate convergence.", "result": "On Occ-Scannet benchmark: achieves 23.2 FPS without depth priors, outperforms baseline OPUS by 36.1%, beats depth-enhanced OPUS\u2020. With depth integration (DiScene\u2020): attains new SOTA, surpassing EmbodiedOcc by 3.7% with 1.62\u00d7 faster inference. Also validated on Occ3D-nuScenes and in-the-wild scenarios.", "conclusion": "DiScene demonstrates efficient and robust occupancy prediction through multi-level distillation, achieving state-of-the-art performance with faster inference across multiple benchmarks and real-world scenarios."}}
{"id": "2602.02334", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.02334", "abs": "https://arxiv.org/abs/2602.02334", "authors": ["Fatemeh Zargarbashi", "Dhruv Agrawal", "Jakob Buhmann", "Martin Guay", "Stelian Coros", "Robert W. Sumner"], "title": "VQ-Style: Disentangling Style and Content in Motion with Residual Quantized Representations", "comment": null, "summary": "Human motion data is inherently rich and complex, containing both semantic content and subtle stylistic features that are challenging to model. We propose a novel method for effective disentanglement of the style and content in human motion data to facilitate style transfer. Our approach is guided by the insight that content corresponds to coarse motion attributes while style captures the finer, expressive details. To model this hierarchy, we employ Residual Vector Quantized Variational Autoencoders (RVQ-VAEs) to learn a coarse-to-fine representation of motion. We further enhance the disentanglement by integrating contrastive learning and a novel information leakage loss with codebook learning to organize the content and the style across different codebooks. We harness this disentangled representation using our simple and effective inference-time technique Quantized Code Swapping, which enables motion style transfer without requiring any fine-tuning for unseen styles. Our framework demonstrates strong versatility across multiple inference applications, including style transfer, style removal, and motion blending.", "AI": {"tldr": "A novel method for disentangling style and content in human motion data using RVQ-VAEs with contrastive learning and information leakage loss, enabling style transfer without fine-tuning via Quantized Code Swapping.", "motivation": "Human motion data contains rich semantic content and subtle stylistic features that are challenging to model and disentangle, making style transfer difficult.", "method": "Uses Residual Vector Quantized Variational Autoencoders (RVQ-VAEs) for coarse-to-fine motion representation, enhanced with contrastive learning and information leakage loss for disentanglement, plus Quantized Code Swapping for inference-time style transfer.", "result": "The framework demonstrates strong versatility across multiple applications including style transfer, style removal, and motion blending without requiring fine-tuning for unseen styles.", "conclusion": "The proposed method effectively disentangles style and content in human motion, enabling flexible style manipulation through a simple inference-time technique that works for unseen styles."}}
{"id": "2602.02341", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.02341", "abs": "https://arxiv.org/abs/2602.02341", "authors": ["Zhenpeng Huang", "Jiaqi Li", "Zihan Jia", "Xinhao Li", "Desen Meng", "Lingxue Song", "Xi Chen", "Liang Li", "Limin Wang"], "title": "LongVPO: From Anchored Cues to Self-Reasoning for Long-Form Video Preference Optimization", "comment": "NeurIPS 2025", "summary": "We present LongVPO, a novel two-stage Direct Preference Optimization framework that enables short-context vision-language models to robustly understand ultra-long videos without any long-video annotations. In Stage 1, we synthesize preference triples by anchoring questions to individual short clips, interleaving them with distractors, and applying visual-similarity and question-specificity filtering to mitigate positional bias and ensure unambiguous supervision. We also approximate the reference model's scoring over long contexts by evaluating only the anchor clip, reducing computational overhead. In Stage 2, we employ a recursive captioning pipeline on long videos to generate scene-level metadata, then use a large language model to craft multi-segment reasoning queries and dispreferred responses, aligning the model's preferences through multi-segment reasoning tasks. With only 16K synthetic examples and no costly human labels, LongVPO outperforms the state-of-the-art open-source models on multiple long-video benchmarks, while maintaining strong short-video performance (e.g., on MVBench), offering a scalable paradigm for efficient long-form video understanding.", "AI": {"tldr": "LongVPO is a two-stage DPO framework that enables short-context VLMs to understand ultra-long videos without long-video annotations, using synthetic preference triples and multi-segment reasoning tasks.", "motivation": "Existing vision-language models struggle with ultra-long video understanding due to computational constraints and lack of long-video annotations. There's a need for scalable methods that can extend short-context models to handle long videos without costly human labeling.", "method": "Two-stage approach: Stage 1 synthesizes preference triples by anchoring questions to short clips with distractors, using filtering to mitigate bias and approximating reference model scoring. Stage 2 uses recursive captioning on long videos to generate scene metadata, then employs LLMs to create multi-segment reasoning queries and dispreferred responses for alignment through multi-segment reasoning tasks.", "result": "LongVPO outperforms state-of-the-art open-source models on multiple long-video benchmarks while maintaining strong short-video performance on MVBench, achieving this with only 16K synthetic examples and no human labels.", "conclusion": "LongVPO offers a scalable paradigm for efficient long-form video understanding, demonstrating that synthetic data generation and multi-segment reasoning alignment can effectively extend short-context models to handle ultra-long videos without expensive human annotation."}}
{"id": "2602.02354", "categories": ["cs.CV", "cs.AI", "cs.GR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.02354", "abs": "https://arxiv.org/abs/2602.02354", "authors": ["Albert Kwok", "Zheyuan Hu", "Dounia Hammou"], "title": "Implicit neural representation of textures", "comment": "Albert Kwok and Zheyuan Hu contributed equally to this work", "summary": "Implicit neural representation (INR) has proven to be accurate and efficient in various domains. In this work, we explore how different neural networks can be designed as a new texture INR, which operates in a continuous manner rather than a discrete one over the input UV coordinate space. Through thorough experiments, we demonstrate that these INRs perform well in terms of image quality, with considerable memory usage and rendering inference time. We analyze the balance between these objectives. In addition, we investigate various related applications in real-time rendering and down-stream tasks, e.g. mipmap fitting and INR-space generation.", "AI": {"tldr": "The paper explores using implicit neural representations (INRs) as continuous texture representations, analyzing their performance in image quality, memory usage, and rendering speed, with applications in real-time rendering and downstream tasks.", "motivation": "To develop continuous texture representations using implicit neural networks that overcome limitations of discrete texture representations, enabling more efficient and flexible texture handling in rendering pipelines.", "method": "Design different neural networks as texture INRs that operate continuously over UV coordinate space, conducting thorough experiments to evaluate image quality, memory usage, and rendering inference time.", "result": "INRs perform well in terms of image quality while offering considerable memory usage and rendering inference time advantages, with analysis of the trade-offs between these objectives.", "conclusion": "Implicit neural representations provide effective continuous texture representations with good performance trade-offs, enabling various applications in real-time rendering and downstream tasks like mipmap fitting and INR-space generation."}}
{"id": "2602.02356", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.02356", "abs": "https://arxiv.org/abs/2602.02356", "authors": ["Wangduo Xie", "Matthew B. Blaschko"], "title": "NAB: Neural Adaptive Binning for Sparse-View CT reconstruction", "comment": null, "summary": "Computed Tomography (CT) plays a vital role in inspecting the internal structures of industrial objects. Furthermore, achieving high-quality CT reconstruction from sparse views is essential for reducing production costs. While classic implicit neural networks have shown promising results for sparse reconstruction, they are unable to leverage shape priors of objects. Motivated by the observation that numerous industrial objects exhibit rectangular structures, we propose a novel \\textbf{N}eural \\textbf{A}daptive \\textbf{B}inning (\\textbf{NAB}) method that effectively integrates rectangular priors into the reconstruction process. Specifically, our approach first maps coordinate space into a binned vector space. This mapping relies on an innovative binning mechanism based on differences between shifted hyperbolic tangent functions, with our extension enabling rotations around the input-plane normal vector. The resulting representations are then processed by a neural network to predict CT attenuation coefficients. This design enables end-to-end optimization of the encoding parameters -- including position, size, steepness, and rotation -- via gradient flow from the projection data, thus enhancing reconstruction accuracy. By adjusting the smoothness of the binning function, NAB can generalize to objects with more complex geometries. This research provides a new perspective on integrating shape priors into neural network-based reconstruction. Extensive experiments demonstrate that NAB achieves superior performance on two industrial datasets. It also maintains robust on medical datasets when the binning function is extended to more general expression. The code will be made available.", "AI": {"tldr": "NAB: Neural Adaptive Binning method for CT reconstruction that integrates rectangular shape priors via learnable binning functions to improve sparse-view reconstruction of industrial objects.", "motivation": "CT is vital for industrial inspection but sparse-view reconstruction is needed to reduce costs. Existing implicit neural networks for sparse reconstruction cannot leverage shape priors, even though many industrial objects have rectangular structures.", "method": "Proposes Neural Adaptive Binning (NAB) that maps coordinate space to binned vector space using a novel binning mechanism based on shifted hyperbolic tangent functions with rotation extension. The encoding parameters (position, size, steepness, rotation) are optimized end-to-end via gradient flow from projection data.", "result": "NAB achieves superior performance on two industrial datasets and maintains robustness on medical datasets when extended to more general binning functions. The method effectively integrates rectangular priors to enhance reconstruction accuracy.", "conclusion": "NAB provides a new perspective on integrating shape priors into neural network-based reconstruction, offering improved sparse-view CT reconstruction for industrial objects with rectangular structures while maintaining generalization capability."}}
{"id": "2602.02370", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.02370", "abs": "https://arxiv.org/abs/2602.02370", "authors": ["Uma Meleti", "Jeffrey J. Nirschl"], "title": "Uncertainty-Aware Image Classification In Biomedical Imaging Using Spectral-normalized Neural Gaussian Processes", "comment": "Accepted for publication at the IEEE International Symposium on Biomedical Imaging (ISBI) 2026", "summary": "Accurate histopathologic interpretation is key for clinical decision-making; however, current deep learning models for digital pathology are often overconfident and poorly calibrated in out-of-distribution (OOD) settings, which limit trust and clinical adoption. Safety-critical medical imaging workflows benefit from intrinsic uncertainty-aware properties that can accurately reject OOD input. We implement the Spectral-normalized Neural Gaussian Process (SNGP), a set of lightweight modifications that apply spectral normalization and replace the final dense layer with a Gaussian process layer to improve single-model uncertainty estimation and OOD detection. We evaluate SNGP vs. deterministic and MonteCarlo dropout on six datasets across three biomedical classification tasks: white blood cells, amyloid plaques, and colorectal histopathology. SNGP has comparable in-distribution performance while significantly improving uncertainty estimation and OOD detection. Thus, SNGP or related models offer a useful framework for uncertainty-aware classification in digital pathology, supporting safe deployment and building trust with pathologists.", "AI": {"tldr": "SNGP improves uncertainty estimation and OOD detection in digital pathology models while maintaining comparable in-distribution performance.", "motivation": "Current deep learning models for digital pathology are overconfident and poorly calibrated in out-of-distribution settings, limiting clinical trust and adoption. Medical imaging workflows need uncertainty-aware properties to safely reject OOD inputs.", "method": "Implement Spectral-normalized Neural Gaussian Process (SNGP) - lightweight modifications applying spectral normalization and replacing final dense layer with Gaussian process layer to improve single-model uncertainty estimation and OOD detection.", "result": "SNGP shows comparable in-distribution performance while significantly improving uncertainty estimation and OOD detection compared to deterministic and Monte Carlo dropout models across six datasets and three biomedical classification tasks.", "conclusion": "SNGP offers a useful framework for uncertainty-aware classification in digital pathology, supporting safe deployment and building trust with pathologists."}}
{"id": "2602.02380", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.02380", "abs": "https://arxiv.org/abs/2602.02380", "authors": ["Yibin Wang", "Yuhang Zang", "Feng Han", "Jiazi Bu", "Yujie Zhou", "Cheng Jin", "Jiaqi Wang"], "title": "Unified Personalized Reward Model for Vision Generation", "comment": "Website: https://codegoat24.github.io/UnifiedReward/flex", "summary": "Recent advancements in multimodal reward models (RMs) have significantly propelled the development of visual generation. Existing frameworks typically adopt Bradley-Terry-style preference modeling or leverage generative VLMs as judges, and subsequently optimize visual generation models via reinforcement learning. However, current RMs suffer from inherent limitations: they often follow a one-size-fits-all paradigm that assumes a monolithic preference distribution or relies on fixed evaluation rubrics. As a result, they are insensitive to content-specific visual cues, leading to systematic misalignment with subjective and context-dependent human preferences. To this end, inspired by human assessment, we propose UnifiedReward-Flex, a unified personalized reward model for vision generation that couples reward modeling with flexible and context-adaptive reasoning. Specifically, given a prompt and the generated visual content, it first interprets the semantic intent and grounds on visual evidence, then dynamically constructs a hierarchical assessment by instantiating fine-grained criteria under both predefined and self-generated high-level dimensions. Our training pipeline follows a two-stage process: (1) we first distill structured, high-quality reasoning traces from advanced closed-source VLMs to bootstrap SFT, equipping the model with flexible and context-adaptive reasoning behaviors; (2) we then perform direct preference optimization (DPO) on carefully curated preference pairs to further strengthen reasoning fidelity and discriminative alignment. To validate the effectiveness, we integrate UnifiedReward-Flex into the GRPO framework for image and video synthesis, and extensive results demonstrate its superiority.", "AI": {"tldr": "UnifiedReward-Flex is a personalized reward model for vision generation that uses context-adaptive reasoning to assess visual content based on semantic intent and fine-grained criteria, addressing limitations of one-size-fits-all reward models.", "motivation": "Current multimodal reward models for visual generation follow a one-size-fits-all paradigm that assumes monolithic preference distributions or uses fixed evaluation rubrics. This makes them insensitive to content-specific visual cues and leads to systematic misalignment with subjective, context-dependent human preferences.", "method": "The model first interprets semantic intent and grounds on visual evidence, then dynamically constructs hierarchical assessment by instantiating fine-grained criteria under both predefined and self-generated high-level dimensions. Training involves: (1) distilling structured reasoning traces from advanced VLMs for SFT to bootstrap flexible reasoning behaviors, and (2) performing DPO on curated preference pairs to strengthen reasoning fidelity and discriminative alignment.", "result": "When integrated into the GRPO framework for image and video synthesis, UnifiedReward-Flex demonstrates superiority over existing approaches, showing improved alignment with human preferences through its personalized, context-adaptive assessment.", "conclusion": "UnifiedReward-Flex addresses the limitations of current reward models by introducing personalized, context-adaptive reasoning for visual generation assessment, leading to better alignment with subjective human preferences through hierarchical, fine-grained evaluation."}}
{"id": "2602.02388", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.02388", "abs": "https://arxiv.org/abs/2602.02388", "authors": ["Rajalaxmi Rajagopalan", "Debottam Dutta", "Yu-Lin Wei", "Romit Roy Choudhury"], "title": "Personalized Image Generation via Human-in-the-loop Bayesian Optimization", "comment": null, "summary": "Imagine Alice has a specific image $x^\\ast$ in her mind, say, the view of the street in which she grew up during her childhood. To generate that exact image, she guides a generative model with multiple rounds of prompting and arrives at an image $x^{p*}$. Although $x^{p*}$ is reasonably close to $x^\\ast$, Alice finds it difficult to close that gap using language prompts. This paper aims to narrow this gap by observing that even after language has reached its limits, humans can still tell when a new image $x^+$ is closer to $x^\\ast$ than $x^{p*}$. Leveraging this observation, we develop MultiBO (Multi-Choice Preferential Bayesian Optimization) that carefully generates $K$ new images as a function of $x^{p*}$, gets preferential feedback from the user, uses the feedback to guide the diffusion model, and ultimately generates a new set of $K$ images. We show that within $B$ rounds of user feedback, it is possible to arrive much closer to $x^\\ast$, even though the generative model has no information about $x^\\ast$. Qualitative scores from $30$ users, combined with quantitative metrics compared across $5$ baselines, show promising results, suggesting that multi-choice feedback from humans can be effectively harnessed for personalized image generation.", "AI": {"tldr": "MultiBO uses multi-choice preferential Bayesian optimization to refine AI-generated images when language prompts alone can't achieve the exact image a user has in mind.", "motivation": "When users have a specific mental image they want to generate with AI, language prompts often fall short of capturing the exact details. Even after extensive prompting, there remains a gap between the generated image and what the user imagines, but humans can still recognize when new images are closer to their mental target.", "method": "MultiBO (Multi-Choice Preferential Bayesian Optimization) generates K new images based on the best prompt-generated image, collects preferential feedback from users about which images are closer to their mental target, uses this feedback to guide the diffusion model, and iteratively refines the images over B rounds of user feedback.", "result": "The method shows promising results with qualitative scores from 30 users and quantitative metrics outperforming 5 baselines. Users can get much closer to their mental image within limited feedback rounds, even though the generative model has no direct information about the target image.", "conclusion": "Multi-choice preferential feedback from humans can be effectively harnessed for personalized image generation, bridging the gap between language prompts and specific mental images that users want to create."}}
{"id": "2602.02393", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.02393", "abs": "https://arxiv.org/abs/2602.02393", "authors": ["Ruiqi Wu", "Xuanhua He", "Meng Cheng", "Tianyu Yang", "Yong Zhang", "Zhuoliang Kang", "Xunliang Cai", "Xiaoming Wei", "Chunle Guo", "Chongyi Li", "Ming-Ming Cheng"], "title": "Infinite-World: Scaling Interactive World Models to 1000-Frame Horizons via Pose-Free Hierarchical Memory", "comment": "14 pages, 8 figures", "summary": "We propose Infinite-World, a robust interactive world model capable of maintaining coherent visual memory over 1000+ frames in complex real-world environments. While existing world models can be efficiently optimized on synthetic data with perfect ground-truth, they lack an effective training paradigm for real-world videos due to noisy pose estimations and the scarcity of viewpoint revisits. To bridge this gap, we first introduce a Hierarchical Pose-free Memory Compressor (HPMC) that recursively distills historical latents into a fixed-budget representation. By jointly optimizing the compressor with the generative backbone, HPMC enables the model to autonomously anchor generations in the distant past with bounded computational cost, eliminating the need for explicit geometric priors. Second, we propose an Uncertainty-aware Action Labeling module that discretizes continuous motion into a tri-state logic. This strategy maximizes the utilization of raw video data while shielding the deterministic action space from being corrupted by noisy trajectories, ensuring robust action-response learning. Furthermore, guided by insights from a pilot toy study, we employ a Revisit-Dense Finetuning Strategy using a compact, 30-minute dataset to efficiently activate the model's long-range loop-closure capabilities. Extensive experiments, including objective metrics and user studies, demonstrate that Infinite-World achieves superior performance in visual quality, action controllability, and spatial consistency.", "AI": {"tldr": "Infinite-World is a robust interactive world model that maintains coherent visual memory over 1000+ frames in complex real-world environments, addressing limitations of existing models in handling noisy pose estimations and viewpoint scarcity.", "motivation": "Existing world models work well on synthetic data with perfect ground-truth but lack effective training paradigms for real-world videos due to noisy pose estimations and scarcity of viewpoint revisits, creating a gap between synthetic and real-world performance.", "method": "Three key innovations: 1) Hierarchical Pose-free Memory Compressor (HPMC) that recursively distills historical latents into fixed-budget representation, 2) Uncertainty-aware Action Labeling that discretizes continuous motion into tri-state logic, and 3) Revisit-Dense Finetuning Strategy using compact dataset to activate long-range loop-closure capabilities.", "result": "Extensive experiments show Infinite-World achieves superior performance in visual quality, action controllability, and spatial consistency, demonstrating robust memory maintenance over 1000+ frames in complex real-world environments.", "conclusion": "Infinite-World successfully bridges the gap between synthetic and real-world world modeling by eliminating need for explicit geometric priors, handling noisy trajectories, and efficiently activating long-range loop-closure capabilities through novel architectural and training innovations."}}
{"id": "2602.02401", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.02401", "abs": "https://arxiv.org/abs/2602.02401", "authors": ["Xinshun Wang", "Peiming Li", "Ziyi Wang", "Zhongbin Fang", "Zhichao Deng", "Songtao Wu", "Jason Li", "Mengyuan Liu"], "title": "Superman: Unifying Skeleton and Vision for Human Motion Perception and Generation", "comment": null, "summary": "Human motion analysis tasks, such as temporal 3D pose estimation, motion prediction, and motion in-betweening, play an essential role in computer vision. However, current paradigms suffer from severe fragmentation. First, the field is split between ``perception'' models that understand motion from video but only output text, and ``generation'' models that cannot perceive from raw visual input. Second, generative MLLMs are often limited to single-frame, static poses using dense, parametric SMPL models, failing to handle temporal motion. Third, existing motion vocabularies are built from skeleton data alone, severing the link to the visual domain. To address these challenges, we introduce Superman, a unified framework that bridges visual perception with temporal, skeleton-based motion generation. Our solution is twofold. First, to overcome the modality disconnect, we propose a Vision-Guided Motion Tokenizer. Leveraging the natural geometric alignment between 3D skeletons and visual data, this module pioneers robust joint learning from both modalities, creating a unified, cross-modal motion vocabulary. Second, grounded in this motion language, a single, unified MLLM architecture is trained to handle all tasks. This module flexibly processes diverse, temporal inputs, unifying 3D skeleton pose estimation from video (perception) with skeleton-based motion prediction and in-betweening (generation). Extensive experiments on standard benchmarks, including Human3.6M, demonstrate that our unified method achieves state-of-the-art or competitive performance across all motion tasks. This showcases a more efficient and scalable path for generative motion analysis using skeletons.", "AI": {"tldr": "Superman is a unified framework that bridges visual perception with temporal skeleton-based motion generation using a vision-guided motion tokenizer and single MLLM architecture.", "motivation": "Current motion analysis suffers from fragmentation: perception models only output text from video, generation models can't perceive visual input, generative MLLMs are limited to single-frame static poses, and motion vocabularies are built from skeleton data alone, severing visual domain links.", "method": "Two-fold solution: 1) Vision-Guided Motion Tokenizer leverages geometric alignment between 3D skeletons and visual data for joint learning, creating unified cross-modal motion vocabulary; 2) Single unified MLLM architecture trained to handle all tasks, processing diverse temporal inputs for both perception (3D skeleton pose estimation from video) and generation (motion prediction and in-betweening).", "result": "Extensive experiments on standard benchmarks including Human3.6M demonstrate state-of-the-art or competitive performance across all motion tasks, showing efficient and scalable path for generative motion analysis using skeletons.", "conclusion": "Superman successfully bridges the gap between visual perception and temporal motion generation, creating a unified framework that outperforms fragmented approaches and offers a more efficient path forward for motion analysis."}}
{"id": "2602.02408", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.02408", "abs": "https://arxiv.org/abs/2602.02408", "authors": ["Jiaxing Qiu", "Kaihua Hou", "Roxana Daneshjou", "Ahmed Alaa", "Thomas Hartvigsen"], "title": "ReasonEdit: Editing Vision-Language Models using Human Reasoning", "comment": null, "summary": "Model editing aims to correct errors in large, pretrained models without altering unrelated behaviors. While some recent works have edited vision-language models (VLMs), no existing editors tackle reasoning-heavy tasks, which typically require humans and models to reason about images.We therefore propose ReasonEdit, the first VLM editor to let users explain their reasoning during editing, introducing a new, practical model editing setup. ReasonEdit continuously stores human reasoning in a codebook, and retrieves only relevant facts during inference using a novel topology-balanced multimodal embedding method inspired by network science. Across four VLMs on multiple rationale-based visual question answering datasets, ReasonEdit achieves state-of-the-art editing performance, ultimately showing that using human reasoning during editing greatly improves edit generalization.", "AI": {"tldr": "ReasonEdit is the first vision-language model editor that incorporates human reasoning during editing, achieving SOTA performance on reasoning-heavy visual question answering tasks.", "motivation": "Existing VLM editors don't handle reasoning-heavy tasks that require both humans and models to reason about images. There's a need for editors that can incorporate human reasoning explanations during the editing process.", "method": "ReasonEdit continuously stores human reasoning in a codebook and retrieves relevant facts during inference using a novel topology-balanced multimodal embedding method inspired by network science.", "result": "Across four VLMs on multiple rationale-based visual question answering datasets, ReasonEdit achieves state-of-the-art editing performance and demonstrates that using human reasoning during editing greatly improves edit generalization.", "conclusion": "ReasonEdit introduces a practical model editing setup that successfully incorporates human reasoning, showing significant improvements in edit generalization for vision-language models on reasoning-heavy tasks."}}
{"id": "2602.02409", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.02409", "abs": "https://arxiv.org/abs/2602.02409", "authors": ["Abid Hassan", "Tuan Ngo", "Saad Shafiq", "Nenad Medvidovic"], "title": "Catalyst: Out-of-Distribution Detection via Elastic Scaling", "comment": null, "summary": "Out-of-distribution (OOD) detection is critical for the safe deployment of deep neural networks. State-of-the-art post-hoc methods typically derive OOD scores from the output logits or penultimate feature vector obtained via global average pooling (GAP). We contend that this exclusive reliance on the logit or feature vector discards a rich, complementary signal: the raw channel-wise statistics of the pre-pooling feature map lost in GAP. In this paper, we introduce Catalyst, a post-hoc framework that exploits these under-explored signals. Catalyst computes an input-dependent scaling factor ($\u03b3$) on-the-fly from these raw statistics (e.g., mean, standard deviation, and maximum activation). This $\u03b3$ is then fused with the existing baseline score, multiplicatively modulating it -- an ``elastic scaling'' -- to push the ID and OOD distributions further apart. We demonstrate Catalyst is a generalizable framework: it seamlessly integrates with logit-based methods (e.g., Energy, ReAct, SCALE) and also provides a significant boost to distance-based detectors like KNN. As a result, Catalyst achieves substantial and consistent performance gains, reducing the average False Positive Rate by 32.87 on CIFAR-10 (ResNet-18), 27.94% on CIFAR-100 (ResNet-18), and 22.25% on ImageNet (ResNet-50). Our results highlight the untapped potential of pre-pooling statistics and demonstrate that Catalyst is complementary to existing OOD detection approaches.", "AI": {"tldr": "Catalyst is a post-hoc OOD detection framework that uses pre-pooling feature map statistics to compute an input-dependent scaling factor, which multiplicatively modulates existing OOD scores to better separate ID and OOD distributions.", "motivation": "Current post-hoc OOD detection methods rely only on logits or GAP features, discarding valuable channel-wise statistics from pre-pooling feature maps. These raw statistics contain complementary signals that could improve OOD detection performance.", "method": "Catalyst extracts raw channel-wise statistics (mean, standard deviation, maximum activation) from pre-pooling feature maps, computes an input-dependent scaling factor \u03b3, and fuses it multiplicatively with existing baseline OOD scores via \"elastic scaling\" to push ID and OOD distributions apart.", "result": "Catalyst achieves substantial performance gains: reduces average False Positive Rate by 32.87% on CIFAR-10 (ResNet-18), 27.94% on CIFAR-100 (ResNet-18), and 22.25% on ImageNet (ResNet-50). It works with both logit-based methods (Energy, ReAct, SCALE) and distance-based detectors like KNN.", "conclusion": "Pre-pooling feature map statistics contain untapped potential for OOD detection. Catalyst is a generalizable framework that complements existing approaches and consistently improves performance across different datasets and network architectures."}}
{"id": "2602.02426", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.02426", "abs": "https://arxiv.org/abs/2602.02426", "authors": ["Simon-Olivier Duguay", "Hugo Baudchon", "Etienne Lalibert\u00e9", "Helene Muller-Landau", "Gonzalo Rivas-Torres", "Arthur Ouaknine"], "title": "SelvaMask: Segmenting Trees in Tropical Forests and Beyond", "comment": "22 pages, 8 figures", "summary": "Tropical forests harbor most of the planet's tree biodiversity and are critical to global ecological balance. Canopy trees in particular play a disproportionate role in carbon storage and functioning of these ecosystems. Studying canopy trees at scale requires accurate delineation of individual tree crowns, typically performed using high-resolution aerial imagery. Despite advances in transformer-based models for individual tree crown segmentation, performance remains low in most forests, especially tropical ones. To this end, we introduce SelvaMask, a new tropical dataset containing over 8,800 manually delineated tree crowns across three Neotropical forest sites in Panama, Brazil, and Ecuador. SelvaMask features comprehensive annotations, including an inter-annotator agreement evaluation, capturing the dense structure of tropical forests and highlighting the difficulty of the task. Leveraging this benchmark, we propose a modular detection-segmentation pipeline that adapts vision foundation models (VFMs), using domain-specific detection-prompter. Our approach reaches state-of-the-art performance, outperforming both zero-shot generalist models and fully supervised end-to-end methods in dense tropical forests. We validate these gains on external tropical and temperate datasets, demonstrating that SelvaMask serves as both a challenging benchmark and a key enabler for generalized forest monitoring. Our code and dataset will be released publicly.", "AI": {"tldr": "SelvaMask: A new tropical forest dataset with 8,800+ manually delineated tree crowns across three Neotropical sites, enabling improved individual tree crown segmentation using a modular detection-segmentation pipeline that adapts vision foundation models.", "motivation": "Tropical forests are critical for global ecological balance and carbon storage, but current transformer-based models for individual tree crown segmentation perform poorly in tropical forests due to their dense structure and lack of comprehensive datasets.", "method": "Created SelvaMask dataset with comprehensive annotations and inter-annotator agreement evaluation. Developed a modular detection-segmentation pipeline that adapts vision foundation models using domain-specific detection-prompter for improved tree crown delineation.", "result": "Achieved state-of-the-art performance, outperforming both zero-shot generalist models and fully supervised end-to-end methods in dense tropical forests. Validated gains on external tropical and temperate datasets.", "conclusion": "SelvaMask serves as both a challenging benchmark and key enabler for generalized forest monitoring, with publicly released code and dataset to advance tropical forest research."}}
{"id": "2602.02437", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.02437", "abs": "https://arxiv.org/abs/2602.02437", "authors": ["Dianyi Wang", "Chaofan Ma", "Feng Han", "Size Wu", "Wei Song", "Yibin Wang", "Zhixiong Zhang", "Tianhang Wang", "Siyuan Wang", "Zhongyu Wei", "Jiaqi Wang"], "title": "UniReason 1.0: A Unified Reasoning Framework for World Knowledge Aligned Image Generation and Editing", "comment": null, "summary": "Unified multimodal models often struggle with complex synthesis tasks that demand deep reasoning, and typically treat text-to-image generation and image editing as isolated capabilities rather than interconnected reasoning steps. To address this, we propose UniReason, a unified framework that harmonizes these two tasks through a dual reasoning paradigm. We formulate generation as world knowledge-enhanced planning to inject implicit constraints, and leverage editing capabilities for fine-grained visual refinement to further correct visual errors via self-reflection. This approach unifies generation and editing within a shared representation, mirroring the human cognitive process of planning followed by refinement. We support this framework by systematically constructing a large-scale reasoning-centric dataset (~300k samples) covering five major knowledge domains (e.g., cultural commonsense, physics, etc.) for planning, alongside an agent-generated corpus for visual self-correction. Extensive experiments demonstrate that UniReason achieves advanced performance on reasoning-intensive benchmarks such as WISE, KrisBench and UniREditBench, while maintaining superior general synthesis capabilities.", "AI": {"tldr": "UniReason is a unified framework that connects text-to-image generation and image editing through a dual reasoning paradigm, treating them as interconnected steps rather than isolated capabilities.", "motivation": "Current unified multimodal models struggle with complex synthesis tasks requiring deep reasoning and treat generation and editing as separate capabilities rather than interconnected reasoning steps.", "method": "Proposes UniReason with a dual reasoning paradigm: 1) generation as world knowledge-enhanced planning with implicit constraints, and 2) editing for fine-grained visual refinement via self-reflection. Unifies both tasks within a shared representation. Also constructs a large-scale reasoning-centric dataset (~300k samples) covering five knowledge domains for planning and an agent-generated corpus for visual self-correction.", "result": "UniReason achieves advanced performance on reasoning-intensive benchmarks (WISE, KrisBench, UniREditBench) while maintaining superior general synthesis capabilities.", "conclusion": "The framework successfully unifies generation and editing through a reasoning-based approach that mirrors human cognitive processes of planning followed by refinement, demonstrating improved performance on complex synthesis tasks."}}
{"id": "2602.02471", "categories": ["cs.CV", "cs.AI", "physics.med-ph"], "pdf": "https://arxiv.org/pdf/2602.02471", "abs": "https://arxiv.org/abs/2602.02471", "authors": ["Edwin Kys", "Febian Febian"], "title": "Multi-head automated segmentation by incorporating detection head into the contextual layer neural network", "comment": "8 pages, 3 figures, 1 table", "summary": "Deep learning based auto segmentation is increasingly used in radiotherapy, but conventional models often produce anatomically implausible false positives, or hallucinations, in slices lacking target structures. We propose a gated multi-head Transformer architecture based on Swin U-Net, augmented with inter-slice context integration and a parallel detection head, which jointly performs slice-level structure detection via a multi-layer perceptron and pixel-level segmentation through a context-enhanced stream. Detection outputs gate the segmentation predictions to suppress false positives in anatomically invalid slices, and training uses slice-wise Tversky loss to address class imbalance. Experiments on the Prostate-Anatomical-Edge-Cases dataset from The Cancer Imaging Archive demonstrate that the gated model substantially outperforms a non-gated segmentation-only baseline, achieving a mean Dice loss of $0.013 \\pm 0.036$ versus $0.732 \\pm 0.314$, with detection probabilities strongly correlated with anatomical presence, effectively eliminating spurious segmentations. In contrast, the non-gated model exhibited higher variability and persistent false positives across all slices. These results indicate that detection-based gating enhances robustness and anatomical plausibility in automated segmentation applications, reducing hallucinated predictions without compromising segmentation quality in valid slices, and offers a promising approach for improving the reliability of clinical radiotherapy auto-contouring workflows.", "AI": {"tldr": "A gated multi-head Transformer architecture for radiotherapy auto-segmentation that uses slice-level detection to gate segmentation predictions, eliminating false positives in slices without target structures.", "motivation": "Conventional deep learning segmentation models in radiotherapy often produce anatomically implausible false positives (hallucinations) in slices that don't contain the target structures, which reduces clinical reliability.", "method": "A gated multi-head Transformer based on Swin U-Net with inter-slice context integration and a parallel detection head. The model jointly performs slice-level structure detection via MLP and pixel-level segmentation through a context-enhanced stream. Detection outputs gate segmentation predictions to suppress false positives in invalid slices, using slice-wise Tversky loss to address class imbalance.", "result": "The gated model substantially outperforms non-gated baseline, achieving mean Dice loss of 0.013 \u00b1 0.036 vs 0.732 \u00b1 0.314 on Prostate-Anatomical-Edge-Cases dataset. Detection probabilities strongly correlate with anatomical presence, effectively eliminating spurious segmentations, while non-gated model showed higher variability and persistent false positives.", "conclusion": "Detection-based gating enhances robustness and anatomical plausibility in automated segmentation, reducing hallucinated predictions without compromising segmentation quality in valid slices. This offers a promising approach for improving reliability of clinical radiotherapy auto-contouring workflows."}}
{"id": "2602.02493", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.02493", "abs": "https://arxiv.org/abs/2602.02493", "authors": ["Zehong Ma", "Ruihan Xu", "Shiliang Zhang"], "title": "PixelGen: Pixel Diffusion Beats Latent Diffusion with Perceptual Loss", "comment": "Project Pages: https://zehong-ma.github.io/PixelGen/", "summary": "Pixel diffusion generates images directly in pixel space in an end-to-end manner, avoiding the artifacts and bottlenecks introduced by VAEs in two-stage latent diffusion. However, it is challenging to optimize high-dimensional pixel manifolds that contain many perceptually irrelevant signals, leaving existing pixel diffusion methods lagging behind latent diffusion models. We propose PixelGen, a simple pixel diffusion framework with perceptual supervision. Instead of modeling the full image manifold, PixelGen introduces two complementary perceptual losses to guide diffusion model towards learning a more meaningful perceptual manifold. An LPIPS loss facilitates learning better local patterns, while a DINO-based perceptual loss strengthens global semantics. With perceptual supervision, PixelGen surpasses strong latent diffusion baselines. It achieves an FID of 5.11 on ImageNet-256 without classifier-free guidance using only 80 training epochs, and demonstrates favorable scaling performance on large-scale text-to-image generation with a GenEval score of 0.79. PixelGen requires no VAEs, no latent representations, and no auxiliary stages, providing a simpler yet more powerful generative paradigm. Codes are publicly available at https://github.com/Zehong-Ma/PixelGen.", "AI": {"tldr": "PixelGen is a pixel diffusion framework that uses perceptual supervision (LPIPS and DINO losses) to improve image generation quality, surpassing latent diffusion models without needing VAEs or latent representations.", "motivation": "Existing pixel diffusion methods struggle with high-dimensional pixel manifolds containing perceptually irrelevant signals, causing them to lag behind latent diffusion models that use VAEs. The authors want to create a simpler end-to-end pixel diffusion approach that matches or exceeds latent diffusion performance.", "method": "PixelGen introduces two complementary perceptual losses: LPIPS loss for better local patterns and DINO-based perceptual loss for stronger global semantics. These losses guide the diffusion model to learn a more meaningful perceptual manifold instead of modeling the full image manifold.", "result": "PixelGen achieves FID of 5.11 on ImageNet-256 without classifier-free guidance using only 80 training epochs, and demonstrates favorable scaling performance on large-scale text-to-image generation with GenEval score of 0.79, surpassing strong latent diffusion baselines.", "conclusion": "PixelGen provides a simpler yet more powerful generative paradigm that requires no VAEs, no latent representations, and no auxiliary stages, offering an end-to-end pixel diffusion approach that outperforms latent diffusion models."}}
