{"id": "2601.00812", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.00812", "abs": "https://arxiv.org/abs/2601.00812", "authors": ["Takashi Ushio", "Kazuhiro Onishi", "Hideyoshi Yanagisawa"], "title": "Free Energy-Based Modeling of Emotional Dynamics in Video Advertisements", "comment": "This article has been accepted for publication in IEEE Access and will be published shortly", "summary": "Emotional responses during advertising video viewing are recognized as essential for understanding media effects because they have influenced attention, memory, and purchase intention. To establish a methodological basis for explainable emotion estimation without relying on external information such as physiological signals or subjective ratings, we have quantified \"pleasantness,\" \"surprise,\" and \"habituation\" solely from scene-level expression features of advertising videos, drawing on the free energy(FE) principle, which has provided a unified account of perception, learning, and behavior. In this framework, Kullback-Leibler divergence (KLD) has captured prediction error, Bayesian surprise (BS) has captured belief updates, and uncertainty (UN) has reflected prior ambiguity, and together they have formed the core components of FE. Using 1,059 15 s food video advertisements, the experiments have shown that KLD has reflected \"pleasantness\" associated with brand presentation, BS has captured \"surprise\" arising from informational complexity, and UN has reflected \"surprise\" driven by uncertainty in element types and spatial arrangements, as well as by the variability and quantity of presented elements. This study also identified three characteristic emotional patterns, namely uncertain stimulus, sustained high emotion, and momentary peak and decay, demonstrating the usefulness of the proposed method. Robustness across nine hyperparameter settings and generalization tests with six types of Japanese advertising videos (three genres and two durations) confirmed that these tendencies remained stable. This work can be extended by integrating a wider range of expression elements and validating the approach through subjective ratings, ultimately guiding the development of technologies that can support the creation of more engaging advertising videos.", "AI": {"tldr": "This paper proposes a method to estimate emotional responses (pleasantness, surprise, habituation) from advertising videos using only scene-level expression features, based on the free energy principle without needing external physiological or subjective data.", "motivation": "Emotional responses during advertising viewing are crucial for understanding media effects on attention, memory, and purchase intention. Current methods often rely on external information like physiological signals or subjective ratings, which can be impractical. The authors aim to establish an explainable emotion estimation method that doesn't require such external data.", "method": "The method quantifies three emotional dimensions using the free energy principle: Kullback-Leibler divergence (KLD) captures prediction error (pleasantness), Bayesian surprise (BS) captures belief updates (surprise), and uncertainty (UN) reflects prior ambiguity (habituation). These are derived solely from scene-level expression features of advertising videos. The approach was tested on 1,059 15-second food video advertisements.", "result": "KLD reflected \"pleasantness\" associated with brand presentation, BS captured \"surprise\" from informational complexity, and UN reflected \"surprise\" driven by uncertainty in element types/spatial arrangements and variability/quantity of presented elements. Three characteristic emotional patterns were identified: uncertain stimulus, sustained high emotion, and momentary peak and decay. The method showed robustness across hyperparameter settings and generalization across different Japanese ad genres and durations.", "conclusion": "The proposed method successfully estimates emotional responses from advertising videos without external data, providing explainable emotion estimation. It demonstrates practical applications for understanding ad effectiveness and could guide the development of technologies to create more engaging advertising videos. Future work includes integrating more expression elements and validation through subjective ratings."}}
{"id": "2601.00829", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.00829", "abs": "https://arxiv.org/abs/2601.00829", "authors": ["Alexander Vinogradov"], "title": "Can Generative Models Actually Forge Realistic Identity Documents?", "comment": "11 pages, 16 figures", "summary": "Generative image models have recently shown significant progress in image realism, leading to public concerns about their potential misuse for document forgery. This paper explores whether contemporary open-source and publicly accessible diffusion-based generative models can produce identity document forgeries that could realistically bypass human or automated verification systems. We evaluate text-to-image and image-to-image generation pipelines using multiple publicly available generative model families, including Stable Diffusion, Qwen, Flux, Nano-Banana, and others. The findings indicate that while current generative models can simulate surface-level document aesthetics, they fail to reproduce structural and forensic authenticity. Consequently, the risk of generative identity document deepfakes achieving forensic-level authenticity may be overestimated, underscoring the value of collaboration between machine learning practitioners and document-forensics experts in realistic risk assessment.", "AI": {"tldr": "Current generative models can create visually convincing ID documents but fail to achieve forensic-level authenticity needed to bypass verification systems.", "motivation": "To assess whether publicly available diffusion-based generative models can produce identity document forgeries that could realistically bypass human or automated verification systems, addressing public concerns about misuse for document forgery.", "method": "Evaluated text-to-image and image-to-image generation pipelines using multiple publicly available generative model families including Stable Diffusion, Qwen, Flux, Nano-Banana, and others to test document forgery capabilities.", "result": "While current generative models can simulate surface-level document aesthetics, they fail to reproduce structural and forensic authenticity needed to bypass verification systems.", "conclusion": "The risk of generative identity document deepfakes achieving forensic-level authenticity may be overestimated, highlighting the importance of collaboration between machine learning practitioners and document-forensics experts for realistic risk assessment."}}
{"id": "2601.00837", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.00837", "abs": "https://arxiv.org/abs/2601.00837", "authors": ["Agniv Roy Choudhury"], "title": "Pediatric Pneumonia Detection from Chest X-Rays:A Comparative Study of Transfer Learning and Custom CNNs", "comment": null, "summary": "Pneumonia is a leading cause of mortality in children under five, with over 700,000 deaths annually. Accurate diagnosis from chest X-rays is limited by radiologist availability and variability.\n  Objective: This study compares custom CNNs trained from scratch with transfer learning (ResNet50, DenseNet121, EfficientNet-B0) for pediatric pneumonia detection, evaluating frozen-backbone and fine-tuning regimes.\n  Methods: A dataset of 5,216 pediatric chest X-rays was split 80/10/10 for training, validation, and testing. Seven models were trained and assessed using accuracy, F1-score, and AUC. Grad-CAM visualizations provided explainability.\n  Results: Fine-tuned ResNet50 achieved the best performance: 99.43\\% accuracy, 99.61\\% F1-score, and 99.93\\% AUC, with only 3 misclassifications. Fine-tuning outperformed frozen-backbone models by 5.5 percentage points on average. Grad-CAM confirmed clinically relevant lung regions guided predictions.\n  Conclusions: Transfer learning with fine-tuning substantially outperforms CNNs trained from scratch for pediatric pneumonia detection, showing near-perfect accuracy. This system has strong potential as a screening tool in resource-limited settings. Future work should validate these findings on multi-center and adult datasets.\n  Keywords: Pneumonia detection, deep learning, transfer learning, CNN, chest X-ray, pediatric diagnosis, ResNet, DenseNet, EfficientNet, Grad-CAM.", "AI": {"tldr": "Fine-tuned ResNet50 achieves near-perfect accuracy (99.43%) for pediatric pneumonia detection from chest X-rays, outperforming custom CNNs and frozen-backbone transfer learning approaches.", "motivation": "Pneumonia causes over 700,000 annual deaths in children under five, with diagnosis limited by radiologist availability and variability. There's a need for accurate automated screening tools, especially in resource-limited settings.", "method": "Used 5,216 pediatric chest X-rays split 80/10/10. Compared custom CNNs with transfer learning (ResNet50, DenseNet121, EfficientNet-B0) in both frozen-backbone and fine-tuning regimes. Evaluated with accuracy, F1-score, AUC, and Grad-CAM visualizations for explainability.", "result": "Fine-tuned ResNet50 achieved best performance: 99.43% accuracy, 99.61% F1-score, 99.93% AUC with only 3 misclassifications. Fine-tuning outperformed frozen-backbone models by 5.5 percentage points on average. Grad-CAM confirmed clinically relevant lung regions guided predictions.", "conclusion": "Transfer learning with fine-tuning substantially outperforms CNNs trained from scratch for pediatric pneumonia detection, showing near-perfect accuracy. The system has strong potential as a screening tool in resource-limited settings. Future validation needed on multi-center and adult datasets."}}
{"id": "2601.00839", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.00839", "abs": "https://arxiv.org/abs/2601.00839", "authors": ["Zahid Ullah", "Muhammad Hilal", "Eunsoo Lee", "Dragan Pamucar", "Jihie Kim"], "title": "Unified Review and Benchmark of Deep Segmentation Architectures for Cardiac Ultrasound on CAMUS", "comment": null, "summary": "Several review papers summarize cardiac imaging and DL advances, few works connect this overview to a unified and reproducible experimental benchmark. In this study, we combine a focused review of cardiac ultrasound segmentation literature with a controlled comparison of three influential architectures, U-Net, Attention U-Net, and TransUNet, on the Cardiac Acquisitions for Multi-Structure Ultrasound Segmentation (CAMUS) echocardiography dataset. Our benchmark spans multiple preprocessing routes, including native NIfTI volumes, 16-bit PNG exports, GPT-assisted polygon-based pseudo-labels, and self-supervised pretraining (SSL) on thousands of unlabeled cine frames. Using identical training splits, losses, and evaluation criteria, a plain U-Net achieved a 94% mean Dice when trained directly on NIfTI data (preserving native dynamic range), while the PNG-16-bit workflow reached 91% under similar conditions. Attention U-Net provided modest improvements on small or low-contrast regions, reducing boundary leakage, whereas TransUNet demonstrated the strongest generalization on challenging frames due to its ability to model global spatial context, particularly when initialized with SSL. Pseudo-labeling expanded the training set and improved robustness after confidence filtering. Overall, our contributions are threefold: a harmonized, apples-to-apples benchmark of U-Net, Attention U-Net, and TransUNet under standardized CAMUS preprocessing and evaluation; practical guidance on maintaining intensity fidelity, resolution consistency, and alignment when preparing ultrasound data; and an outlook on scalable self-supervision and emerging multimodal GPT-based annotation pipelines for rapid labeling, quality assurance, and targeted dataset curation.", "AI": {"tldr": "Benchmark study comparing U-Net, Attention U-Net, and TransUNet for cardiac ultrasound segmentation on CAMUS dataset with standardized preprocessing and evaluation protocols.", "motivation": "To address the lack of unified, reproducible experimental benchmarks in cardiac imaging and deep learning literature by providing an apples-to-apples comparison of influential architectures.", "method": "Combined literature review with controlled comparison of three architectures (U-Net, Attention U-Net, TransUNet) on CAMUS dataset using identical training splits, losses, and evaluation. Tested multiple preprocessing routes including native NIfTI volumes, 16-bit PNG exports, GPT-assisted pseudo-labels, and self-supervised pretraining on unlabeled cine frames.", "result": "Plain U-Net achieved 94% mean Dice on native NIfTI data, PNG workflow reached 91%. Attention U-Net improved small/low-contrast region segmentation and reduced boundary leakage. TransUNet showed strongest generalization on challenging frames, especially with SSL initialization. Pseudo-labeling improved robustness after confidence filtering.", "conclusion": "Provides harmonized benchmark of three architectures, practical guidance on ultrasound data preparation, and outlook on scalable self-supervision and GPT-based annotation pipelines for rapid labeling and dataset curation."}}
{"id": "2601.00854", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.00854", "abs": "https://arxiv.org/abs/2601.00854", "authors": ["Igor Lodin", "Sergii Filatov", "Vira Filatova", "Dmytro Filatov"], "title": "Motion-Compensated Latent Semantic Canvases for Visual Situational Awareness on Edge", "comment": "11 pages, 5 figures", "summary": "We propose Motion-Compensated Latent Semantic Canvases (MCLSC) for visual situational awareness on resource-constrained edge devices. The core idea is to maintain persistent semantic metadata in two latent canvases - a slowly accumulating static layer and a rapidly updating dynamic layer - defined in a baseline coordinate frame stabilized from the video stream. Expensive panoptic segmentation (Mask2Former) runs asynchronously and is motion-gated: inference is triggered only when motion indicates new information, while stabilization/motion compensation preserves a consistent coordinate system for latent semantic memory. On prerecorded 480p clips, our prototype reduces segmentation calls by >30x and lowers mean end-to-end processing time by >20x compared to naive per-frame segmentation, while maintaining coherent static/dynamic semantic overlays.", "AI": {"tldr": "MCLSC reduces computational load on edge devices by using motion-gated segmentation and persistent semantic canvases instead of per-frame processing.", "motivation": "Enable visual situational awareness on resource-constrained edge devices by reducing computational overhead of continuous panoptic segmentation while maintaining semantic understanding.", "method": "Maintains two latent semantic canvases (static and dynamic layers) in stabilized baseline coordinates. Uses motion-gated triggering of expensive Mask2Former segmentation only when motion indicates new information, with motion compensation for consistent coordinate system.", "result": "On 480p clips, reduces segmentation calls by >30x and lowers mean end-to-end processing time by >20x compared to naive per-frame segmentation, while maintaining coherent semantic overlays.", "conclusion": "MCLSC enables efficient visual situational awareness on edge devices through motion-gated segmentation and persistent semantic memory, dramatically reducing computational requirements while preserving semantic coherence."}}
{"id": "2601.00879", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.00879", "abs": "https://arxiv.org/abs/2601.00879", "authors": ["Zahid Ullah", "Jihie Kim"], "title": "VL-OrdinalFormer: Vision Language Guided Ordinal Transformers for Interpretable Knee Osteoarthritis Grading", "comment": null, "summary": "Knee osteoarthritis (KOA) is a leading cause of disability worldwide, and accurate severity assessment using the Kellgren Lawrence (KL) grading system is critical for clinical decision making. However, radiographic distinctions between early disease stages, particularly KL1 and KL2, are subtle and frequently lead to inter-observer variability among radiologists. To address these challenges, we propose VLOrdinalFormer, a vision language guided ordinal learning framework for fully automated KOA grading from knee radiographs. The proposed method combines a ViT L16 backbone with CORAL based ordinal regression and a Contrastive Language Image Pretraining (CLIP) driven semantic alignment module, allowing the model to incorporate clinically meaningful textual concepts related to joint space narrowing, osteophyte formation, and subchondral sclerosis. To improve robustness and mitigate overfitting, we employ stratified five fold cross validation, class aware re weighting to emphasize challenging intermediate grades, and test time augmentation with global threshold optimization. Experiments conducted on the publicly available OAI kneeKL224 dataset demonstrate that VLOrdinalFormer achieves state of the art performance, outperforming CNN and ViT baselines in terms of macro F1 score and overall accuracy. Notably, the proposed framework yields substantial performance gains for KL1 and KL2 without compromising classification accuracy for mild or severe cases. In addition, interpretability analyses using Grad CAM and CLIP similarity maps confirm that the model consistently attends to clinically relevant anatomical regions. These results highlight the potential of vision language aligned ordinal transformers as reliable and interpretable tools for KOA grading and disease progression assessment in routine radiological practice.", "AI": {"tldr": "VLOrdinalFormer: A vision-language ordinal transformer framework for automated knee osteoarthritis grading that combines ViT backbone with CLIP semantic alignment and ordinal regression to improve accuracy on subtle early-stage distinctions (KL1 vs KL2).", "motivation": "Knee osteoarthritis (KOA) severity assessment using KL grading is crucial but challenging due to subtle radiographic distinctions between early stages (KL1 and KL2), leading to high inter-observer variability among radiologists.", "method": "Proposes VLOrdinalFormer combining ViT-L16 backbone with CORAL-based ordinal regression and CLIP-driven semantic alignment module to incorporate clinically meaningful textual concepts (joint space narrowing, osteophyte formation, subchondral sclerosis). Uses stratified 5-fold cross-validation, class-aware reweighting, and test-time augmentation with global threshold optimization.", "result": "Achieves state-of-the-art performance on OAI kneeKL224 dataset, outperforming CNN and ViT baselines in macro F1 score and overall accuracy. Shows substantial gains for KL1 and KL2 grades without compromising accuracy for mild/severe cases. Interpretability analyses confirm attention to clinically relevant anatomical regions.", "conclusion": "Vision-language aligned ordinal transformers show potential as reliable and interpretable tools for KOA grading and disease progression assessment in routine radiological practice."}}
{"id": "2601.00887", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.00887", "abs": "https://arxiv.org/abs/2601.00887", "authors": ["Hongbo Jin", "Kuanwei Lin", "Wenhao Zhang", "Yichen Jin", "Ge Li"], "title": "VideoCuRL: Video Curriculum Reinforcement Learning with Orthogonal Difficulty Decomposition", "comment": null, "summary": "Reinforcement Learning (RL) is crucial for empowering VideoLLMs with complex spatiotemporal reasoning. However, current RL paradigms predominantly rely on random data shuffling or naive curriculum strategies based on scalar difficulty metrics. We argue that scalar metrics fail to disentangle two orthogonal challenges in video understanding: Visual Temporal Perception Load and Cognitive Reasoning Depth. To address this, we propose VideoCuRL, a novel framework that decomposes difficulty into these two axes. We employ efficient, training-free proxies, optical flow and keyframe entropy for visual complexity, Calibrated Surprisal for cognitive complexity, to map data onto a 2D curriculum grid. A competence aware Diagonal Wavefront strategy then schedules training from base alignment to complex reasoning. Furthermore, we introduce Dynamic Sparse KL and Structured Revisiting to stabilize training against reward collapse and catastrophic forgetting. Extensive experiments show that VideoCuRL surpasses strong RL baselines on reasoning (+2.5 on VSI-Bench) and perception (+2.9 on VideoMME) tasks. Notably, VideoCuRL eliminates the prohibitive inference overhead of generation-based curricula, offering a scalable solution for robust video post-training.", "AI": {"tldr": "VideoCuRL is a novel RL framework for VideoLLMs that decomposes training difficulty into visual perception load and cognitive reasoning depth axes, using efficient proxies to create a 2D curriculum grid with diagonal wavefront scheduling and stabilization techniques.", "motivation": "Current RL paradigms for VideoLLMs rely on random data shuffling or naive scalar difficulty metrics, which fail to disentangle the orthogonal challenges of visual temporal perception load and cognitive reasoning depth in video understanding.", "method": "Proposes VideoCuRL framework that: 1) decomposes difficulty into visual perception load (using optical flow & keyframe entropy) and cognitive reasoning depth (using Calibrated Surprisal), 2) maps data onto 2D curriculum grid, 3) uses competence-aware Diagonal Wavefront strategy for training scheduling, 4) implements Dynamic Sparse KL and Structured Revisiting to stabilize training.", "result": "VideoCuRL surpasses strong RL baselines on reasoning (+2.5 on VSI-Bench) and perception (+2.9 on VideoMME) tasks, while eliminating prohibitive inference overhead of generation-based curricula.", "conclusion": "VideoCuRL offers a scalable solution for robust video post-training by effectively disentangling and addressing the dual challenges of visual perception and cognitive reasoning in video understanding through a novel 2D curriculum approach."}}
{"id": "2601.00888", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.00888", "abs": "https://arxiv.org/abs/2601.00888", "authors": ["Happy Gery Pangestu", "Andi Prademon Yunus", "Siti Khomsah"], "title": "Comparative Evaluation of CNN Architectures for Neural Style Transfer in Indonesian Batik Motif Generation: A Comprehensive Study", "comment": "29 pages, 9 figures, submitted in VCIBA", "summary": "Neural Style Transfer (NST) provides a computational framework for the digital preservation and generative exploration of Indonesian batik motifs; however, existing approaches remain largely centered on VGG-based architectures whose strong stylistic expressiveness comes at the cost of high computational and memory demands, that limits practical deployment in resource-limited environments. This study presents a systematic comparative analysis of five widely used CNN backbones, namely VGG16, VGG19, Inception V3, ResNet50, and ResNet101, based on 245 controlled experiments combining quantitative metrics, qualitative assessment, and statistical analysis to examine the trade-off between structural preservation, stylistic behavior, and computational efficiency. The results show that backbone selection does not yield statistically significant differences in structural similarity, as confirmed by ANOVA on SSIM (p= 0.83), indicating comparable levels of structural preservation rather than equivalent stylistic quality. Within this context, ResNet-based architectures achieve approximately 5-6x faster convergence than VGG models while maintaining similar perceptual similarity (LPIPS = 0.53) and requiring over 16x fewer FLOPs (0.63 vs 10.12 GFLOPs). Qualitative analysis reveals consistent stylistic trade-offs, with VGG producing denser painterly textures, ResNet favoring geometric stability and canting stroke preservation with milder stylization, and Inception V3 exhibiting intermediate but noisier behavior. These findings reposition architectural choice in NST from maximizing stylistic intensity toward efficiency-aware and structure-preserving deployment, highlighting ResNet-based backbones as a practical foundation for scalable, industry-oriented batik generation.", "AI": {"tldr": "Comparative analysis shows ResNet backbones offer 5-6x faster convergence and 16x fewer FLOPs than VGG for neural style transfer of Indonesian batik, with comparable structural preservation but different stylistic trade-offs.", "motivation": "Existing VGG-based NST approaches for Indonesian batik preservation are computationally expensive, limiting practical deployment in resource-constrained environments. There's a need to evaluate alternative CNN backbones for better efficiency while maintaining quality.", "method": "Systematic comparative analysis of five CNN backbones (VGG16, VGG19, Inception V3, ResNet50, ResNet101) using 245 controlled experiments with quantitative metrics (SSIM, LPIPS), qualitative assessment, and statistical analysis (ANOVA) to evaluate structural preservation, stylistic behavior, and computational efficiency.", "result": "Backbone selection doesn't significantly affect structural similarity (ANOVA p=0.83). ResNet architectures achieve 5-6x faster convergence than VGG, require 16x fewer FLOPs (0.63 vs 10.12 GFLOPs), and maintain similar perceptual similarity (LPIPS=0.53). VGG produces denser painterly textures, ResNet favors geometric stability and stroke preservation with milder stylization, and Inception V3 shows intermediate but noisier behavior.", "conclusion": "Architectural choice in NST should shift from maximizing stylistic intensity toward efficiency-aware and structure-preserving deployment. ResNet-based backbones provide a practical foundation for scalable, industry-oriented batik generation due to their computational efficiency while maintaining structural preservation."}}
{"id": "2601.00897", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.00897", "abs": "https://arxiv.org/abs/2601.00897", "authors": ["Sai Teja Erukude", "Jane Mascarenhas", "Lior Shamir"], "title": "CornViT: A Multi-Stage Convolutional Vision Transformer Framework for Hierarchical Corn Kernel Analysis", "comment": "23 pages", "summary": "Accurate grading of corn kernels is critical for seed certification, directional seeding, and breeding, yet it is still predominantly performed by manual inspection. This work introduces CornViT, a three-stage Convolutional Vision Transformer (CvT) framework that emulates the hierarchical reasoning of human seed analysts for single-kernel evaluation. Three sequential CvT-13 classifiers operate on 384x384 RGB images: Stage 1 distinguishes pure from impure kernels; Stage 2 categorizes pure kernels into flat and round morphologies; and Stage 3 determines the embryo orientation (up vs. down) for pure, flat kernels. Starting from a public corn seed image collection, we manually relabeled and filtered images to construct three stage-specific datasets: 7265 kernels for purity, 3859 pure kernels for morphology, and 1960 pure-flat kernels for embryo orientation, all released as benchmarks. Head-only fine-tuning of ImageNet-22k pretrained CvT-13 backbones yields test accuracies of 93.76% for purity, 94.11% for shape, and 91.12% for embryo-orientation detection. Under identical training conditions, ResNet-50 reaches only 76.56 to 81.02 percent, whereas DenseNet-121 attains 86.56 to 89.38 percent accuracy. These results highlight the advantages of convolution-augmented self-attention for kernel analysis. To facilitate adoption, we deploy CornViT in a Flask-based web application that performs stage-wise inference and exposes interpretable outputs through a browser interface. Together, the CornViT framework, curated datasets, and web application provide a deployable solution for automated corn kernel quality assessment in seed quality workflows. Source code and data are publicly available.", "AI": {"tldr": "CornViT: A three-stage Convolutional Vision Transformer framework for automated corn kernel quality assessment, achieving high accuracy in purity, morphology, and embryo orientation classification.", "motivation": "Corn kernel grading is critical for seed certification, directional seeding, and breeding, but still relies on manual inspection which is time-consuming and subjective. There's a need for automated, accurate solutions.", "method": "Three-stage hierarchical CvT framework: Stage 1 (purity classification), Stage 2 (morphology classification for pure kernels), Stage 3 (embryo orientation for pure-flat kernels). Uses CvT-13 classifiers on 384x384 RGB images with head-only fine-tuning from ImageNet-22k pretrained backbones.", "result": "Achieved test accuracies: 93.76% for purity, 94.11% for shape, 91.12% for embryo orientation. Outperformed ResNet-50 (76.56-81.02%) and DenseNet-121 (86.56-89.38%). Created three curated datasets and deployed as Flask web application.", "conclusion": "CornViT demonstrates superior performance for corn kernel analysis, highlighting advantages of convolution-augmented self-attention. Provides complete solution with framework, datasets, and web application for automated quality assessment in seed workflows."}}
{"id": "2601.00905", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.00905", "abs": "https://arxiv.org/abs/2601.00905", "authors": ["Eliot Park", "Abhi Kumar", "Pranav Rajpurkar"], "title": "Evaluating Contextual Intelligence in Recyclability: A Comprehensive Study of Image-Based Reasoning Systems", "comment": "x", "summary": "While the importance of efficient recycling is widely acknowledged, accurately determining the recyclability of items and their proper disposal remains a complex task for the general public. In this study, we explore the application of cutting-edge vision-language models (GPT-4o, GPT-4o-mini, and Claude 3.5) for predicting the recyclability of commonly disposed items. Utilizing a curated dataset of images, we evaluated the models' ability to match objects to appropriate recycling bins, including assessing whether the items could physically fit into the available bins. Additionally, we investigated the models' performance across several challenging scenarios: (i) adjusting predictions based on location-specific recycling guidelines; (ii) accounting for contamination or structural damage; and (iii) handling objects composed of multiple materials. Our findings highlight the significant advancements in contextual understanding offered by these models compared to previous iterations, while also identifying areas where they still fall short. The continued refinement of context-aware models is crucial for enhancing public recycling practices and advancing environmental sustainability.", "AI": {"tldr": "Vision-language models (GPT-4o, GPT-4o-mini, Claude 3.5) show improved contextual understanding for recycling classification but still have limitations in handling complex scenarios like location-specific rules, contamination, and multi-material objects.", "motivation": "Accurate recycling classification remains challenging for the public despite its environmental importance, creating a need for automated solutions using advanced AI models.", "method": "Evaluated GPT-4o, GPT-4o-mini, and Claude 3.5 vision-language models on a curated image dataset for recyclability prediction, including bin matching and physical fit assessment. Tested three challenging scenarios: location-specific guidelines, contamination/damage handling, and multi-material object classification.", "result": "Models demonstrated significant improvements in contextual understanding compared to previous versions, but still showed limitations in handling complex real-world recycling scenarios like location-specific rules, contaminated items, and multi-material objects.", "conclusion": "Continued refinement of context-aware vision-language models is essential for improving public recycling practices and advancing environmental sustainability, though current models still need further development to handle complex recycling scenarios."}}
{"id": "2601.00913", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.00913", "abs": "https://arxiv.org/abs/2601.00913", "authors": ["Subhankar Mishra"], "title": "Clean-GS: Semantic Mask-Guided Pruning for 3D Gaussian Splatting", "comment": null, "summary": "3D Gaussian Splatting produces high-quality scene reconstructions but generates hundreds of thousands of spurious Gaussians (floaters) scattered throughout the environment. These artifacts obscure objects of interest and inflate model sizes, hindering deployment in bandwidth-constrained applications. We present Clean-GS, a method for removing background clutter and floaters from 3DGS reconstructions using sparse semantic masks. Our approach combines whitelist-based spatial filtering with color-guided validation and outlier removal to achieve 60-80\\% model compression while preserving object quality. Unlike existing 3DGS pruning methods that rely on global importance metrics, Clean-GS uses semantic information from as few as 3 segmentation masks (1\\% of views) to identify and remove Gaussians not belonging to the target object. Our multi-stage approach consisting of (1) whitelist filtering via projection to masked regions, (2) depth-buffered color validation, and (3) neighbor-based outlier removal isolates monuments and objects from complex outdoor scenes. Experiments on Tanks and Temples show that Clean-GS reduces file sizes from 125MB to 47MB while maintaining rendering quality, making 3DGS models practical for web deployment and AR/VR applications. Our code is available at https://github.com/smlab-niser/clean-gs", "AI": {"tldr": "Clean-GS removes spurious Gaussians (floaters) from 3D Gaussian Splatting reconstructions using sparse semantic masks, achieving 60-80% model compression while preserving object quality.", "motivation": "3D Gaussian Splatting produces high-quality reconstructions but generates many spurious Gaussians (floaters) that obscure objects and inflate model sizes, hindering deployment in bandwidth-constrained applications like web and AR/VR.", "method": "Multi-stage approach: (1) whitelist filtering via projection to masked regions, (2) depth-buffered color validation, and (3) neighbor-based outlier removal. Uses semantic information from as few as 3 segmentation masks (1% of views) to identify and remove Gaussians not belonging to target objects.", "result": "Achieves 60-80% model compression, reducing file sizes from 125MB to 47MB on Tanks and Temples dataset while maintaining rendering quality. Enables practical deployment for web and AR/VR applications.", "conclusion": "Clean-GS effectively removes background clutter and floaters from 3DGS reconstructions using sparse semantic masks, offering significant compression while preserving object quality, making 3DGS models practical for real-world applications."}}
{"id": "2601.00918", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.00918", "abs": "https://arxiv.org/abs/2601.00918", "authors": ["Faisal Ahmed"], "title": "Four-Stage Alzheimer's Disease Classification from MRI Using Topological Feature Extraction, Feature Selection, and Ensemble Learning", "comment": "15 pages, 7 figures", "summary": "Accurate and efficient classification of Alzheimer's disease (AD) severity from brain magnetic resonance imaging (MRI) remains a critical challenge, particularly when limited data and model interpretability are of concern. In this work, we propose TDA-Alz, a novel framework for four-stage Alzheimer's disease severity classification (non-demented, moderate dementia, mild, and very mild) using topological data analysis (TDA) and ensemble learning. Instead of relying on deep convolutional architectures or extensive data augmentation, our approach extracts topological descriptors that capture intrinsic structural patterns of brain MRI, followed by feature selection to retain the most discriminative topological features. These features are then classified using an ensemble learning strategy to achieve robust multiclass discrimination.\n  Experiments conducted on the OASIS-1 MRI dataset demonstrate that the proposed method achieves an accuracy of 98.19% and an AUC of 99.75%, outperforming or matching state-of-the-art deep learning--based methods reported on OASIS and OASIS-derived datasets. Notably, the proposed framework does not require data augmentation, pretrained networks, or large-scale computational resources, making it computationally efficient and fast compared to deep neural network approaches. Furthermore, the use of topological descriptors provides greater interpretability, as the extracted features are directly linked to the underlying structural characteristics of brain MRI rather than opaque latent representations. These results indicate that TDA-Alz offers a powerful, lightweight, and interpretable alternative to deep learning models for MRI-based Alzheimer's disease severity classification, with strong potential for real-world clinical decision-support systems.", "AI": {"tldr": "TDA-Alz: A lightweight, interpretable framework using topological data analysis and ensemble learning for four-stage Alzheimer's disease severity classification from brain MRI, achieving 98.19% accuracy without data augmentation or large computational resources.", "motivation": "Current Alzheimer's disease classification methods face challenges with limited data, lack of interpretability, and computational inefficiency. Deep learning approaches often require extensive data augmentation, pretrained networks, and large computational resources while providing opaque latent representations.", "method": "Proposes TDA-Alz framework using topological data analysis (TDA) to extract topological descriptors capturing intrinsic structural patterns from brain MRI. Performs feature selection to retain discriminative topological features, then uses ensemble learning for robust multiclass classification of four AD severity stages.", "result": "Achieved 98.19% accuracy and 99.75% AUC on OASIS-1 MRI dataset, outperforming or matching state-of-the-art deep learning methods. The framework requires no data augmentation, pretrained networks, or large computational resources, making it computationally efficient and fast.", "conclusion": "TDA-Alz offers a powerful, lightweight, and interpretable alternative to deep learning models for MRI-based Alzheimer's disease severity classification. The topological descriptors provide greater interpretability by linking features directly to structural characteristics, with strong potential for clinical decision-support systems."}}
{"id": "2601.00925", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.00925", "abs": "https://arxiv.org/abs/2601.00925", "authors": ["I-Hsien Ting", "Yi-Jun Tseng", "Yu-Sheng Lin"], "title": "Application of deep learning techniques in non-contrast computed tomography pulmonary angiogram for pulmonary embolism diagnosis", "comment": null, "summary": "Pulmonary embolism is a life-threatening disease, early detection and treatment can significantly reduce mortality. In recent years, many studies have been using deep learning in the diagnosis of pulmonary embolism with contrast medium computed tomography pulmonary angiography, but the contrast medium is likely to cause acute kidney injury in patients with pulmonary embolism and chronic kidney disease, and the contrast medium takes time to work, patients with acute pulmonary embolism may miss the golden treatment time.\n  This study aims to use deep learning techniques to automatically classify pulmonary embolism in CT images without contrast medium by using a 3D convolutional neural network model. The deep learning model used in this study had a significant impact on the pulmonary embolism classification of computed tomography images without contrast with 85\\% accuracy and 0.84 AUC, which confirms the feasibility of the model in the diagnosis of pulmonary embolism.", "AI": {"tldr": "Deep learning model achieves 85% accuracy in classifying pulmonary embolism from non-contrast CT images, offering potential to avoid contrast-related risks and delays.", "motivation": "Contrast-enhanced CT for pulmonary embolism diagnosis carries risks of acute kidney injury in CKD patients and delays treatment due to contrast medium preparation time, potentially missing the golden treatment window for acute cases.", "method": "Used a 3D convolutional neural network model to automatically classify pulmonary embolism in CT images without contrast medium.", "result": "The model achieved 85% accuracy and 0.84 AUC in classifying pulmonary embolism from non-contrast CT images, demonstrating significant diagnostic capability.", "conclusion": "The study confirms the feasibility of using deep learning models for pulmonary embolism diagnosis without contrast medium, potentially reducing risks and treatment delays associated with contrast-enhanced imaging."}}
{"id": "2601.00928", "categories": ["cs.CV", "cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2601.00928", "abs": "https://arxiv.org/abs/2601.00928", "authors": ["Luis Yoichi Morales", "Francesco Zanlungo", "David M. Woollard"], "title": "Analyzing the Shopping Journey: Computing Shelf Browsing Visits in a Physical Retail Store", "comment": null, "summary": "Motivated by recent challenges in the deployment of robots into customer-facing roles within retail, this work introduces a study of customer activity in physical stores as a step toward autonomous understanding of shopper intent. We introduce an algorithm that computes shoppers' ``shelf visits'' -- capturing their browsing behavior in the store. Shelf visits are extracted from trajectories obtained via machine vision-based 3D tracking and overhead cameras. We perform two independent calibrations of the shelf visit algorithm, using distinct sets of trajectories (consisting of 8138 and 15129 trajectories), collected in different stores and labeled by human reviewers. The calibrated models are then evaluated on trajectories held out of the calibration process both from the same store on which calibration was performed and from the other store. An analysis of the results shows that the algorithm can recognize customers' browsing activity when evaluated in an environment different from the one on which calibration was performed. We then use the model to analyze the customers' ``browsing patterns'' on a large set of trajectories and their relation to actual purchases in the stores. Finally, we discuss how shelf browsing information could be used for retail planning and in the domain of human-robot interaction scenarios.", "AI": {"tldr": "Algorithm for detecting shopper browsing behavior (\"shelf visits\") from 3D tracking data, validated across different retail stores, and applied to analyze browsing-purchase relationships.", "motivation": "To enable autonomous understanding of shopper intent in retail environments, particularly for customer-facing robot deployment, by analyzing customer activity in physical stores.", "method": "Developed an algorithm that computes \"shelf visits\" from trajectories obtained via machine vision-based 3D tracking and overhead cameras. Performed two independent calibrations using distinct trajectory datasets (8138 and 15129 trajectories) from different stores, labeled by human reviewers.", "result": "The algorithm successfully recognizes customers' browsing activity when evaluated in environments different from calibration stores. The model was used to analyze browsing patterns on large trajectory datasets and their relation to actual purchases.", "conclusion": "Shelf browsing information has practical applications for retail planning and human-robot interaction scenarios, demonstrating cross-store generalization capability for shopper intent understanding."}}
{"id": "2601.00939", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.00939", "abs": "https://arxiv.org/abs/2601.00939", "authors": ["Feng Luo", "Hongbo Pan", "Xiang Yang", "Baoyu Jiang", "Fengqing Liu", "Tao Huang"], "title": "ShadowGS: Shadow-Aware 3D Gaussian Splatting for Satellite Imagery", "comment": null, "summary": "3D Gaussian Splatting (3DGS) has emerged as a novel paradigm for 3D reconstruction from satellite imagery. However, in multi-temporal satellite images, prevalent shadows exhibit significant inconsistencies due to varying illumination conditions. To address this, we propose ShadowGS, a novel framework based on 3DGS. It leverages a physics-based rendering equation from remote sensing, combined with an efficient ray marching technique, to precisely model geometrically consistent shadows while maintaining efficient rendering. Additionally, it effectively disentangles different illumination components and apparent attributes in the scene. Furthermore, we introduce a shadow consistency constraint that significantly enhances the geometric accuracy of 3D reconstruction. We also incorporate a novel shadow map prior to improve performance with sparse-view inputs. Extensive experiments demonstrate that ShadowGS outperforms current state-of-the-art methods in shadow decoupling accuracy, 3D reconstruction precision, and novel view synthesis quality, with only a few minutes of training. ShadowGS exhibits robust performance across various settings, including RGB, pansharpened, and sparse-view satellite inputs.", "AI": {"tldr": "ShadowGS is a 3DGS-based framework that addresses inconsistent shadows in multi-temporal satellite imagery using physics-based rendering and shadow consistency constraints for improved 3D reconstruction.", "motivation": "Multi-temporal satellite images have inconsistent shadows due to varying illumination conditions, which degrades 3D reconstruction quality in current 3D Gaussian Splatting methods.", "method": "Uses physics-based rendering equation from remote sensing with efficient ray marching to model geometrically consistent shadows. Includes shadow consistency constraint for geometric accuracy and shadow map prior for sparse-view inputs.", "result": "Outperforms state-of-the-art methods in shadow decoupling accuracy, 3D reconstruction precision, and novel view synthesis quality with only minutes of training. Works robustly with RGB, pansharpened, and sparse-view satellite inputs.", "conclusion": "ShadowGS effectively addresses shadow inconsistencies in satellite 3D reconstruction, achieving superior performance in multiple metrics while maintaining efficient training and rendering."}}
{"id": "2601.00940", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.00940", "abs": "https://arxiv.org/abs/2601.00940", "authors": ["Jonas Li", "Michelle Li", "Luke Liu", "Heng Fan"], "title": "Learning to Segment Liquids in Real-world Images", "comment": "9 pages, 7 figures", "summary": "Different types of liquids such as water, wine and medicine appear in all aspects of daily life. However, limited attention has been given to the task, hindering the ability of robots to avoid or interact with liquids safely. The segmentation of liquids is difficult because liquids come in diverse appearances and shapes; moreover, they can be both transparent or reflective, taking on arbitrary objects and scenes from the background or surroundings. To take on this challenge, we construct a large-scale dataset of liquids named LQDS consisting of 5000 real-world images annotated into 14 distinct classes, and design a novel liquid detection model named LQDM, which leverages cross-attention between a dedicated boundary branch and the main segmentation branch to enhance segmentation predictions. Extensive experiments demonstrate the effectiveness of LQDM on the test set of LQDS, outperforming state-of-the-art methods and establishing a strong baseline for the semantic segmentation of liquids.", "AI": {"tldr": "The paper introduces LQDS, a large-scale liquid segmentation dataset with 5000 real-world images across 14 classes, and proposes LQDM, a novel detection model using cross-attention between boundary and segmentation branches to improve liquid segmentation performance.", "motivation": "Liquids (water, wine, medicine) are ubiquitous in daily life but receive limited attention in robotics, hindering robots' ability to safely avoid or interact with liquids. Liquid segmentation is challenging due to diverse appearances, shapes, transparency, reflectivity, and background adaptation.", "method": "Constructed LQDS dataset with 5000 real-world images annotated into 14 distinct liquid classes. Designed LQDM detection model that leverages cross-attention between a dedicated boundary branch and the main segmentation branch to enhance segmentation predictions.", "result": "Extensive experiments demonstrate LQDM's effectiveness on the LQDS test set, outperforming state-of-the-art methods and establishing a strong baseline for semantic segmentation of liquids.", "conclusion": "The paper addresses the important but neglected problem of liquid segmentation by providing both a comprehensive dataset and an effective model, advancing robotic perception capabilities for safe liquid interaction."}}
{"id": "2601.00943", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.00943", "abs": "https://arxiv.org/abs/2601.00943", "authors": ["Megha Mariam K. M", "Aditya Arun", "Zakaria Laskar", "C. V. Jawahar"], "title": "PhyEduVideo: A Benchmark for Evaluating Text-to-Video Models for Physics Education", "comment": "Accepted at IEEE/CVF Winter Conference on Applications of Computer Vision (WACV) 2026", "summary": "Generative AI models, particularly Text-to-Video (T2V) systems, offer a promising avenue for transforming science education by automating the creation of engaging and intuitive visual explanations. In this work, we take a first step toward evaluating their potential in physics education by introducing a dedicated benchmark for explanatory video generation. The benchmark is designed to assess how well T2V models can convey core physics concepts through visual illustrations. Each physics concept in our benchmark is decomposed into granular teaching points, with each point accompanied by a carefully crafted prompt intended for visual explanation of the teaching point. T2V models are evaluated on their ability to generate accurate videos in response to these prompts. Our aim is to systematically explore the feasibility of using T2V models to generate high-quality, curriculum-aligned educational content-paving the way toward scalable, accessible, and personalized learning experiences powered by AI. Our evaluation reveals that current models produce visually coherent videos with smooth motion and minimal flickering, yet their conceptual accuracy is less reliable. Performance in areas such as mechanics, fluids, and optics is encouraging, but models struggle with electromagnetism and thermodynamics, where abstract interactions are harder to depict. These findings underscore the gap between visual quality and conceptual correctness in educational video generation. We hope this benchmark helps the community close that gap and move toward T2V systems that can deliver accurate, curriculum-aligned physics content at scale. The benchmark and accompanying codebase are publicly available at https://github.com/meghamariamkm/PhyEduVideo.", "AI": {"tldr": "Researchers introduce a benchmark to evaluate Text-to-Video (T2V) models for generating physics educational videos, finding current models produce visually coherent content but struggle with conceptual accuracy, especially for abstract topics.", "motivation": "To systematically evaluate the potential of generative AI Text-to-Video systems for transforming science education by automating the creation of engaging visual explanations, particularly for physics concepts.", "method": "Created a dedicated benchmark for physics education video generation that decomposes core physics concepts into granular teaching points, each with carefully crafted prompts for visual explanation. T2V models are evaluated on their ability to generate accurate videos in response to these prompts.", "result": "Current T2V models produce visually coherent videos with smooth motion and minimal flickering, but conceptual accuracy is unreliable. Performance is encouraging for mechanics, fluids, and optics, but models struggle with electromagnetism and thermodynamics where abstract interactions are harder to depict.", "conclusion": "There's a significant gap between visual quality and conceptual correctness in educational video generation. The benchmark aims to help the community close this gap and develop T2V systems that can deliver accurate, curriculum-aligned physics content at scale for personalized learning experiences."}}
{"id": "2601.00963", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.00963", "abs": "https://arxiv.org/abs/2601.00963", "authors": ["Bishwajit Saha", "Dmitry Krotov", "Mohammed J. Zaki", "Parikshit Ram"], "title": "Deep Clustering with Associative Memories", "comment": null, "summary": "Deep clustering - joint representation learning and latent space clustering - is a well studied problem especially in computer vision and text processing under the deep learning framework. While the representation learning is generally differentiable, clustering is an inherently discrete optimization task, requiring various approximations and regularizations to fit in a standard differentiable pipeline. This leads to a somewhat disjointed representation learning and clustering. In this work, we propose a novel loss function utilizing energy-based dynamics via Associative Memories to formulate a new deep clustering method, DCAM, which ties together the representation learning and clustering aspects more intricately in a single objective. Our experiments showcase the advantage of DCAM, producing improved clustering quality for various architecture choices (convolutional, residual or fully-connected) and data modalities (images or text).", "AI": {"tldr": "DCAM is a deep clustering method that uses energy-based dynamics via Associative Memories to better integrate representation learning and clustering in a single objective, improving clustering quality across architectures and data types.", "motivation": "Current deep clustering methods have a disjointed approach where differentiable representation learning is combined with discrete clustering optimization through approximations, leading to suboptimal integration between the two components.", "method": "Proposes DCAM (Deep Clustering with Associative Memories) using a novel loss function based on energy-based dynamics via Associative Memories to formulate a single objective that intricately ties representation learning and clustering together.", "result": "DCAM produces improved clustering quality across various architecture choices (convolutional, residual, fully-connected) and data modalities (images and text).", "conclusion": "The proposed DCAM method successfully integrates representation learning and clustering more effectively through energy-based dynamics, offering a superior approach to deep clustering that works well across different architectures and data types."}}
{"id": "2601.00964", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.00964", "abs": "https://arxiv.org/abs/2601.00964", "authors": ["Md. Maksudul Haque", "Rahnuma Akter", "A S M Ahsanul Sarkar Akib", "Abdul Hasib"], "title": "A Deep Learning Approach for Automated Skin Lesion Diagnosis with Explainable AI", "comment": null, "summary": "Skin cancer is also one of the most common and dangerous types of cancer in the world that requires timely and precise diagnosis. In this paper, a deep-learning architecture of the multi-class skin lesion classification on the HAM10000 dataset will be described. The system suggested combines high-quality data balancing methods, large-scale data augmentation, hybridized EfficientNetV2-L framework with channel attention, and a three-stage progressive learning approach. Moreover, we also use explainable AI (XAI) techniques such as Grad-CAM and saliency maps to come up with intelligible visual representations of model predictions. Our strategy is with a total accuracy of 91.15 per cent, macro F1 of 85.45\\% and micro-average AUC of 99.33\\%. The model has shown high performance in all the seven lesion classes with specific high performance of melanoma and melanocytic nevi. In addition to enhancing diagnostic transparency, XAI also helps to find out the visual characteristics that cause the classifications, which enhances clinical trustworthiness.", "AI": {"tldr": "Deep learning system for multi-class skin lesion classification achieves 91.15% accuracy on HAM10000 dataset using EfficientNetV2-L with attention mechanisms, data balancing, augmentation, and explainable AI techniques.", "motivation": "Skin cancer requires timely and precise diagnosis, but current methods may lack accuracy and transparency. The paper aims to develop a reliable deep learning system for skin lesion classification that is both accurate and clinically interpretable.", "method": "Combines data balancing methods, large-scale data augmentation, hybridized EfficientNetV2-L framework with channel attention, and a three-stage progressive learning approach. Uses explainable AI techniques (Grad-CAM and saliency maps) for visual interpretability.", "result": "Achieved 91.15% total accuracy, 85.45% macro F1 score, and 99.33% micro-average AUC. Showed high performance across all seven lesion classes, with particularly strong results for melanoma and melanocytic nevi.", "conclusion": "The proposed deep learning system provides accurate skin lesion classification while enhancing diagnostic transparency through explainable AI, which helps identify visual characteristics driving classifications and improves clinical trustworthiness."}}
{"id": "2601.00988", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.00988", "abs": "https://arxiv.org/abs/2601.00988", "authors": ["Lin Xi", "Yingliang Ma", "Xiahai Zhuang"], "title": "Few-Shot Video Object Segmentation in X-Ray Angiography Using Local Matching and Spatio-Temporal Consistency Loss", "comment": null, "summary": "We introduce a novel FSVOS model that employs a local matching strategy to restrict the search space to the most relevant neighboring pixels. Rather than relying on inefficient standard im2col-like implementations (e.g., spatial convolutions, depthwise convolutions and feature-shifting mechanisms) or hardware-specific CUDA kernels (e.g., deformable and neighborhood attention), which often suffer from limited portability across non-CUDA devices, we reorganize the local sampling process through a direction-based sampling perspective. Specifically, we implement a non-parametric sampling mechanism that enables dynamically varying sampling regions. This approach provides the flexibility to adapt to diverse spatial structures without the computational costs of parametric layers and the need for model retraining. To further enhance feature coherence across frames, we design a supervised spatio-temporal contrastive learning scheme that enforces consistency in feature representations. In addition, we introduce a publicly available benchmark dataset for multi-object segmentation in X-ray angiography videos (MOSXAV), featuring detailed, manually labeled segmentation ground truth. Extensive experiments on the CADICA, XACV, and MOSXAV datasets show that our proposed FSVOS method outperforms current state-of-the-art video segmentation methods in terms of segmentation accuracy and generalization capability (i.e., seen and unseen categories). This work offers enhanced flexibility and potential for a wide range of clinical applications.", "AI": {"tldr": "A novel FSVOS model using local matching with direction-based sampling and spatio-temporal contrastive learning, plus a new X-ray angiography dataset, achieving SOTA performance.", "motivation": "To address limitations of existing methods: inefficient standard implementations (spatial/depthwise convolutions, feature-shifting) and hardware-specific CUDA kernels (deformable/neighborhood attention) that lack portability across non-CUDA devices. Also to improve feature coherence across frames in video segmentation.", "method": "1) Local matching strategy restricting search space to relevant neighboring pixels; 2) Direction-based sampling perspective reorganizing local sampling process; 3) Non-parametric sampling mechanism enabling dynamically varying sampling regions; 4) Supervised spatio-temporal contrastive learning scheme for feature consistency across frames; 5) Introduction of MOSXAV dataset for multi-object segmentation in X-ray angiography videos.", "result": "Extensive experiments on CADICA, XACV, and MOSXAV datasets show the proposed FSVOS method outperforms current state-of-the-art video segmentation methods in segmentation accuracy and generalization capability (both seen and unseen categories).", "conclusion": "The approach offers enhanced flexibility without computational costs of parametric layers or need for retraining, provides better portability across devices, and has potential for wide range of clinical applications in medical video segmentation."}}
{"id": "2601.00991", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.00991", "abs": "https://arxiv.org/abs/2601.00991", "authors": ["Joshua Kawaguchi", "Saad Manzur", "Emily Gao Wang", "Maitreyi Sinha", "Bryan Vela", "Yunxi Wang", "Brandon Vela", "Wayne B. Hayes"], "title": "UnrealPose: Leveraging Game Engine Kinematics for Large-Scale Synthetic Human Pose Data", "comment": "CVPR 2026 submission. Introduces UnrealPose-1M dataset and UnrealPose-Gen pipeline", "summary": "Diverse, accurately labeled 3D human pose data is expensive and studio-bound, while in-the-wild datasets lack known ground truth. We introduce UnrealPose-Gen, an Unreal Engine 5 pipeline built on Movie Render Queue for high-quality offline rendering. Our generated frames include: (i) 3D joints in world and camera coordinates, (ii) 2D projections and COCO-style keypoints with occlusion and joint-visibility flags, (iii) person bounding boxes, and (iv) camera intrinsics and extrinsics. We use UnrealPose-Gen to present UnrealPose-1M, an approximately one million frame corpus comprising eight sequences: five scripted \"coherent\" sequences spanning five scenes, approximately 40 actions, and five subjects; and three randomized sequences across three scenes, approximately 100 actions, and five subjects, all captured from diverse camera trajectories for broad viewpoint coverage. As a fidelity check, we report real-to-synthetic results on four tasks: image-to-3D pose, 2D keypoint detection, 2D-to-3D lifting, and person detection/segmentation. Though time and resources constrain us from an unlimited dataset, we release the UnrealPose-1M dataset, as well as the UnrealPose-Gen pipeline to support third-party generation of human pose data.", "AI": {"tldr": "UnrealPose-Gen is an Unreal Engine 5 pipeline for generating synthetic human pose data with comprehensive annotations, used to create the 1M-frame UnrealPose-1M dataset for training pose estimation models.", "motivation": "Real-world 3D human pose datasets are expensive to create and lack ground truth annotations, while synthetic data can provide diverse, accurately labeled training data for pose estimation tasks.", "method": "Developed UnrealPose-Gen pipeline using Unreal Engine 5 and Movie Render Queue for offline rendering, generating frames with 3D joints, 2D projections, COCO-style keypoints, bounding boxes, and camera parameters.", "result": "Created UnrealPose-1M dataset with ~1M frames from 8 sequences (5 coherent, 3 randomized) across multiple scenes, actions, and subjects, with validation showing real-to-synthetic performance on 4 pose tasks.", "conclusion": "The released UnrealPose-Gen pipeline and UnrealPose-1M dataset provide valuable synthetic resources for human pose estimation research, addressing data scarcity and annotation challenges in the field."}}
{"id": "2601.00993", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.00993", "abs": "https://arxiv.org/abs/2601.00993", "authors": ["Julian D. Santamaria", "Claudia Isaza", "Jhony H. Giraldo"], "title": "WildIng: A Wildlife Image Invariant Representation Model for Geographical Domain Shift", "comment": null, "summary": "Wildlife monitoring is crucial for studying biodiversity loss and climate change. Camera trap images provide a non-intrusive method for analyzing animal populations and identifying ecological patterns over time. However, manual analysis is time-consuming and resource-intensive. Deep learning, particularly foundation models, has been applied to automate wildlife identification, achieving strong performance when tested on data from the same geographical locations as their training sets. Yet, despite their promise, these models struggle to generalize to new geographical areas, leading to significant performance drops. For example, training an advanced vision-language model, such as CLIP with an adapter, on an African dataset achieves an accuracy of 84.77%. However, this performance drops significantly to 16.17% when the model is tested on an American dataset. This limitation partly arises because existing models rely predominantly on image-based representations, making them sensitive to geographical data distribution shifts, such as variation in background, lighting, and environmental conditions. To address this, we introduce WildIng, a Wildlife image Invariant representation model for geographical domain shift. WildIng integrates text descriptions with image features, creating a more robust representation to geographical domain shifts. By leveraging textual descriptions, our approach captures consistent semantic information, such as detailed descriptions of the appearance of the species, improving generalization across different geographical locations. Experiments show that WildIng enhances the accuracy of foundation models such as BioCLIP by 30% under geographical domain shift conditions. We evaluate WildIng on two datasets collected from different regions, namely America and Africa. The code and models are publicly available at https://github.com/Julian075/CATALOG/tree/WildIng.", "AI": {"tldr": "WildIng improves wildlife image classification across geographical domains by integrating text descriptions with image features, addressing domain shift issues that cause performance drops in existing models.", "motivation": "Current wildlife monitoring using deep learning models suffers from poor generalization to new geographical areas due to sensitivity to environmental variations like background, lighting, and conditions. Models trained on one region perform poorly when tested on another region.", "method": "WildIng integrates text descriptions with image features to create invariant representations for geographical domain shift. It leverages textual descriptions of species appearance to capture consistent semantic information across different locations.", "result": "WildIng enhances foundation model accuracy by 30% under geographical domain shift conditions. It addresses the performance drop from 84.77% to 16.17% when models trained on African data are tested on American data.", "conclusion": "Integrating textual descriptions with image features creates more robust representations that generalize better across geographical domains, significantly improving wildlife monitoring automation in diverse environments."}}
{"id": "2601.00998", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.00998", "abs": "https://arxiv.org/abs/2601.00998", "authors": ["Yue Zhou", "Jue Chen", "Zilun Zhang", "Penghui Huang", "Ran Ding", "Zhentao Zou", "PengFei Gao", "Yuchen Wei", "Ke Li", "Xue Yang", "Xue Jiang", "Hongxin Yang", "Jonathan Li"], "title": "DVGBench: Implicit-to-Explicit Visual Grounding Benchmark in UAV Imagery with Large Vision-Language Models", "comment": "20 pages, 17 figures", "summary": "Remote sensing (RS) large vision-language models (LVLMs) have shown strong promise across visual grounding (VG) tasks. However, existing RS VG datasets predominantly rely on explicit referring expressions-such as relative position, relative size, and color cues-thereby constraining performance on implicit VG tasks that require scenario-specific domain knowledge. This article introduces DVGBench, a high-quality implicit VG benchmark for drones, covering six major application scenarios: traffic, disaster, security, sport, social activity, and productive activity. Each object provides both explicit and implicit queries. Based on the dataset, we design DroneVG-R1, an LVLM that integrates the novel Implicit-to-Explicit Chain-of-Thought (I2E-CoT) within a reinforcement learning paradigm. This enables the model to take advantage of scene-specific expertise, converting implicit references into explicit ones and thus reducing grounding difficulty. Finally, an evaluation of mainstream models on both explicit and implicit VG tasks reveals substantial limitations in their reasoning capabilities. These findings provide actionable insights for advancing the reasoning capacity of LVLMs for drone-based agents. The code and datasets will be released at https://github.com/zytx121/DVGBench", "AI": {"tldr": "DVGBench is a new implicit visual grounding benchmark for drone vision, with DroneVG-R1 model using Implicit-to-Explicit Chain-of-Thought to convert implicit references to explicit ones via reinforcement learning.", "motivation": "Existing remote sensing visual grounding datasets rely too heavily on explicit referring expressions (position, size, color), limiting performance on implicit tasks requiring domain knowledge about drone application scenarios.", "method": "Created DVGBench dataset covering six drone scenarios with both explicit and implicit queries. Developed DroneVG-R1 model integrating Implicit-to-Explicit Chain-of-Thought (I2E-CoT) within reinforcement learning to convert implicit references to explicit ones.", "result": "Evaluation shows mainstream models have substantial limitations in reasoning capabilities for implicit visual grounding tasks, highlighting the need for improved reasoning in drone-based LVLMs.", "conclusion": "DVGBench provides actionable insights for advancing reasoning capacity of large vision-language models for drone agents, with code and datasets to be released publicly."}}
{"id": "2601.01002", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.01002", "abs": "https://arxiv.org/abs/2601.01002", "authors": ["Prem Babu Kanaparthi", "Tulasi Venkata Sri Varshini Padamata"], "title": "Lightweight Channel Attention for Efficient CNNs", "comment": "6 pages, 5 figures", "summary": "Attention mechanisms have become integral to modern convolutional neural networks (CNNs), delivering notable performance improvements with minimal computational overhead. However, the efficiency accuracy trade off of different channel attention designs remains underexplored. This work presents an empirical study comparing Squeeze and Excitation (SE), Efficient Channel Attention (ECA), and a proposed Lite Channel Attention (LCA) module across ResNet 18 and MobileNetV2 architectures on CIFAR 10. LCA employs adaptive one dimensional convolutions with grouped operations to reduce parameter usage while preserving effective attention behavior. Experimental results show that LCA achieves competitive accuracy, reaching 94.68 percent on ResNet 18 and 93.10 percent on MobileNetV2, while matching ECA in parameter efficiency and maintaining favorable inference latency. Comprehensive benchmarks including FLOPs, parameter counts, and GPU latency measurements are provided, offering practical insights for deploying attention enhanced CNNs in resource constrained environments.", "AI": {"tldr": "The paper presents an empirical study comparing channel attention mechanisms (SE, ECA, LCA) in CNNs, proposing a lightweight LCA module that achieves competitive accuracy with improved parameter efficiency for resource-constrained environments.", "motivation": "While attention mechanisms improve CNN performance with minimal computational overhead, the efficiency-accuracy trade-off of different channel attention designs remains underexplored, particularly for deployment in resource-constrained environments.", "method": "The study compares Squeeze-and-Excitation (SE), Efficient Channel Attention (ECA), and a proposed Lite Channel Attention (LCA) module across ResNet-18 and MobileNetV2 architectures on CIFAR-10. LCA employs adaptive 1D convolutions with grouped operations to reduce parameters while preserving effective attention behavior.", "result": "LCA achieves competitive accuracy (94.68% on ResNet-18, 93.10% on MobileNetV2) while matching ECA in parameter efficiency and maintaining favorable inference latency. Comprehensive benchmarks include FLOPs, parameter counts, and GPU latency measurements.", "conclusion": "LCA provides a practical solution for deploying attention-enhanced CNNs in resource-constrained environments, offering competitive performance with improved efficiency through adaptive 1D convolutions and grouped operations."}}
{"id": "2601.01022", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.01022", "abs": "https://arxiv.org/abs/2601.01022", "authors": ["Shiao Wang", "Xiao Wang", "Haonan Zhao", "Jiarui Xu", "Bo Jiang", "Lin Zhu", "Xin Zhao", "Yonghong Tian", "Jin Tang"], "title": "Decoupling Amplitude and Phase Attention in Frequency Domain for RGB-Event based Visual Object Tracking", "comment": null, "summary": "Existing RGB-Event visual object tracking approaches primarily rely on conventional feature-level fusion, failing to fully exploit the unique advantages of event cameras. In particular, the high dynamic range and motion-sensitive nature of event cameras are often overlooked, while low-information regions are processed uniformly, leading to unnecessary computational overhead for the backbone network. To address these issues, we propose a novel tracking framework that performs early fusion in the frequency domain, enabling effective aggregation of high-frequency information from the event modality. Specifically, RGB and event modalities are transformed from the spatial domain to the frequency domain via the Fast Fourier Transform, with their amplitude and phase components decoupled. High-frequency event information is selectively fused into RGB modality through amplitude and phase attention, enhancing feature representation while substantially reducing backbone computation. In addition, a motion-guided spatial sparsification module leverages the motion-sensitive nature of event cameras to capture the relationship between target motion cues and spatial probability distribution, filtering out low-information regions and enhancing target-relevant features. Finally, a sparse set of target-relevant features is fed into the backbone network for learning, and the tracking head predicts the final target position. Extensive experiments on three widely used RGB-Event tracking benchmark datasets, including FE108, FELT, and COESOT, demonstrate the high performance and efficiency of our method. The source code of this paper will be released on https://github.com/Event-AHU/OpenEvTracking", "AI": {"tldr": "A novel RGB-Event tracking framework using frequency-domain early fusion and motion-guided spatial sparsification to reduce computational overhead while improving performance.", "motivation": "Existing RGB-Event tracking methods fail to fully exploit event cameras' advantages (high dynamic range, motion sensitivity) and process low-information regions uniformly, causing unnecessary computational overhead.", "method": "1) Early fusion in frequency domain via FFT with amplitude/phase attention; 2) Motion-guided spatial sparsification module to filter low-information regions; 3) Sparse feature feeding to backbone network.", "result": "Extensive experiments on FE108, FELT, and COESOT benchmarks demonstrate high performance and efficiency improvements over existing methods.", "conclusion": "The proposed frequency-domain fusion and motion-guided sparsification effectively exploit event camera advantages while reducing computational costs, achieving state-of-the-art RGB-Event tracking performance."}}
{"id": "2601.01024", "categories": ["cs.CV", "cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2601.01024", "abs": "https://arxiv.org/abs/2601.01024", "authors": ["Tien-Huy Nguyen", "Huu-Loc Tran", "Thanh Duc Ngo"], "title": "ITSELF: Attention Guided Fine-Grained Alignment for Vision-Language Retrieval", "comment": "Accepted at WACV Main Track 2026", "summary": "Vision Language Models (VLMs) have rapidly advanced and show strong promise for text-based person search (TBPS), a task that requires capturing fine-grained relationships between images and text to distinguish individuals. Previous methods address these challenges through local alignment, yet they are often prone to shortcut learning and spurious correlations, yielding misalignment. Moreover, injecting prior knowledge can distort intra-modality structure. Motivated by our finding that encoder attention surfaces spatially precise evidence from the earliest training epochs, and to alleviate these issues, we introduceITSELF, an attention-guided framework for implicit local alignment. At its core, Guided Representation with Attentive Bank (GRAB) converts the model's own attention into an Attentive Bank of high-saliency tokens and applies local objectives on this bank, learning fine-grained correspondences without extra supervision. To make the selection reliable and non-redundant, we introduce Multi-Layer Attention for Robust Selection (MARS), which aggregates attention across layers and performs diversity-aware top-k selection; and Adaptive Token Scheduler (ATS), which schedules the retention budget from coarse to fine over training, preserving context early while progressively focusing on discriminative details. Extensive experiments on three widely used TBPS benchmarks showstate-of-the-art performance and strong cross-dataset generalization, confirming the effectiveness and robustness of our approach without additional prior supervision. Our project is publicly available at https://trhuuloc.github.io/itself", "AI": {"tldr": "ITSELF is an attention-guided framework for text-based person search that uses the model's own attention to create an attentive bank of high-saliency tokens for implicit local alignment, achieving state-of-the-art performance without extra supervision.", "motivation": "Previous methods for text-based person search suffer from shortcut learning, spurious correlations, and misalignment issues with local alignment approaches. Additionally, injecting prior knowledge can distort intra-modality structure. The authors discovered that encoder attention surfaces spatially precise evidence early in training, motivating an attention-guided approach.", "method": "ITSELF uses Guided Representation with Attentive Bank (GRAB) to convert model attention into an attentive bank of high-saliency tokens for local objectives. Multi-Layer Attention for Robust Selection (MARS) aggregates attention across layers with diversity-aware top-k selection. Adaptive Token Scheduler (ATS) schedules retention budget from coarse to fine over training, preserving context early while focusing on discriminative details later.", "result": "Extensive experiments on three widely used TBPS benchmarks show state-of-the-art performance and strong cross-dataset generalization, confirming effectiveness and robustness without additional prior supervision.", "conclusion": "The attention-guided framework ITSELF effectively addresses limitations of previous local alignment methods by leveraging the model's own attention for implicit alignment, achieving superior performance in text-based person search without requiring extra supervision."}}
{"id": "2601.01026", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.SE"], "pdf": "https://arxiv.org/pdf/2601.01026", "abs": "https://arxiv.org/abs/2601.01026", "authors": ["Douglas Costa Braga", "Daniel Oliveira Dantas"], "title": "Enhanced Leukemic Cell Classification Using Attention-Based CNN and Data Augmentation", "comment": "9 pages, 5 figures, 4 tables. Submitted to VISAPP 2025", "summary": "We present a reproducible deep learning pipeline for leukemic cell classification, focusing on system architecture, experimental robustness, and software design choices for medical image analysis. Acute lymphoblastic leukemia (ALL) is the most common childhood cancer, requiring expert microscopic diagnosis that suffers from inter-observer variability and time constraints. The proposed system integrates an attention-based convolutional neural network combining EfficientNetV2-B3 with Squeeze-and-Excitation mechanisms for automated ALL cell classification. Our approach employs comprehensive data augmentation, focal loss for class imbalance, and patient-wise data splitting to ensure robust and reproducible evaluation. On the C-NMC 2019 dataset (12,528 original images from 62 patients), the system achieves a 97.89% F1-score and 97.89% accuracy on the test set, with statistical validation through 100-iteration Monte Carlo experiments confirming significant improvements (p < 0.001) over baseline methods. The proposed pipeline outperforms existing approaches by up to 4.67% while using 89% fewer parameters than VGG16 (15.2M vs. 138M). The attention mechanism provides interpretable visualizations of diagnostically relevant cellular features, demonstrating that modern attention-based architectures can improve leukemic cell classification while maintaining computational efficiency suitable for clinical deployment.", "AI": {"tldr": "A reproducible deep learning pipeline for leukemic cell classification achieves 97.89% accuracy using an attention-based CNN with EfficientNetV2-B3 and SE mechanisms, outperforming baselines while using 89% fewer parameters than VGG16.", "motivation": "Acute lymphoblastic leukemia (ALL) is the most common childhood cancer requiring microscopic diagnosis, which suffers from inter-observer variability and time constraints, necessitating automated, reliable classification systems.", "method": "Integrates attention-based CNN combining EfficientNetV2-B3 with Squeeze-and-Excitation mechanisms, employs comprehensive data augmentation, focal loss for class imbalance, and patient-wise data splitting for robust evaluation on C-NMC 2019 dataset.", "result": "Achieves 97.89% F1-score and 97.89% accuracy on test set, with statistical validation through 100-iteration Monte Carlo experiments confirming significant improvements (p < 0.001) over baseline methods, using 89% fewer parameters than VGG16.", "conclusion": "Modern attention-based architectures can improve leukemic cell classification while maintaining computational efficiency suitable for clinical deployment, with interpretable visualizations of diagnostically relevant cellular features."}}
{"id": "2601.01036", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.01036", "abs": "https://arxiv.org/abs/2601.01036", "authors": ["Kiet Dang Vu", "Trung Thai Tran", "Kien Nguyen Do Trung", "Duc Dung Nguyen"], "title": "Mono3DV: Monocular 3D Object Detection with 3D-Aware Bipartite Matching and Variational Query DeNoising", "comment": null, "summary": "While DETR-like architectures have demonstrated significant potential for monocular 3D object detection, they are often hindered by a critical limitation: the exclusion of 3D attributes from the bipartite matching process. This exclusion arises from the inherent ill-posed nature of 3D estimation from monocular image, which introduces instability during training. Consequently, high-quality 3D predictions can be erroneously suppressed by 2D-only matching criteria, leading to suboptimal results. To address this, we propose Mono3DV, a novel Transformer-based framework. Our approach introduces three key innovations. First, we develop a 3D-Aware Bipartite Matching strategy that directly incorporates 3D geometric information into the matching cost, resolving the misalignment caused by purely 2D criteria. Second, it is important to stabilize the Bipartite Matching to resolve the instability occurring when integrating 3D attributes. Therefore, we propose 3D-DeNoising scheme in the training phase. Finally, recognizing the gradient vanishing issue associated with conventional denoising techniques, we propose a novel Variational Query DeNoising mechanism to overcome this limitation, which significantly enhances model performance. Without leveraging any external data, our method achieves state-of-the-art results on the KITTI 3D object detection benchmark.", "AI": {"tldr": "Mono3DV introduces a Transformer-based framework for monocular 3D object detection that addresses limitations of DETR-like architectures by incorporating 3D geometric information into bipartite matching and stabilizing training with novel denoising techniques.", "motivation": "Current DETR-like architectures for monocular 3D object detection exclude 3D attributes from bipartite matching due to the ill-posed nature of 3D estimation from monocular images. This causes instability during training and leads to suppression of high-quality 3D predictions by 2D-only matching criteria, resulting in suboptimal performance.", "method": "Proposes Mono3DV with three key innovations: 1) 3D-Aware Bipartite Matching that incorporates 3D geometric information into matching cost, 2) 3D-DeNoising scheme to stabilize training when integrating 3D attributes, and 3) Variational Query DeNoising mechanism to overcome gradient vanishing issues of conventional denoising techniques.", "result": "Achieves state-of-the-art results on the KITTI 3D object detection benchmark without using any external data.", "conclusion": "Mono3DV successfully addresses the limitations of existing DETR-like architectures for monocular 3D object detection by properly integrating 3D information into the matching process and stabilizing training, leading to superior performance on standard benchmarks."}}
{"id": "2601.01041", "categories": ["cs.CV", "cs.MM"], "pdf": "https://arxiv.org/pdf/2601.01041", "abs": "https://arxiv.org/abs/2601.01041", "authors": ["Xiang Zhang", "Wenliang Weng", "Daoyong Fu", "Ziqiang Li", "Zhangjie Fu"], "title": "Deepfake Detection with Multi-Artifact Subspace Fine-Tuning and Selective Layer Masking", "comment": null, "summary": "Deepfake detection still faces significant challenges in cross-dataset and real-world complex scenarios. The root cause lies in the high diversity of artifact distributions introduced by different forgery methods, while pretrained models tend to disrupt their original general semantic structures when adapting to new artifacts. Existing approaches usually rely on indiscriminate global parameter updates or introduce additional supervision signals, making it difficult to effectively model diverse forgery artifacts while preserving semantic stability. To address these issues, this paper proposes a deepfake detection method based on Multi-Artifact Subspaces and selective layer masks (MASM), which explicitly decouples semantic representations from artifact representations and constrains the fitting strength of artifact subspaces, thereby improving generalization robustness in cross-dataset scenarios. Specifically, MASM applies singular value decomposition to model weights, partitioning pretrained weights into a stable semantic principal subspace and multiple learnable artifact subspaces. This design enables decoupled modeling of different forgery artifact patterns while preserving the general semantic subspace. On this basis, a selective layer mask strategy is introduced to adaptively regulate the update behavior of corresponding network layers according to the learning state of each artifact subspace, suppressing overfitting to any single forgery characteristic. Furthermore, orthogonality constraints and spectral consistency constraints are imposed to jointly regularize multiple artifact subspaces, guiding them to learn complementary and diverse artifact representations while maintaining a stable overall spectral structure.", "AI": {"tldr": "MASM method for deepfake detection uses multi-artifact subspaces and selective layer masks to decouple semantic and artifact representations, improving cross-dataset generalization by preventing overfitting to specific forgery patterns.", "motivation": "Deepfake detection struggles with cross-dataset and real-world scenarios due to diverse artifact distributions from different forgery methods. Pretrained models lose their general semantic structure when adapting to new artifacts, and existing approaches fail to effectively model diverse artifacts while maintaining semantic stability.", "method": "Proposes MASM: 1) Uses singular value decomposition to partition pretrained weights into stable semantic principal subspace and multiple learnable artifact subspaces; 2) Introduces selective layer masks to adaptively regulate layer updates based on each artifact subspace's learning state; 3) Applies orthogonality and spectral consistency constraints to regularize artifact subspaces for complementary, diverse representations.", "result": "The method enables decoupled modeling of different forgery artifact patterns while preserving general semantic structure, suppresses overfitting to single forgery characteristics, and improves generalization robustness in cross-dataset scenarios.", "conclusion": "MASM addresses deepfake detection challenges by explicitly separating semantic and artifact representations, constraining artifact subspace fitting, and adaptively regulating network updates, leading to better cross-dataset generalization performance."}}
{"id": "2601.01044", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.01044", "abs": "https://arxiv.org/abs/2601.01044", "authors": ["Jin Wang", "Angelo De Castro", "Yuxi Zhang", "Lucas Basolli Borsatto", "Yuechen Guo", "Victoria Bastos Primo", "Ana Beatriz Montevecchio Bernardino", "Gota Morota", "Ricardo C Chebel", "Haipeng Yu"], "title": "Evaluating transfer learning strategies for improving dairy cattle body weight prediction in small farms using depth-image and point-cloud data", "comment": null, "summary": "Computer vision provides automated, non-invasive, and scalable tools for monitoring dairy cattle, thereby supporting management, health assessment, and phenotypic data collection. Although transfer learning is commonly used for predicting body weight from images, its effectiveness and optimal fine-tuning strategies remain poorly understood in livestock applications, particularly beyond the use of pretrained ImageNet or COCO weights. In addition, while both depth images and three-dimensional point-cloud data have been explored for body weight prediction, direct comparisons of these two modalities in dairy cattle are limited. Therefore, the objectives of this study were to 1) evaluate whether transfer learning from a large farm enhances body weight prediction on a small farm with limited data, and 2) compare the predictive performance of depth-image- and point-cloud-based approaches under three experimental designs. Top-view depth images and point-cloud data were collected from 1,201, 215, and 58 cows at large, medium, and small dairy farms, respectively. Four deep learning models were evaluated: ConvNeXt and MobileViT for depth images, and PointNet and DGCNN for point clouds. Transfer learning markedly improved body weight prediction on the small farm across all four models, outperforming single-source learning and achieving gains comparable to or greater than joint learning. These results indicate that pretrained representations generalize well across farms with differing imaging conditions and dairy cattle populations. No consistent performance difference was observed between depth-image- and point-cloud-based models. Overall, these findings suggest that transfer learning is well suited for small farm prediction scenarios where cross-farm data sharing is limited by privacy, logistical, or policy constraints, as it requires access only to pretrained model weights rather than raw data.", "AI": {"tldr": "Transfer learning from large farms significantly improves body weight prediction on small farms using depth images and point clouds, with no clear advantage between the two modalities.", "motivation": "Computer vision offers scalable tools for dairy cattle monitoring, but transfer learning effectiveness and optimal fine-tuning strategies are poorly understood in livestock applications. There's also limited direct comparison between depth images and point clouds for body weight prediction in dairy cattle.", "method": "Collected top-view depth images and point-cloud data from 1,201, 215, and 58 cows at large, medium, and small dairy farms. Evaluated four deep learning models: ConvNeXt and MobileViT for depth images, and PointNet and DGCNN for point clouds. Tested transfer learning from large farm to small farm under three experimental designs.", "result": "Transfer learning markedly improved body weight prediction on the small farm across all four models, outperforming single-source learning and achieving gains comparable to or greater than joint learning. No consistent performance difference was observed between depth-image- and point-cloud-based models.", "conclusion": "Transfer learning is well-suited for small farm prediction scenarios where cross-farm data sharing is limited by privacy, logistical, or policy constraints, as it requires access only to pretrained model weights rather than raw data. Pretrained representations generalize well across farms with differing imaging conditions and dairy cattle populations."}}
{"id": "2601.01050", "categories": ["cs.CV", "cs.AI", "cs.GR"], "pdf": "https://arxiv.org/pdf/2601.01050", "abs": "https://arxiv.org/abs/2601.01050", "authors": ["Hongming Fu", "Wenjia Wang", "Xiaozhen Qiao", "Shuo Yang", "Zheng Liu", "Bo Zhao"], "title": "EgoGrasp: World-Space Hand-Object Interaction Estimation from Egocentric Videos", "comment": null, "summary": "We propose EgoGrasp, the first method to reconstruct world-space hand-object interactions (W-HOI) from egocentric monocular videos with dynamic cameras in the wild. Accurate W-HOI reconstruction is critical for understanding human behavior and enabling applications in embodied intelligence and virtual reality. However, existing hand-object interactions (HOI) methods are limited to single images or camera coordinates, failing to model temporal dynamics or consistent global trajectories. Some recent approaches attempt world-space hand estimation but overlook object poses and HOI constraints. Their performance also suffers under severe camera motion and frequent occlusions common in egocentric in-the-wild videos. To address these challenges, we introduce a multi-stage framework with a robust pre-process pipeline built on newly developed spatial intelligence models, a whole-body HOI prior model based on decoupled diffusion models, and a multi-objective test-time optimization paradigm. Our HOI prior model is template-free and scalable to multiple objects. In experiments, we prove our method achieving state-of-the-art performance in W-HOI reconstruction.", "AI": {"tldr": "EgoGrasp: First method to reconstruct world-space hand-object interactions from egocentric monocular videos with dynamic cameras in the wild.", "motivation": "Accurate world-space hand-object interaction (W-HOI) reconstruction is critical for understanding human behavior and enabling applications in embodied intelligence and VR. Existing methods are limited to single images or camera coordinates, failing to model temporal dynamics or consistent global trajectories, and suffer under severe camera motion and occlusions in egocentric videos.", "method": "Multi-stage framework with: 1) robust pre-process pipeline using spatial intelligence models, 2) whole-body HOI prior model based on decoupled diffusion models (template-free and scalable to multiple objects), and 3) multi-objective test-time optimization paradigm.", "result": "Achieves state-of-the-art performance in W-HOI reconstruction from egocentric monocular videos with dynamic cameras in the wild.", "conclusion": "EgoGrasp successfully addresses the challenges of reconstructing world-space hand-object interactions from challenging egocentric videos, overcoming limitations of previous methods through a novel multi-stage framework with specialized components for handling camera motion and occlusions."}}
{"id": "2601.01056", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.01056", "abs": "https://arxiv.org/abs/2601.01056", "authors": ["Ifeanyi Ezuma", "Ugochukwu Ugwu"], "title": "Enhancing Histopathological Image Classification via Integrated HOG and Deep Features with Robust Noise Performance", "comment": "10 pages, 8 figures. Code and datasets available upon request", "summary": "The era of digital pathology has advanced histopathological examinations, making automated image analysis essential in clinical practice. This study evaluates the classification performance of machine learning and deep learning models on the LC25000 dataset, which includes five classes of histopathological images. We used the fine-tuned InceptionResNet-v2 network both as a classifier and for feature extraction. Our results show that the fine-tuned InceptionResNet-v2 achieved a classification accuracy of 96.01\\% and an average AUC of 96.8\\%. Models trained on deep features from InceptionResNet-v2 outperformed those using only the pre-trained network, with the Neural Network model achieving an AUC of 99.99\\% and accuracy of 99.84\\%. Evaluating model robustness under varying SNR conditions revealed that models using deep features exhibited greater resilience, particularly GBM and KNN. The combination of HOG and deep features showed enhanced performance, however, less so in noisy environments.", "AI": {"tldr": "Fine-tuned InceptionResNet-v2 achieves 96.01% accuracy on LC25000 histopathology dataset; neural networks using deep features reach 99.84% accuracy; deep features provide better noise robustness than HOG features.", "motivation": "Digital pathology advancements require automated image analysis for clinical practice, necessitating evaluation of ML/DL models for histopathological image classification.", "method": "Used fine-tuned InceptionResNet-v2 as both classifier and feature extractor on LC25000 dataset; compared models using deep features vs pre-trained network; evaluated robustness under varying SNR conditions; tested HOG+deep feature combinations.", "result": "Fine-tuned InceptionResNet-v2: 96.01% accuracy, 96.8% AUC. Neural Network with deep features: 99.84% accuracy, 99.99% AUC. Deep feature models showed superior noise robustness (especially GBM and KNN). HOG+deep features improved performance but less effective in noisy environments.", "conclusion": "Deep features from fine-tuned InceptionResNet-v2 significantly outperform pre-trained networks for histopathology classification; models using these features demonstrate excellent accuracy and superior robustness to noise, making them promising for clinical digital pathology applications."}}
{"id": "2601.01064", "categories": ["cs.CV", "eess.IV"], "pdf": "https://arxiv.org/pdf/2601.01064", "abs": "https://arxiv.org/abs/2601.01064", "authors": ["Jianan Li", "Wangcai Zhao", "Tingfa Xu"], "title": "Efficient Hyperspectral Image Reconstruction Using Lightweight Separate Spectral Transformers", "comment": null, "summary": "Hyperspectral imaging (HSI) is essential across various disciplines for its capacity to capture rich spectral information. However, efficiently reconstructing hyperspectral images from compressive sensing measurements presents significant challenges. To tackle these, we adopt a divide-and-conquer strategy that capitalizes on the unique spectral and spatial characteristics of hyperspectral images. We introduce the Lightweight Separate Spectral Transformer (LSST), an innovative architecture tailored for efficient hyperspectral image reconstruction. This architecture consists of Separate Spectral Transformer Blocks (SSTB) for modeling spectral relationships and Lightweight Spatial Convolution Blocks (LSCB) for spatial processing. The SSTB employs Grouped Spectral Self-attention and a Spectrum Shuffle operation to effectively manage both local and non-local spectral relationships. Simultaneously, the LSCB utilizes depth-wise separable convolutions and strategic ordering to enhance spatial information processing. Furthermore, we implement the Focal Spectrum Loss, a novel loss weighting mechanism that dynamically adjusts during training to improve reconstruction across spectrally complex bands. Extensive testing demonstrates that our LSST achieves superior performance while requiring fewer FLOPs and parameters, underscoring its efficiency and effectiveness. The source code is available at: https://github.com/wcz1124/LSST.", "AI": {"tldr": "LSST is a lightweight transformer-based architecture for efficient hyperspectral image reconstruction from compressive sensing measurements, using separate spectral and spatial processing blocks with novel attention mechanisms and focal spectrum loss.", "motivation": "Hyperspectral imaging captures rich spectral information but faces challenges in efficient reconstruction from compressive sensing measurements. Existing methods need to better leverage the unique spectral and spatial characteristics of HSI data while maintaining computational efficiency.", "method": "Proposes Lightweight Separate Spectral Transformer (LSST) with two main components: Separate Spectral Transformer Blocks (SSTB) using Grouped Spectral Self-attention and Spectrum Shuffle for spectral modeling, and Lightweight Spatial Convolution Blocks (LSCB) using depth-wise separable convolutions for spatial processing. Also introduces Focal Spectrum Loss for dynamic weighting during training.", "result": "Extensive testing shows LSST achieves superior reconstruction performance while requiring fewer FLOPs and parameters compared to existing methods, demonstrating both efficiency and effectiveness.", "conclusion": "LSST provides an efficient and effective solution for hyperspectral image reconstruction by leveraging a divide-and-conquer strategy that separately processes spectral and spatial information with lightweight transformer and convolution blocks, achieving state-of-the-art performance with reduced computational complexity."}}
{"id": "2601.01084", "categories": ["cs.CV", "eess.IV"], "pdf": "https://arxiv.org/pdf/2601.01084", "abs": "https://arxiv.org/abs/2601.01084", "authors": ["Adari Rama Sukanya", "Puvvula Roopesh Naga Sri Sai", "Kota Moses", "Rimalapudi Sarvendranath"], "title": "A UAV-Based Multispectral and RGB Dataset for Multi-Stage Paddy Crop Monitoring in Indian Agricultural Fields", "comment": "10-page dataset explanation paper", "summary": "We present a large-scale unmanned aerial vehicle (UAV)-based RGB and multispectral image dataset collected over paddy fields in the Vijayawada region, Andhra Pradesh, India, covering nursery to harvesting stages. We used a 20-megapixel RGB camera and a 5-megapixel four-band multispectral camera capturing red, green, red-edge, and near-infrared bands. Standardised operating procedure (SOP) and checklists were developed to ensure repeatable data acquisition. Our dataset comprises of 42,430 raw images (415 GB) captured over 5 acres with 1 cm/pixel ground sampling distance (GSD) with associated metadata such as GPS coordinates, flight altitude, and environmental conditions. Captured images were validated using Pix4D Fields to generate orthomosaic maps and vegetation index maps, such as normalised difference vegetation index (NDVI) and normalised difference red-edge (NDRE) index. Our dataset is one of the few datasets that provide high-resolution images with rich metadata that cover all growth stages of Indian paddy crops. The dataset is available on IEEE DataPort with DOI, . It can support studies on targeted spraying, disease analysis, and yield estimation.", "AI": {"tldr": "A large-scale UAV dataset of paddy fields in India covering all growth stages with RGB and multispectral imagery at 1 cm/pixel resolution, including rich metadata for agricultural research applications.", "motivation": "To address the lack of comprehensive, high-resolution UAV datasets covering all growth stages of Indian paddy crops with rich metadata for agricultural research applications like targeted spraying, disease analysis, and yield estimation.", "method": "Used UAVs equipped with 20MP RGB and 5MP four-band multispectral cameras to capture images over 5 acres of paddy fields. Developed SOPs and checklists for repeatable data acquisition. Collected 42,430 raw images with GPS coordinates, flight altitude, and environmental metadata. Validated images using Pix4D Fields to generate orthomosaic and vegetation index maps (NDVI, NDRE).", "result": "Created a 415 GB dataset with 1 cm/pixel GSD covering nursery to harvesting stages. The dataset includes RGB and multispectral (red, green, red-edge, near-infrared) imagery with comprehensive metadata. Generated orthomosaic maps and vegetation index maps. Dataset is publicly available on IEEE DataPort with DOI.", "conclusion": "This dataset fills a gap by providing high-resolution, multi-temporal UAV imagery covering all growth stages of Indian paddy crops with rich metadata, supporting agricultural research in precision farming applications like targeted spraying, disease analysis, and yield estimation."}}
{"id": "2601.01085", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.01085", "abs": "https://arxiv.org/abs/2601.01085", "authors": ["Jiayi Xu", "Zhang Zhang", "Yuanrui Zhang", "Ruitao Chen", "Yixian Xu", "Tianyu He", "Di He"], "title": "Luminark: Training-free, Probabilistically-Certified Watermarking for General Vision Generative Models", "comment": null, "summary": "In this paper, we introduce \\emph{Luminark}, a training-free and probabilistically-certified watermarking method for general vision generative models. Our approach is built upon a novel watermark definition that leverages patch-level luminance statistics. Specifically, the service provider predefines a binary pattern together with corresponding patch-level thresholds. To detect a watermark in a given image, we evaluate whether the luminance of each patch surpasses its threshold and then verify whether the resulting binary pattern aligns with the target one. A simple statistical analysis demonstrates that the false positive rate of the proposed method can be effectively controlled, thereby ensuring certified detection. To enable seamless watermark injection across different paradigms, we leverage the widely adopted guidance technique as a plug-and-play mechanism and develop the \\emph{watermark guidance}. This design enables Luminark to achieve generality across state-of-the-art generative models without compromising image quality. Empirically, we evaluate our approach on nine models spanning diffusion, autoregressive, and hybrid frameworks. Across all evaluations, Luminark consistently demonstrates high detection accuracy, strong robustness against common image transformations, and good performance on visual quality.", "AI": {"tldr": "Luminark is a training-free, probabilistically-certified watermarking method for vision generative models that uses patch-level luminance statistics and watermark guidance for seamless integration.", "motivation": "There's a need for watermarking methods for vision generative models that are training-free, probabilistically certified, and generalizable across different model architectures without compromising image quality.", "method": "Uses patch-level luminance statistics with predefined binary patterns and thresholds. Leverages guidance technique as plug-and-play watermark guidance for seamless injection across different generative paradigms.", "result": "Evaluated on 9 models spanning diffusion, autoregressive, and hybrid frameworks. Consistently demonstrates high detection accuracy, strong robustness against image transformations, and good visual quality performance.", "conclusion": "Luminark provides an effective, generalizable watermarking solution for vision generative models with certified detection and minimal impact on image quality."}}
{"id": "2601.01088", "categories": ["cs.CV", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.01088", "abs": "https://arxiv.org/abs/2601.01088", "authors": ["Haq Nawaz Malik"], "title": "600k-ks-ocr: a large-scale synthetic dataset for optical character recognition in kashmiri script", "comment": null, "summary": "This technical report presents the 600K-KS-OCR Dataset, a large-scale synthetic corpus comprising approximately 602,000 word-level segmented images designed for training and evaluating optical character recognition systems targeting Kashmiri script. The dataset addresses a critical resource gap for Kashmiri, an endangered Dardic language utilizing a modified Perso-Arabic writing system spoken by approximately seven million people. Each image is rendered at 256x64 pixels with corresponding ground-truth transcriptions provided in multiple formats compatible with CRNN, TrOCR, and generalpurpose machine learning pipelines. The generation methodology incorporates three traditional Kashmiri typefaces, comprehensive data augmentation simulating real-world document degradation, and diverse background textures to enhance model robustness. The dataset is distributed across ten partitioned archives totaling approximately 10.6 GB and is released under the CC-BY-4.0 license to facilitate research in low-resource language optical character recognition.", "AI": {"tldr": "A large-scale synthetic OCR dataset for Kashmiri script with 602K word-level images, addressing resource gaps for this endangered language.", "motivation": "To address the critical resource gap for Kashmiri, an endangered Dardic language spoken by ~7 million people that uses a modified Perso-Arabic writing system, which lacks sufficient OCR training data.", "method": "Created synthetic corpus using three traditional Kashmiri typefaces with comprehensive data augmentation simulating real-world document degradation, diverse background textures, and rendered at 256x64 pixels with ground-truth transcriptions in multiple formats.", "result": "Produced the 600K-KS-OCR Dataset containing approximately 602,000 word-level segmented images distributed across ten partitioned archives totaling ~10.6 GB, released under CC-BY-4.0 license.", "conclusion": "This dataset facilitates research in low-resource language optical character recognition for Kashmiri script and addresses a significant resource gap for an endangered language community."}}
{"id": "2601.01095", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.01095", "abs": "https://arxiv.org/abs/2601.01095", "authors": ["Hyeonjeong Ha", "Jinjin Ge", "Bo Feng", "Kaixin Ma", "Gargi Chakraborty"], "title": "NarrativeTrack: Evaluating Video Language Models Beyond the Frame", "comment": "VideoLLM Fine-Grained Evaluation", "summary": "Multimodal large language models (MLLMs) have achieved impressive progress in vision-language reasoning, yet their ability to understand temporally unfolding narratives in videos remains underexplored. True narrative understanding requires grounding who is doing what, when, and where, maintaining coherent entity representations across dynamic visual and temporal contexts. We introduce NarrativeTrack, the first benchmark to evaluate narrative understanding in MLLMs through fine-grained entity-centric reasoning. Unlike existing benchmarks limited to short clips or coarse scene-level semantics, we decompose videos into constituent entities and examine their continuity via a Compositional Reasoning Progression (CRP), a structured evaluation framework that progressively increases narrative complexity across three dimensions: entity existence, entity changes, and entity ambiguity. CRP challenges models to advance from temporal persistence to contextual evolution and fine-grained perceptual reasoning. A fully automated entity-centric pipeline enables scalable extraction of temporally grounded entity representations, providing the foundation for CRP. Evaluations of state-of-the-art MLLMs reveal that models fail to robustly track entities across visual transitions and temporal dynamics, often hallucinating identity under context shifts. Open-source general-purpose MLLMs exhibit strong perceptual grounding but weak temporal coherence, while video-specific MLLMs capture temporal context yet hallucinate entity's contexts. These findings uncover a fundamental trade-off between perceptual grounding and temporal reasoning, indicating that narrative understanding emerges only from their integration. NarrativeTrack provides the first systematic framework to diagnose and advance temporally grounded narrative comprehension in MLLMs.", "AI": {"tldr": "NarrativeTrack is the first benchmark for evaluating multimodal LLMs' narrative understanding in videos through fine-grained entity-centric reasoning, revealing a trade-off between perceptual grounding and temporal coherence.", "motivation": "Current MLLMs have impressive vision-language reasoning but lack ability to understand temporally unfolding narratives in videos. True narrative understanding requires grounding entities (who, what, when, where) across dynamic visual and temporal contexts.", "method": "Introduces NarrativeTrack benchmark with Compositional Reasoning Progression (CRP) framework that progressively increases narrative complexity across three dimensions: entity existence, entity changes, and entity ambiguity. Uses automated entity-centric pipeline for scalable extraction of temporally grounded entity representations.", "result": "Evaluations show MLLMs fail to robustly track entities across visual transitions and temporal dynamics, often hallucinating identity under context shifts. Open-source general-purpose MLLMs have strong perceptual grounding but weak temporal coherence, while video-specific MLLMs capture temporal context but hallucinate entity contexts.", "conclusion": "Reveals fundamental trade-off between perceptual grounding and temporal reasoning - narrative understanding emerges only from their integration. NarrativeTrack provides first systematic framework to diagnose and advance temporally grounded narrative comprehension in MLLMs."}}
{"id": "2601.01099", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.01099", "abs": "https://arxiv.org/abs/2601.01099", "authors": ["Mahmudul Hasan", "Mabsur Fatin Bin Hossain"], "title": "Evolving CNN Architectures: From Custom Designs to Deep Residual Models for Diverse Image Classification and Detection Tasks", "comment": null, "summary": "This paper presents a comparative study of a custom convolutional neural network (CNN) architecture against widely used pretrained and transfer learning CNN models across five real-world image datasets. The datasets span binary classification, fine-grained multiclass recognition, and object detection scenarios. We analyze how architectural factors, such as network depth, residual connections, and feature extraction strategies, influence classification and localization performance. The results show that deeper CNN architectures provide substantial performance gains on fine-grained multiclass datasets, while lightweight pretrained and transfer learning models remain highly effective for simpler binary classification tasks. Additionally, we extend the proposed architecture to an object detection setting, demonstrating its adaptability in identifying unauthorized auto-rickshaws in real-world traffic scenes. Building upon a systematic analysis of custom CNN architectures alongside pretrained and transfer learning models, this study provides practical guidance for selecting suitable network designs based on task complexity and resource constraints.", "AI": {"tldr": "Custom CNN vs pretrained models compared across 5 real-world image datasets, showing deeper networks excel at fine-grained tasks while lightweight models work for simpler binary classification.", "motivation": "To provide practical guidance for selecting appropriate CNN architectures by systematically comparing custom designs with pretrained/transfer learning models across different task complexities and resource constraints.", "method": "Comparative study using a custom CNN architecture against widely used pretrained and transfer learning CNN models across five real-world image datasets spanning binary classification, fine-grained multiclass recognition, and object detection scenarios.", "result": "Deeper CNN architectures provide substantial performance gains on fine-grained multiclass datasets, while lightweight pretrained and transfer learning models remain highly effective for simpler binary classification tasks. The custom architecture was successfully extended to object detection for identifying unauthorized auto-rickshaws.", "conclusion": "The study provides practical guidance for selecting suitable network designs based on task complexity and resource constraints, demonstrating the adaptability of custom architectures across different computer vision tasks."}}
{"id": "2601.01103", "categories": ["cs.CV", "eess.IV"], "pdf": "https://arxiv.org/pdf/2601.01103", "abs": "https://arxiv.org/abs/2601.01103", "authors": ["Abhinav Attri", "Rajeev Ranjan Dwivedi", "Samiran Das", "Vinod Kumar Kurmi"], "title": "Histogram Assisted Quality Aware Generative Model for Resolution Invariant NIR Image Colorization", "comment": "Accepted at WACV 2026", "summary": "We present HAQAGen, a unified generative model for resolution-invariant NIR-to-RGB colorization that balances chromatic realism with structural fidelity. The proposed model introduces (i) a combined loss term aligning the global color statistics through differentiable histogram matching, perceptual image quality measure, and feature based similarity to preserve texture information, (ii) local hue-saturation priors injected via Spatially Adaptive Denormalization (SPADE) to stabilize chromatic reconstruction, and (iii) texture-aware supervision within a Mamba backbone to preserve fine details. We introduce an adaptive-resolution inference engine that further enables high-resolution translation without sacrificing quality. Our proposed NIR-to-RGB translation model simultaneously enforces global color statistics and local chromatic consistency, while scaling to native resolutions without compromising texture fidelity or generalization. Extensive evaluations on FANVID, OMSIV, VCIP2020, and RGB2NIR using different evaluation metrics demonstrate consistent improvements over state-of-the-art baseline methods. HAQAGen produces images with sharper textures, natural colors, attaining significant gains as per perceptual metrics. These results position HAQAGen as a scalable and effective solution for NIR-to-RGB translation across diverse imaging scenarios. Project Page: https://rajeev-dw9.github.io/HAQAGen/", "AI": {"tldr": "HAQAGen is a unified generative model for resolution-invariant NIR-to-RGB colorization that balances chromatic realism with structural fidelity using histogram matching, SPADE priors, and Mamba backbone.", "motivation": "The paper aims to solve NIR-to-RGB colorization with a focus on balancing chromatic realism with structural fidelity, addressing the challenge of preserving both global color statistics and local texture details while enabling resolution-invariant translation.", "method": "The model introduces: (1) a combined loss term with differentiable histogram matching, perceptual quality measures, and feature similarity; (2) local hue-saturation priors via SPADE for chromatic stabilization; (3) texture-aware supervision within a Mamba backbone; and (4) adaptive-resolution inference engine for high-resolution translation.", "result": "Extensive evaluations on FANVID, OMSIV, VCIP2020, and RGB2NIR datasets show consistent improvements over state-of-the-art baselines, producing images with sharper textures and natural colors with significant gains in perceptual metrics.", "conclusion": "HAQAGen is positioned as a scalable and effective solution for NIR-to-RGB translation across diverse imaging scenarios, achieving resolution-invariant colorization while maintaining texture fidelity and generalization."}}
{"id": "2601.01167", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.01167", "abs": "https://arxiv.org/abs/2601.01167", "authors": ["Tianheng Cheng", "Xinggang Wang", "Junchao Liao", "Wenyu Liu"], "title": "Cross-Layer Attentive Feature Upsampling for Low-latency Semantic Segmentation", "comment": null, "summary": "Semantic segmentation is a fundamental problem in computer vision and it requires high-resolution feature maps for dense prediction. Current coordinate-guided low-resolution feature interpolation methods, e.g., bilinear interpolation, produce coarse high-resolution features which suffer from feature misalignment and insufficient context information. Moreover, enriching semantics to high-resolution features requires a high computation burden, so that it is challenging to meet the requirement of lowlatency inference. We propose a novel Guided Attentive Interpolation (GAI) method to adaptively interpolate fine-grained high-resolution features with semantic features to tackle these issues. Guided Attentive Interpolation determines both spatial and semantic relations of pixels from features of different resolutions and then leverages these relations to interpolate high-resolution features with rich semantics. GAI can be integrated with any deep convolutional network for efficient semantic segmentation. In experiments, the GAI-based semantic segmentation networks, i.e., GAIN, can achieve78.8 mIoU with 22.3 FPS on Cityscapes and 80.6 mIoU with 64.5 on CamVid using an NVIDIA 1080Ti GPU, which are the new state-of-the-art results of low-latency semantic segmentation. Code and models are available at: https://github.com/hustvl/simpleseg.", "AI": {"tldr": "GAIN introduces Guided Attentive Interpolation (GAI) for efficient semantic segmentation, achieving state-of-the-art accuracy with low latency by adaptively interpolating high-resolution features with semantic guidance.", "motivation": "Current interpolation methods for semantic segmentation (like bilinear interpolation) produce coarse high-resolution features with feature misalignment and insufficient context, while enriching semantics requires high computational burden, making low-latency inference challenging.", "method": "Proposes Guided Attentive Interpolation (GAI) that determines both spatial and semantic relations of pixels from features of different resolutions, then leverages these relations to interpolate high-resolution features with rich semantics. GAI can be integrated with any deep convolutional network.", "result": "GAIN achieves 78.8 mIoU with 22.3 FPS on Cityscapes and 80.6 mIoU with 64.5 FPS on CamVid using NVIDIA 1080Ti GPU, establishing new state-of-the-art results for low-latency semantic segmentation.", "conclusion": "GAI effectively addresses feature misalignment and insufficient context in high-resolution feature interpolation while maintaining computational efficiency, enabling high-performance semantic segmentation with low latency."}}
{"id": "2601.01176", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.01176", "abs": "https://arxiv.org/abs/2601.01176", "authors": ["Andr\u00e9s Bell-Navas", "Jes\u00fas Garicano-Mena", "Antonella Ausiello", "Soledad Le Clainche", "Mar\u00eda Villalba-Orero", "Enrique Lara-Pezzi"], "title": "CardioMOD-Net: A Modal Decomposition-Neural Network Framework for Diagnosis and Prognosis of HFpEF from Echocardiography Cine Loops", "comment": "9 pages; 1 figure; letter", "summary": "Introduction: Heart failure with preserved ejection fraction (HFpEF) arises from diverse comorbidities and progresses through prolonged subclinical stages, making early diagnosis and prognosis difficult. Current echocardiography-based Artificial Intelligence (AI) models focus primarily on binary HFpEF detection in humans and do not provide comorbidity-specific phenotyping or temporal estimates of disease progression towards decompensation. We aimed to develop a unified AI framework, CardioMOD-Net, to perform multiclass diagnosis and continuous prediction of HFpEF onset directly from standard echocardiography cine loops in preclinical models.\n  Methods: Mouse echocardiography videos from four groups were used: control (CTL), hyperglycaemic (HG), obesity (OB), and systemic arterial hypertension (SAH). Two-dimensional parasternal long-axis cine loops were decomposed using Higher Order Dynamic Mode Decomposition (HODMD) to extract temporal features for downstream analysis. A shared latent representation supported Vision Transformers, one for a classifier for diagnosis and another for a regression module for predicting the age at HFpEF onset.\n  Results: Overall diagnostic accuracy across the four groups was 65%, with all classes exceeding 50% accuracy. Misclassifications primarily reflected early-stage overlap between OB or SAH and CTL. The prognostic module achieved a root-mean-square error of 21.72 weeks for time-to-HFpEF prediction, with OB and SAH showing the most accurate estimates. Predicted HFpEF onset closely matched true distributions in all groups.\n  Discussion: This unified framework demonstrates that multiclass phenotyping and continuous HFpEF onset prediction can be obtained from a single cine loop, even under small-data conditions. The approach offers a foundation for integrating diagnostic and prognostic modelling in preclinical HFpEF research.", "AI": {"tldr": "CardioMOD-Net: AI framework using echocardiography videos for multiclass HFpEF diagnosis and continuous prediction of disease onset in preclinical mouse models.", "motivation": "HFpEF has diverse comorbidities and prolonged subclinical stages, making early diagnosis and prognosis difficult. Current AI models only do binary detection without comorbidity-specific phenotyping or temporal progression estimates.", "method": "Used mouse echocardiography videos from control, hyperglycemic, obesity, and hypertension groups. Applied Higher Order Dynamic Mode Decomposition to extract temporal features, then used Vision Transformers - one for multiclass diagnosis and another for regression to predict HFpEF onset age.", "result": "65% overall diagnostic accuracy across four groups, with all classes >50% accuracy. Prognostic module achieved RMSE of 21.72 weeks for time-to-HFpEF prediction, with obesity and hypertension showing most accurate estimates.", "conclusion": "Unified framework demonstrates multiclass phenotyping and continuous HFpEF onset prediction from single cine loop, even with small data. Provides foundation for integrating diagnostic and prognostic modeling in preclinical HFpEF research."}}
{"id": "2601.01181", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.01181", "abs": "https://arxiv.org/abs/2601.01181", "authors": ["Chenglizhao Chen", "Shaojiang Yuan", "Xiaoxue Lu", "Mengke Song", "Jia Song", "Zhenyu Wu", "Wenfeng Song", "Shuai Li"], "title": "GenCAMO: Scene-Graph Contextual Decoupling for Environment-aware and Mask-free Camouflage Image-Dense Annotation Generation", "comment": null, "summary": "Conceal dense prediction (CDP), especially RGB-D camouflage object detection and open-vocabulary camouflage object segmentation, plays a crucial role in advancing the understanding and reasoning of complex camouflage scenes. However, high-quality and large-scale camouflage datasets with dense annotation remain scarce due to expensive data collection and labeling costs. To address this challenge, we explore leveraging generative models to synthesize realistic camouflage image-dense data for training CDP models with fine-grained representations, prior knowledge, and auxiliary reasoning. Concretely, our contributions are threefold: (i) we introduce GenCAMO-DB, a large-scale camouflage dataset with multi-modal annotations, including depth maps, scene graphs, attribute descriptions, and text prompts; (ii) we present GenCAMO, an environment-aware and mask-free generative framework that produces high-fidelity camouflage image-dense annotations; (iii) extensive experiments across multiple modalities demonstrate that GenCAMO significantly improves dense prediction performance on complex camouflage scenes by providing high-quality synthetic data. The code and datasets will be released after paper acceptance.", "AI": {"tldr": "GenCAMO introduces a generative framework to create synthetic camouflage datasets with dense annotations (depth, scene graphs, attributes) to improve RGB-D camouflage object detection and segmentation, addressing data scarcity in complex camouflage scenes.", "motivation": "High-quality, large-scale camouflage datasets with dense annotations are scarce due to expensive collection and labeling costs, limiting progress in camouflage dense prediction tasks like object detection and segmentation.", "method": "Proposes GenCAMO, an environment-aware and mask-free generative framework that synthesizes realistic camouflage images with multi-modal dense annotations (depth maps, scene graphs, attribute descriptions, text prompts), creating the GenCAMO-DB dataset.", "result": "Extensive experiments show GenCAMO significantly improves dense prediction performance on complex camouflage scenes by providing high-quality synthetic training data across multiple modalities.", "conclusion": "Generative models can effectively address data scarcity in camouflage analysis by creating realistic synthetic datasets with dense annotations, advancing the understanding and reasoning of complex camouflage scenes."}}
{"id": "2601.01192", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.01192", "abs": "https://arxiv.org/abs/2601.01192", "authors": ["Hao Lu", "Xuhui Zhu", "Wenjing Zhang", "Yanan Li", "Xiang Bai"], "title": "Crowded Video Individual Counting Informed by Social Grouping and Spatial-Temporal Displacement Priors", "comment": "Journal Extension of arXiv:2506.13067", "summary": "Video Individual Counting (VIC) is a recently introduced task aiming to estimate pedestrian flux from a video. It extends Video Crowd Counting (VCC) beyond the per-frame pedestrian count. In contrast to VCC that learns to count pedestrians across frames, VIC must identify co-existent pedestrians between frames, which turns out to be a correspondence problem. Existing VIC approaches, however, can underperform in congested scenes such as metro commuting. To address this, we build WuhanMetroCrowd, one of the first VIC datasets that characterize crowded, dynamic pedestrian flows. It features sparse-to-dense density levels, short-to-long video clips, slow-to-fast flow variations, front-to-back appearance changes, and light-to-heavy occlusions. To better adapt VIC approaches to crowds, we rethink the nature of VIC and recognize two informative priors: i) the social grouping prior that indicates pedestrians tend to gather in groups and ii) the spatial-temporal displacement prior that informs an individual cannot teleport physically. The former inspires us to relax the standard one-to-one (O2O) matching used by VIC to one-to-many (O2M) matching, implemented by an implicit context generator and a O2M matcher; the latter facilitates the design of a displacement prior injector, which strengthens not only O2M matching but also feature extraction and model training. These designs jointly form a novel and strong VIC baseline OMAN++. Extensive experiments show that OMAN++ not only outperforms state-of-the-art VIC baselines on the standard SenseCrowd, CroHD, and MovingDroneCrowd benchmarks, but also indicates a clear advantage in crowded scenes, with a 38.12% error reduction on our WuhanMetroCrowd dataset. Code, data, and pretrained models are available at https://github.com/tiny-smart/OMAN.", "AI": {"tldr": "OMAN++ is a novel Video Individual Counting method that improves performance in crowded scenes by introducing one-to-many matching and spatial-temporal displacement priors, validated on a new challenging dataset WuhanMetroCrowd.", "motivation": "Existing Video Individual Counting (VIC) approaches underperform in congested scenes like metro commuting. The paper addresses this limitation by recognizing the need for better handling of crowded, dynamic pedestrian flows.", "method": "Proposes OMAN++ with two key innovations: 1) Relaxes standard one-to-one matching to one-to-many matching using social grouping prior, implemented via implicit context generator and O2M matcher; 2) Uses spatial-temporal displacement prior via displacement prior injector to enhance matching, feature extraction, and training.", "result": "OMAN++ outperforms state-of-the-art VIC baselines on SenseCrowd, CroHD, and MovingDroneCrowd benchmarks, and achieves 38.12% error reduction on the new WuhanMetroCrowd dataset, showing clear advantage in crowded scenes.", "conclusion": "The paper introduces a strong VIC baseline OMAN++ that effectively handles crowded scenes by leveraging social grouping and spatial-temporal displacement priors, with validation on a new challenging dataset featuring realistic metro commuting scenarios."}}
{"id": "2601.01200", "categories": ["cs.CV", "eess.IV"], "pdf": "https://arxiv.org/pdf/2601.01200", "abs": "https://arxiv.org/abs/2601.01200", "authors": ["Zhang Chen", "Shuai Wan", "Yuezhe Zhang", "Siyu Ren", "Fuzheng Yang", "Junhui Hou"], "title": "MS-ISSM: Objective Quality Assessment of Point Clouds Using Multi-scale Implicit Structural Similarity", "comment": null, "summary": "The unstructured and irregular nature of point clouds poses a significant challenge for objective quality assessment (PCQA), particularly in establishing accurate perceptual feature correspondence. To tackle this, we propose the Multi-scale Implicit Structural Similarity Measurement (MS-ISSM). Unlike traditional point-to-point matching, MS-ISSM utilizes Radial Basis Functions (RBF) to represent local features continuously, transforming distortion measurement into a comparison of implicit function coefficients. This approach effectively circumvents matching errors inherent in irregular data. Additionally, we propose a ResGrouped-MLP quality assessment network, which robustly maps multi-scale feature differences to perceptual scores. The network architecture departs from traditional flat MLPs by adopting a grouped encoding strategy integrated with Residual Blocks and Channel-wise Attention mechanisms. This hierarchical design allows the model to preserve the distinct physical semantics of luma, chroma, and geometry while adaptively focusing on the most salient distortion features across High, Medium, and Low scales. Experimental results on multiple benchmarks demonstrate that MS-ISSM outperforms state-of-the-art metrics in both reliability and generalization. The source code is available at: https://github.com/ZhangChen2022/MS-ISSM.", "AI": {"tldr": "MS-ISSM is a novel point cloud quality assessment method that uses Radial Basis Functions for implicit structural similarity measurement and a ResGrouped-MLP network for robust quality prediction.", "motivation": "Point clouds have unstructured and irregular nature which makes objective quality assessment challenging, especially in establishing accurate perceptual feature correspondence between reference and distorted point clouds.", "method": "Proposes Multi-scale Implicit Structural Similarity Measurement (MS-ISSM) using Radial Basis Functions to represent local features continuously, transforming distortion measurement into comparison of implicit function coefficients. Also introduces ResGrouped-MLP quality assessment network with grouped encoding strategy integrated with Residual Blocks and Channel-wise Attention mechanisms.", "result": "Experimental results on multiple benchmarks demonstrate that MS-ISSM outperforms state-of-the-art metrics in both reliability and generalization.", "conclusion": "The proposed MS-ISSM effectively addresses matching errors in irregular point cloud data and provides superior quality assessment through implicit structural similarity measurement and hierarchical network design."}}
{"id": "2601.01202", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.01202", "abs": "https://arxiv.org/abs/2601.01202", "authors": ["Jiazhu Dai", "Huihui Jiang"], "title": "RefSR-Adv: Adversarial Attack on Reference-based Image Super-Resolution Models", "comment": null, "summary": "Single Image Super-Resolution (SISR) aims to recover high-resolution images from low-resolution inputs. Unlike SISR, Reference-based Super-Resolution (RefSR) leverages an additional high-resolution reference image to facilitate the recovery of high-frequency textures. However, existing research mainly focuses on backdoor attacks targeting RefSR, while the vulnerability of the adversarial attacks targeting RefSR has not been fully explored. To fill this research gap, we propose RefSR-Adv, an adversarial attack that degrades SR outputs by perturbing only the reference image. By maximizing the difference between adversarial and clean outputs, RefSR-Adv induces significant performance degradation and generates severe artifacts across CNN, Transformer, and Mamba architectures on the CUFED5, WR-SR, and DRefSR datasets. Importantly, experiments confirm a positive correlation between the similarity of the low-resolution input and the reference image and attack effectiveness, revealing that the model's over-reliance on reference features is a key security flaw. This study reveals a security vulnerability in RefSR systems, aiming to urge researchers to pay attention to the robustness of RefSR.", "AI": {"tldr": "RefSR-Adv: An adversarial attack on Reference-based Super-Resolution that degrades SR outputs by perturbing only the reference image, revealing security vulnerabilities in RefSR systems.", "motivation": "Existing research focuses on backdoor attacks targeting RefSR, but the vulnerability to adversarial attacks has not been fully explored. The paper aims to fill this research gap and reveal security flaws in RefSR systems.", "method": "Proposes RefSR-Adv, an adversarial attack that maximizes the difference between adversarial and clean outputs by perturbing only the reference image. The attack is tested across CNN, Transformer, and Mamba architectures on multiple datasets.", "result": "RefSR-Adv induces significant performance degradation and generates severe artifacts across different architectures on CUFED5, WR-SR, and DRefSR datasets. Experiments show a positive correlation between input-reference similarity and attack effectiveness, revealing the model's over-reliance on reference features as a key security flaw.", "conclusion": "This study reveals a security vulnerability in RefSR systems where adversarial perturbations to reference images can severely degrade performance. The findings aim to urge researchers to pay attention to the robustness of RefSR models against such attacks."}}
{"id": "2601.01204", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.01204", "abs": "https://arxiv.org/abs/2601.01204", "authors": ["Zunhai Su", "Weihao Ye", "Hansen Feng", "Keyu Fan", "Jing Zhang", "Dahai Yu", "Zhengwu Liu", "Ngai Wong"], "title": "XStreamVGGT: Extremely Memory-Efficient Streaming Vision Geometry Grounded Transformer with KV Cache Compression", "comment": null, "summary": "Learning-based 3D visual geometry models have benefited substantially from large-scale transformers. Among these, StreamVGGT leverages frame-wise causal attention for strong streaming reconstruction, but suffers from unbounded KV cache growth, leading to escalating memory consumption and inference latency as input frames accumulate. We propose XStreamVGGT, a tuning-free approach that systematically compresses the KV cache through joint pruning and quantization, enabling extremely memory-efficient streaming inference. Specifically, redundant KVs originating from multi-view inputs are pruned through efficient token importance identification, enabling a fixed memory budget. Leveraging the unique distribution of KV tensors, we incorporate KV quantization to further reduce memory consumption. Extensive evaluations show that XStreamVGGT achieves mostly negligible performance degradation while substantially reducing memory usage by 4.42$\\times$ and accelerating inference by 5.48$\\times$, enabling scalable and practical streaming 3D applications. The code is available at https://github.com/ywh187/XStreamVGGT/.", "AI": {"tldr": "XStreamVGGT compresses KV cache in streaming 3D vision transformers via joint pruning and quantization, achieving 4.42\u00d7 memory reduction and 5.48\u00d7 speedup with minimal performance loss.", "motivation": "StreamVGGT's frame-wise causal attention causes unbounded KV cache growth, leading to escalating memory consumption and inference latency as frames accumulate, limiting practical streaming 3D applications.", "method": "Systematic KV cache compression through: 1) pruning redundant KVs from multi-view inputs via efficient token importance identification for fixed memory budget, and 2) KV quantization leveraging unique tensor distributions.", "result": "Achieves mostly negligible performance degradation while reducing memory usage by 4.42\u00d7 and accelerating inference by 5.48\u00d7, enabling scalable streaming 3D applications.", "conclusion": "XStreamVGGT provides a tuning-free approach for memory-efficient streaming inference in 3D visual geometry models, making streaming 3D applications more practical and scalable."}}
{"id": "2601.01210", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2601.01210", "abs": "https://arxiv.org/abs/2601.01210", "authors": ["Kazuhiko Murasaki", "Shunsuke Konagai", "Masakatsu Aoki", "Taiga Yoshida", "Ryuichi Tanida"], "title": "Real-Time LiDAR Point Cloud Densification for Low-Latency Spatial Data Transmission", "comment": null, "summary": "To realize low-latency spatial transmission system for immersive telepresence, there are two major problems: capturing dynamic 3D scene densely and processing them in real time. LiDAR sensors capture 3D in real time, but produce sparce point clouds. Therefore, this paper presents a high-speed LiDAR point cloud densification method to generate dense 3D scene with minimal latency, addressing the need for on-the-fly depth completion while maintaining real-time performance. Our approach combines multiple LiDAR inputs with high-resolution color images and applies a joint bilateral filtering strategy implemented through a convolutional neural network architecture. Experiments demonstrate that the proposed method produces dense depth maps at full HD resolution in real time (30 fps), which is over 15x faster than a recent training-based depth completion approach. The resulting dense point clouds exhibit accurate geometry without multiview inconsistencies or ghosting artifacts.", "AI": {"tldr": "High-speed LiDAR point cloud densification method for real-time dense 3D scene generation using joint bilateral filtering with CNN architecture.", "motivation": "To enable low-latency immersive telepresence systems by solving the problem of sparse LiDAR point clouds while maintaining real-time performance for dynamic 3D scene capture.", "method": "Combines multiple LiDAR inputs with high-resolution color images using joint bilateral filtering implemented through a convolutional neural network architecture for on-the-fly depth completion.", "result": "Achieves dense depth maps at full HD resolution in real time (30 fps), 15x faster than recent training-based approaches, with accurate geometry and no multiview inconsistencies or ghosting artifacts.", "conclusion": "The method successfully addresses the need for high-speed LiDAR point cloud densification for immersive telepresence applications, providing real-time dense 3D reconstruction with minimal latency."}}
{"id": "2601.01213", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.01213", "abs": "https://arxiv.org/abs/2601.01213", "authors": ["Riccardo Gelato", "Carlo Sgaravatti", "Jakob Grahn", "Giacomo Boracchi", "Filippo Maria Bianchi"], "title": "Promptable Foundation Models for SAR Remote Sensing: Adapting the Segment Anything Model for Snow Avalanche Segmentation", "comment": null, "summary": "Remote sensing solutions for avalanche segmentation and mapping are key to supporting risk forecasting and mitigation in mountain regions. Synthetic Aperture Radar (SAR) imagery from Sentinel-1 can be effectively used for this task, but training an effective detection model requires gathering a large dataset with high-quality annotations from domain experts, which is prohibitively time-consuming. In this work, we aim to facilitate and accelerate the annotation of SAR images for avalanche mapping. We build on the Segment Anything Model (SAM), a segmentation foundation model trained on natural images, and tailor it to Sentinel-1 SAR data. Adapting SAM to our use-case requires addressing several domain-specific challenges: (i) domain mismatch, since SAM was not trained on satellite/SAR imagery; (ii) input adaptation, because SAR products typically provide more than three channels, while SAM is constrained to RGB images; (iii) robustness to imprecise prompts that can affect target identification and degrade the segmentation quality, an issue exacerbated in small, low-contrast avalanches; and (iv) training efficiency, since standard fine-tuning is computationally demanding for SAM. We tackle these challenges through a combination of adapters to mitigate the domain gap, multiple encoders to handle multi-channel SAR inputs, prompt-engineering strategies to improve avalanche localization accuracy, and a training algorithm that limits the training time of the encoder, which is recognized as the major bottleneck. We integrate the resulting model into an annotation tool and show experimentally that it speeds up the annotation of SAR images.", "AI": {"tldr": "Adapting Segment Anything Model (SAM) to Sentinel-1 SAR data for faster avalanche annotation through domain adaptation, multi-channel handling, prompt engineering, and efficient training.", "motivation": "Avalanche segmentation in SAR imagery requires expert annotations which are time-consuming. Need to accelerate annotation process for avalanche mapping using Sentinel-1 SAR data.", "method": "Adapt SAM foundation model to SAR domain using: (1) adapters to reduce domain gap, (2) multiple encoders for multi-channel SAR inputs, (3) prompt-engineering for better avalanche localization, (4) efficient training algorithm focusing on encoder bottleneck.", "result": "Developed an integrated annotation tool that speeds up SAR image annotation for avalanche mapping, addressing domain-specific challenges of SAR data.", "conclusion": "Successfully adapted SAM to Sentinel-1 SAR data for avalanche segmentation, creating a practical tool that accelerates expert annotation workflow."}}
{"id": "2601.01222", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.01222", "abs": "https://arxiv.org/abs/2601.01222", "authors": ["Mengfei Li", "Peng Li", "Zheng Zhang", "Jiahao Lu", "Chengfeng Zhao", "Wei Xue", "Qifeng Liu", "Sida Peng", "Wenxiao Zhang", "Wenhan Luo", "Yuan Liu", "Yike Guo"], "title": "UniSH: Unifying Scene and Human Reconstruction in a Feed-Forward Pass", "comment": null, "summary": "We present UniSH, a unified, feed-forward framework for joint metric-scale 3D scene and human reconstruction. A key challenge in this domain is the scarcity of large-scale, annotated real-world data, forcing a reliance on synthetic datasets. This reliance introduces a significant sim-to-real domain gap, leading to poor generalization, low-fidelity human geometry, and poor alignment on in-the-wild videos. To address this, we propose an innovative training paradigm that effectively leverages unlabeled in-the-wild data. Our framework bridges strong, disparate priors from scene reconstruction and HMR, and is trained with two core components: (1) a robust distillation strategy to refine human surface details by distilling high-frequency details from an expert depth model, and (2) a two-stage supervision scheme, which first learns coarse localization on synthetic data, then fine-tunes on real data by directly optimizing the geometric correspondence between the SMPL mesh and the human point cloud. This approach enables our feed-forward model to jointly recover high-fidelity scene geometry, human point clouds, camera parameters, and coherent, metric-scale SMPL bodies, all in a single forward pass. Extensive experiments demonstrate that our model achieves state-of-the-art performance on human-centric scene reconstruction and delivers highly competitive results on global human motion estimation, comparing favorably against both optimization-based frameworks and HMR-only methods. Project page: https://murphylmf.github.io/UniSH/", "AI": {"tldr": "UniSH is a unified feed-forward framework for joint metric-scale 3D scene and human reconstruction that addresses sim-to-real domain gaps through innovative training with unlabeled real-world data.", "motivation": "The key challenge is the scarcity of large-scale annotated real-world data, forcing reliance on synthetic datasets which creates significant sim-to-real domain gaps, leading to poor generalization, low-fidelity human geometry, and poor alignment on in-the-wild videos.", "method": "Proposes a training paradigm leveraging unlabeled in-the-wild data with two core components: (1) robust distillation strategy to refine human surface details by distilling high-frequency details from an expert depth model, and (2) two-stage supervision scheme that first learns coarse localization on synthetic data, then fine-tunes on real data by optimizing geometric correspondence between SMPL mesh and human point cloud.", "result": "The model achieves state-of-the-art performance on human-centric scene reconstruction and delivers highly competitive results on global human motion estimation, comparing favorably against both optimization-based frameworks and HMR-only methods.", "conclusion": "UniSH enables joint recovery of high-fidelity scene geometry, human point clouds, camera parameters, and coherent metric-scale SMPL bodies in a single forward pass, effectively bridging the sim-to-real domain gap for 3D scene and human reconstruction."}}
{"id": "2601.01224", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.01224", "abs": "https://arxiv.org/abs/2601.01224", "authors": ["Bac Nguyen", "Yuhta Takida", "Naoki Murata", "Chieh-Hsin Lai", "Toshimitsu Uesaka", "Stefano Ermon", "Yuki Mitsufuji"], "title": "Improved Object-Centric Diffusion Learning with Registers and Contrastive Alignment", "comment": null, "summary": "Slot Attention (SA) with pretrained diffusion models has recently shown promise for object-centric learning (OCL), but suffers from slot entanglement and weak alignment between object slots and image content. We propose Contrastive Object-centric Diffusion Alignment (CODA), a simple extension that (i) employs register slots to absorb residual attention and reduce interference between object slots, and (ii) applies a contrastive alignment loss to explicitly encourage slot-image correspondence. The resulting training objective serves as a tractable surrogate for maximizing mutual information (MI) between slots and inputs, strengthening slot representation quality. On both synthetic (MOVi-C/E) and real-world datasets (VOC, COCO), CODA improves object discovery (e.g., +6.1% FG-ARI on COCO), property prediction, and compositional image generation over strong baselines. Register slots add negligible overhead, keeping CODA efficient and scalable. These results indicate potential applications of CODA as an effective framework for robust OCL in complex, real-world scenes.", "AI": {"tldr": "CODA improves Slot Attention with diffusion models by adding register slots to reduce slot entanglement and using contrastive alignment to strengthen slot-image correspondence, achieving better object discovery and generation.", "motivation": "Slot Attention with pretrained diffusion models suffers from slot entanglement and weak alignment between object slots and image content, limiting its effectiveness for object-centric learning in complex scenes.", "method": "CODA introduces two key components: (1) register slots that absorb residual attention to reduce interference between object slots, and (2) a contrastive alignment loss to explicitly encourage slot-image correspondence, serving as a tractable surrogate for maximizing mutual information between slots and inputs.", "result": "CODA improves object discovery (+6.1% FG-ARI on COCO), property prediction, and compositional image generation over strong baselines on both synthetic (MOVi-C/E) and real-world datasets (VOC, COCO), with register slots adding negligible overhead.", "conclusion": "CODA provides an effective and efficient framework for robust object-centric learning in complex, real-world scenes, demonstrating potential applications for improving slot representation quality and alignment."}}
{"id": "2601.01228", "categories": ["cs.CV", "math.NA"], "pdf": "https://arxiv.org/pdf/2601.01228", "abs": "https://arxiv.org/abs/2601.01228", "authors": ["Markus Haltmeier", "Lukas Neumann", "Nadja Gruber", "Johannes Schwab", "Gyeongha Hwang"], "title": "HyDRA: Hybrid Denoising Regularization for Measurement-Only DEQ Training", "comment": null, "summary": "Solving image reconstruction problems of the form \\(\\mathbf{A} \\mathbf{x} = \\mathbf{y}\\) remains challenging due to ill-posedness and the lack of large-scale supervised datasets. Deep Equilibrium (DEQ) models have been used successfully but typically require supervised pairs \\((\\mathbf{x},\\mathbf{y})\\). In many practical settings, only measurements \\(\\mathbf{y}\\) are available. We introduce HyDRA (Hybrid Denoising Regularization Adaptation), a measurement-only framework for DEQ training that combines measurement consistency with an adaptive denoising regularization term, together with a data-driven early stopping criterion. Experiments on sparse-view CT demonstrate competitive reconstruction quality and fast inference.", "AI": {"tldr": "HyDRA enables training Deep Equilibrium models without ground truth data by combining measurement consistency with adaptive denoising regularization and data-driven early stopping.", "motivation": "Image reconstruction problems are challenging due to ill-posedness and lack of large supervised datasets. Traditional DEQ models require supervised pairs (x,y), but in practice only measurements y are available.", "method": "HyDRA combines measurement consistency with adaptive denoising regularization and uses a data-driven early stopping criterion. It's a measurement-only framework for DEQ training.", "result": "Experiments on sparse-view CT demonstrate competitive reconstruction quality and fast inference compared to supervised methods.", "conclusion": "HyDRA provides an effective measurement-only training approach for DEQ models that doesn't require ground truth data, making it practical for real-world applications where only measurements are available."}}
{"id": "2601.01240", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.01240", "abs": "https://arxiv.org/abs/2601.01240", "authors": ["Ziqian Guan", "Xieyi Fu", "Yuting Wang", "Haowen Xiao", "Jiarui Zhu", "Yingying Zhu", "Yongtao Liu", "Lin Gu"], "title": "RFAssigner: A Generic Label Assignment Strategy for Dense Object Detection", "comment": null, "summary": "Label assignment is a critical component in training dense object detectors. State-of-the-art methods typically assign each training sample a positive and a negative weight, optimizing the assignment scheme during training. However, these strategies often assign an insufficient number of positive samples to small objects, leading to a scale imbalance during training. To address this limitation, we introduce RFAssigner, a novel assignment strategy designed to enhance the multi-scale learning capabilities of dense detectors. RFAssigner first establishes an initial set of positive samples using a point-based prior. It then leverages a Gaussian Receptive Field (GRF) distance to measure the similarity between the GRFs of unassigned candidate locations and the ground-truth objects. Based on this metric, RFAssigner adaptively selects supplementary positive samples from the unassigned pool, promoting a more balanced learning process across object scales. Comprehensive experiments on three datasets with distinct object scale distributions validate the effectiveness and generalizability of our method. Notably, a single FCOS-ResNet-50 detector equipped with RFAssigner achieves state-of-the-art performance across all object scales, consistently outperforming existing strategies without requiring auxiliary modules or heuristics.", "AI": {"tldr": "RFAssigner is a novel label assignment strategy for dense object detectors that addresses scale imbalance by adaptively selecting supplementary positive samples for small objects using Gaussian Receptive Field similarity.", "motivation": "Current label assignment methods in dense object detectors often assign insufficient positive samples to small objects, creating a scale imbalance during training that hinders performance across different object sizes.", "method": "RFAssigner first establishes initial positive samples using point-based prior, then measures similarity between unassigned candidate locations and ground-truth objects using Gaussian Receptive Field (GRF) distance, and adaptively selects supplementary positive samples from unassigned pool to balance learning across scales.", "result": "Experiments on three datasets with distinct object scale distributions show RFAssigner achieves state-of-the-art performance across all object scales. A single FCOS-ResNet-50 detector with RFAssigner consistently outperforms existing strategies without needing auxiliary modules or heuristics.", "conclusion": "RFAssigner effectively addresses scale imbalance in dense object detectors through adaptive label assignment based on Gaussian Receptive Field similarity, demonstrating strong generalizability and performance improvements across diverse object scale distributions."}}
{"id": "2601.01260", "categories": ["cs.CV", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.01260", "abs": "https://arxiv.org/abs/2601.01260", "authors": ["Hamad Khan", "Saddam Hussain Khan"], "title": "MambaFormer: Token-Level Guided Routing Mixture-of-Experts for Accurate and Efficient Clinical Assistance", "comment": "28 Pages, Tables 12, Figure 09", "summary": "The deployment of large language models (LLMs) in real-world clinical applications is constrained by the fundamental trade-off between computational cost and the efficiency of linear-time models. To address this, we propose an LLM-based MambaFormer hybrid Mixture-of-Experts (MoE) framework for efficient medical question-answering (QA) and clinical assistance. The MambaFormer employs a lightweight gating mechanism that performs token-level dynamic routing to a customized Transformer expert (ET5) for short, complex queries or to a State Space Model expert (EMamba) for long, high-throughput sequences. The customized EMamba and ET5 models are tailored to accommodate input sequence dimensionality, embedding structure, sequence length, and target-specific output heads, and are fine-tuned through transfer learning on a new, custom-designed DentalQA dataset. Moreover, intelligent routing decisions are driven by the contextual complexity of token embeddings, normalized sequence length, and domain-aware features, thereby enforcing a Pareto-optimal trade-off between inference latency and prediction accuracy. Furthermore, a novel utility-guided multi-objective loss jointly optimizes decisions, router parameters, routing behavior, expert utilization, and computational cost by adaptively regulating token-level expert activation. Finally, the proposed MambaFormer is cross-validated (holdout) for medical QA on the new, custom-designed DentalQA and PubMedQA datasets and compared with state-of-the-art techniques. The proposed MambaFormer outperforms (BERTScore = 0.9180) with ultra-low latency (0.077 s), delivering a 24.4 speedup over T5-Large and establishing a scalable solution for resource-constrained clinical deployment.", "AI": {"tldr": "LLM-based MambaFormer hybrid MoE framework for efficient medical QA using dynamic routing between Transformer and State Space Model experts optimized for computational cost vs. accuracy trade-off.", "motivation": "Address the computational cost vs. efficiency trade-off that constrains LLM deployment in real-world clinical applications, enabling scalable solutions for resource-constrained clinical environments.", "method": "Propose MambaFormer hybrid MoE framework with lightweight gating mechanism for token-level dynamic routing: ET5 Transformer expert for short complex queries, EMamba State Space Model expert for long high-throughput sequences. Customized experts tailored to input characteristics, fine-tuned on DentalQA dataset. Intelligent routing based on contextual complexity, normalized sequence length, and domain-aware features. Novel utility-guided multi-objective loss jointly optimizes routing decisions, parameters, behavior, expert utilization, and computational cost.", "result": "Outperforms state-of-the-art with BERTScore = 0.9180 and ultra-low latency (0.077 s), achieving 24.4\u00d7 speedup over T5-Large on DentalQA and PubMedQA datasets, demonstrating superior efficiency-accuracy trade-off.", "conclusion": "MambaFormer establishes a scalable, efficient solution for clinical LLM deployment by intelligently routing queries to specialized experts, achieving Pareto-optimal trade-off between inference latency and prediction accuracy for resource-constrained clinical environments."}}
{"id": "2601.01281", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.01281", "abs": "https://arxiv.org/abs/2601.01281", "authors": ["Sifatullah Sheikh Urmi", "Kirtonia Nuzath Tabassum Arthi", "Md Al-Imran"], "title": "AI-Powered Deepfake Detection Using CNN and Vision Transformer Architectures", "comment": "6 pages, 6 figures, 3 tables. Conference paper", "summary": "The increasing use of artificial intelligence generated deepfakes creates major challenges in maintaining digital authenticity. Four AI-based models, consisting of three CNNs and one Vision Transformer, were evaluated using large face image datasets. Data preprocessing and augmentation techniques improved model performance across different scenarios. VFDNET demonstrated superior accuracy with MobileNetV3, showing efficient performance, thereby demonstrating AI's capabilities for dependable deepfake detection.", "AI": {"tldr": "AI models (3 CNNs + 1 Vision Transformer) evaluated for deepfake detection, with VFDNET+MobileNetV3 showing best accuracy and efficiency.", "motivation": "The increasing prevalence of AI-generated deepfakes poses significant threats to digital authenticity and trust, necessitating reliable detection methods.", "method": "Evaluated four AI models (three convolutional neural networks and one Vision Transformer) using large face image datasets, with data preprocessing and augmentation techniques to enhance performance.", "result": "VFDNET with MobileNetV3 architecture demonstrated superior accuracy and efficient performance across different scenarios, outperforming other evaluated models.", "conclusion": "AI-based approaches, particularly the VFDNET+MobileNetV3 combination, show strong capabilities for dependable deepfake detection, offering promising solutions to maintain digital authenticity."}}
{"id": "2601.01285", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.01285", "abs": "https://arxiv.org/abs/2601.01285", "authors": ["Md. Sanaullah Chowdhury Lameya Sabrin"], "title": "S2M-Net: Spectral-Spatial Mixing for Medical Image Segmentation with Morphology-Aware Adaptive Loss", "comment": null, "summary": "Medical image segmentation requires balancing local precision for boundary-critical clinical applications, global context for anatomical coherence, and computational efficiency for deployment on limited data and hardware a trilemma that existing architectures fail to resolve. Although convolutional networks provide local precision at $\\mathcal{O}(n)$ cost but limited receptive fields, vision transformers achieve global context through $\\mathcal{O}(n^2)$ self-attention at prohibitive computational expense, causing overfitting on small clinical datasets. We propose S2M-Net, a 4.7M-parameter architecture that achieves $\\mathcal{O}(HW \\log HW)$ global context through two synergistic innovations: (i) Spectral-Selective Token Mixer (SSTM), which exploits the spectral concentration of medical images via truncated 2D FFT with learnable frequency filtering and content-gated spatial projection, avoiding quadratic attention cost while maintaining global receptive fields; and (ii) Morphology-Aware Adaptive Segmentation Loss (MASL), which automatically analyzes structure characteristics (compactness, tubularity, irregularity, scale) to modulate five complementary loss components through constrained learnable weights, eliminating manual per-dataset tuning. Comprehensive evaluation in 16 medical imaging datasets that span 8 modalities demonstrates state-of-the-art performance: 96.12\\% Dice on polyp segmentation, 83.77\\% on surgical instruments (+17.85\\% over the prior art) and 80.90\\% on brain tumors, with consistent 3-18\\% improvements over specialized baselines while using 3.5--6$\\times$ fewer parameters than transformer-based methods.", "AI": {"tldr": "S2M-Net is a lightweight medical image segmentation network that achieves global context with O(HW log HW) complexity using spectral token mixing and adaptive loss, outperforming transformers with 3.5-6x fewer parameters.", "motivation": "Existing architectures fail to balance local precision, global context, and computational efficiency for medical image segmentation. Convolutional networks have limited receptive fields, while vision transformers have prohibitive O(n\u00b2) computational cost that causes overfitting on small clinical datasets.", "method": "Two key innovations: (1) Spectral-Selective Token Mixer (SSTM) uses truncated 2D FFT with learnable frequency filtering and content-gated spatial projection for O(HW log HW) global context; (2) Morphology-Aware Adaptive Segmentation Loss (MASL) automatically analyzes structure characteristics to modulate five complementary loss components with constrained learnable weights.", "result": "State-of-the-art performance across 16 medical imaging datasets spanning 8 modalities: 96.12% Dice on polyp segmentation, 83.77% on surgical instruments (+17.85% over prior art), 80.90% on brain tumors, with consistent 3-18% improvements over specialized baselines using only 4.7M parameters.", "conclusion": "S2M-Net resolves the trilemma of medical image segmentation by providing efficient global context through spectral processing and adaptive loss tuning, achieving superior performance with significantly fewer parameters than transformer-based methods."}}
{"id": "2601.01312", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.01312", "abs": "https://arxiv.org/abs/2601.01312", "authors": ["Kailash A. Hambarde", "Hugo Proen\u00e7a", "Md Rashidunnabi", "Pranita Samale", "Qiwei Yang", "Pingping Zhang", "Zijing Gong", "Yuhao Wang", "Xi Zhang", "Ruoshui Qu", "Qiaoyun He", "Yuhang Zhang", "Thi Ngoc Ha Nguyen", "Tien-Dung Mai", "Cheng-Jun Kang", "Yu-Fan Lin", "Jin-Hui Jiang", "Chih-Chung Hsu", "Tam\u00e1s Endrei", "Gy\u00f6rgy Cserey", "Ashwat Rajbhandari"], "title": "VReID-XFD: Video-based Person Re-identification at Extreme Far Distance Challenge Results", "comment": null, "summary": "Person re-identification (ReID) across aerial and ground views at extreme far distances introduces a distinct operating regime where severe resolution degradation, extreme viewpoint changes, unstable motion cues, and clothing variation jointly undermine the appearance-based assumptions of existing ReID systems. To study this regime, we introduce VReID-XFD, a video-based benchmark and community challenge for extreme far-distance (XFD) aerial-to-ground person re-identification. VReID-XFD is derived from the DetReIDX dataset and comprises 371 identities, 11,288 tracklets, and 11.75 million frames, captured across altitudes from 5.8 m to 120 m, viewing angles from oblique (30 degrees) to nadir (90 degrees), and horizontal distances up to 120 m. The benchmark supports aerial-to-aerial, aerial-to-ground, and ground-to-aerial evaluation under strict identity-disjoint splits, with rich physical metadata. The VReID-XFD-25 Challenge attracted 10 teams with hundreds of submissions. Systematic analysis reveals monotonic performance degradation with altitude and distance, a universal disadvantage of nadir views, and a trade-off between peak performance and robustness. Even the best-performing SAS-PReID method achieves only 43.93 percent mAP in the aerial-to-ground setting. The dataset, annotations, and official evaluation protocols are publicly available at https://www.it.ubi.pt/DetReIDX/ .", "AI": {"tldr": "VReID-XFD is a new benchmark for extreme far-distance aerial-to-ground person re-identification, featuring 371 identities across 11.75M frames with challenging conditions like severe resolution degradation and extreme viewpoint changes.", "motivation": "Existing person re-identification systems fail in extreme far-distance scenarios where severe resolution degradation, extreme viewpoint changes, unstable motion cues, and clothing variation undermine appearance-based assumptions.", "method": "Created VReID-XFD benchmark from DetReIDX dataset with 371 identities, 11,288 tracklets, and 11.75M frames captured across altitudes (5.8m-120m), viewing angles (30\u00b0-90\u00b0), and distances up to 120m. Includes aerial-to-aerial, aerial-to-ground, and ground-to-aerial evaluation with strict identity-disjoint splits.", "result": "The VReID-XFD-25 Challenge attracted 10 teams with hundreds of submissions. Analysis shows monotonic performance degradation with altitude/distance, universal disadvantage of nadir views, and trade-off between peak performance and robustness. Best method (SAS-PReID) achieved only 43.93% mAP in aerial-to-ground setting.", "conclusion": "Extreme far-distance aerial-to-ground person re-identification remains extremely challenging, with current methods performing poorly. The VReID-XFD benchmark provides a standardized testbed for advancing research in this difficult domain."}}
{"id": "2601.01322", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.MM", "eess.IV"], "pdf": "https://arxiv.org/pdf/2601.01322", "abs": "https://arxiv.org/abs/2601.01322", "authors": ["Hongjie Wang", "Niraj K. Jha"], "title": "LinMU: Multimodal Understanding Made Linear", "comment": "23 pages, 7 figures", "summary": "Modern Vision-Language Models (VLMs) achieve impressive performance but are limited by the quadratic complexity of self-attention, which prevents their deployment on edge devices and makes their understanding of high-resolution images and long-context videos prohibitively expensive. To address this challenge, we introduce LinMU (Linear-complexity Multimodal Understanding), a VLM design that achieves linear complexity without using any quadratic-complexity modules while maintaining the performance of global-attention-based VLMs. LinMU replaces every self-attention layer in the VLM with the M-MATE block: a dual-branch module that combines a bidirectional state-space model for global context (Flex-MA branch) with localized Swin-style window attention (Local-Swin branch) for adjacent correlations. To transform a pre-trained VLM into the LinMU architecture, we propose a three-stage distillation framework that (i) initializes both branches with self-attention weights and trains the Flex-MA branch alone, (ii) unfreezes the Local-Swin branch and fine-tunes it jointly with the Flex-MA branch, and (iii) unfreezes the remaining blocks and fine-tunes them using LoRA adapters, while regressing on hidden states and token-level logits of the frozen VLM teacher. On MMMU, TextVQA, LongVideoBench, Video-MME, and other benchmarks, LinMU matches the performance of teacher models, yet reduces Time-To-First-Token (TTFT) by up to 2.7$\\times$ and improves token throughput by up to 9.0$\\times$ on minute-length videos. Ablations confirm the importance of each distillation stage and the necessity of the two branches of the M-MATE block. The proposed framework demonstrates that state-of-the-art multimodal reasoning can be achieved without quadratic attention, thus opening up avenues for long-context VLMs that can deal with high-resolution images and long videos.", "AI": {"tldr": "LinMU is a linear-complexity Vision-Language Model that replaces quadratic self-attention with a dual-branch M-MATE block, achieving similar performance to global-attention VLMs while being 2.7-9.0\u00d7 faster for long videos.", "motivation": "Current VLMs suffer from quadratic complexity of self-attention, making them expensive for high-resolution images and long videos, and preventing deployment on edge devices.", "method": "Replace self-attention layers with M-MATE blocks (Flex-MA branch for global context via bidirectional SSM + Local-Swin branch for adjacent correlations), using three-stage distillation: initialize branches with self-attention weights, train Flex-MA alone, then jointly fine-tune both branches, finally fine-tune remaining blocks with LoRA while regressing on teacher's hidden states and logits.", "result": "Matches teacher model performance on MMMU, TextVQA, LongVideoBench, Video-MME; reduces TTFT by up to 2.7\u00d7 and improves token throughput by up to 9.0\u00d7 on minute-length videos.", "conclusion": "State-of-the-art multimodal reasoning can be achieved without quadratic attention, enabling efficient long-context VLMs for high-resolution images and videos."}}
{"id": "2601.01339", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.01339", "abs": "https://arxiv.org/abs/2601.01339", "authors": ["Weihang You", "Hanqi Jiang", "Yi Pan", "Junhao Chen", "Tianming Liu", "Fei Dou"], "title": "Achieving Fine-grained Cross-modal Understanding through Brain-inspired Hierarchical Representation Learning", "comment": null, "summary": "Understanding neural responses to visual stimuli remains challenging due to the inherent complexity of brain representations and the modality gap between neural data and visual inputs. Existing methods, mainly based on reducing neural decoding to generation tasks or simple correlations, fail to reflect the hierarchical and temporal processes of visual processing in the brain. To address these limitations, we present NeuroAlign, a novel framework for fine-grained fMRI-video alignment inspired by the hierarchical organization of the human visual system. Our framework implements a two-stage mechanism that mirrors biological visual pathways: global semantic understanding through Neural-Temporal Contrastive Learning (NTCL) and fine-grained pattern matching through enhanced vector quantization. NTCL explicitly models temporal dynamics through bidirectional prediction between modalities, while our DynaSyncMM-EMA approach enables dynamic multi-modal fusion with adaptive weighting. Experiments demonstrate that NeuroAlign significantly outperforms existing methods in cross-modal retrieval tasks, establishing a new paradigm for understanding visual cognitive mechanisms.", "AI": {"tldr": "NeuroAlign is a novel fMRI-video alignment framework that mimics the hierarchical visual system, using neural-temporal contrastive learning and enhanced vector quantization for better cross-modal retrieval.", "motivation": "Existing methods fail to capture the hierarchical and temporal nature of visual processing in the brain, reducing neural decoding to simple generation tasks or correlations without reflecting biological visual pathways.", "method": "Two-stage framework: 1) Global semantic understanding via Neural-Temporal Contrastive Learning (NTCL) with bidirectional prediction between modalities, and 2) Fine-grained pattern matching through enhanced vector quantization with DynaSyncMM-EMA for dynamic multi-modal fusion with adaptive weighting.", "result": "NeuroAlign significantly outperforms existing methods in cross-modal retrieval tasks, demonstrating superior alignment between fMRI data and video stimuli.", "conclusion": "The framework establishes a new paradigm for understanding visual cognitive mechanisms by better modeling the hierarchical and temporal processes of the human visual system."}}
{"id": "2601.01352", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.01352", "abs": "https://arxiv.org/abs/2601.01352", "authors": ["Yixuan Lai", "He Wang", "Kun Zhou", "Tianjia Shao"], "title": "Slot-ID: Identity-Preserving Video Generation from Reference Videos via Slot-Based Temporal Identity Encoding", "comment": null, "summary": "Producing prompt-faithful videos that preserve a user-specified identity remains challenging: models need to extrapolate facial dynamics from sparse reference while balancing the tension between identity preservation and motion naturalness. Conditioning on a single image completely ignores the temporal signature, which leads to pose-locked motions, unnatural warping, and \"average\" faces when viewpoints and expressions change. To this end, we introduce an identity-conditioned variant of a diffusion-transformer video generator which uses a short reference video rather than a single portrait. Our key idea is to incorporate the dynamics in the reference. A short clip reveals subject-specific patterns, e.g., how smiles form, across poses and lighting. From this clip, a Sinkhorn-routed encoder learns compact identity tokens that capture characteristic dynamics while remaining pretrained backbone-compatible. Despite adding only lightweight conditioning, the approach consistently improves identity retention under large pose changes and expressive facial behavior, while maintaining prompt faithfulness and visual realism across diverse subjects and prompts.", "AI": {"tldr": "A video generation method that uses short reference videos instead of single images to better preserve subject identity and natural motion in generated videos.", "motivation": "Current methods using single reference images fail to capture temporal dynamics, leading to pose-locked motions, unnatural warping, and generic \"average\" faces when viewpoints and expressions change. There's a need to balance identity preservation with motion naturalness.", "method": "Introduces an identity-conditioned diffusion-transformer video generator that uses short reference videos. A Sinkhorn-routed encoder learns compact identity tokens from the reference clip that capture subject-specific dynamics while remaining compatible with pretrained backbones.", "result": "The approach consistently improves identity retention under large pose changes and expressive facial behavior, while maintaining prompt faithfulness and visual realism across diverse subjects and prompts.", "conclusion": "Using short reference videos with learned identity tokens that capture temporal dynamics significantly improves identity preservation in generated videos while maintaining natural motion and prompt faithfulness."}}
{"id": "2601.01356", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.01356", "abs": "https://arxiv.org/abs/2601.01356", "authors": ["Dang H. Pham", "Tu N. Nguyen", "Hoa N. Nguyen"], "title": "Advanced Machine Learning Approaches for Enhancing Person Re-Identification Performance", "comment": "in Vietnamese language", "summary": "Person re-identification (ReID) plays a critical role in intelligent surveillance systems by linking identities across multiple cameras in complex environments. However, ReID faces significant challenges such as appearance variations, domain shifts, and limited labeled data. This dissertation proposes three advanced approaches to enhance ReID performance under supervised, unsupervised domain adaptation (UDA), and fully unsupervised settings. First, SCM-ReID integrates supervised contrastive learning with hybrid loss optimization (classification, center, triplet, and centroid-triplet losses), improving discriminative feature representation and achieving state-of-the-art accuracy on Market-1501 and CUHK03 datasets. Second, for UDA, IQAGA and DAPRH combine GAN-based image augmentation, domain-invariant mapping, and pseudo-label refinement to mitigate domain discrepancies and enhance cross-domain generalization. Experiments demonstrate substantial gains over baseline methods, with mAP and Rank-1 improvements up to 12% in challenging transfer scenarios. Finally, ViTC-UReID leverages Vision Transformer-based feature encoding and camera-aware proxy learning to boost unsupervised ReID. By integrating global and local attention with camera identity constraints, this method significantly outperforms existing unsupervised approaches on large-scale benchmarks. Comprehensive evaluations across CUHK03, Market-1501, DukeMTMC-reID, and MSMT17 confirm the effectiveness of the proposed methods. The contributions advance ReID research by addressing key limitations in feature learning, domain adaptation, and label noise handling, paving the way for robust deployment in real-world surveillance systems.", "AI": {"tldr": "This dissertation proposes three advanced person re-identification methods for supervised, unsupervised domain adaptation, and fully unsupervised settings, achieving state-of-the-art performance on major benchmarks.", "motivation": "Person re-identification faces challenges including appearance variations, domain shifts, and limited labeled data, which hinder robust deployment in real-world surveillance systems.", "method": "Three approaches: 1) SCM-ReID with supervised contrastive learning and hybrid loss optimization; 2) IQAGA and DAPRH for UDA using GAN-based augmentation and pseudo-label refinement; 3) ViTC-UReID with Vision Transformer encoding and camera-aware proxy learning.", "result": "Achieved state-of-the-art accuracy on Market-1501 and CUHK03 datasets, with up to 12% mAP/Rank-1 improvements in cross-domain scenarios, and outperformed existing unsupervised approaches on large-scale benchmarks.", "conclusion": "The proposed methods advance ReID research by addressing feature learning, domain adaptation, and label noise limitations, enabling more robust deployment in real-world surveillance systems."}}
{"id": "2601.01360", "categories": ["cs.CV", "cs.HC"], "pdf": "https://arxiv.org/pdf/2601.01360", "abs": "https://arxiv.org/abs/2601.01360", "authors": ["Jiawei Fang", "Ruonan Zheng", "Xiaoxia Gao", "Shifan Jiang", "Anjun Chen", "Qi Ye", "Shihui Guo"], "title": "Garment Inertial Denoiser (GID): Endowing Accurate Motion Capture via Loose IMU Denoiser", "comment": "11 pages, 4 figures", "summary": "Wearable inertial motion capture (MoCap) provides a portable, occlusion-free, and privacy-preserving alternative to camera-based systems, but its accuracy depends on tightly attached sensors - an intrusive and uncomfortable requirement for daily use. Embedding IMUs into loose-fitting garments is a desirable alternative, yet sensor-body displacement introduces severe, structured, and location-dependent corruption that breaks standard inertial pipelines. We propose GID (Garment Inertial Denoiser), a lightweight, plug-and-play Transformer that factorizes loose-wear MoCap into three stages: (i) location-specific denoising, (ii) adaptive cross-wear fusion, and (iii) general pose prediction. GID uses a location-aware expert architecture, where a shared spatio-temporal backbone models global motion while per-IMU expert heads specialize in local garment dynamics, and a lightweight fusion module ensures cross-part consistency. This inductive bias enables stable training and effective learning from limited paired loose-tight IMU data. We also introduce GarMoCap, a combined public and newly collected dataset covering diverse users, motions, and garments. Experiments show that GID enables accurate, real-time denoising from single-user training and generalizes across unseen users, motions, and garment types, consistently improving state-of-the-art inertial MoCap methods when used as a drop-in module.", "AI": {"tldr": "GID is a lightweight Transformer-based system that enables accurate inertial motion capture with loose-fitting garments by denoising sensor-body displacement artifacts through location-aware expert architecture and cross-wear fusion.", "motivation": "Traditional wearable inertial MoCap requires tightly attached sensors which are intrusive and uncomfortable for daily use. Loose-fitting garments with embedded IMUs are desirable but introduce severe, structured corruption from sensor-body displacement that breaks standard inertial pipelines.", "method": "GID uses a three-stage approach: (1) location-specific denoising, (2) adaptive cross-wear fusion, and (3) general pose prediction. It employs a location-aware expert architecture with shared spatio-temporal backbone for global motion and per-IMU expert heads for local garment dynamics, plus a lightweight fusion module for cross-part consistency.", "result": "GID enables accurate, real-time denoising from single-user training and generalizes across unseen users, motions, and garment types. It consistently improves state-of-the-art inertial MoCap methods when used as a drop-in module.", "conclusion": "GID provides an effective solution for loose-wear inertial MoCap by addressing sensor-body displacement through specialized denoising architecture, enabling comfortable, privacy-preserving motion capture suitable for daily use."}}
{"id": "2601.01364", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.01364", "abs": "https://arxiv.org/abs/2601.01364", "authors": ["Mostofa Rafid Uddin", "Mahek Vora", "Qifeng Wu", "Muyuan Chen", "Min Xu"], "title": "Unsupervised SE(3) Disentanglement for in situ Macromolecular Morphology Identification from Cryo-Electron Tomography", "comment": null, "summary": "Cryo-electron tomography (cryo-ET) provides direct 3D visualization of macromolecules inside the cell, enabling analysis of their in situ morphology. This morphology can be regarded as an SE(3)-invariant, denoised volumetric representation of subvolumes extracted from tomograms. Inferring morphology is therefore an inverse problem of estimating both a template morphology and its SE(3) transformation. Existing expectation-maximization based solution to this problem often misses rare but important morphologies and requires extensive manual hyperparameter tuning. Addressing this issue, we present a disentangled deep representation learning framework that separates SE(3) transformations from morphological content in the representation space. The framework includes a novel multi-choice learning module that enables this disentanglement for highly noisy cryo-ET data, and the learned morphological content is used to generate template morphologies. Experiments on simulated and real cryo-ET datasets demonstrate clear improvements over prior methods, including the discovery of previously unidentified macromolecular morphologies.", "AI": {"tldr": "A disentangled deep learning framework for cryo-ET that separates SE(3) transformations from morphological content, enabling discovery of rare macromolecular morphologies without extensive manual tuning.", "motivation": "Existing expectation-maximization methods for cryo-ET often miss rare but important macromolecular morphologies and require extensive manual hyperparameter tuning, limiting their effectiveness in discovering novel biological structures.", "method": "A disentangled deep representation learning framework with a novel multi-choice learning module that separates SE(3) transformations from morphological content in the representation space, using learned morphological content to generate template morphologies.", "result": "Experiments on simulated and real cryo-ET datasets demonstrate clear improvements over prior methods, including the discovery of previously unidentified macromolecular morphologies.", "conclusion": "The proposed framework effectively addresses limitations of existing methods by enabling automated discovery of rare macromolecular morphologies in cryo-ET data through disentangled representation learning."}}
{"id": "2601.01386", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.01386", "abs": "https://arxiv.org/abs/2601.01386", "authors": ["Xiaobao Wei", "Zhangjie Ye", "Yuxiang Gu", "Zunjie Zhu", "Yunfei Guo", "Yingying Shen", "Shan Zhao", "Ming Lu", "Haiyang Sun", "Bing Wang", "Guang Chen", "Rongfeng Lu", "Hangjun Ye"], "title": "ParkGaussian: Surround-view 3D Gaussian Splatting for Autonomous Parking", "comment": null, "summary": "Parking is a critical task for autonomous driving systems (ADS), with unique challenges in crowded parking slots and GPS-denied environments. However, existing works focus on 2D parking slot perception, mapping, and localization, 3D reconstruction remains underexplored, which is crucial for capturing complex spatial geometry in parking scenarios. Naively improving the visual quality of reconstructed parking scenes does not directly benefit autonomous parking, as the key entry point for parking is the slots perception module. To address these limitations, we curate the first benchmark named ParkRecon3D, specifically designed for parking scene reconstruction. It includes sensor data from four surround-view fisheye cameras with calibrated extrinsics and dense parking slot annotations. We then propose ParkGaussian, the first framework that integrates 3D Gaussian Splatting (3DGS) for parking scene reconstruction. To further improve the alignment between reconstruction and downstream parking slot detection, we introduce a slot-aware reconstruction strategy that leverages existing parking perception methods to enhance the synthesis quality of slot regions. Experiments on ParkRecon3D demonstrate that ParkGaussian achieves state-of-the-art reconstruction quality and better preserves perception consistency for downstream tasks. The code and dataset will be released at: https://github.com/wm-research/ParkGaussian", "AI": {"tldr": "First 3D parking scene reconstruction benchmark (ParkRecon3D) and framework (ParkGaussian) using 3D Gaussian Splatting with slot-aware strategy to improve downstream parking perception.", "motivation": "Existing autonomous parking research focuses on 2D perception, mapping, and localization, but 3D reconstruction remains underexplored despite being crucial for capturing complex spatial geometry in parking scenarios. Simply improving visual quality doesn't directly benefit autonomous parking since the key entry point is parking slot perception.", "method": "1) Created ParkRecon3D benchmark with sensor data from four surround-view fisheye cameras with calibrated extrinsics and dense parking slot annotations. 2) Proposed ParkGaussian framework integrating 3D Gaussian Splatting (3DGS) for parking scene reconstruction. 3) Introduced slot-aware reconstruction strategy that leverages existing parking perception methods to enhance synthesis quality of slot regions.", "result": "Experiments on ParkRecon3D demonstrate that ParkGaussian achieves state-of-the-art reconstruction quality and better preserves perception consistency for downstream tasks compared to existing methods.", "conclusion": "The work addresses the gap in 3D parking scene reconstruction by introducing the first benchmark and framework specifically designed for this task, with a slot-aware strategy that bridges reconstruction quality with downstream parking slot detection performance."}}
{"id": "2601.01393", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.01393", "abs": "https://arxiv.org/abs/2601.01393", "authors": ["Shamik Shafkat Avro", "Nazira Jesmin Lina", "Shahanaz Sharmin"], "title": "Evaluation of Convolutional Neural Network For Image Classification with Agricultural and Urban Datasets", "comment": "All authors contributed equally to this work", "summary": "This paper presents the development and evaluation of a custom Convolutional Neural Network (CustomCNN) created to study how architectural design choices affect multi-domain image classification tasks. The network uses residual connections, Squeeze-and-Excitation attention mechanisms, progressive channel scaling, and Kaiming initialization to improve its ability to represent data and speed up training. The model is trained and tested on five publicly available datasets: unauthorized vehicle detection, footpath encroachment detection, polygon-annotated road damage and manhole detection, MangoImageBD and PaddyVarietyBD. A comparison with popular CNN architectures shows that the CustomCNN delivers competitive performance while remaining efficient in computation. The results underscore the importance of thoughtful architectural design for real-world Smart City and agricultural imaging applications.", "AI": {"tldr": "CustomCNN with residual connections, SE attention, progressive scaling, and Kaiming initialization achieves competitive multi-domain image classification performance across Smart City and agricultural datasets.", "motivation": "To study how architectural design choices affect multi-domain image classification tasks, particularly for real-world applications in Smart City infrastructure monitoring and agricultural imaging.", "method": "Developed a custom CNN architecture incorporating residual connections, Squeeze-and-Excitation attention mechanisms, progressive channel scaling, and Kaiming initialization. Trained and tested on five diverse datasets covering unauthorized vehicle detection, footpath encroachment, road damage/manhole detection, and agricultural (mango/paddy) classification.", "result": "CustomCNN delivers competitive performance compared to popular CNN architectures while maintaining computational efficiency across all five datasets.", "conclusion": "Thoughtful architectural design is crucial for effective multi-domain image classification in real-world Smart City and agricultural applications, with the proposed CustomCNN demonstrating both performance and efficiency."}}
{"id": "2601.01406", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.01406", "abs": "https://arxiv.org/abs/2601.01406", "authors": ["Habiba Kausar", "Saeed Anwar", "Omar Jamal Hammad", "Abdul Bais"], "title": "SwinIFS: Landmark Guided Swin Transformer For Identity Preserving Face Super Resolution", "comment": null, "summary": "Face super-resolution aims to recover high-quality facial images from severely degraded low-resolution inputs, but remains challenging due to the loss of fine structural details and identity-specific features. This work introduces SwinIFS, a landmark-guided super-resolution framework that integrates structural priors with hierarchical attention mechanisms to achieve identity-preserving reconstruction at both moderate and extreme upscaling factors. The method incorporates dense Gaussian heatmaps of key facial landmarks into the input representation, enabling the network to focus on semantically important facial regions from the earliest stages of processing. A compact Swin Transformer backbone is employed to capture long-range contextual information while preserving local geometry, allowing the model to restore subtle facial textures and maintain global structural consistency. Extensive experiments on the CelebA benchmark demonstrate that SwinIFS achieves superior perceptual quality, sharper reconstructions, and improved identity retention; it consistently produces more photorealistic results and exhibits strong performance even under 8x magnification, where most methods fail to recover meaningful structure. SwinIFS also provides an advantageous balance between reconstruction accuracy and computational efficiency, making it suitable for real-world applications in facial enhancement, surveillance, and digital restoration. Our code, model weights, and results are available at https://github.com/Habiba123-stack/SwinIFS.", "AI": {"tldr": "SwinIFS is a landmark-guided face super-resolution framework that uses facial landmarks and Swin Transformer architecture to preserve identity and structural details at extreme upscaling factors (up to 8x).", "motivation": "Face super-resolution faces challenges in recovering fine structural details and identity-specific features from severely degraded low-resolution inputs, especially at extreme upscaling factors where most methods fail.", "method": "Integrates dense Gaussian heatmaps of facial landmarks into input representation, uses a compact Swin Transformer backbone to capture long-range contextual information while preserving local geometry, and focuses on semantically important facial regions from early processing stages.", "result": "Superior perceptual quality, sharper reconstructions, and improved identity retention on CelebA benchmark; performs well even at 8x magnification where most methods fail; balances reconstruction accuracy with computational efficiency.", "conclusion": "SwinIFS achieves identity-preserving face super-resolution at extreme upscaling factors, making it suitable for real-world applications in facial enhancement, surveillance, and digital restoration."}}
{"id": "2601.01408", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.01408", "abs": "https://arxiv.org/abs/2601.01408", "authors": ["Gong Gao", "Zekai Wang", "Jian Zhao", "Ziqi Xie", "Xianhui Liu", "Weidong Zhao"], "title": "Mask-Guided Multi-Task Network for Face Attribute Recognition", "comment": "23 pages, 9 figures", "summary": "Face Attribute Recognition (FAR) plays a crucial role in applications such as person re-identification, face retrieval, and face editing. Conventional multi-task attribute recognition methods often process the entire feature map for feature extraction and attribute classification, which can produce redundant features due to reliance on global regions. To address these challenges, we propose a novel approach emphasizing the selection of specific feature regions for efficient feature learning. We introduce the Mask-Guided Multi-Task Network (MGMTN), which integrates Adaptive Mask Learning (AML) and Group-Global Feature Fusion (G2FF) to address the aforementioned limitations. Leveraging a pre-trained keypoint annotation model and a fully convolutional network, AML accurately localizes critical facial parts (e.g., eye and mouth groups) and generates group masks that delineate meaningful feature regions, thereby mitigating negative transfer from global region usage. Furthermore, G2FF combines group and global features to enhance FAR learning, enabling more precise attribute identification. Extensive experiments on two challenging facial attribute recognition datasets demonstrate the effectiveness of MGMTN in improving FAR performance.", "AI": {"tldr": "MGMTN improves face attribute recognition by focusing on specific facial regions using adaptive mask learning and group-global feature fusion to reduce redundancy and negative transfer.", "motivation": "Conventional multi-task attribute recognition methods process entire feature maps, producing redundant features from global regions and suffering from negative transfer, which limits performance in face attribute recognition applications like person re-identification and face editing.", "method": "Proposes Mask-Guided Multi-Task Network (MGMTN) with two key components: Adaptive Mask Learning (AML) that uses pre-trained keypoint annotation and fully convolutional networks to localize critical facial parts and generate group masks; and Group-Global Feature Fusion (G2FF) that combines group and global features for enhanced attribute learning.", "result": "Extensive experiments on two challenging facial attribute recognition datasets demonstrate MGMTN's effectiveness in improving FAR performance compared to conventional methods.", "conclusion": "MGMTN addresses limitations of global feature processing by focusing on specific facial regions, reducing redundancy and negative transfer while enhancing attribute recognition through adaptive mask learning and feature fusion."}}
{"id": "2601.01416", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.01416", "abs": "https://arxiv.org/abs/2601.01416", "authors": ["Yue Zhou", "Ran Ding", "Xue Yang", "Xue Jiang", "Xingzhao Liu"], "title": "AirSpatialBot: A Spatially-Aware Aerial Agent for Fine-Grained Vehicle Attribute Recognization and Retrieval", "comment": "12 pages, 9 figures", "summary": "Despite notable advancements in remote sensing vision-language models (VLMs), existing models often struggle with spatial understanding, limiting their effectiveness in real-world applications. To push the boundaries of VLMs in remote sensing, we specifically address vehicle imagery captured by drones and introduce a spatially-aware dataset AirSpatial, which comprises over 206K instructions and introduces two novel tasks: Spatial Grounding and Spatial Question Answering. It is also the first remote sensing grounding dataset to provide 3DBB. To effectively leverage existing image understanding of VLMs to spatial domains, we adopt a two-stage training strategy comprising Image Understanding Pre-training and Spatial Understanding Fine-tuning. Utilizing this trained spatially-aware VLM, we develop an aerial agent, AirSpatialBot, which is capable of fine-grained vehicle attribute recognition and retrieval. By dynamically integrating task planning, image understanding, spatial understanding, and task execution capabilities, AirSpatialBot adapts to diverse query requirements. Experimental results validate the effectiveness of our approach, revealing the spatial limitations of existing VLMs while providing valuable insights. The model, code, and datasets will be released at https://github.com/VisionXLab/AirSpatialBot", "AI": {"tldr": "Researchers introduce AirSpatial, a spatially-aware dataset for drone-captured vehicle imagery with 206K+ instructions and novel spatial tasks, plus AirSpatialBot - an aerial agent for vehicle attribute recognition and retrieval.", "motivation": "Existing remote sensing vision-language models struggle with spatial understanding, limiting their real-world effectiveness for applications like drone-based vehicle analysis.", "method": "Two-stage training: Image Understanding Pre-training followed by Spatial Understanding Fine-tuning using the AirSpatial dataset with 3D bounding boxes, spatial grounding, and spatial QA tasks.", "result": "AirSpatialBot demonstrates effective fine-grained vehicle attribute recognition and retrieval, revealing spatial limitations of existing VLMs while providing valuable insights.", "conclusion": "The proposed spatially-aware VLM and AirSpatialBot advance remote sensing capabilities by addressing spatial understanding gaps, with model, code, and datasets made publicly available."}}
{"id": "2601.01425", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.01425", "abs": "https://arxiv.org/abs/2601.01425", "authors": ["Xu Guo", "Fulong Ye", "Xinghui Li", "Pengqi Tu", "Pengze Zhang", "Qichao Sun", "Songtao Zhao", "Xiangwang Hou", "Qian He"], "title": "DreamID-V:Bridging the Image-to-Video Gap for High-Fidelity Face Swapping via Diffusion Transformer", "comment": "Project: https://guoxu1233.github.io/DreamID-V/", "summary": "Video Face Swapping (VFS) requires seamlessly injecting a source identity into a target video while meticulously preserving the original pose, expression, lighting, background, and dynamic information. Existing methods struggle to maintain identity similarity and attribute preservation while preserving temporal consistency. To address the challenge, we propose a comprehensive framework to seamlessly transfer the superiority of Image Face Swapping (IFS) to the video domain. We first introduce a novel data pipeline SyncID-Pipe that pre-trains an Identity-Anchored Video Synthesizer and combines it with IFS models to construct bidirectional ID quadruplets for explicit supervision. Building upon paired data, we propose the first Diffusion Transformer-based framework DreamID-V, employing a core Modality-Aware Conditioning module to discriminatively inject multi-model conditions. Meanwhile, we propose a Synthetic-to-Real Curriculum mechanism and an Identity-Coherence Reinforcement Learning strategy to enhance visual realism and identity consistency under challenging scenarios. To address the issue of limited benchmarks, we introduce IDBench-V, a comprehensive benchmark encompassing diverse scenes. Extensive experiments demonstrate DreamID-V outperforms state-of-the-art methods and further exhibits exceptional versatility, which can be seamlessly adapted to various swap-related tasks.", "AI": {"tldr": "DreamID-V is a diffusion transformer framework for video face swapping that transfers image face swapping superiority to video domain while maintaining identity similarity, temporal consistency, and visual realism.", "motivation": "Existing video face swapping methods struggle to maintain identity similarity and attribute preservation while preserving temporal consistency. There's a need to transfer the superiority of image face swapping to video domain and address limited benchmarks.", "method": "1) SyncID-Pipe data pipeline for pre-training Identity-Anchored Video Synthesizer and constructing bidirectional ID quadruplets; 2) DreamID-V diffusion transformer framework with Modality-Aware Conditioning module; 3) Synthetic-to-Real Curriculum mechanism; 4) Identity-Coherence Reinforcement Learning strategy; 5) IDBench-V benchmark for evaluation.", "result": "Extensive experiments show DreamID-V outperforms state-of-the-art methods and exhibits exceptional versatility for various swap-related tasks.", "conclusion": "The proposed comprehensive framework successfully addresses video face swapping challenges by transferring image face swapping superiority to video domain while maintaining identity consistency and temporal coherence."}}
{"id": "2601.01431", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.01431", "abs": "https://arxiv.org/abs/2601.01431", "authors": ["Weiqi Yu", "Yiyang Yao", "Lin He", "Jianming Lv"], "title": "EdgeNeRF: Edge-Guided Regularization for Neural Radiance Fields from Sparse Views", "comment": "PRCV 2025", "summary": "Neural Radiance Fields (NeRF) achieve remarkable performance in dense multi-view scenarios, but their reconstruction quality degrades significantly under sparse inputs due to geometric artifacts. Existing methods utilize global depth regularization to mitigate artifacts, leading to the loss of geometric boundary details. To address this problem, we propose EdgeNeRF, an edge-guided sparse-view 3D reconstruction algorithm. Our method leverages the prior that abrupt changes in depth and normals generate edges. Specifically, we first extract edges from input images, then apply depth and normal regularization constraints to non-edge regions, enhancing geometric consistency while preserving high-frequency details at boundaries. Experiments on LLFF and DTU datasets demonstrate EdgeNeRF's superior performance, particularly in retaining sharp geometric boundaries and suppressing artifacts. Additionally, the proposed edge-guided depth regularization module can be seamlessly integrated into other methods in a plug-and-play manner, significantly improving their performance without substantially increasing training time. Code is available at https://github.com/skyhigh404/edgenerf.", "AI": {"tldr": "EdgeNeRF improves sparse-view 3D reconstruction by using edge-guided depth regularization to preserve geometric boundaries while reducing artifacts.", "motivation": "NeRF performs poorly with sparse inputs, suffering from geometric artifacts. Existing methods use global depth regularization which loses boundary details.", "method": "Extract edges from input images, then apply depth and normal regularization only to non-edge regions to preserve boundary details while enhancing geometric consistency.", "result": "Superior performance on LLFF and DTU datasets, especially in retaining sharp geometric boundaries and suppressing artifacts. The edge-guided module can be integrated into other methods as a plug-and-play component.", "conclusion": "EdgeNeRF effectively addresses sparse-view reconstruction by preserving geometric boundaries through edge-guided regularization, offering a versatile solution that can enhance existing methods."}}
{"id": "2601.01439", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.01439", "abs": "https://arxiv.org/abs/2601.01439", "authors": ["Wenqi Ren", "Weijie Wang", "Meng Zheng", "Ziyan Wu", "Yang Tang", "Zhun Zhong", "Nicu Sebe"], "title": "In defense of the two-stage framework for open-set domain adaptive semantic segmentation", "comment": null, "summary": "Open-Set Domain Adaptation for Semantic Segmentation (OSDA-SS) presents a significant challenge, as it requires both domain adaptation for known classes and the distinction of unknowns. Existing methods attempt to address both tasks within a single unified stage. We question this design, as the annotation imbalance between known and unknown classes often leads to negative transfer of known classes and underfitting for unknowns. To overcome these issues, we propose SATS, a Separating-then-Adapting Training Strategy, which addresses OSDA-SS through two sequential steps: known/unknown separation and unknown-aware domain adaptation. By providing the model with more accurate and well-aligned unknown classes, our method ensures a balanced learning of discriminative features for both known and unknown classes, steering the model toward discovering truly unknown objects. Additionally, we present hard unknown exploration, an innovative data augmentation method that exposes the model to more challenging unknowns, strengthening its ability to capture more comprehensive understanding of target unknowns. We evaluate our method on public OSDA-SS benchmarks. Experimental results demonstrate that our method achieves a substantial advancement, with a +3.85% H-Score improvement for GTA5-to-Cityscapes and +18.64% for SYNTHIA-to-Cityscapes, outperforming previous state-of-the-art methods.", "AI": {"tldr": "SATS proposes a two-stage Separating-then-Adapting Training Strategy for Open-Set Domain Adaptation in Semantic Segmentation, addressing annotation imbalance issues by first separating known/unknown classes then performing unknown-aware domain adaptation.", "motivation": "Existing single-stage methods for Open-Set Domain Adaptation in Semantic Segmentation suffer from negative transfer of known classes and underfitting for unknowns due to annotation imbalance between known and unknown classes.", "method": "SATS uses a two-step approach: 1) known/unknown separation, and 2) unknown-aware domain adaptation. It also introduces hard unknown exploration, a data augmentation method that exposes the model to more challenging unknown examples.", "result": "Achieves substantial improvements: +3.85% H-Score for GTA5-to-Cityscapes and +18.64% for SYNTHIA-to-Cityscapes, outperforming previous state-of-the-art methods.", "conclusion": "Separating the tasks of known/unknown separation and domain adaptation leads to better balanced learning and more accurate identification of unknown objects in semantic segmentation across domains."}}
{"id": "2601.01454", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.01454", "abs": "https://arxiv.org/abs/2601.01454", "authors": ["Xiao Li", "Zilong Liu", "Yining Liu", "Zhuhong Li", "Na Dong", "Sitian Qin", "Xiaolin Hu"], "title": "PartImageNet++ Dataset: Enhancing Visual Models with High-Quality Part Annotations", "comment": "arXiv admin note: substantial text overlap with arXiv:2407.10918", "summary": "To address the scarcity of high-quality part annotations in existing datasets, we introduce PartImageNet++ (PIN++), a dataset that provides detailed part annotations for all categories in ImageNet-1K. With 100 annotated images per category, totaling 100K images, PIN++ represents the most comprehensive dataset covering a diverse range of object categories. Leveraging PIN++, we propose a Multi-scale Part-supervised recognition Model (MPM) for robust classification on ImageNet-1K. We first trained a part segmentation network using PIN++ and used it to generate pseudo part labels for the remaining unannotated images. MPM then integrated a conventional recognition architecture with auxiliary bypass layers, jointly supervised by both pseudo part labels and the original part annotations. Furthermore, we conducted extensive experiments on PIN++, including part segmentation, object segmentation, and few-shot learning, exploring various ways to leverage part annotations in downstream tasks. Experimental results demonstrated that our approach not only enhanced part-based models for robust object recognition but also established strong baselines for multiple downstream tasks, highlighting the potential of part annotations in improving model performance. The dataset and the code are available at https://github.com/LixiaoTHU/PartImageNetPP.", "AI": {"tldr": "PIN++ is a comprehensive ImageNet-1K dataset with detailed part annotations (100 images per category, 100K total) used to train MPM model for robust classification and explore part annotations in downstream tasks.", "motivation": "Addressing the scarcity of high-quality part annotations in existing datasets, which limits the development of part-based models and downstream applications.", "method": "1) Created PIN++ dataset with detailed part annotations for all ImageNet-1K categories; 2) Trained part segmentation network on PIN++ to generate pseudo labels for unannotated images; 3) Developed MPM model integrating conventional recognition architecture with auxiliary bypass layers, jointly supervised by both pseudo part labels and original annotations.", "result": "MPM enhanced part-based models for robust object recognition and established strong baselines for multiple downstream tasks (part segmentation, object segmentation, few-shot learning), demonstrating the potential of part annotations to improve model performance.", "conclusion": "PIN++ provides valuable part annotations for ImageNet-1K, enabling development of more robust part-based models and exploration of part annotations in various downstream applications, with promising results showing performance improvements."}}
{"id": "2601.01456", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.01456", "abs": "https://arxiv.org/abs/2601.01456", "authors": ["Wentao Bian", "Fenglei Xu"], "title": "Rethinking Multimodal Few-Shot 3D Point Cloud Segmentation: From Fused Refinement to Decoupled Arbitration", "comment": "10 pages, 4 figures, 3 tables", "summary": "In this paper, we revisit multimodal few-shot 3D point cloud semantic segmentation (FS-PCS), identifying a conflict in \"Fuse-then-Refine\" paradigms: the \"Plasticity-Stability Dilemma.\" In addition, CLIP's inter-class confusion can result in semantic blindness. To address these issues, we present the Decoupled-experts Arbitration Few-Shot SegNet (DA-FSS), a model that effectively distinguishes between semantic and geometric paths and mutually regularizes their gradients to achieve better generalization. DA-FSS employs the same backbone and pre-trained text encoder as MM-FSS to generate text embeddings, which can increase free modalities' utilization rate and better leverage each modality's information space. To achieve this, we propose a Parallel Expert Refinement module to generate each modal correlation. We also propose a Stacked Arbitration Module (SAM) to perform convolutional fusion and arbitrate correlations for each modality pathway. The Parallel Experts decouple two paths: a Geometric Expert maintains plasticity, and a Semantic Expert ensures stability. They are coordinated via a Decoupled Alignment Module (DAM) that transfers knowledge without propagating confusion. Experiments on popular datasets (S3DIS, ScanNet) demonstrate the superiority of DA-FSS over MM-FSS. Meanwhile, geometric boundaries, completeness, and texture differentiation are all superior to the baseline. The code is available at: https://github.com/MoWenQAQ/DA-FSS.", "AI": {"tldr": "DA-FSS addresses multimodal few-shot 3D point cloud segmentation by decoupling semantic and geometric pathways to resolve the Plasticity-Stability Dilemma and CLIP's semantic blindness issues.", "motivation": "The paper identifies two key problems in existing multimodal few-shot 3D segmentation: 1) The \"Plasticity-Stability Dilemma\" in Fuse-then-Refine paradigms where early fusion creates conflicts, and 2) CLIP's inter-class confusion leading to semantic blindness in multimodal approaches.", "method": "Proposes DA-FSS with three main components: 1) Parallel Expert Refinement module to generate modal correlations, 2) Stacked Arbitration Module for convolutional fusion and arbitration, and 3) Decoupled Alignment Module that separates geometric and semantic pathways - Geometric Expert maintains plasticity while Semantic Expert ensures stability, with knowledge transfer without confusion propagation.", "result": "Experiments on S3DIS and ScanNet datasets show DA-FSS outperforms MM-FSS baseline, with superior geometric boundaries, completeness, and texture differentiation. The model better utilizes multimodal information and achieves better generalization.", "conclusion": "Decoupling semantic and geometric pathways with specialized experts and proper arbitration effectively addresses the Plasticity-Stability Dilemma and CLIP's semantic blindness, leading to improved multimodal few-shot 3D point cloud segmentation performance."}}
{"id": "2601.01457", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.01457", "abs": "https://arxiv.org/abs/2601.01457", "authors": ["Mingxing Zhan", "Li Zhang", "Beibei Wang", "Yingjie Wang", "Zenglin Shi"], "title": "Language as Prior, Vision as Calibration: Metric Scale Recovery for Monocular Depth Estimation", "comment": null, "summary": "Relative-depth foundation models transfer well, yet monocular metric depth remains ill-posed due to unidentifiable global scale and heightened domain-shift sensitivity. Under a frozen-backbone calibration setting, we recover metric depth via an image-specific affine transform in inverse depth and train only lightweight calibration heads while keeping the relative-depth backbone and the CLIP text encoder fixed. Since captions provide coarse but noisy scale cues that vary with phrasing and missing objects, we use language to predict an uncertainty-aware envelope that bounds feasible calibration parameters in an unconstrained space, rather than committing to a text-only point estimate. We then use pooled multi-scale frozen visual features to select an image-specific calibration within this envelope. During training, a closed-form least-squares oracle in inverse depth provides per-image supervision for learning the envelope and the selected calibration. Experiments on NYUv2 and KITTI improve in-domain accuracy, while zero-shot transfer to SUN-RGBD and DDAD demonstrates improved robustness over strong language-only baselines.", "AI": {"tldr": "A method that uses language cues to predict uncertainty-aware calibration envelopes for monocular metric depth estimation, training only lightweight calibration heads while keeping relative-depth backbone and CLIP text encoder frozen.", "motivation": "Monocular metric depth estimation remains ill-posed due to unidentifiable global scale and heightened domain-shift sensitivity, despite relative-depth foundation models transferring well. Captions provide coarse but noisy scale cues that vary with phrasing and missing objects.", "method": "Under frozen-backbone calibration setting, recover metric depth via image-specific affine transform in inverse depth. Use language to predict uncertainty-aware envelope bounding feasible calibration parameters rather than text-only point estimate. Use pooled multi-scale frozen visual features to select image-specific calibration within envelope. Train with closed-form least-squares oracle in inverse depth providing per-image supervision.", "result": "Experiments on NYUv2 and KITTI show improved in-domain accuracy. Zero-shot transfer to SUN-RGBD and DDAD demonstrates improved robustness over strong language-only baselines.", "conclusion": "The proposed uncertainty-aware language-guided calibration approach effectively addresses the ill-posed nature of monocular metric depth estimation, improving both in-domain accuracy and cross-domain robustness while maintaining efficiency through lightweight calibration heads."}}
{"id": "2601.01460", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.01460", "abs": "https://arxiv.org/abs/2601.01460", "authors": ["Mohd Usama", "Belal Ahmad", "Christer Gronlund", "Faleh Menawer R Althiyabi"], "title": "Domain Adaptation of Carotid Ultrasound Images using Generative Adversarial Network", "comment": "15 pages, 9 figures, 4 tables", "summary": "Deep learning has been extensively used in medical imaging applications, assuming that the test and training datasets belong to the same probability distribution. However, a common challenge arises when working with medical images generated by different systems or even the same system with different parameter settings. Such images contain diverse textures and reverberation noise that violate the aforementioned assumption. Consequently, models trained on data from one device or setting often struggle to perform effectively with data from other devices or settings. In addition, retraining models for each specific device or setting is labor-intensive and costly. To address these issues in ultrasound images, we propose a novel Generative Adversarial Network (GAN)-based model. We formulated the domain adaptation tasks as an image-to-image translation task, in which we modified the texture patterns and removed reverberation noise in the test data images from the source domain to align with those in the target domain images while keeping the image content unchanged. We applied the proposed method to two datasets containing carotid ultrasound images from three different domains. The experimental results demonstrate that the model successfully translated the texture pattern of images and removed reverberation noise from the ultrasound images. Furthermore, we evaluated the CycleGAN approaches for a comparative study with the proposed model. The experimental findings conclusively demonstrated that the proposed model achieved domain adaptation (histogram correlation (0.960 (0.019), & 0.920 (0.043) and bhattacharya distance (0.040 (0.020), & 0.085 (0.048)), compared to no adaptation (0.916 (0.062) & 0.890 (0.077), 0.090 (0.070) & 0.121 (0.095)) for both datasets.", "AI": {"tldr": "A GAN-based domain adaptation method for ultrasound images that translates texture patterns and removes reverberation noise to address distribution shifts between different medical imaging devices/settings.", "motivation": "Deep learning models in medical imaging assume consistent probability distributions between training and test data, but this assumption is violated when images come from different devices or parameter settings. Models trained on one device struggle with others, and retraining for each specific device is costly and labor-intensive.", "method": "Proposed a novel GAN-based model that formulates domain adaptation as an image-to-image translation task. The model modifies texture patterns and removes reverberation noise in test data from source domains to align with target domains while preserving image content. Applied to carotid ultrasound images from three different domains.", "result": "The model successfully translated texture patterns and removed reverberation noise. Quantitative evaluation showed superior domain adaptation performance compared to no adaptation: histogram correlation (0.960\u00b10.019 & 0.920\u00b10.043 vs 0.916\u00b10.062 & 0.890\u00b10.077) and Bhattacharya distance (0.040\u00b10.020 & 0.085\u00b10.048 vs 0.090\u00b10.070 & 0.121\u00b10.095). Also compared favorably with CycleGAN approaches.", "conclusion": "The proposed GAN-based domain adaptation method effectively addresses the distribution shift problem in medical ultrasound images by translating texture patterns and removing noise, enabling models to work across different imaging devices and settings without costly retraining."}}
{"id": "2601.01481", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.01481", "abs": "https://arxiv.org/abs/2601.01481", "authors": ["Mohammad Hassan Saghafi", "Seyed Majid Noorhosseini", "Seyed Abolfazl Seyed Javadein", "Hadi Khalili"], "title": "Robust Ship Detection and Tracking Using Modified ViBe and Backwash Cancellation Algorithm", "comment": null, "summary": "In this paper, we propose a robust real time detection and tracking method for detecting ships in a coastal video sequences. Since coastal scenarios are unpredictable and scenes have dynamic properties it is essential to apply detection methods that are robust to these conditions. This paper presents modified ViBe for moving object detection which detects ships and backwash. In the modified ViBe the probability of losing ships is decreased in comparison with the original ViBe. It is robust to natural sea waves and variation of lights and is capable of quickly updating the background. Based on geometrical properties of ship and some concepts such as brightness distortion, a new method for backwash cancellation is proposed. Experimental results demonstrate that the proposed strategy and methods have outstanding performance in ship detection and tracking. These results also illustrate real time and precise performance of the proposed strategy.", "AI": {"tldr": "A robust real-time ship detection and tracking method for coastal videos using modified ViBe for moving object detection and backwash cancellation based on ship geometry and brightness distortion.", "motivation": "Coastal scenarios are unpredictable with dynamic properties, requiring robust detection methods that can handle natural sea waves, light variations, and backwash interference for accurate ship detection.", "method": "Modified ViBe algorithm for moving object detection that reduces probability of losing ships, quickly updates background, and is robust to sea waves and light variations. Plus a new backwash cancellation method based on ship geometrical properties and brightness distortion concepts.", "result": "Experimental results demonstrate outstanding performance in ship detection and tracking, with real-time and precise operation in coastal video sequences.", "conclusion": "The proposed strategy provides robust, real-time ship detection and tracking in challenging coastal environments, effectively handling dynamic conditions and backwash interference."}}
{"id": "2601.01483", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.01483", "abs": "https://arxiv.org/abs/2601.01483", "authors": ["Xinyu Qiu", "Heng Jia", "Zhengwen Zeng", "Shuheng Shen", "Changhua Meng", "Yi Yang", "Linchao Zhu"], "title": "Unified Generation and Self-Verification for Vision-Language Models via Advantage Decoupled Preference Optimization", "comment": null, "summary": "Parallel test-time scaling typically trains separate generation and verification models, incurring high training and inference costs. We propose Advantage Decoupled Preference Optimization (ADPO), a unified reinforcement learning framework that jointly learns answer generation and self-verification within a single policy. ADPO introduces two innovations: a preference verification reward improving verification capability and a decoupled optimization mechanism enabling synergistic optimization of generation and verification. Specifically, the preference verification reward computes mean verification scores from positive and negative samples as decision thresholds, providing positive feedback when prediction correctness aligns with answer correctness. Meanwhile, the advantage decoupled optimization computes separate advantages for generation and verification, applies token masks to isolate gradients, and combines masked GRPO objectives, preserving generation quality while calibrating verification scores. ADPO achieves up to +34.1% higher verification AUC and -53.5% lower inference time, with significant gains of +2.8%/+1.4% accuracy on MathVista/MMMU, +1.9 cIoU on ReasonSeg, and +1.7%/+1.0% step success rate on AndroidControl/GUI Odyssey.", "AI": {"tldr": "ADPO is a unified RL framework that jointly learns answer generation and self-verification in a single policy, reducing training/inference costs while improving performance.", "motivation": "Parallel test-time scaling typically requires separate generation and verification models, which incurs high training and inference costs. There's a need for a more efficient unified approach.", "method": "ADPO introduces two key innovations: 1) Preference verification reward that uses mean verification scores as decision thresholds, providing feedback when prediction correctness aligns with answer correctness. 2) Advantage decoupled optimization that computes separate advantages for generation and verification, applies token masks to isolate gradients, and combines masked GRPO objectives.", "result": "ADPO achieves up to +34.1% higher verification AUC, -53.5% lower inference time, +2.8%/+1.4% accuracy on MathVista/MMMU, +1.9 cIoU on ReasonSeg, and +1.7%/+1.0% step success rate on AndroidControl/GUI Odyssey.", "conclusion": "ADPO provides an efficient unified framework that synergistically optimizes generation and verification within a single policy, significantly reducing costs while improving performance across multiple benchmarks."}}
{"id": "2601.01485", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.01485", "abs": "https://arxiv.org/abs/2601.01485", "authors": ["Zobia Batool", "Diala Lteif", "Vijaya B. Kolachalama", "Huseyin Ozkan", "Erchan Aptoula"], "title": "Higher-Order Domain Generalization in Magnetic Resonance-Based Assessment of Alzheimer's Disease", "comment": null, "summary": "Despite progress in deep learning for Alzheimer's disease (AD) diagnostics, models trained on structural magnetic resonance imaging (sMRI) often do not perform well when applied to new cohorts due to domain shifts from varying scanners, protocols and patient demographics. AD, the primary driver of dementia, manifests through progressive cognitive and neuroanatomical changes like atrophy and ventricular expansion, making robust, generalizable classification essential for real-world use. While convolutional neural networks and transformers have advanced feature extraction via attention and fusion techniques, single-domain generalization (SDG) remains underexplored yet critical, given the fragmented nature of AD datasets. To bridge this gap, we introduce Extended MixStyle (EM), a framework for blending higher-order feature moments (skewness and kurtosis) to mimic diverse distributional variations. Trained on sMRI data from the National Alzheimer's Coordinating Center (NACC; n=4,647) to differentiate persons with normal cognition (NC) from those with mild cognitive impairment (MCI) or AD and tested on three unseen cohorts (total n=3,126), EM yields enhanced cross-domain performance, improving macro-F1 on average by 2.4 percentage points over state-of-the-art SDG benchmarks, underscoring its promise for invariant, reliable AD detection in heterogeneous real-world settings. The source code will be made available upon acceptance at https://github.com/zobia111/Extended-Mixstyle.", "AI": {"tldr": "Extended MixStyle (EM) framework improves Alzheimer's disease classification across different MRI datasets by blending higher-order feature moments to handle domain shifts from varying scanners and protocols.", "motivation": "Existing deep learning models for Alzheimer's disease diagnostics using structural MRI often fail to generalize to new cohorts due to domain shifts from varying scanners, protocols, and patient demographics. Single-domain generalization remains underexplored despite being critical for real-world application given fragmented AD datasets.", "method": "Extended MixStyle (EM) framework that blends higher-order feature moments (skewness and kurtosis) to mimic diverse distributional variations, enabling better domain generalization. Trained on NACC dataset (n=4,647) to differentiate normal cognition from mild cognitive impairment or AD, then tested on three unseen cohorts (total n=3,126).", "result": "EM yields enhanced cross-domain performance, improving macro-F1 on average by 2.4 percentage points over state-of-the-art single-domain generalization benchmarks, demonstrating promise for invariant, reliable AD detection in heterogeneous real-world settings.", "conclusion": "The Extended MixStyle framework effectively addresses domain shift challenges in Alzheimer's disease diagnostics by blending higher-order feature moments, providing improved generalization across different MRI datasets and making AD detection more robust for real-world clinical applications."}}
{"id": "2601.01487", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.01487", "abs": "https://arxiv.org/abs/2601.01487", "authors": ["Ziyue Zhang", "Luxi Lin", "Xiaolin Hu", "Chao Chang", "HuaiXi Wang", "Yiyi Zhou", "Rongrong Ji"], "title": "DeepInv: A Novel Self-supervised Learning Approach for Fast and Accurate Diffusion Inversion", "comment": null, "summary": "Diffusion inversion is a task of recovering the noise of an image in a diffusion model, which is vital for controllable diffusion image editing. At present, diffusion inversion still remains a challenging task due to the lack of viable supervision signals. Thus, most existing methods resort to approximation-based solutions, which however are often at the cost of performance or efficiency. To remedy these shortcomings, we propose a novel self-supervised diffusion inversion approach in this paper, termed Deep Inversion (DeepInv). Instead of requiring ground-truth noise annotations, we introduce a self-supervised objective as well as a data augmentation strategy to generate high-quality pseudo noises from real images without manual intervention. Based on these two innovative designs, DeepInv is also equipped with an iterative and multi-scale training regime to train a parameterized inversion solver, thereby achieving the fast and accurate image-to-noise mapping. To the best of our knowledge, this is the first attempt of presenting a trainable solver to predict inversion noise step by step. The extensive experiments show that our DeepInv can achieve much better performance and inference speed than the compared methods, e.g., +40.435% SSIM than EasyInv and +9887.5% speed than ReNoise on COCO dataset. Moreover, our careful designs of trainable solvers can also provide insights to the community. Codes and model parameters will be released in https://github.com/potato-kitty/DeepInv.", "AI": {"tldr": "DeepInv: A self-supervised diffusion inversion method that trains a parameterized solver to predict inversion noise step-by-step without ground-truth annotations, achieving superior performance and speed.", "motivation": "Diffusion inversion is crucial for controllable image editing but remains challenging due to lack of viable supervision signals. Existing approximation-based methods sacrifice performance or efficiency.", "method": "Proposes DeepInv with: 1) self-supervised objective and data augmentation to generate pseudo noises from real images, 2) iterative multi-scale training regime to train a parameterized inversion solver for fast image-to-noise mapping.", "result": "Achieves +40.435% SSIM over EasyInv and +9887.5% speed improvement over ReNoise on COCO dataset. First trainable solver for step-by-step inversion noise prediction.", "conclusion": "DeepInv provides a novel self-supervised approach to diffusion inversion that significantly outperforms existing methods in both accuracy and efficiency, offering new insights for trainable inversion solvers."}}
{"id": "2601.01507", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.01507", "abs": "https://arxiv.org/abs/2601.01507", "authors": ["Tao Li", "Qing Li", "Na Li", "Hui Xie"], "title": "DiffKD-DCIS: Predicting Upgrade of Ductal Carcinoma In Situ with Diffusion Augmentation and Knowledge Distillation", "comment": null, "summary": "Accurately predicting the upgrade of ductal carcinoma in situ (DCIS) to invasive ductal carcinoma (IDC) is crucial for surgical planning. However, traditional deep learning methods face challenges due to limited ultrasound data and poor generalization ability. This study proposes the DiffKD-DCIS framework, integrating conditional diffusion modeling with teacher-student knowledge distillation.\n  The framework operates in three stages: First, a conditional diffusion model generates high-fidelity ultrasound images using multimodal conditions for data augmentation. Then, a deep teacher network extracts robust features from both original and synthetic data. Finally, a compact student network learns from the teacher via knowledge distillation, balancing generalization and computational efficiency.\n  Evaluated on a multi-center dataset of 1,435 cases, the synthetic images were of good quality. The student network had fewer parameters and faster inference. On external test sets, it outperformed partial combinations, and its accuracy was comparable to senior radiologists and superior to junior ones, showing significant clinical potential.", "AI": {"tldr": "DiffKD-DCIS framework uses conditional diffusion modeling for data augmentation and knowledge distillation to predict DCIS upgrade to IDC from ultrasound images, achieving radiologist-level performance with computational efficiency.", "motivation": "Accurate prediction of DCIS upgrade to invasive carcinoma is crucial for surgical planning, but traditional deep learning methods struggle with limited ultrasound data and poor generalization.", "method": "Three-stage framework: 1) Conditional diffusion model generates high-fidelity synthetic ultrasound images using multimodal conditions for data augmentation; 2) Deep teacher network extracts robust features from both original and synthetic data; 3) Compact student network learns from teacher via knowledge distillation for balance between generalization and efficiency.", "result": "Synthetic images showed good quality; student network had fewer parameters and faster inference; outperformed partial combinations on external test sets; accuracy comparable to senior radiologists and superior to junior ones.", "conclusion": "The DiffKD-DCIS framework demonstrates significant clinical potential for predicting DCIS upgrade to IDC, achieving radiologist-level accuracy with computational efficiency through synthetic data augmentation and knowledge distillation."}}
{"id": "2601.01512", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.01512", "abs": "https://arxiv.org/abs/2601.01512", "authors": ["Wenhui Chu", "Aobo Jin", "Hardik A. Gohel"], "title": "A Novel Deep Learning Method for Segmenting the Left Ventricle in Cardiac Cine MRI", "comment": "9 pages, 5 figures", "summary": "This research aims to develop a novel deep learning network, GBU-Net, utilizing a group-batch-normalized U-Net framework, specifically designed for the precise semantic segmentation of the left ventricle in short-axis cine MRI scans. The methodology includes a down-sampling pathway for feature extraction and an up-sampling pathway for detail restoration, enhanced for medical imaging. Key modifications include techniques for better contextual understanding crucial in cardiac MRI segmentation. The dataset consists of 805 left ventricular MRI scans from 45 patients, with comparative analysis using established metrics such as the dice coefficient and mean perpendicular distance. GBU-Net significantly improves the accuracy of left ventricle segmentation in cine MRI scans. Its innovative design outperforms existing methods in tests, surpassing standard metrics like the dice coefficient and mean perpendicular distance. The approach is unique in its ability to capture contextual information, often missed in traditional CNN-based segmentation. An ensemble of the GBU-Net attains a 97% dice score on the SunnyBrook testing dataset. GBU-Net offers enhanced precision and contextual understanding in left ventricle segmentation for surgical robotics and medical analysis.", "AI": {"tldr": "GBU-Net is a novel deep learning network using group-batch-normalized U-Net architecture for precise left ventricle segmentation in cardiac MRI, achieving 97% dice score on SunnyBrook dataset.", "motivation": "To improve the accuracy of left ventricle segmentation in short-axis cine MRI scans, addressing limitations of traditional CNN-based segmentation that often miss contextual information crucial for cardiac imaging analysis.", "method": "Developed GBU-Net based on group-batch-normalized U-Net framework with down-sampling pathway for feature extraction and up-sampling pathway for detail restoration, specifically enhanced for medical imaging with modifications for better contextual understanding in cardiac MRI segmentation.", "result": "GBU-Net significantly outperforms existing methods, achieving 97% dice score on SunnyBrook testing dataset and surpassing standard metrics like dice coefficient and mean perpendicular distance, demonstrating enhanced precision in left ventricle segmentation.", "conclusion": "GBU-Net offers superior precision and contextual understanding for left ventricle segmentation in cardiac MRI, making it valuable for surgical robotics and medical analysis applications."}}
{"id": "2601.01513", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.01513", "abs": "https://arxiv.org/abs/2601.01513", "authors": ["Gen Li", "Peiyu Liu"], "title": "FastV-RAG: Towards Fast and Fine-Grained Video QA with Retrieval-Augmented Generation", "comment": null, "summary": "Vision-Language Models (VLMs) excel at visual reasoning but still struggle with integrating external knowledge. Retrieval-Augmented Generation (RAG) is a promising solution, but current methods remain inefficient and often fail to maintain high answer quality. To address these challenges, we propose VideoSpeculateRAG, an efficient VLM-based RAG framework built on two key ideas. First, we introduce a speculative decoding pipeline: a lightweight draft model quickly generates multiple answer candidates, which are then verified and refined by a more accurate heavyweight model, substantially reducing inference latency without sacrificing correctness. Second, we identify a major source of error - incorrect entity recognition in retrieved knowledge - and mitigate it with a simple yet effective similarity-based filtering strategy that improves entity alignment and boosts overall answer accuracy. Experiments demonstrate that VideoSpeculateRAG achieves comparable or higher accuracy than standard RAG approaches while accelerating inference by approximately 2x. Our framework highlights the potential of combining speculative decoding with retrieval-augmented reasoning to enhance efficiency and reliability in complex, knowledge-intensive multimodal tasks.", "AI": {"tldr": "VideoSpeculateRAG: Efficient VLM-based RAG framework using speculative decoding and similarity filtering to improve speed and accuracy in knowledge-intensive multimodal tasks.", "motivation": "VLMs struggle with integrating external knowledge efficiently. Current RAG methods are inefficient and often fail to maintain high answer quality, creating a need for better solutions.", "method": "Two key innovations: 1) Speculative decoding pipeline with lightweight draft model generating answer candidates verified by heavyweight model, 2) Similarity-based filtering strategy to fix incorrect entity recognition in retrieved knowledge.", "result": "Achieves comparable or higher accuracy than standard RAG approaches while accelerating inference by approximately 2x.", "conclusion": "Combining speculative decoding with retrieval-augmented reasoning enhances efficiency and reliability in complex, knowledge-intensive multimodal tasks."}}
{"id": "2601.01526", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.01526", "abs": "https://arxiv.org/abs/2601.01526", "authors": ["Hongbing Li", "Linhui Xiao", "Zihan Zhao", "Qi Shen", "Yixiang Huang", "Bo Xiao", "Zhanyu Ma"], "title": "BARE: Towards Bias-Aware and Reasoning-Enhanced One-Tower Visual Grounding", "comment": null, "summary": "Visual Grounding (VG), which aims to locate a specific region referred to by expressions, is a fundamental yet challenging task in the multimodal understanding fields. While recent grounding transfer works have advanced the field through one-tower architectures, they still suffer from two primary limitations: (1) over-entangled multimodal representations that exacerbate deceptive modality biases, and (2) insufficient semantic reasoning that hinders the comprehension of referential cues. In this paper, we propose BARE, a bias-aware and reasoning-enhanced framework for one-tower visual grounding. BARE introduces a mechanism that preserves modality-specific features and constructs referential semantics through three novel modules: (i) language salience modulator, (ii) visual bias correction and (iii) referential relationship enhancement, which jointly mitigate multimodal distractions and enhance referential comprehension. Extensive experimental results on five benchmarks demonstrate that BARE not only achieves state-of-the-art performance but also delivers superior computational efficiency compared to existing approaches. The code is publicly accessible at https://github.com/Marloweeee/BARE.", "AI": {"tldr": "BARE is a bias-aware and reasoning-enhanced one-tower visual grounding framework that addresses over-entangled multimodal representations and insufficient semantic reasoning through three novel modules.", "motivation": "Current one-tower visual grounding architectures suffer from two main limitations: (1) over-entangled multimodal representations that exacerbate deceptive modality biases, and (2) insufficient semantic reasoning that hinders comprehension of referential cues.", "method": "BARE introduces a mechanism that preserves modality-specific features and constructs referential semantics through three modules: language salience modulator, visual bias correction, and referential relationship enhancement, which jointly mitigate multimodal distractions and enhance referential comprehension.", "result": "Extensive experiments on five benchmarks demonstrate that BARE achieves state-of-the-art performance while delivering superior computational efficiency compared to existing approaches.", "conclusion": "BARE effectively addresses the limitations of current one-tower visual grounding methods by reducing modality biases and enhancing semantic reasoning, resulting in improved performance and efficiency."}}
{"id": "2601.01528", "categories": ["cs.CV", "cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2601.01528", "abs": "https://arxiv.org/abs/2601.01528", "authors": ["Yang Zhou", "Hao Shao", "Letian Wang", "Zhuofan Zong", "Hongsheng Li", "Steven L. Waslander"], "title": "DrivingGen: A Comprehensive Benchmark for Generative Video World Models in Autonomous Driving", "comment": "10 pages, 4 figures; Project Website: https://drivinggen-bench.github.io/", "summary": "Video generation models, as one form of world models, have emerged as one of the most exciting frontiers in AI, promising agents the ability to imagine the future by modeling the temporal evolution of complex scenes. In autonomous driving, this vision gives rise to driving world models: generative simulators that imagine ego and agent futures, enabling scalable simulation, safe testing of corner cases, and rich synthetic data generation. Yet, despite fast-growing research activity, the field lacks a rigorous benchmark to measure progress and guide priorities. Existing evaluations remain limited: generic video metrics overlook safety-critical imaging factors; trajectory plausibility is rarely quantified; temporal and agent-level consistency is neglected; and controllability with respect to ego conditioning is ignored. Moreover, current datasets fail to cover the diversity of conditions required for real-world deployment. To address these gaps, we present DrivingGen, the first comprehensive benchmark for generative driving world models. DrivingGen combines a diverse evaluation dataset curated from both driving datasets and internet-scale video sources, spanning varied weather, time of day, geographic regions, and complex maneuvers, with a suite of new metrics that jointly assess visual realism, trajectory plausibility, temporal coherence, and controllability. Benchmarking 14 state-of-the-art models reveals clear trade-offs: general models look better but break physics, while driving-specific ones capture motion realistically but lag in visual quality. DrivingGen offers a unified evaluation framework to foster reliable, controllable, and deployable driving world models, enabling scalable simulation, planning, and data-driven decision-making.", "AI": {"tldr": "DrivingGen is the first comprehensive benchmark for generative driving world models, addressing gaps in current evaluation methods with diverse datasets and new metrics for visual realism, trajectory plausibility, temporal coherence, and controllability.", "motivation": "Current driving world model evaluations are inadequate: generic video metrics overlook safety factors, trajectory plausibility is rarely quantified, temporal/agent consistency is neglected, controllability is ignored, and datasets lack real-world diversity. There's no rigorous benchmark to measure progress in this emerging field.", "method": "Created DrivingGen benchmark combining: 1) Diverse evaluation dataset from driving datasets and internet-scale video sources covering varied weather, time of day, geographic regions, and complex maneuvers; 2) New metric suite assessing visual realism, trajectory plausibility, temporal coherence, and controllability.", "result": "Benchmarked 14 state-of-the-art models revealing clear trade-offs: general video generation models produce better visuals but break physics, while driving-specific models capture realistic motion but lag in visual quality.", "conclusion": "DrivingGen provides a unified evaluation framework to foster reliable, controllable, and deployable driving world models, enabling scalable simulation, planning, and data-driven decision-making for autonomous driving applications."}}
{"id": "2601.01535", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.01535", "abs": "https://arxiv.org/abs/2601.01535", "authors": ["Zixuan Fu", "Lanqing Guo", "Chong Wang", "Binbin Song", "Ding Liu", "Bihan Wen"], "title": "Improving Flexible Image Tokenizers for Autoregressive Image Generation", "comment": null, "summary": "Flexible image tokenizers aim to represent an image using an ordered 1D variable-length token sequence. This flexible tokenization is typically achieved through nested dropout, where a portion of trailing tokens is randomly truncated during training, and the image is reconstructed using the remaining preceding sequence. However, this tail-truncation strategy inherently concentrates the image information in the early tokens, limiting the effectiveness of downstream AutoRegressive (AR) image generation as the token length increases. To overcome these limitations, we propose \\textbf{ReToK}, a flexible tokenizer with \\underline{Re}dundant \\underline{Tok}en Padding and Hierarchical Semantic Regularization, designed to fully exploit all tokens for enhanced latent modeling. Specifically, we introduce \\textbf{Redundant Token Padding} to activate tail tokens more frequently, thereby alleviating information over-concentration in the early tokens. In addition, we apply \\textbf{Hierarchical Semantic Regularization} to align the decoding features of earlier tokens with those from a pre-trained vision foundation model, while progressively reducing the regularization strength toward the tail to allow finer low-level detail reconstruction. Extensive experiments demonstrate the effectiveness of ReTok: on ImageNet 256$\\times$256, our method achieves superior generation performance compared with both flexible and fixed-length tokenizers. Code will be available at: \\href{https://github.com/zfu006/ReTok}{https://github.com/zfu006/ReTok}", "AI": {"tldr": "ReToK introduces redundant token padding and hierarchical semantic regularization to improve flexible image tokenizers by distributing information across all tokens instead of concentrating it in early tokens.", "motivation": "Current flexible image tokenizers using nested dropout concentrate image information in early tokens, limiting effectiveness for autoregressive image generation as token length increases.", "method": "Proposes ReToK with two key components: 1) Redundant Token Padding to activate tail tokens more frequently, 2) Hierarchical Semantic Regularization that aligns earlier tokens with pre-trained vision foundation model features while reducing regularization strength toward tail tokens.", "result": "Achieves superior generation performance on ImageNet 256\u00d7256 compared to both flexible and fixed-length tokenizers.", "conclusion": "ReToK effectively overcomes limitations of existing flexible tokenizers by better distributing information across all tokens, enabling improved latent modeling for autoregressive image generation."}}
{"id": "2601.01537", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.01537", "abs": "https://arxiv.org/abs/2601.01537", "authors": ["Gong Gao", "Zekai Wang", "Xianhui Liu", "Weidong Zhao"], "title": "FAR-AMTN: Attention Multi-Task Network for Face Attribute Recognition", "comment": "28 pages, 8figures", "summary": "To enhance the generalization performance of Multi-Task Networks (MTN) in Face Attribute Recognition (FAR), it is crucial to share relevant information across multiple related prediction tasks effectively. Traditional MTN methods create shared low-level modules and distinct high-level modules, causing an exponential increase in model parameters with the addition of tasks. This approach also limits feature interaction at the high level, hindering the exploration of semantic relations among attributes, thereby affecting generalization negatively. In response, this study introduces FAR-AMTN, a novel Attention Multi-Task Network for FAR. It incorporates a Weight-Shared Group-Specific Attention (WSGSA) module with shared parameters to minimize complexity while improving group feature representation. Furthermore, a Cross-Group Feature Fusion (CGFF) module is utilized to foster interactions between attribute groups, enhancing feature learning. A Dynamic Weighting Strategy (DWS) is also introduced for synchronized task convergence. Experiments on the CelebA and LFWA datasets demonstrate that the proposed FAR-AMTN demonstrates superior accuracy with significantly fewer parameters compared to existing models.", "AI": {"tldr": "FAR-AMTN: An attention-based multi-task network for face attribute recognition that reduces parameters while improving generalization through weight-shared attention, cross-group feature fusion, and dynamic task weighting.", "motivation": "Traditional multi-task networks for face attribute recognition suffer from exponential parameter growth with added tasks and limited high-level feature interaction, which hinders exploration of semantic relations among attributes and negatively affects generalization performance.", "method": "Proposes FAR-AMTN with three key components: 1) Weight-Shared Group-Specific Attention (WSGSA) module with shared parameters to reduce complexity while improving group feature representation, 2) Cross-Group Feature Fusion (CGFF) module to enable interactions between attribute groups for enhanced feature learning, and 3) Dynamic Weighting Strategy (DWS) for synchronized task convergence.", "result": "Experiments on CelebA and LFWA datasets show that FAR-AMTN achieves superior accuracy with significantly fewer parameters compared to existing models.", "conclusion": "The proposed FAR-AMTN effectively addresses parameter explosion and limited feature interaction in traditional multi-task networks, demonstrating improved generalization for face attribute recognition through attention mechanisms and cross-group feature fusion."}}
{"id": "2601.01547", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.01547", "abs": "https://arxiv.org/abs/2601.01547", "authors": ["Tianjun Gu", "Chenghua Gong", "Jingyu Gong", "Zhizhong Zhang", "Yuan Xie", "Lizhuang Ma", "Xin Tan"], "title": "EscherVerse: An Open World Benchmark and Dataset for Teleo-Spatial Intelligence with Physical-Dynamic and Intent-Driven Understanding", "comment": null, "summary": "The ability to reason about spatial dynamics is a cornerstone of intelligence, yet current research overlooks the human intent behind spatial changes. To address these limitations, we introduce Teleo-Spatial Intelligence (TSI), a new paradigm that unifies two critical pillars: Physical-Dynamic Reasoning--understanding the physical principles of object interactions--and Intent-Driven Reasoning--inferring the human goals behind these actions. To catalyze research in TSI, we present EscherVerse, consisting of a large-scale, open-world benchmark (Escher-Bench), a dataset (Escher-35k), and models (Escher series). Derived from real-world videos, EscherVerse moves beyond constrained settings to explicitly evaluate an agent's ability to reason about object permanence, state transitions, and trajectory prediction in dynamic, human-centric scenarios. Crucially, it is the first benchmark to systematically assess Intent-Driven Reasoning, challenging models to connect physical events to their underlying human purposes. Our work, including a novel data curation pipeline, provides a foundational resource to advance spatial intelligence from passive scene description toward a holistic, purpose-driven understanding of the world.", "AI": {"tldr": "The paper introduces Teleo-Spatial Intelligence (TSI), a new paradigm combining physical-dynamic reasoning and intent-driven reasoning, and presents EscherVerse benchmark, dataset, and models to advance spatial intelligence research.", "motivation": "Current research overlooks human intent behind spatial changes, focusing only on physical dynamics. There's a need to unify understanding of physical object interactions with inference of human goals behind actions.", "method": "Introduces Teleo-Spatial Intelligence (TSI) paradigm and creates EscherVerse: Escher-Bench benchmark, Escher-35k dataset, and Escher series models. Uses real-world videos and novel data curation pipeline to evaluate object permanence, state transitions, trajectory prediction, and intent-driven reasoning.", "result": "EscherVerse is presented as the first benchmark to systematically assess Intent-Driven Reasoning, moving beyond constrained settings to dynamic, human-centric scenarios. It provides foundational resources for advancing spatial intelligence.", "conclusion": "The work advances spatial intelligence from passive scene description toward holistic, purpose-driven understanding by unifying physical dynamics with human intent reasoning through the TSI paradigm and EscherVerse resources."}}
{"id": "2601.01593", "categories": ["cs.CV", "cs.MM"], "pdf": "https://arxiv.org/pdf/2601.01593", "abs": "https://arxiv.org/abs/2601.01593", "authors": ["Haonan Cai", "Yuxuan Luo", "Zhouhui Lian"], "title": "Beyond Patches: Global-aware Autoregressive Model for Multimodal Few-Shot Font Generation", "comment": "25 pages", "summary": "Manual font design is an intricate process that transforms a stylistic visual concept into a coherent glyph set. This challenge persists in automated Few-shot Font Generation (FFG), where models often struggle to preserve both the structural integrity and stylistic fidelity from limited references. While autoregressive (AR) models have demonstrated impressive generative capabilities, their application to FFG is constrained by conventional patch-level tokenization, which neglects global dependencies crucial for coherent font synthesis. Moreover, existing FFG methods remain within the image-to-image paradigm, relying solely on visual references and overlooking the role of language in conveying stylistic intent during font design. To address these limitations, we propose GAR-Font, a novel AR framework for multimodal few-shot font generation. GAR-Font introduces a global-aware tokenizer that effectively captures both local structures and global stylistic patterns, a multimodal style encoder offering flexible style control through a lightweight language-style adapter without requiring intensive multimodal pretraining, and a post-refinement pipeline that further enhances structural fidelity and style coherence. Extensive experiments show that GAR-Font outperforms existing FFG methods, excelling in maintaining global style faithfulness and achieving higher-quality results with textual stylistic guidance.", "AI": {"tldr": "GAR-Font is a novel autoregressive framework for few-shot font generation that uses global-aware tokenization, multimodal style encoding with language guidance, and post-refinement to improve structural integrity and stylistic fidelity.", "motivation": "Current few-shot font generation methods struggle with preserving structural integrity and stylistic fidelity from limited references. Autoregressive models are limited by patch-level tokenization that ignores global dependencies, and existing approaches overlook the role of language in conveying stylistic intent during font design.", "method": "Proposes GAR-Font with three key components: 1) Global-aware tokenizer capturing both local structures and global stylistic patterns, 2) Multimodal style encoder with lightweight language-style adapter for flexible style control without intensive pretraining, 3) Post-refinement pipeline enhancing structural fidelity and style coherence.", "result": "Extensive experiments show GAR-Font outperforms existing few-shot font generation methods, excelling in maintaining global style faithfulness and achieving higher-quality results with textual stylistic guidance.", "conclusion": "GAR-Font successfully addresses limitations in few-shot font generation by incorporating global dependencies, multimodal style control, and refinement techniques, demonstrating superior performance in preserving both structural integrity and stylistic fidelity from limited references."}}
{"id": "2601.01608", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.01608", "abs": "https://arxiv.org/abs/2601.01608", "authors": ["Felix Krause", "Stefan Andreas Baumann", "Johannes Schusterbauer", "Olga Grebenkova", "Ming Gui", "Vincent Tao Hu", "Bj\u00f6rn Ommer"], "title": "Guiding Token-Sparse Diffusion Models", "comment": null, "summary": "Diffusion models deliver high quality in image synthesis but remain expensive during training and inference. Recent works have leveraged the inherent redundancy in visual content to make training more affordable by training only on a subset of visual information. While these methods were successful in providing cheaper and more effective training, sparsely trained diffusion models struggle in inference. This is due to their lacking response to Classifier-free Guidance (CFG) leading to underwhelming performance during inference. To overcome this, we propose Sparse Guidance (SG). Instead of using conditional dropout as a signal to guide diffusion models, SG uses token-level sparsity. As a result, SG preserves the high-variance of the conditional prediction better, achieving good quality and high variance outputs. Leveraging token-level sparsity at inference, SG improves fidelity at lower compute, achieving 1.58 FID on the commonly used ImageNet-256 benchmark with 25% fewer FLOPs, and yields up to 58% FLOP savings at matched baseline quality. To demonstrate the effectiveness of Sparse Guidance, we train a 2.5B text-to-image diffusion model using training time sparsity and leverage SG during inference. SG achieves improvements in composition and human preference score while increasing throughput at the same time.", "AI": {"tldr": "Sparse Guidance (SG) improves inference efficiency for sparsely trained diffusion models by using token-level sparsity instead of conditional dropout, achieving better quality with fewer FLOPs.", "motivation": "Sparsely trained diffusion models are cheaper to train but struggle during inference due to poor response to Classifier-free Guidance (CFG), leading to underwhelming performance.", "method": "Proposes Sparse Guidance (SG) which uses token-level sparsity instead of conditional dropout to guide diffusion models, preserving high-variance of conditional predictions.", "result": "SG achieves 1.58 FID on ImageNet-256 with 25% fewer FLOPs, yields up to 58% FLOP savings at matched baseline quality, and improves composition/human preference scores in 2.5B text-to-image models.", "conclusion": "Sparse Guidance enables efficient inference for sparsely trained diffusion models while maintaining or improving output quality, making diffusion models more practical for real-world applications."}}
{"id": "2601.01613", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.01613", "abs": "https://arxiv.org/abs/2601.01613", "authors": ["Kazi Ramisa Rifa", "Jie Zhang", "Abdullah Imran"], "title": "CAP-IQA: Context-Aware Prompt-Guided CT Image Quality Assessment", "comment": "18 pages, 9 figures, 5 tables", "summary": "Prompt-based methods, which encode medical priors through descriptive text, have been only minimally explored for CT Image Quality Assessment (IQA). While such prompts can embed prior knowledge about diagnostic quality, they often introduce bias by reflecting idealized definitions that may not hold under real-world degradations such as noise, motion artifacts, or scanner variability. To address this, we propose the Context-Aware Prompt-guided Image Quality Assessment (CAP-IQA) framework, which integrates text-level priors with instance-level context prompts and applies causal debiasing to separate idealized knowledge from factual, image-specific degradations. Our framework combines a CNN-based visual encoder with a domain-specific text encoder to assess diagnostic visibility, anatomical clarity, and noise perception in abdominal CT images. The model leverages radiology-style prompts and context-aware fusion to align semantic and perceptual representations. On the 2023 LDCTIQA challenge benchmark, CAP-IQA achieves an overall correlation score of 2.8590 (sum of PLCC, SROCC, and KROCC), surpassing the top-ranked leaderboard team (2.7427) by 4.24%. Moreover, our comprehensive ablation experiments confirm that prompt-guided fusion and the simplified encoder-only design jointly enhance feature alignment and interpretability. Furthermore, evaluation on an in-house dataset of 91,514 pediatric CT images demonstrates the true generalizability of CAP-IQA in assessing perceptual fidelity in a different patient population.", "AI": {"tldr": "CAP-IQA framework integrates text-level priors with instance-level context prompts and causal debiasing for CT image quality assessment, outperforming previous methods on benchmark datasets.", "motivation": "Existing prompt-based methods for CT IQA often introduce bias by reflecting idealized definitions that don't hold under real-world degradations like noise, motion artifacts, or scanner variability.", "method": "Proposes Context-Aware Prompt-guided IQA (CAP-IQA) framework that integrates text-level priors with instance-level context prompts and applies causal debiasing to separate idealized knowledge from factual image-specific degradations. Combines CNN-based visual encoder with domain-specific text encoder to assess diagnostic visibility, anatomical clarity, and noise perception.", "result": "On 2023 LDCTIQA challenge benchmark, CAP-IQA achieves overall correlation score of 2.8590 (sum of PLCC, SROCC, KROCC), surpassing top-ranked leaderboard team (2.7427) by 4.24%. Evaluation on in-house dataset of 91,514 pediatric CT images demonstrates generalizability to different patient populations.", "conclusion": "CAP-IQA effectively addresses bias in prompt-based CT IQA through context-aware fusion and causal debiasing, achieving state-of-the-art performance and demonstrating strong generalizability across different patient populations."}}
{"id": "2601.01639", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.01639", "abs": "https://arxiv.org/abs/2601.01639", "authors": ["Gaurav Sekar"], "title": "An Empirical Study of Monocular Human Body Measurement Under Weak Calibration", "comment": "The paper consists of 8 pages, 2 figures (on pages 4 and 7), and 2 tables (both on page 6)", "summary": "Estimating human body measurements from monocular RGB imagery remains challenging due to scale ambiguity, viewpoint sensitivity, and the absence of explicit depth information. This work presents a systematic empirical study of three weakly calibrated monocular strategies: landmark-based geometry, pose-driven regression, and object-calibrated silhouettes, evaluated under semi-constrained conditions using consumer-grade cameras. Rather than pursuing state-of-the-art accuracy, the study analyzes how differing calibration assumptions influence measurement behavior, robustness, and failure modes across varied body types. The results reveal a clear trade-off between user effort during calibration and the stability of resulting circumferential quantities. This paper serves as an empirical design reference for lightweight monocular human measurement systems intended for deployment on consumer devices.", "AI": {"tldr": "Systematic empirical study comparing three weakly calibrated monocular strategies for human body measurement from RGB images, analyzing calibration assumptions' impact on measurement behavior rather than pursuing state-of-the-art accuracy.", "motivation": "Human body measurement from monocular RGB imagery is challenging due to scale ambiguity, viewpoint sensitivity, and lack of depth information. The paper aims to provide empirical design guidance for lightweight systems deployable on consumer devices.", "method": "Empirical study of three weakly calibrated monocular strategies: landmark-based geometry, pose-driven regression, and object-calibrated silhouettes. Evaluated under semi-constrained conditions using consumer-grade cameras, focusing on how calibration assumptions affect measurement behavior.", "result": "Reveals clear trade-off between user effort during calibration and stability of resulting circumferential quantities. Analyzes how different calibration assumptions influence measurement robustness and failure modes across varied body types.", "conclusion": "The paper serves as an empirical design reference for lightweight monocular human measurement systems intended for deployment on consumer devices, providing insights into calibration trade-offs rather than pursuing state-of-the-art accuracy."}}
{"id": "2601.01660", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.01660", "abs": "https://arxiv.org/abs/2601.01660", "authors": ["Aymen Mir", "Riza Alp Guler", "Jian Wang", "Gerard Pons-Moll", "Bing Zhou"], "title": "Animated 3DGS Avatars in Diverse Scenes with Consistent Lighting and Shadows", "comment": "Our project page is available at https://miraymen.github.io/dgsm", "summary": "We present a method for consistent lighting and shadows when animated 3D Gaussian Splatting (3DGS) avatars interact with 3DGS scenes or with dynamic objects inserted into otherwise static scenes. Our key contribution is Deep Gaussian Shadow Maps (DGSM), a modern analogue of the classical shadow mapping algorithm tailored to the volumetric 3DGS representation. Building on the classic deep shadow mapping idea, we show that 3DGS admits closed form light accumulation along light rays, enabling volumetric shadow computation without meshing. For each estimated light, we tabulate transmittance over concentric radial shells and store them in octahedral atlases, which modern GPUs can sample in real time per query to attenuate affected scene Gaussians and thus cast and receive shadows consistently. To relight moving avatars, we approximate the local environment illumination with HDRI probes represented in a spherical harmonic (SH) basis and apply a fast per Gaussian radiance transfer, avoiding explicit BRDF estimation or offline optimization. We demonstrate environment consistent lighting for avatars from AvatarX and ActorsHQ, composited into ScanNet++, DL3DV, and SuperSplat scenes, and show interactions with inserted objects. Across single and multi avatar settings, DGSM and SH relighting operate fully in the volumetric 3DGS representation, yielding coherent shadows and relighting while avoiding meshing.", "AI": {"tldr": "Deep Gaussian Shadow Maps (DGSM) enable consistent lighting and shadows for animated 3D Gaussian Splatting avatars interacting with 3DGS scenes using volumetric shadow computation without meshing, combined with spherical harmonic relighting.", "motivation": "To achieve consistent lighting and shadows when animated 3D Gaussian Splatting avatars interact with 3DGS scenes or dynamic objects in static scenes, avoiding the need for meshing while maintaining real-time performance.", "method": "DGSM adapts classical shadow mapping to 3DGS by computing closed-form light accumulation along rays and storing transmittance in octahedral atlases. For relighting, HDRI probes in spherical harmonic basis enable fast per-Gaussian radiance transfer without BRDF estimation.", "result": "Demonstrated environment-consistent lighting for avatars from AvatarX and ActorsHQ composited into ScanNet++, DL3DV, and SuperSplat scenes, showing coherent shadows and relighting in single and multi-avatar settings without meshing.", "conclusion": "DGSM and SH relighting provide a complete volumetric solution for consistent lighting and shadows in 3DGS scenes, enabling real-time avatar-scene interactions while avoiding the limitations of mesh-based approaches."}}
{"id": "2601.01676", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.01676", "abs": "https://arxiv.org/abs/2601.01676", "authors": ["Jin Yao", "Radowan Mahmud Redoy", "Sebastian Elbaum", "Matthew B. Dwyer", "Zezhou Cheng"], "title": "LabelAny3D: Label Any Object 3D in the Wild", "comment": "NeurIPS 2025. Project page: https://uva-computer-vision-lab.github.io/LabelAny3D/", "summary": "Detecting objects in 3D space from monocular input is crucial for applications ranging from robotics to scene understanding. Despite advanced performance in the indoor and autonomous driving domains, existing monocular 3D detection models struggle with in-the-wild images due to the lack of 3D in-the-wild datasets and the challenges of 3D annotation. We introduce LabelAny3D, an \\emph{analysis-by-synthesis} framework that reconstructs holistic 3D scenes from 2D images to efficiently produce high-quality 3D bounding box annotations. Built on this pipeline, we present COCO3D, a new benchmark for open-vocabulary monocular 3D detection, derived from the MS-COCO dataset and covering a wide range of object categories absent from existing 3D datasets. Experiments show that annotations generated by LabelAny3D improve monocular 3D detection performance across multiple benchmarks, outperforming prior auto-labeling approaches in quality. These results demonstrate the promise of foundation-model-driven annotation for scaling up 3D recognition in realistic, open-world settings.", "AI": {"tldr": "LabelAny3D is an analysis-by-synthesis framework that generates 3D bounding box annotations from 2D images, enabling creation of COCO3D benchmark for open-vocabulary monocular 3D detection.", "motivation": "Existing monocular 3D detection models struggle with in-the-wild images due to lack of 3D datasets and challenges in 3D annotation. There's a need for scalable 3D recognition in realistic, open-world settings.", "method": "LabelAny3D uses an analysis-by-synthesis framework that reconstructs holistic 3D scenes from 2D images to efficiently produce high-quality 3D bounding box annotations. This pipeline is used to create COCO3D from MS-COCO dataset.", "result": "Annotations from LabelAny3D improve monocular 3D detection performance across multiple benchmarks, outperforming prior auto-labeling approaches in quality. COCO3D covers wide range of object categories absent from existing 3D datasets.", "conclusion": "Foundation-model-driven annotation shows promise for scaling up 3D recognition in realistic, open-world settings, addressing the data bottleneck in monocular 3D detection."}}
{"id": "2601.01677", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.01677", "abs": "https://arxiv.org/abs/2601.01677", "authors": ["Zhengsen Xu", "Lanying Wang", "Sibo Cheng", "Xue Rui", "Kyle Gao", "Yimin Zhu", "Mabel Heffring", "Zack Dewis", "Saeid Taleghanidoozdoozan", "Megan Greenwood", "Motasem Alkayid", "Quinn Ledingham", "Hongjie He", "Jonathan Li", "Lincoln Linlin Xu"], "title": "Trustworthy Data-Driven Wildfire Risk Prediction and Understanding in Western Canada", "comment": null, "summary": "In recent decades, the intensification of wildfire activity in western Canada has resulted in substantial socio-economic and environmental losses. Accurate wildfire risk prediction is hindered by the intrinsic stochasticity of ignition and spread and by nonlinear interactions among fuel conditions, meteorology, climate variability, topography, and human activities, challenging the reliability and interpretability of purely data-driven models. We propose a trustworthy data-driven wildfire risk prediction framework based on long-sequence, multi-scale temporal modeling, which integrates heterogeneous drivers while explicitly quantifying predictive uncertainty and enabling process-level interpretation. Evaluated over western Canada during the record-breaking 2023 and 2024 fire seasons, the proposed model outperforms existing time-series approaches, achieving an F1 score of 0.90 and a PR-AUC of 0.98 with low computational cost. Uncertainty-aware analysis reveals structured spatial and seasonal patterns in predictive confidence, highlighting increased uncertainty associated with ambiguous predictions and spatiotemporal decision boundaries. SHAP-based interpretation provides mechanistic understanding of wildfire controls, showing that temperature-related drivers dominate wildfire risk in both years, while moisture-related constraints play a stronger role in shaping spatial and land-cover-specific contrasts in 2024 compared to the widespread hot and dry conditions of 2023. Data and code are available at https://github.com/SynUW/mmFire.", "AI": {"tldr": "A trustworthy data-driven wildfire risk prediction framework using long-sequence, multi-scale temporal modeling that integrates heterogeneous drivers, quantifies uncertainty, and enables process-level interpretation, achieving state-of-the-art performance for western Canada wildfires.", "motivation": "Wildfire intensification in western Canada causes substantial socio-economic and environmental losses, but accurate prediction is challenging due to stochastic ignition/spread and nonlinear interactions among multiple factors, limiting reliability and interpretability of purely data-driven models.", "method": "A trustworthy data-driven framework based on long-sequence, multi-scale temporal modeling that integrates heterogeneous drivers (fuel conditions, meteorology, climate variability, topography, human activities) while explicitly quantifying predictive uncertainty and enabling process-level interpretation.", "result": "Outperforms existing time-series approaches with F1 score of 0.90 and PR-AUC of 0.98 on 2023-2024 western Canada fire seasons, with low computational cost. Uncertainty analysis reveals spatial/seasonal patterns in predictive confidence. SHAP interpretation shows temperature dominates wildfire risk, with moisture constraints playing stronger role in 2024 spatial contrasts.", "conclusion": "The proposed framework provides accurate, computationally efficient wildfire risk prediction with uncertainty quantification and mechanistic interpretation, offering valuable insights for wildfire management and demonstrating the importance of trustworthy AI approaches for complex environmental systems."}}
{"id": "2601.01680", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.01680", "abs": "https://arxiv.org/abs/2601.01680", "authors": ["Afzal Hossain", "Mst Rumana Sumi", "Stephanie Schuckers"], "title": "Evaluating Deep Learning-Based Face Recognition for Infants and Toddlers: Impact of Age Across Developmental Stages", "comment": "Accepted and presented at IEEE IJCB 2025 conference; final published version forthcoming", "summary": "Face recognition for infants and toddlers presents unique challenges due to rapid facial morphology changes, high inter-class similarity, and limited dataset availability. This study evaluates the performance of four deep learning-based face recognition models FaceNet, ArcFace, MagFace, and CosFace on a newly developed longitudinal dataset collected over a 24 month period in seven sessions involving children aged 0 to 3 years. Our analysis examines recognition accuracy across developmental stages, showing that the True Accept Rate (TAR) is only 30.7% at 0.1% False Accept Rate (FAR) for infants aged 0 to 6 months, due to unstable facial features. Performance improves significantly in older children, reaching 64.7% TAR at 0.1% FAR in the 2.5 to 3 year age group. We also evaluate verification performance over different time intervals, revealing that shorter time gaps result in higher accuracy due to reduced embedding drift. To mitigate this drift, we apply a Domain Adversarial Neural Network (DANN) approach that improves TAR by over 12%, yielding features that are more temporally stable and generalizable. These findings are critical for building biometric systems that function reliably over time in smart city applications such as public healthcare, child safety, and digital identity services. The challenges observed in early age groups highlight the importance of future research on privacy preserving biometric authentication systems that can address temporal variability, particularly in secure and regulated urban environments where child verification is essential.", "AI": {"tldr": "Face recognition for infants/toddlers is challenging due to rapid facial changes, high similarity, and limited data. This study evaluates FaceNet, ArcFace, MagFace, and CosFace on a longitudinal 0-3 year dataset, showing poor performance in infants (30.7% TAR at 0.1% FAR) but improvement with age (64.7% TAR at 2.5-3 years). DANN reduces embedding drift, improving TAR by 12%.", "motivation": "Infant/toddler face recognition faces unique challenges: rapid facial morphology changes during development, high inter-class similarity among young children, and limited availability of longitudinal datasets. These challenges are critical for biometric systems in smart city applications like public healthcare, child safety, and digital identity services that require reliable child verification over time.", "method": "The study evaluates four deep learning face recognition models (FaceNet, ArcFace, MagFace, CosFace) on a newly developed longitudinal dataset collected over 24 months in seven sessions with children aged 0-3 years. They analyze recognition accuracy across developmental stages and time intervals, and apply Domain Adversarial Neural Network (DANN) to mitigate embedding drift caused by temporal changes.", "result": "Infants aged 0-6 months show poor performance (30.7% TAR at 0.1% FAR) due to unstable facial features. Performance improves significantly with age, reaching 64.7% TAR at 0.1% FAR for children aged 2.5-3 years. Shorter time gaps yield higher accuracy due to reduced embedding drift. DANN improves TAR by over 12%, producing more temporally stable and generalizable features.", "conclusion": "Infant/toddler face recognition is challenging but improves with age. Temporal drift significantly affects performance, but DANN effectively mitigates this issue. The findings are crucial for developing reliable biometric systems for smart city applications requiring child verification. Future research should focus on privacy-preserving biometric authentication systems that address temporal variability in secure urban environments."}}
{"id": "2601.01687", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.01687", "abs": "https://arxiv.org/abs/2601.01687", "authors": ["Abdur R. Fayjie", "Pankhi Kashyap", "Jutika Borah", "Patrick Vandewalle"], "title": "FALCON: Few-Shot Adversarial Learning for Cross-Domain Medical Image Segmentation", "comment": "20 pages, 6 figures, 7 tables", "summary": "Precise delineation of anatomical and pathological structures within 3D medical volumes is crucial for accurate diagnosis, effective surgical planning, and longitudinal disease monitoring. Despite advancements in AI, clinically viable segmentation is often hindered by the scarcity of 3D annotations, patient-specific variability, data privacy concerns, and substantial computational overhead. In this work, we propose FALCON, a cross-domain few-shot segmentation framework that achieves high-precision 3D volume segmentation by processing data as 2D slices. The framework is first meta-trained on natural images to learn-to-learn generalizable segmentation priors, then transferred to the medical domain via adversarial fine-tuning and boundary-aware learning. Task-aware inference, conditioned on support cues, allows FALCON to adapt dynamically to patient-specific anatomical variations across slices. Experiments on four benchmarks demonstrate that FALCON consistently achieves the lowest Hausdorff Distance scores, indicating superior boundary accuracy while maintaining a Dice Similarity Coefficient comparable to the state-of-the-art models. Notably, these results are achieved with significantly less labeled data, no data augmentation, and substantially lower computational overhead.", "AI": {"tldr": "FALCON is a cross-domain few-shot 3D medical segmentation framework that uses 2D slices, meta-trained on natural images and fine-tuned on medical data, achieving superior boundary accuracy with minimal labeled data and computational overhead.", "motivation": "Current 3D medical segmentation faces challenges: scarcity of 3D annotations, patient-specific variability, data privacy concerns, and high computational costs. These limitations hinder clinically viable AI segmentation solutions.", "method": "FALCON processes 3D volumes as 2D slices. It's first meta-trained on natural images to learn generalizable segmentation priors, then transferred to medical domain via adversarial fine-tuning and boundary-aware learning. Task-aware inference adapts dynamically to patient-specific variations across slices.", "result": "On four benchmarks, FALCON consistently achieves the lowest Hausdorff Distance scores (superior boundary accuracy) while maintaining comparable Dice Similarity Coefficient to state-of-the-art models. Achieved with significantly less labeled data, no data augmentation, and substantially lower computational overhead.", "conclusion": "FALCON provides an efficient, data-efficient solution for precise 3D medical segmentation by leveraging cross-domain learning and few-shot adaptation, addressing key clinical challenges of annotation scarcity and computational demands."}}
{"id": "2601.01689", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.01689", "abs": "https://arxiv.org/abs/2601.01689", "authors": ["Afzal Hossain", "Stephanie Schuckers"], "title": "Mitigating Longitudinal Performance Degradation in Child Face Recognition Using Synthetic Data", "comment": null, "summary": "Longitudinal face recognition in children remains challenging due to rapid and nonlinear facial growth, which causes template drift and increasing verification errors over time. This work investigates whether synthetic face data can act as a longitudinal stabilizer by improving temporal robustness of child face recognition models. Using an identity disjoint protocol on the Young Face Aging (YFA) dataset, we evaluate three settings: (i) pretrained MagFace embeddings without dataset specific fine-tuning, (ii) MagFace fine-tuned using authentic training faces only, and (iii) MagFace fine-tuned using a combination of authentic and synthetically generated training faces. Synthetic data is generated using StyleGAN2 ADA and incorporated exclusively within the training identities; a post generation filtering step is applied to mitigate identity leakage and remove artifact affected samples. Experimental results across enrollment verification gaps from 6 to 36 months show that synthetic-augmented fine tuning substantially reduces error rates relative to both the pretrained baseline and real only fine tuning. These findings provide a risk aware assessment of synthetic augmentation for improving identity persistence in pediatric face recognition.", "AI": {"tldr": "Synthetic face data improves child face recognition by reducing verification errors caused by facial growth over time.", "motivation": "Longitudinal face recognition in children is challenging due to rapid facial growth causing template drift and increasing verification errors over time.", "method": "Used identity disjoint protocol on YFA dataset with three settings: 1) pretrained MagFace embeddings, 2) MagFace fine-tuned with authentic faces only, 3) MagFace fine-tuned with authentic + synthetic faces. Synthetic data generated using StyleGAN2 ADA with post-generation filtering to prevent identity leakage.", "result": "Synthetic-augmented fine tuning substantially reduces error rates across enrollment verification gaps (6-36 months) compared to pretrained baseline and real-only fine tuning.", "conclusion": "Synthetic augmentation provides a risk-aware approach to improving identity persistence in pediatric face recognition by acting as a longitudinal stabilizer."}}
{"id": "2601.01695", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.01695", "abs": "https://arxiv.org/abs/2601.01695", "authors": ["Ruiyu Mao", "Baoming Zhang", "Nicholas Ruozzi", "Yunhui Guo"], "title": "Learnability-Driven Submodular Optimization for Active Roadside 3D Detection", "comment": "10 pages, 7 figures. Submitted to CVPR 2026", "summary": "Roadside perception datasets are typically constructed via cooperative labeling between synchronized vehicle and roadside frame pairs. However, real deployment often requires annotation of roadside-only data due to hardware and privacy constraints. Even human experts struggle to produce accurate labels without vehicle-side data (image, LIDAR), which not only increases annotation difficulty and cost, but also reveals a fundamental learnability problem: many roadside-only scenes contain distant, blurred, or occluded objects whose 3D properties are ambiguous from a single view and can only be reliably annotated by cross-checking paired vehicle--roadside frames. We refer to such cases as inherently ambiguous samples. To reduce wasted annotation effort on inherently ambiguous samples while still obtaining high-performing models, we turn to active learning. This work focuses on active learning for roadside monocular 3D object detection and proposes a learnability-driven framework that selects scenes which are both informative and reliably labelable, suppressing inherently ambiguous samples while ensuring coverage. Experiments demonstrate that our method, LH3D, achieves 86.06%, 67.32%, and 78.67% of full-performance for vehicles, pedestrians, and cyclists respectively, using only 25% of the annotation budget on DAIR-V2X-I, significantly outperforming uncertainty-based baselines. This confirms that learnability, not uncertainty, matters for roadside 3D perception.", "AI": {"tldr": "LH3D: Active learning framework for roadside monocular 3D detection that selects informative and reliably labelable scenes, suppressing inherently ambiguous samples to reduce annotation waste while maintaining performance.", "motivation": "Real-world roadside perception often requires annotation of roadside-only data due to hardware/privacy constraints, but many scenes contain distant, blurred, or occluded objects whose 3D properties are ambiguous from single view. Human experts struggle without paired vehicle data, increasing annotation difficulty/cost and revealing fundamental learnability problems.", "method": "Proposes learnability-driven active learning framework for roadside monocular 3D object detection. Selects scenes that are both informative and reliably labelable, suppressing inherently ambiguous samples while ensuring coverage. Focuses on learnability rather than uncertainty for sample selection.", "result": "LH3D achieves 86.06%, 67.32%, and 78.67% of full-performance for vehicles, pedestrians, and cyclists respectively using only 25% of annotation budget on DAIR-V2X-I dataset, significantly outperforming uncertainty-based baselines.", "conclusion": "Learnability, not uncertainty, matters for roadside 3D perception. The proposed framework effectively reduces wasted annotation effort on inherently ambiguous samples while obtaining high-performing models through strategic sample selection."}}
{"id": "2601.01696", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2601.01696", "abs": "https://arxiv.org/abs/2601.01696", "authors": ["Yian Liu", "Xiong Wang", "Ping Xu", "Lei Zhu", "Ming Yan", "Linyun Xue"], "title": "Real-Time Lane Detection via Efficient Feature Alignment and Covariance Optimization for Low-Power Embedded Systems", "comment": null, "summary": "Real-time lane detection in embedded systems encounters significant challenges due to subtle and sparse visual signals in RGB images, often constrained by limited computational resources and power consumption. Although deep learning models for lane detection categorized into segmentation-based, anchor-based, and curve-based methods there remains a scarcity of universally applicable optimization techniques tailored for low-power embedded environments. To overcome this, we propose an innovative Covariance Distribution Optimization (CDO) module specifically designed for efficient, real-time applications. The CDO module aligns lane feature distributions closely with ground-truth labels, significantly enhancing detection accuracy without increasing computational complexity. Evaluations were conducted on six diverse models across all three method categories, including two optimized for real-time applications and four state-of-the-art (SOTA) models, tested comprehensively on three major datasets: CULane, TuSimple, and LLAMAS. Experimental results demonstrate accuracy improvements ranging from 0.01% to 1.5%. The proposed CDO module is characterized by ease of integration into existing systems without structural modifications and utilizes existing model parameters to facilitate ongoing training, thus offering substantial benefits in performance, power efficiency, and operational flexibility in embedded systems.", "AI": {"tldr": "A Covariance Distribution Optimization (CDO) module is proposed to improve lane detection accuracy in embedded systems without increasing computational complexity, achieving 0.01-1.5% accuracy gains across various models and datasets.", "motivation": "Real-time lane detection in embedded systems faces challenges due to subtle/sparse visual signals in RGB images, limited computational resources, and power constraints. There's a scarcity of universally applicable optimization techniques tailored for low-power embedded environments.", "method": "Proposes a Covariance Distribution Optimization (CDO) module that aligns lane feature distributions with ground-truth labels to enhance detection accuracy without adding computational complexity. The module is easily integrated into existing systems without structural modifications and uses existing model parameters for ongoing training.", "result": "Evaluated on six diverse models across three method categories (segmentation-based, anchor-based, curve-based) including real-time optimized and SOTA models. Tested on three major datasets (CULane, TuSimple, LLAMAS), achieving accuracy improvements ranging from 0.01% to 1.5%.", "conclusion": "The CDO module offers substantial benefits in performance, power efficiency, and operational flexibility for embedded systems, providing an effective optimization solution for real-time lane detection without increasing computational burden."}}
{"id": "2601.01720", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.01720", "abs": "https://arxiv.org/abs/2601.01720", "authors": ["Xijie Huang", "Chengming Xu", "Donghao Luo", "Xiaobin Hu", "Peng Tang", "Xu Peng", "Jiangning Zhang", "Chengjie Wang", "Yanwei Fu"], "title": "FFP-300K: Scaling First-Frame Propagation for Generalizable Video Editing", "comment": null, "summary": "First-Frame Propagation (FFP) offers a promising paradigm for controllable video editing, but existing methods are hampered by a reliance on cumbersome run-time guidance. We identify the root cause of this limitation as the inadequacy of current training datasets, which are often too short, low-resolution, and lack the task diversity required to teach robust temporal priors. To address this foundational data gap, we first introduce FFP-300K, a new large-scale dataset comprising 300K high-fidelity video pairs at 720p resolution and 81 frames in length, constructed via a principled two-track pipeline for diverse local and global edits. Building on this dataset, we propose a novel framework designed for true guidance-free FFP that resolves the critical tension between maintaining first-frame appearance and preserving source video motion. Architecturally, we introduce Adaptive Spatio-Temporal RoPE (AST-RoPE), which dynamically remaps positional encodings to disentangle appearance and motion references. At the objective level, we employ a self-distillation strategy where an identity propagation task acts as a powerful regularizer, ensuring long-term temporal stability and preventing semantic drift. Comprehensive experiments on the EditVerseBench benchmark demonstrate that our method significantly outperforming existing academic and commercial models by receiving about 0.2 PickScore and 0.3 VLM score improvement against these competitors.", "AI": {"tldr": "The paper introduces FFP-300K dataset and a guidance-free First-Frame Propagation framework with AST-RoPE and self-distillation for superior video editing performance.", "motivation": "Existing First-Frame Propagation methods rely on cumbersome run-time guidance due to inadequate training datasets that are too short, low-resolution, and lack task diversity, preventing robust temporal priors learning.", "method": "1) Create FFP-300K dataset with 300K high-fidelity 720p video pairs (81 frames) via two-track pipeline for diverse edits. 2) Propose guidance-free FFP framework with Adaptive Spatio-Temporal RoPE (AST-RoPE) to disentangle appearance/motion references. 3) Use self-distillation with identity propagation as regularizer for temporal stability.", "result": "Significantly outperforms existing academic and commercial models on EditVerseBench benchmark with ~0.2 PickScore and ~0.3 VLM score improvements.", "conclusion": "The proposed dataset and framework enable true guidance-free First-Frame Propagation by resolving appearance-motion tension through architectural (AST-RoPE) and objective (self-distillation) innovations, achieving state-of-the-art video editing performance."}}
{"id": "2601.01746", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.01746", "abs": "https://arxiv.org/abs/2601.01746", "authors": ["Lintong Wei", "Jian Lu", "Haozhe Cheng", "Jihua Zhu", "Kaibing Zhang"], "title": "Point-SRA: Self-Representation Alignment for 3D Representation Learning", "comment": "This is an AAAI 2026 accepted paper titled \"Point-SRA: Self-Representation Alignment for 3D Representation Learning\", spanning 13 pages in total. The submission includes 7 figures (fig1 to fig7) that visually support the technical analysis", "summary": "Masked autoencoders (MAE) have become a dominant paradigm in 3D representation learning, setting new performance benchmarks across various downstream tasks. Existing methods with fixed mask ratio neglect multi-level representational correlations and intrinsic geometric structures, while relying on point-wise reconstruction assumptions that conflict with the diversity of point cloud. To address these issues, we propose a 3D representation learning method, termed Point-SRA, which aligns representations through self-distillation and probabilistic modeling. Specifically, we assign different masking ratios to the MAE to capture complementary geometric and semantic information, while the MeanFlow Transformer (MFT) leverages cross-modal conditional embeddings to enable diverse probabilistic reconstruction. Our analysis further reveals that representations at different time steps in MFT also exhibit complementarity. Therefore, a Dual Self-Representation Alignment mechanism is proposed at both the MAE and MFT levels. Finally, we design a Flow-Conditioned Fine-Tuning Architecture to fully exploit the point cloud distribution learned via MeanFlow. Point-SRA outperforms Point-MAE by 5.37% on ScanObjectNN. On intracranial aneurysm segmentation, it reaches 96.07% mean IoU for arteries and 86.87% for aneurysms. For 3D object detection, Point-SRA achieves 47.3% AP@50, surpassing MaskPoint by 5.12%.", "AI": {"tldr": "Point-SRA improves 3D representation learning by using multi-level masking ratios and probabilistic modeling with MeanFlow Transformer, achieving state-of-the-art performance on various 3D tasks.", "motivation": "Existing masked autoencoder methods for 3D point clouds have limitations: fixed mask ratios ignore multi-level correlations, point-wise reconstruction conflicts with point cloud diversity, and they neglect intrinsic geometric structures.", "method": "1) Uses different masking ratios in MAE to capture complementary geometric/semantic information; 2) MeanFlow Transformer with cross-modal conditional embeddings for diverse probabilistic reconstruction; 3) Dual Self-Representation Alignment at both MAE and MFT levels; 4) Flow-Conditioned Fine-Tuning Architecture.", "result": "Outperforms Point-MAE by 5.37% on ScanObjectNN; achieves 96.07% mean IoU for arteries and 86.87% for aneurysms in intracranial aneurysm segmentation; reaches 47.3% AP@50 for 3D object detection, surpassing MaskPoint by 5.12%.", "conclusion": "Point-SRA effectively addresses limitations of existing 3D MAE methods by capturing multi-level representations and modeling point cloud diversity through probabilistic approaches, demonstrating superior performance across diverse downstream tasks."}}
{"id": "2601.01749", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.01749", "abs": "https://arxiv.org/abs/2601.01749", "authors": ["Lei Zhu", "Lijian Lin", "Ye Zhu", "Jiahao Wu", "Xuehan Hou", "Yu Li", "Yunfei Liu", "Jie Chen"], "title": "MANGO:Natural Multi-speaker 3D Talking Head Generation via 2D-Lifted Enhancement", "comment": "20 pages, 11i figures", "summary": "Current audio-driven 3D head generation methods mainly focus on single-speaker scenarios, lacking natural, bidirectional listen-and-speak interaction. Achieving seamless conversational behavior, where speaking and listening states transition fluidly remains a key challenge. Existing 3D conversational avatar approaches rely on error-prone pseudo-3D labels that fail to capture fine-grained facial dynamics. To address these limitations, we introduce a novel two-stage framework MANGO, which leveraging pure image-level supervision by alternately training to mitigate the noise introduced by pseudo-3D labels, thereby achieving better alignment with real-world conversational behaviors. Specifically, in the first stage, a diffusion-based transformer with a dual-audio interaction module models natural 3D motion from multi-speaker audio. In the second stage, we use a fast 3D Gaussian Renderer to generate high-fidelity images and provide 2D-level photometric supervision for the 3D motions through alternate training. Additionally, we introduce MANGO-Dialog, a high-quality dataset with over 50 hours of aligned 2D-3D conversational data across 500+ identities. Extensive experiments demonstrate that our method achieves exceptional accuracy and realism in modeling two-person 3D dialogue motion, significantly advancing the fidelity and controllability of audio-driven talking heads.", "AI": {"tldr": "MANGO is a two-stage framework for generating realistic 3D conversational avatars from audio, using pure image-level supervision instead of error-prone pseudo-3D labels, achieving better alignment with real-world conversational behaviors.", "motivation": "Current audio-driven 3D head generation methods focus on single-speaker scenarios and lack natural bidirectional listen-and-speak interaction. Existing approaches rely on error-prone pseudo-3D labels that fail to capture fine-grained facial dynamics, making seamless conversational behavior transitions challenging.", "method": "Two-stage framework: 1) Diffusion-based transformer with dual-audio interaction module models natural 3D motion from multi-speaker audio. 2) Fast 3D Gaussian Renderer generates high-fidelity images and provides 2D-level photometric supervision for 3D motions through alternate training, using pure image-level supervision to mitigate noise from pseudo-3D labels.", "result": "Extensive experiments demonstrate exceptional accuracy and realism in modeling two-person 3D dialogue motion, significantly advancing fidelity and controllability of audio-driven talking heads. The method also introduces MANGO-Dialog dataset with over 50 hours of aligned 2D-3D conversational data across 500+ identities.", "conclusion": "MANGO addresses key limitations in conversational avatar generation by eliminating reliance on pseudo-3D labels through image-level supervision, enabling more natural bidirectional interaction and better alignment with real-world conversational behaviors, representing significant progress in audio-driven 3D head generation."}}
{"id": "2601.01769", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.01769", "abs": "https://arxiv.org/abs/2601.01769", "authors": ["Hao Lu", "Ziniu Qian", "Yifu Li", "Yang Zhou", "Bingzheng Wei", "Yan Xu"], "title": "CTIS-QA: Clinical Template-Informed Slide-level Question Answering for Pathology", "comment": "The paper has been accepted by BIBM 2025", "summary": "In this paper, we introduce a clinical diagnosis template-based pipeline to systematically collect and structure pathological information. In collaboration with pathologists and guided by the the College of American Pathologists (CAP) Cancer Protocols, we design a Clinical Pathology Report Template (CPRT) that ensures comprehensive and standardized extraction of diagnostic elements from pathology reports. We validate the effectiveness of our pipeline on TCGA-BRCA. First, we extract pathological features from reports using CPRT. These features are then used to build CTIS-Align, a dataset of 80k slide-description pairs from 804 WSIs for vision-language alignment training, and CTIS-Bench, a rigorously curated VQA benchmark comprising 977 WSIs and 14,879 question-answer pairs. CTIS-Bench emphasizes clinically grounded, closed-ended questions (e.g., tumor grade, receptor status) that reflect real diagnostic workflows, minimize non-visual reasoning, and require genuine slide understanding. We further propose CTIS-QA, a Slide-level Question Answering model, featuring a dual-stream architecture that mimics pathologists' diagnostic approach. One stream captures global slide-level context via clustering-based feature aggregation, while the other focuses on salient local regions through attention-guided patch perception module. Extensive experiments on WSI-VQA, CTIS-Bench, and slide-level diagnostic tasks show that CTIS-QA consistently outperforms existing state-of-the-art models across multiple metrics. Code and data are available at https://github.com/HLSvois/CTIS-QA.", "AI": {"tldr": "A clinical diagnosis template-based pipeline for structured pathology information extraction, creating vision-language datasets and a slide-level QA model that outperforms SOTA on diagnostic tasks.", "motivation": "To systematically collect and structure pathological information from pathology reports using standardized templates, enabling better vision-language alignment and clinically grounded slide understanding for diagnostic workflows.", "method": "1) Design Clinical Pathology Report Template (CPRT) based on CAP Cancer Protocols; 2) Extract pathological features from TCGA-BRCA reports; 3) Build CTIS-Align dataset (80k slide-description pairs) and CTIS-Bench VQA benchmark (977 WSIs, 14,879 QA pairs); 4) Propose CTIS-QA model with dual-stream architecture: global slide-level context via clustering-based aggregation and local regions via attention-guided patch perception.", "result": "CTIS-QA consistently outperforms existing state-of-the-art models across multiple metrics on WSI-VQA, CTIS-Bench, and slide-level diagnostic tasks. The pipeline successfully extracts comprehensive pathological features and creates valuable datasets for vision-language alignment.", "conclusion": "The clinical diagnosis template-based pipeline effectively structures pathology information, and the proposed CTIS-QA model with its dual-stream architecture demonstrates superior performance in slide-level question answering, advancing clinically grounded computational pathology."}}
{"id": "2601.01781", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.01781", "abs": "https://arxiv.org/abs/2601.01781", "authors": ["Lakshay Sharma", "Alex Marin"], "title": "Subimage Overlap Prediction: Task-Aligned Self-Supervised Pretraining For Semantic Segmentation In Remote Sensing Imagery", "comment": "Accepted at CV4EO Workshop at WACV 2026", "summary": "Self-supervised learning (SSL) methods have become a dominant paradigm for creating general purpose models whose capabilities can be transferred to downstream supervised learning tasks. However, most such methods rely on vast amounts of pretraining data. This work introduces Subimage Overlap Prediction, a novel self-supervised pretraining task to aid semantic segmentation in remote sensing imagery that uses significantly lesser pretraining imagery. Given an image, a sub-image is extracted and the model is trained to produce a semantic mask of the location of the extracted sub-image within the original image. We demonstrate that pretraining with this task results in significantly faster convergence, and equal or better performance (measured via mIoU) on downstream segmentation. This gap in convergence and performance widens when labeled training data is reduced. We show this across multiple architecture types, and with multiple downstream datasets. We also show that our method matches or exceeds performance while requiring significantly lesser pretraining data relative to other SSL methods. Code and model weights are provided at \\href{https://github.com/sharmalakshay93/subimage-overlap-prediction}{github.com/sharmalakshay93/subimage-overlap-prediction}.", "AI": {"tldr": "Subimage Overlap Prediction is a novel SSL method for remote sensing segmentation that uses less pretraining data by predicting sub-image locations within original images.", "motivation": "Most SSL methods require vast pretraining data, which is problematic for remote sensing where labeled data is scarce. Need efficient SSL that works with limited pretraining imagery.", "method": "Extract sub-images from original images and train model to predict semantic mask of sub-image location within original image. This creates a self-supervised pretext task for segmentation.", "result": "Faster convergence and equal/better mIoU on downstream segmentation tasks. Performance gap widens with reduced labeled data. Works across multiple architectures and datasets, requiring significantly less pretraining data than other SSL methods.", "conclusion": "Subimage Overlap Prediction is an effective SSL approach for remote sensing segmentation that reduces pretraining data requirements while maintaining or improving performance."}}
{"id": "2601.01784", "categories": ["cs.CV", "cs.MM", "eess.IV"], "pdf": "https://arxiv.org/pdf/2601.01784", "abs": "https://arxiv.org/abs/2601.01784", "authors": ["Boyang Zhao", "Xin Liao", "Jiaxin Chen", "Xiaoshuai Wu", "Yufeng Wu"], "title": "DDNet: A Dual-Stream Graph Learning and Disentanglement Framework for Temporal Forgery Localization", "comment": null, "summary": "The rapid evolution of AIGC technology enables misleading viewers by tampering mere small segments within a video, rendering video-level detection inaccurate and unpersuasive. Consequently, temporal forgery localization (TFL), which aims to precisely pinpoint tampered segments, becomes critical. However, existing methods are often constrained by \\emph{local view}, failing to capture global anomalies. To address this, we propose a \\underline{d}ual-stream graph learning and \\underline{d}isentanglement framework for temporal forgery localization (DDNet). By coordinating a \\emph{Temporal Distance Stream} for local artifacts and a \\emph{Semantic Content Stream} for long-range connections, DDNet prevents global cues from being drowned out by local smoothness. Furthermore, we introduce Trace Disentanglement and Adaptation (TDA) to isolate generic forgery fingerprints, alongside Cross-Level Feature Embedding (CLFE) to construct a robust feature foundation via deep fusion of hierarchical features. Experiments on ForgeryNet and TVIL benchmarks demonstrate that our method outperforms state-of-the-art approaches by approximately 9\\% in AP@0.95, with significant improvements in cross-domain robustness.", "AI": {"tldr": "DDNet: A dual-stream graph learning framework for temporal forgery localization that combines local artifact detection with global semantic analysis to pinpoint tampered video segments.", "motivation": "AIGC technology enables subtle video tampering where only small segments are altered, making video-level detection inaccurate. Existing methods fail to capture global anomalies due to their local view limitations.", "method": "Proposes DDNet with two streams: Temporal Distance Stream for local artifacts and Semantic Content Stream for long-range connections. Includes Trace Disentanglement and Adaptation (TDA) to isolate forgery fingerprints, and Cross-Level Feature Embedding (CLFE) for hierarchical feature fusion.", "result": "Outperforms state-of-the-art by ~9% in AP@0.95 on ForgeryNet and TVIL benchmarks, with significant cross-domain robustness improvements.", "conclusion": "DDNet effectively addresses the limitations of local-view methods by integrating global semantic analysis with local artifact detection, achieving superior temporal forgery localization performance."}}
{"id": "2601.01798", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.01798", "abs": "https://arxiv.org/abs/2601.01798", "authors": ["Syed Abdul Hannan", "Hazim Bukhari", "Thomas Cantalapiedra", "Eman Ansar", "Massa Baali", "Rita Singh", "Bhiksha Raj"], "title": "VerLM: Explaining Face Verification Using Natural Language", "comment": null, "summary": "Face verification systems have seen substantial advancements; however, they often lack transparency in their decision-making processes. In this paper, we introduce an innovative Vision-Language Model (VLM) for Face Verification, which not only accurately determines if two face images depict the same individual but also explicitly explains the rationale behind its decisions. Our model is uniquely trained using two complementary explanation styles: (1) concise explanations that summarize the key factors influencing its decision, and (2) comprehensive explanations detailing the specific differences observed between the images. We adapt and enhance a state-of-the-art modeling approach originally designed for audio-based differentiation to suit visual inputs effectively. This cross-modal transfer significantly improves our model's accuracy and interpretability. The proposed VLM integrates sophisticated feature extraction techniques with advanced reasoning capabilities, enabling clear articulation of its verification process. Our approach demonstrates superior performance, surpassing baseline methods and existing models. These findings highlight the immense potential of vision language models in face verification set up, contributing to more transparent, reliable, and explainable face verification systems.", "AI": {"tldr": "A Vision-Language Model for face verification that not only determines if two faces match but also provides explanations for its decisions using two complementary explanation styles.", "motivation": "Face verification systems lack transparency in decision-making processes, creating a need for more explainable and interpretable models that can articulate their reasoning.", "method": "Adapted and enhanced a state-of-the-art modeling approach originally designed for audio-based differentiation to suit visual inputs. The model is trained with two explanation styles: (1) concise explanations summarizing key factors, and (2) comprehensive explanations detailing specific differences between images. Integrates sophisticated feature extraction with advanced reasoning capabilities.", "result": "Demonstrates superior performance, surpassing baseline methods and existing models. The cross-modal transfer significantly improves both accuracy and interpretability of the face verification system.", "conclusion": "The proposed VLM highlights the potential of vision-language models in face verification, contributing to more transparent, reliable, and explainable face verification systems that can articulate their decision-making process."}}
{"id": "2601.01804", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.01804", "abs": "https://arxiv.org/abs/2601.01804", "authors": ["Zhengjian Kang", "Qi Chen", "Rui Liu", "Kangtong Mo", "Xingyu Zhang", "Xiaoyu Deng", "Ye Zhang"], "title": "Causality-Aware Temporal Projection for Video Understanding in Video-LLMs", "comment": "7 pages, 4 figures", "summary": "Recent Video Large Language Models (Video-LLMs) have shown strong multimodal reasoning capabilities, yet remain challenged by video understanding tasks that require consistent temporal ordering and causal coherence. Many parameter-efficient Video-LLMs rely on unconstrained bidirectional projectors to model inter-frame interactions, which can blur temporal ordering by allowing later frames to influence earlier representations, without explicit architectural mechanisms to respect the directional nature of video reasoning. To address this limitation, we propose V-CORE, a parameter-efficient framework that introduces explicit temporal ordering constraints for video understanding. V-CORE consists of two key components: (1) Learnable Spatial Aggregation (LSA), which adaptively selects salient spatial tokens to reduce redundancy, and (2) a Causality-Aware Temporal Projector (CATP), which enforces structured unidirectional information flow via block-causal attention and a terminal dynamic summary token acting as a causal sink. This design preserves intra-frame spatial interactions while ensuring that temporal information is aggregated in a strictly ordered manner. With 4-bit QLoRA and a frozen LLM backbone, V-CORE can be trained efficiently on a single consumer GPU. Experiments show that V-CORE achieves strong performance on the challenging NExT-QA benchmark, reaching 61.2% accuracy, and remains competitive across MSVD-QA, MSRVTT-QA, and TGIF-QA, with gains concentrated in temporal and causal reasoning subcategories (+3.5% and +5.2% respectively), directly validating the importance of explicit temporal ordering constraints.", "AI": {"tldr": "V-CORE introduces explicit temporal ordering constraints for Video-LLMs using learnable spatial aggregation and causality-aware temporal projection to improve video understanding requiring temporal and causal reasoning.", "motivation": "Current Video-LLMs struggle with video understanding tasks requiring consistent temporal ordering and causal coherence because they use unconstrained bidirectional projectors that blur temporal ordering by allowing later frames to influence earlier representations.", "method": "V-CORE uses two components: (1) Learnable Spatial Aggregation (LSA) to adaptively select salient spatial tokens and reduce redundancy, and (2) Causality-Aware Temporal Projector (CATP) with block-causal attention and a terminal dynamic summary token to enforce unidirectional information flow while preserving intra-frame spatial interactions.", "result": "Achieves 61.2% accuracy on challenging NExT-QA benchmark, remains competitive across MSVD-QA, MSRVTT-QA, and TGIF-QA, with significant gains in temporal (+3.5%) and causal reasoning (+5.2%) subcategories.", "conclusion": "Explicit temporal ordering constraints are crucial for video understanding, and V-CORE demonstrates that parameter-efficient frameworks with structured unidirectional information flow can significantly improve temporal and causal reasoning in Video-LLMs."}}
{"id": "2601.01807", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.01807", "abs": "https://arxiv.org/abs/2601.01807", "authors": ["Ubaidullah", "Muhammad Abid Hussain", "Mohsin Raza Jafri", "Rozi Khan", "Moid Sandhu", "Abd Ullah Khan", "Hyundong Shin"], "title": "Adaptive Hybrid Optimizer based Framework for Lumpy Skin Disease Identification", "comment": null, "summary": "Lumpy Skin Disease (LSD) is a contagious viral infection that significantly deteriorates livestock health, thereby posing a serious threat to the global economy and food security. Owing to its rapid spread characteristics, early and precise identification is crucial to prevent outbreaks and ensure timely intervention. In this paper, we propose a hybrid deep learning-based approach called LUMPNet for the early detection of LSD. LUMPNet utilizes image data to detect and classify skin nodules -- the primary indicator of LSD. To this end, LUMPNet uses YOLOv11, EfficientNet-based CNN classifier with compound scaling, and a novel adaptive hybrid optimizer. More precisely, LUMPNet detects and localizes LSD skin nodules and lesions on cattle images. It exploits EfficientNet to classify the localized cattle images into LSD-affected or healthy categories. To stabilize and accelerate the training of YOLOv11 and EfficientNet hybrid model, a novel adaptive hybrid optimizer is proposed and utilized. We evaluate LUMPNet at various stages of LSD using a publicly available dataset. Results indicate that the proposed scheme achieves 99% LSD detection training accuracy, and outperforms existing schemes. The model also achieves validation accuracy of 98%. Moreover, for further evaluation, we conduct a case study using an optimized EfficientNet-B0 model trained with the AdamW optimizer, and compare its performance with LUMPNet. The results show that LUMPNet achieves superior performance.", "AI": {"tldr": "LUMPNet is a hybrid deep learning approach for early detection of Lumpy Skin Disease using YOLOv11 for lesion detection and EfficientNet for classification, achieving 99% training accuracy.", "motivation": "Lumpy Skin Disease is a contagious viral infection that threatens livestock health, global economy, and food security. Early and precise identification is crucial due to its rapid spread characteristics to prevent outbreaks and ensure timely intervention.", "method": "LUMPNet combines YOLOv11 for detecting and localizing LSD skin nodules/lesions on cattle images, EfficientNet-based CNN classifier with compound scaling for classifying images as LSD-affected or healthy, and a novel adaptive hybrid optimizer to stabilize and accelerate training of the hybrid model.", "result": "The model achieves 99% LSD detection training accuracy and 98% validation accuracy, outperforming existing schemes. A case study comparing with optimized EfficientNet-B0 trained with AdamW optimizer shows LUMPNet achieves superior performance.", "conclusion": "LUMPNet provides an effective hybrid deep learning approach for early LSD detection with high accuracy, demonstrating potential for practical application in livestock disease monitoring and prevention."}}
{"id": "2601.01818", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.01818", "abs": "https://arxiv.org/abs/2601.01818", "authors": ["Sungjune Park", "Hongda Mao", "Qingshuang Chen", "Yong Man Ro", "Yelin Kim"], "title": "Robust Egocentric Visual Attention Prediction Through Language-guided Scene Context-aware Learning", "comment": "11 pages, 7 figures, 4 tables", "summary": "As the demand for analyzing egocentric videos grows, egocentric visual attention prediction, anticipating where a camera wearer will attend, has garnered increasing attention. However, it remains challenging due to the inherent complexity and ambiguity of dynamic egocentric scenes. Motivated by evidence that scene contextual information plays a crucial role in modulating human attention, in this paper, we present a language-guided scene context-aware learning framework for robust egocentric visual attention prediction. We first design a context perceiver which is guided to summarize the egocentric video based on a language-based scene description, generating context-aware video representations. We then introduce two training objectives that: 1) encourage the framework to focus on the target point-of-interest regions and 2) suppress distractions from irrelevant regions which are less likely to attract first-person attention. Extensive experiments on Ego4D and Aria Everyday Activities (AEA) datasets demonstrate the effectiveness of our approach, achieving state-of-the-art performance and enhanced robustness across diverse, dynamic egocentric scenarios.", "AI": {"tldr": "A language-guided scene context-aware learning framework for egocentric visual attention prediction that uses language descriptions to generate context-aware video representations and focuses on relevant regions while suppressing distractions.", "motivation": "Egocentric visual attention prediction is challenging due to the complexity and ambiguity of dynamic egocentric scenes. Scene contextual information plays a crucial role in modulating human attention, motivating the need for context-aware approaches.", "method": "1) Design a context perceiver guided by language-based scene descriptions to generate context-aware video representations. 2) Introduce two training objectives: focus on target point-of-interest regions and suppress distractions from irrelevant regions less likely to attract first-person attention.", "result": "Achieves state-of-the-art performance on Ego4D and Aria Everyday Activities (AEA) datasets, demonstrating effectiveness and enhanced robustness across diverse, dynamic egocentric scenarios.", "conclusion": "The language-guided scene context-aware learning framework effectively addresses the challenges of egocentric visual attention prediction by leveraging scene context through language descriptions, resulting in robust performance across various dynamic scenarios."}}
{"id": "2601.01835", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.01835", "abs": "https://arxiv.org/abs/2601.01835", "authors": ["Rashid Iqbal", "Saddam Hussain Khan"], "title": "RSwinV2-MD: An Enhanced Residual SwinV2 Transformer for Monkeypox Detection from Skin Images", "comment": "15 Pages, 7 Figures, 4 Tables", "summary": "In this paper, a deep learning approach for Mpox diagnosis named Customized Residual SwinTransformerV2 (RSwinV2) has been proposed, trying to enhance the capability of lesion classification by employing the RSwinV2 tool-assisted vision approach. In the RSwinV2 method, a hierarchical structure of the transformer has been customized based on the input dimensionality, embedding structure, and output targeted by the method. In this RSwinV2 approach, the input image has been split into non-overlapping patches and processed using shifted windows and attention in these patches. This process has helped the method link all the windows efficiently by avoiding the locality issues of non-overlapping regions in attention, while being computationally efficient. RSwinV2 has further developed based on SwinTransformer and has included patch and position embeddings to take advantage of the transformer global-linking capability by employing multi-head attention in these embeddings. Furthermore, RSwinV2 has developed and incorporated the Inverse Residual Block (IRB) into this method, which utilizes convolutional skip connections with these inclusive designs to address the vanishing gradient issues during processing. RSwinV2 inclusion of IRB has therefore facilitated this method to link global patterns as well as local patterns; hence, its integrity has helped improve lesion classification capability by minimizing variability of Mpox and increasing differences of Mpox, chickenpox, measles, and cowpox. In testing SwinV2, its accuracy of 96.21 and an F1score of 95.62 have been achieved on the Kaggle public dataset, which has outperformed standard CNN models and SwinTransformers; RSwinV2 vector has thus proved its valiance as a computer-assisted tool for Mpox lesion observation interpretation.", "AI": {"tldr": "Proposed RSwinV2, a customized Residual SwinTransformerV2 for Mpox diagnosis, achieving 96.21% accuracy on Kaggle dataset by combining transformer global attention with convolutional skip connections to handle both local and global lesion patterns.", "motivation": "Need for improved computer-assisted diagnosis of Mpox lesions by addressing limitations of standard CNN models and SwinTransformers in handling both local and global patterns while maintaining computational efficiency.", "method": "Customized hierarchical transformer architecture with non-overlapping patch splitting, shifted window attention, patch/position embeddings, and Inverse Residual Blocks (IRB) with convolutional skip connections to address vanishing gradients and link global/local patterns.", "result": "Achieved 96.21% accuracy and 95.62 F1-score on Kaggle public dataset, outperforming standard CNN models and SwinTransformers in classifying Mpox, chickenpox, measles, and cowpox lesions.", "conclusion": "RSwinV2 proves effective as a computer-assisted tool for Mpox diagnosis by successfully integrating transformer global attention with convolutional local processing, demonstrating superior lesion classification capability."}}
{"id": "2601.01847", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.01847", "abs": "https://arxiv.org/abs/2601.01847", "authors": ["Chuhang Ma", "Shuai Tan", "Ye Pan", "Jiaolong Yang", "Xin Tong"], "title": "ESGaussianFace: Emotional and Stylized Audio-Driven Facial Animation via 3D Gaussian Splatting", "comment": "13 pages, 10 figures", "summary": "Most current audio-driven facial animation research primarily focuses on generating videos with neutral emotions. While some studies have addressed the generation of facial videos driven by emotional audio, efficiently generating high-quality talking head videos that integrate both emotional expressions and style features remains a significant challenge. In this paper, we propose ESGaussianFace, an innovative framework for emotional and stylized audio-driven facial animation. Our approach leverages 3D Gaussian Splatting to reconstruct 3D scenes and render videos, ensuring efficient generation of 3D consistent results. We propose an emotion-audio-guided spatial attention method that effectively integrates emotion features with audio content features. Through emotion-guided attention, the model is able to reconstruct facial details across different emotional states more accurately. To achieve emotional and stylized deformations of the 3D Gaussian points through emotion and style features, we introduce two 3D Gaussian deformation predictors. Futhermore, we propose a multi-stage training strategy, enabling the step-by-step learning of the character's lip movements, emotional variations, and style features. Our generated results exhibit high efficiency, high quality, and 3D consistency. Extensive experimental results demonstrate that our method outperforms existing state-of-the-art techniques in terms of lip movement accuracy, expression variation, and style feature expressiveness.", "AI": {"tldr": "ESGaussianFace: An efficient 3D Gaussian Splatting framework for emotional and stylized audio-driven facial animation with high-quality 3D consistent results.", "motivation": "Current audio-driven facial animation research focuses on neutral emotions or basic emotional generation, but lacks efficient methods for high-quality talking head videos that integrate both emotional expressions and style features.", "method": "Uses 3D Gaussian Splatting for 3D scene reconstruction and rendering; proposes emotion-audio-guided spatial attention to integrate emotion and audio features; introduces two 3D Gaussian deformation predictors for emotional and stylized deformations; employs multi-stage training strategy for step-by-step learning of lip movements, emotional variations, and style features.", "result": "The method generates high-efficiency, high-quality, 3D consistent results that outperform state-of-the-art techniques in lip movement accuracy, expression variation, and style feature expressiveness.", "conclusion": "ESGaussianFace successfully addresses the challenge of efficiently generating emotional and stylized audio-driven facial animation with 3D consistency, demonstrating superior performance over existing methods."}}
{"id": "2601.01856", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.01856", "abs": "https://arxiv.org/abs/2601.01856", "authors": ["Joongwon Chae", "Lihui Luo", "Yang Liu", "Runming Wang", "Dongmei Yu", "Zeming Liang", "Xi Yuan", "Dayan Zhang", "Zhenglin Chen", "Peiwu Qin", "Ilmoon Chae"], "title": "GCR: Geometry-Consistent Routing for Task-Agnostic Continual Anomaly Detection", "comment": null, "summary": "Feature-based anomaly detection is widely adopted in industrial inspection due to the strong representational power of large pre-trained vision encoders. While most existing methods focus on improving within-category anomaly scoring, practical deployments increasingly require task-agnostic operation under continual category expansion, where the category identity is unknown at test time. In this setting, overall performance is often dominated by expert selection, namely routing an input to an appropriate normality model before any head-specific scoring is applied. However, routing rules that compare head-specific anomaly scores across independently constructed heads are unreliable in practice, as score distributions can differ substantially across categories in scale and tail behavior.\n  We propose GCR, a lightweight mixture-of-experts framework for stabilizing task-agnostic continual anomaly detection through geometry-consistent routing. GCR routes each test image directly in a shared frozen patch-embedding space by minimizing an accumulated nearest-prototype distance to category-specific prototype banks, and then computes anomaly maps only within the routed expert using a standard prototype-based scoring rule. By separating cross-head decision making from within-head anomaly scoring, GCR avoids cross-head score comparability issues without requiring end-to-end representation learning.\n  Experiments on MVTec AD and VisA show that geometry-consistent routing substantially improves routing stability and mitigates continual performance collapse, achieving near-zero forgetting while maintaining competitive detection and localization performance. These results indicate that many failures previously attributed to representation forgetting can instead be explained by decision-rule instability in cross-head routing. Code is available at https://github.com/jw-chae/GCR", "AI": {"tldr": "GCR introduces geometry-consistent routing for task-agnostic continual anomaly detection, separating routing decisions from anomaly scoring to avoid cross-head comparability issues and prevent continual performance collapse.", "motivation": "Practical anomaly detection deployments increasingly require task-agnostic operation under continual category expansion, where existing methods suffer from unreliable routing when comparing anomaly scores across independently constructed heads due to differing score distributions.", "method": "GCR uses geometry-consistent routing in a shared frozen patch-embedding space, routing test images by minimizing nearest-prototype distance to category-specific prototype banks, then computing anomaly maps only within the routed expert using standard prototype-based scoring.", "result": "Experiments on MVTec AD and VisA show substantial improvement in routing stability, mitigation of continual performance collapse, near-zero forgetting, and competitive detection/localization performance.", "conclusion": "Many failures previously attributed to representation forgetting can instead be explained by decision-rule instability in cross-head routing, and GCR's geometry-consistent routing provides an effective solution for task-agnostic continual anomaly detection."}}
{"id": "2601.01865", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.01865", "abs": "https://arxiv.org/abs/2601.01865", "authors": ["Wenlong Yang", "Canran Jin", "Weihang Yuan", "Chao Wang", "Lifeng Sun"], "title": "RRNet: Configurable Real-Time Video Enhancement with Arbitrary Local Lighting Variations", "comment": null, "summary": "With the growing demand for real-time video enhancement in live applications, existing methods often struggle to balance speed and effective exposure control, particularly under uneven lighting. We introduce RRNet (Rendering Relighting Network), a lightweight and configurable framework that achieves a state-of-the-art tradeoff between visual quality and efficiency. By estimating parameters for a minimal set of virtual light sources, RRNet enables localized relighting through a depth-aware rendering module without requiring pixel-aligned training data. This object-aware formulation preserves facial identity and supports real-time, high-resolution performance using a streamlined encoder and lightweight prediction head. To facilitate training, we propose a generative AI-based dataset creation pipeline that synthesizes diverse lighting conditions at low cost. With its interpretable lighting control and efficient architecture, RRNet is well suited for practical applications such as video conferencing, AR-based portrait enhancement, and mobile photography. Experiments show that RRNet consistently outperforms prior methods in low-light enhancement, localized illumination adjustment, and glare removal.", "AI": {"tldr": "RRNet is a lightweight real-time video enhancement framework that uses virtual light sources and depth-aware rendering for localized relighting, achieving state-of-the-art balance between quality and efficiency.", "motivation": "Existing methods struggle to balance speed and effective exposure control for real-time video enhancement, especially under uneven lighting conditions in live applications.", "method": "Uses a lightweight configurable framework with virtual light source parameter estimation, depth-aware rendering module, object-aware formulation, streamlined encoder, and lightweight prediction head. Includes generative AI-based dataset creation pipeline for training.", "result": "Consistently outperforms prior methods in low-light enhancement, localized illumination adjustment, and glare removal while preserving facial identity and supporting real-time high-resolution performance.", "conclusion": "RRNet's interpretable lighting control and efficient architecture make it well-suited for practical applications like video conferencing, AR portrait enhancement, and mobile photography."}}
{"id": "2601.01870", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.01870", "abs": "https://arxiv.org/abs/2601.01870", "authors": ["Wenyu Shao", "Hongbo Liu", "Yunchuan Ma", "Ruili Wang"], "title": "Entity-Guided Multi-Task Learning for Infrared and Visible Image Fusion", "comment": "Accepted by IEEE Transactions on Multimedia", "summary": "Existing text-driven infrared and visible image fusion approaches often rely on textual information at the sentence level, which can lead to semantic noise from redundant text and fail to fully exploit the deeper semantic value of textual information. To address these issues, we propose a novel fusion approach named Entity-Guided Multi-Task learning for infrared and visible image fusion (EGMT). Our approach includes three key innovative components: (i) A principled method is proposed to extract entity-level textual information from image captions generated by large vision-language models, eliminating semantic noise from raw text while preserving critical semantic information; (ii) A parallel multi-task learning architecture is constructed, which integrates image fusion with a multi-label classification task. By using entities as pseudo-labels, the multi-label classification task provides semantic supervision, enabling the model to achieve a deeper understanding of image content and significantly improving the quality and semantic density of the fused image; (iii) An entity-guided cross-modal interactive module is also developed to facilitate the fine-grained interaction between visual and entity-level textual features, which enhances feature representation by capturing cross-modal dependencies at both inter-visual and visual-entity levels. To promote the wide application of the entity-guided image fusion framework, we release the entity-annotated version of four public datasets (i.e., TNO, RoadScene, M3FD, and MSRS). Extensive experiments demonstrate that EGMT achieves superior performance in preserving salient targets, texture details, and semantic consistency, compared to the state-of-the-art methods. The code and dataset will be publicly available at https://github.com/wyshao-01/EGMT.", "AI": {"tldr": "EGMT: Entity-Guided Multi-Task learning for infrared and visible image fusion that extracts entity-level text from image captions to guide fusion and classification tasks.", "motivation": "Existing text-driven fusion methods use sentence-level text which introduces semantic noise and fails to fully exploit deeper semantic value of textual information.", "method": "Three key innovations: 1) Entity extraction from image captions using large vision-language models, 2) Parallel multi-task learning architecture combining image fusion with entity-guided multi-label classification, 3) Entity-guided cross-modal interactive module for fine-grained visual-textual feature interaction.", "result": "EGMT achieves superior performance in preserving salient targets, texture details, and semantic consistency compared to state-of-the-art methods. The authors release entity-annotated versions of four public datasets (TNO, RoadScene, M3FD, MSRS).", "conclusion": "The proposed EGMT framework effectively addresses semantic noise issues in text-driven fusion by using entity-level guidance, enabling deeper semantic understanding and improved fusion quality. The released datasets promote wider application of entity-guided fusion approaches."}}
{"id": "2601.01874", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.01874", "abs": "https://arxiv.org/abs/2601.01874", "authors": ["Shuhang Chen", "Yunqiu Xu", "Junjie Xie", "Aojun Lu", "Tao Feng", "Zeying Huang", "Ning Zhang", "Yi Sun", "Yi Yang", "Hangjie Yuan"], "title": "CogFlow: Bridging Perception and Reasoning through Knowledge Internalization for Visual Mathematical Problem Solving", "comment": null, "summary": "Despite significant progress, multimodal large language models continue to struggle with visual mathematical problem solving. Some recent works recognize that visual perception is a bottleneck in visual mathematical reasoning, but their solutions are limited to improving the extraction and interpretation of visual inputs. Notably, they all ignore the key issue of whether the extracted visual cues are faithfully integrated and properly utilized in subsequent reasoning. Motivated by this, we present CogFlow, a novel cognitive-inspired three-stage framework that incorporates a knowledge internalization stage, explicitly simulating the hierarchical flow of human reasoning: perception$\\Rightarrow$internalization$\\Rightarrow$reasoning. Inline with this hierarchical flow, we holistically enhance all its stages. We devise Synergistic Visual Rewards to boost perception capabilities in parametric and semantic spaces, jointly improving visual information extraction from symbols and diagrams. To guarantee faithful integration of extracted visual cues into subsequent reasoning, we introduce a Knowledge Internalization Reward model in the internalization stage, bridging perception and reasoning. Moreover, we design a Visual-Gated Policy Optimization algorithm to further enforce the reasoning is grounded with the visual knowledge, preventing models seeking shortcuts that appear coherent but are visually ungrounded reasoning chains. Moreover, we contribute a new dataset MathCog for model training, which contains samples with over 120K high-quality perception-reasoning aligned annotations. Comprehensive experiments and analysis on commonly used visual mathematical reasoning benchmarks validate the superiority of the proposed CogFlow.", "AI": {"tldr": "CogFlow is a cognitive-inspired three-stage framework for visual mathematical reasoning that addresses the gap between visual perception and reasoning through knowledge internalization and visual-gated optimization.", "motivation": "Current multimodal LLMs struggle with visual mathematical problem solving because they focus only on improving visual extraction but ignore whether extracted visual cues are faithfully integrated and properly utilized in subsequent reasoning.", "method": "Three-stage framework: perception\u2192internalization\u2192reasoning. Includes Synergistic Visual Rewards for perception, Knowledge Internalization Reward model for faithful integration, and Visual-Gated Policy Optimization to prevent visually ungrounded reasoning. Also introduces MathCog dataset with 120K+ perception-reasoning aligned annotations.", "result": "Comprehensive experiments on visual mathematical reasoning benchmarks validate the superiority of CogFlow, showing improved performance in visual mathematical problem solving.", "conclusion": "CogFlow successfully addresses the visual perception-reasoning gap in multimodal LLMs through a cognitive-inspired hierarchical framework that ensures faithful integration of visual cues into reasoning processes."}}
{"id": "2601.01891", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.01891", "abs": "https://arxiv.org/abs/2601.01891", "authors": ["Niloufar Alipour Talemi", "Julia Boone", "Fatemeh Afghah"], "title": "Agentic AI in Remote Sensing: Foundations, Taxonomy, and Emerging Systems", "comment": "Accepted to the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV) 2026, GeoCV Workshop", "summary": "The paradigm of Earth Observation analysis is shifting from static deep learning models to autonomous agentic AI. Although recent vision foundation models and multimodal large language models advance representation learning, they often lack the sequential planning and active tool orchestration required for complex geospatial workflows. This survey presents the first comprehensive review of agentic AI in remote sensing. We introduce a unified taxonomy distinguishing between single-agent copilots and multi-agent systems while analyzing architectural foundations such as planning mechanisms, retrieval-augmented generation, and memory structures. Furthermore, we review emerging benchmarks that move the evaluation from pixel-level accuracy to trajectory-aware reasoning correctness. By critically examining limitations in grounding, safety, and orchestration, this work outlines a strategic roadmap for the development of robust, autonomous geospatial intelligence.", "AI": {"tldr": "This survey paper provides the first comprehensive review of agentic AI in remote sensing, analyzing the shift from static models to autonomous systems with planning and tool orchestration capabilities.", "motivation": "The paradigm of Earth Observation analysis is shifting from static deep learning models to autonomous agentic AI. Current vision foundation models and multimodal LLMs lack the sequential planning and active tool orchestration needed for complex geospatial workflows.", "method": "The paper presents a unified taxonomy distinguishing between single-agent copilots and multi-agent systems, analyzing architectural foundations including planning mechanisms, retrieval-augmented generation, and memory structures.", "result": "The survey reviews emerging benchmarks that shift evaluation from pixel-level accuracy to trajectory-aware reasoning correctness, providing a comprehensive framework for understanding agentic AI in remote sensing.", "conclusion": "By examining limitations in grounding, safety, and orchestration, the work outlines a strategic roadmap for developing robust, autonomous geospatial intelligence systems."}}
{"id": "2601.01892", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.01892", "abs": "https://arxiv.org/abs/2601.01892", "authors": ["Arjun Ramesh Kaushik", "Naresh Kumar Devulapally", "Vishnu Suresh Lokhande", "Nalini K. Ratha", "Venu Govindaraju"], "title": "Forget Less by Learning from Parents Through Hierarchical Relationships", "comment": "Accepted at AAAI-26", "summary": "Custom Diffusion Models (CDMs) offer impressive capabilities for personalization in generative modeling, yet they remain vulnerable to catastrophic forgetting when learning new concepts sequentially. Existing approaches primarily focus on minimizing interference between concepts, often neglecting the potential for positive inter-concept interactions. In this work, we present Forget Less by Learning from Parents (FLLP), a novel framework that introduces a parent-child inter-concept learning mechanism in hyperbolic space to mitigate forgetting. By embedding concept representations within a Lorentzian manifold, naturally suited to modeling tree-like hierarchies, we define parent-child relationships in which previously learned concepts serve as guidance for adapting to new ones. Our method not only preserves prior knowledge but also supports continual integration of new concepts. We validate FLLP on three public datasets and one synthetic benchmark, showing consistent improvements in both robustness and generalization.", "AI": {"tldr": "FLLP introduces a parent-child learning mechanism in hyperbolic space to prevent catastrophic forgetting in custom diffusion models during sequential concept learning.", "motivation": "Custom Diffusion Models suffer from catastrophic forgetting when learning new concepts sequentially, and existing approaches focus only on minimizing interference without leveraging positive inter-concept interactions.", "method": "Forget Less by Learning from Parents (FLLP) uses a parent-child inter-concept learning mechanism in hyperbolic space (Lorentzian manifold) where previously learned concepts guide adaptation to new ones, naturally modeling tree-like hierarchies.", "result": "FLLP shows consistent improvements in both robustness and generalization across three public datasets and one synthetic benchmark, effectively mitigating forgetting while supporting continual integration of new concepts.", "conclusion": "The hyperbolic parent-child learning framework successfully addresses catastrophic forgetting in custom diffusion models by preserving prior knowledge and enabling positive inter-concept interactions through hierarchical relationships."}}
{"id": "2601.01908", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.01908", "abs": "https://arxiv.org/abs/2601.01908", "authors": ["Jingjing Wang", "Qianglin Liu", "Zhuo Xiao", "Xinning Yao", "Bo Liu", "Lu Li", "Lijuan Niu", "Fugen Zhou"], "title": "Nodule-DETR: A Novel DETR Architecture with Frequency-Channel Attention for Ultrasound Thyroid Nodule Detection", "comment": null, "summary": "Thyroid cancer is the most common endocrine malignancy, and its incidence is rising globally. While ultrasound is the preferred imaging modality for detecting thyroid nodules, its diagnostic accuracy is often limited by challenges such as low image contrast and blurred nodule boundaries. To address these issues, we propose Nodule-DETR, a novel detection transformer (DETR) architecture designed for robust thyroid nodule detection in ultrasound images. Nodule-DETR introduces three key innovations: a Multi-Spectral Frequency-domain Channel Attention (MSFCA) module that leverages frequency analysis to enhance features of low-contrast nodules; a Hierarchical Feature Fusion (HFF) module for efficient multi-scale integration; and Multi-Scale Deformable Attention (MSDA) to flexibly capture small and irregularly shaped nodules. We conducted extensive experiments on a clinical dataset of real-world thyroid ultrasound images. The results demonstrate that Nodule-DETR achieves state-of-the-art performance, outperforming the baseline model by a significant margin of 0.149 in mAP@0.5:0.95. The superior accuracy of Nodule-DETR highlights its significant potential for clinical application as an effective tool in computer-aided thyroid diagnosis. The code of work is available at https://github.com/wjj1wjj/Nodule-DETR.", "AI": {"tldr": "Nodule-DETR: A novel detection transformer architecture for thyroid nodule detection in ultrasound images, achieving state-of-the-art performance with three key innovations for handling low-contrast and irregular nodules.", "motivation": "Thyroid cancer incidence is rising globally, and ultrasound detection faces challenges with low image contrast and blurred nodule boundaries, limiting diagnostic accuracy.", "method": "Proposes Nodule-DETR with three innovations: 1) Multi-Spectral Frequency-domain Channel Attention (MSFCA) for enhancing low-contrast nodule features, 2) Hierarchical Feature Fusion (HFF) for multi-scale integration, and 3) Multi-Scale Deformable Attention (MSDA) for capturing small/irregular nodules.", "result": "Achieves state-of-the-art performance on clinical thyroid ultrasound dataset, outperforming baseline by 0.149 in mAP@0.5:0.95, demonstrating superior accuracy for clinical application.", "conclusion": "Nodule-DETR shows significant potential as an effective tool for computer-aided thyroid diagnosis, with code publicly available for further research and clinical implementation."}}
{"id": "2601.01914", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.01914", "abs": "https://arxiv.org/abs/2601.01914", "authors": ["Arjun Ramesh Kaushik", "Nalini K. Ratha", "Venu Govindaraju"], "title": "Learning Action Hierarchies via Hybrid Geometric Diffusion", "comment": "Accepted at WACV-26", "summary": "Temporal action segmentation is a critical task in video understanding, where the goal is to assign action labels to each frame in a video. While recent advances leverage iterative refinement-based strategies, they fail to explicitly utilize the hierarchical nature of human actions. In this work, we propose HybridTAS - a novel framework that incorporates a hybrid of Euclidean and hyperbolic geometries into the denoising process of diffusion models to exploit the hierarchical structure of actions. Hyperbolic geometry naturally provides tree-like relationships between embeddings, enabling us to guide the action label denoising process in a coarse-to-fine manner: higher diffusion timesteps are influenced by abstract, high-level action categories (root nodes), while lower timesteps are refined using fine-grained action classes (leaf nodes). Extensive experiments on three benchmark datasets, GTEA, 50Salads, and Breakfast, demonstrate that our method achieves state-of-the-art performance, validating the effectiveness of hyperbolic-guided denoising for the temporal action segmentation task.", "AI": {"tldr": "HybridTAS: A diffusion-based temporal action segmentation framework that combines Euclidean and hyperbolic geometries to exploit hierarchical action structure through coarse-to-fine denoising.", "motivation": "Existing iterative refinement methods for temporal action segmentation fail to explicitly utilize the hierarchical nature of human actions, which have natural tree-like relationships between abstract and fine-grained action categories.", "method": "Proposes HybridTAS framework incorporating both Euclidean and hyperbolic geometries into diffusion model denoising. Hyperbolic geometry provides tree-like embedding relationships, enabling coarse-to-fine guidance: higher diffusion timesteps use abstract high-level action categories (root nodes), while lower timesteps refine with fine-grained action classes (leaf nodes).", "result": "Extensive experiments on GTEA, 50Salads, and Breakfast datasets demonstrate state-of-the-art performance, validating the effectiveness of hyperbolic-guided denoising for temporal action segmentation.", "conclusion": "HybridTAS successfully leverages the hierarchical structure of actions through hyperbolic geometry in diffusion models, achieving superior performance by guiding the denoising process in a coarse-to-fine manner from abstract to fine-grained action categories."}}
{"id": "2601.01915", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.01915", "abs": "https://arxiv.org/abs/2601.01915", "authors": ["Yujie Hu", "Zecheng Tang", "Xu Jiang", "Weiqi Li", "Jian Zhang"], "title": "TalkPhoto: A Versatile Training-Free Conversational Assistant for Intelligent Image Editing", "comment": "a Conversational Assistant for Intelligent Image Editing", "summary": "Thanks to the powerful language comprehension capabilities of Large Language Models (LLMs), existing instruction-based image editing methods have introduced Multimodal Large Language Models (MLLMs) to promote information exchange between instructions and images, ensuring the controllability and flexibility of image editing. However, these frameworks often build a multi-instruction dataset to train the model to handle multiple editing tasks, which is not only time-consuming and labor-intensive but also fails to achieve satisfactory results. In this paper, we present TalkPhoto, a versatile training-free image editing framework that facilitates precise image manipulation through conversational interaction. We instruct the open-source LLM with a specially designed prompt template to analyze user needs after receiving instructions and hierarchically invoke existing advanced editing methods, all without additional training. Moreover, we implement a plug-and-play and efficient invocation of image editing methods, allowing complex and unseen editing tasks to be integrated into the current framework, achieving stable and high-quality editing results. Extensive experiments demonstrate that our method not only provides more accurate invocation with fewer token consumption but also achieves higher editing quality across various image editing tasks.", "AI": {"tldr": "TalkPhoto is a training-free image editing framework that uses LLMs to analyze user instructions and hierarchically invoke existing editing methods without additional training.", "motivation": "Existing MLLM-based image editing methods require building multi-instruction datasets for training, which is time-consuming and labor-intensive, and often fails to achieve satisfactory results.", "method": "Uses open-source LLM with specially designed prompt templates to analyze user needs, then hierarchically invokes existing advanced editing methods in a plug-and-play manner without additional training.", "result": "Achieves more accurate invocation with fewer token consumption and higher editing quality across various image editing tasks compared to existing methods.", "conclusion": "TalkPhoto provides a versatile, training-free framework for precise image manipulation through conversational interaction, enabling stable and high-quality editing results for complex and unseen tasks."}}
{"id": "2601.01925", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.01925", "abs": "https://arxiv.org/abs/2601.01925", "authors": ["Lianjie Jia", "Yuhan Wu", "Binghao Ran", "Yifan Wang", "Lijun Wang", "Huchuan Lu"], "title": "AR-MOT: Autoregressive Multi-object Tracking", "comment": "12 pages, 5 figures", "summary": "As multi-object tracking (MOT) tasks continue to evolve toward more general and multi-modal scenarios, the rigid and task-specific architectures of existing MOT methods increasingly hinder their applicability across diverse tasks and limit flexibility in adapting to new tracking formulations. Most approaches rely on fixed output heads and bespoke tracking pipelines, making them difficult to extend to more complex or instruction-driven tasks. To address these limitations, we propose AR-MOT, a novel autoregressive paradigm that formulates MOT as a sequence generation task within a large language model (LLM) framework. This design enables the model to output structured results through flexible sequence construction, without requiring any task-specific heads. To enhance region-level visual perception, we introduce an Object Tokenizer based on a pretrained detector. To mitigate the misalignment between global and regional features, we propose a Region-Aware Alignment (RAA) module, and to support long-term tracking, we design a Temporal Memory Fusion (TMF) module that caches historical object tokens. AR-MOT offers strong potential for extensibility, as new modalities or instructions can be integrated by simply modifying the output sequence format without altering the model architecture. Extensive experiments on MOT17 and DanceTrack validate the feasibility of our approach, achieving performance comparable to state-of-the-art methods while laying the foundation for more general and flexible MOT systems.", "AI": {"tldr": "AR-MOT reformulates multi-object tracking as an autoregressive sequence generation task using LLMs, eliminating task-specific heads and enabling flexible adaptation to new tracking scenarios through sequence format modifications.", "motivation": "Existing MOT methods have rigid, task-specific architectures that hinder applicability across diverse tasks and limit flexibility in adapting to new tracking formulations, especially as MOT evolves toward more general and multi-modal scenarios.", "method": "Proposes AR-MOT: an autoregressive paradigm that formulates MOT as sequence generation within an LLM framework. Uses Object Tokenizer for region-level perception, Region-Aware Alignment module to mitigate global-regional feature misalignment, and Temporal Memory Fusion module for long-term tracking by caching historical object tokens.", "result": "Extensive experiments on MOT17 and DanceTrack validate feasibility, achieving performance comparable to state-of-the-art methods while demonstrating strong extensibility potential.", "conclusion": "AR-MOT lays foundation for more general and flexible MOT systems by enabling integration of new modalities or instructions through simple sequence format modifications without architectural changes, addressing limitations of traditional task-specific approaches."}}
{"id": "2601.01926", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.01926", "abs": "https://arxiv.org/abs/2601.01926", "authors": ["Zhifei Li", "Yiran Wang", "Chenyi Xiong", "Yujing Xia", "Xiaoju Hou", "Yue Zhao", "Miao Zhang", "Kui Xiao", "Bing Yang"], "title": "MacVQA: Adaptive Memory Allocation and Global Noise Filtering for Continual Visual Question Answering", "comment": "Accepted to AAAI 2026", "summary": "Visual Question Answering (VQA) requires models to reason over multimodal information, combining visual and textual data. With the development of continual learning, significant progress has been made in retaining knowledge and adapting to new information in the VQA domain. However, current methods often struggle with balancing knowledge retention, adaptation, and robust feature representation. To address these challenges, we propose a novel framework with adaptive memory allocation and global noise filtering called MacVQA for visual question answering. MacVQA fuses visual and question information while filtering noise to ensure robust representations, and employs prototype-based memory allocation to optimize feature quality and memory usage. These designs enable MacVQA to balance knowledge acquisition, retention, and compositional generalization in continual VQA learning. Experiments on ten continual VQA tasks show that MacVQA outperforms existing baselines, achieving 43.38% average accuracy and 2.32% average forgetting on standard tasks, and 42.53% average accuracy and 3.60% average forgetting on novel composition tasks.", "AI": {"tldr": "MacVQA: A novel continual learning framework for VQA with adaptive memory allocation and global noise filtering that balances knowledge retention, adaptation, and robust feature representation.", "motivation": "Current continual learning methods for VQA struggle with balancing knowledge retention, adaptation to new information, and robust feature representation, creating challenges for effective multimodal reasoning over time.", "method": "Proposes MacVQA framework with two key components: 1) fusion of visual and question information with global noise filtering for robust representations, and 2) prototype-based memory allocation to optimize feature quality and memory usage.", "result": "Outperforms existing baselines on ten continual VQA tasks, achieving 43.38% average accuracy and 2.32% average forgetting on standard tasks, and 42.53% average accuracy and 3.60% average forgetting on novel composition tasks.", "conclusion": "MacVQA effectively balances knowledge acquisition, retention, and compositional generalization in continual VQA learning through its adaptive memory allocation and noise filtering mechanisms."}}
{"id": "2601.01950", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.01950", "abs": "https://arxiv.org/abs/2601.01950", "authors": ["Meng Wang", "Wenjing Dai", "Jiawan Zhang", "Xiaojie Guo"], "title": "Face Normal Estimation from Rags to Riches", "comment": null, "summary": "Although recent approaches to face normal estimation have achieved promising results, their effectiveness heavily depends on large-scale paired data for training. This paper concentrates on relieving this requirement via developing a coarse-to-fine normal estimator. Concretely, our method first trains a neat model from a small dataset to produce coarse face normals that perform as guidance (called exemplars) for the following refinement. A self-attention mechanism is employed to capture long-range dependencies, thus remedying severe local artifacts left in estimated coarse facial normals. Then, a refinement network is customized for the sake of mapping input face images together with corresponding exemplars to fine-grained high-quality facial normals. Such a logical function split can significantly cut the requirement of massive paired data and computational resource. Extensive experiments and ablation studies are conducted to demonstrate the efficacy of our design and reveal its superiority over state-of-the-art methods in terms of both training expense as well as estimation quality. Our code and models are open-sourced at: https://github.com/AutoHDR/FNR2R.git.", "AI": {"tldr": "A coarse-to-fine face normal estimation method that reduces dependency on large-scale paired training data by first generating coarse normals from a small dataset, then refining them using self-attention to capture long-range dependencies.", "motivation": "Existing face normal estimation methods require large-scale paired data for training, which is resource-intensive. The paper aims to reduce this dependency while maintaining high-quality results.", "method": "Two-stage approach: 1) Train a neat model on small dataset to produce coarse face normals as exemplars; 2) Use refinement network with self-attention mechanism to capture long-range dependencies and map input images with exemplars to high-quality facial normals.", "result": "The method significantly reduces training data requirements and computational resources while achieving superior performance over state-of-the-art methods in both training expense and estimation quality.", "conclusion": "The coarse-to-fine approach with logical function split effectively reduces dependency on massive paired data and computational resources while producing high-quality face normal estimations."}}
{"id": "2601.01955", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.01955", "abs": "https://arxiv.org/abs/2601.01955", "authors": ["Zhexin Zhang", "Yifeng Zhu", "Yangyang Xu", "Long Chen", "Yong Du", "Shengfeng He", "Jun Yu"], "title": "MotionAdapter: Video Motion Transfer via Content-Aware Attention Customization", "comment": null, "summary": "Recent advances in diffusion-based text-to-video models, particularly those built on the diffusion transformer architecture, have achieved remarkable progress in generating high-quality and temporally coherent videos. However, transferring complex motions between videos remains challenging. In this work, we present MotionAdapter, a content-aware motion transfer framework that enables robust and semantically aligned motion transfer within DiT-based T2V models. Our key insight is that effective motion transfer requires \\romannumeral1) explicit disentanglement of motion from appearance and \\romannumeral 2) adaptive customization of motion to target content. MotionAdapter first isolates motion by analyzing cross-frame attention within 3D full-attention modules to extract attention-derived motion fields. To bridge the semantic gap between reference and target videos, we further introduce a DINO-guided motion customization module that rearranges and refines motion fields based on content correspondences. The customized motion field is then used to guide the DiT denoising process, ensuring that the synthesized video inherits the reference motion while preserving target appearance and semantics. Extensive experiments demonstrate that MotionAdapter outperforms state-of-the-art methods in both qualitative and quantitative evaluations. Moreover, MotionAdapter naturally supports complex motion transfer and motion editing tasks such as zooming.", "AI": {"tldr": "MotionAdapter is a framework for transferring complex motions between videos using diffusion-based text-to-video models, featuring explicit motion-appearance disentanglement and adaptive motion customization.", "motivation": "While diffusion-based text-to-video models have advanced significantly, transferring complex motions between videos remains challenging. Current approaches struggle with robust and semantically aligned motion transfer.", "method": "MotionAdapter uses cross-frame attention analysis in 3D full-attention modules to extract motion fields, then employs a DINO-guided motion customization module to rearrange and refine motion fields based on content correspondences between reference and target videos.", "result": "Extensive experiments show MotionAdapter outperforms state-of-the-art methods in both qualitative and quantitative evaluations, and naturally supports complex motion transfer and editing tasks like zooming.", "conclusion": "MotionAdapter provides an effective content-aware motion transfer framework for DiT-based T2V models that successfully addresses motion-appearance disentanglement and semantic alignment challenges."}}
{"id": "2601.01957", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.01957", "abs": "https://arxiv.org/abs/2601.01957", "authors": ["Tianbo Wang", "Yuqing Ma", "Kewei Liao", "Zhange Zhang", "Simin Li", "Jinyang Guo", "Xianglong Liu"], "title": "AFTER: Mitigating the Object Hallucination of LVLM via Adaptive Factual-Guided Activation Editing", "comment": null, "summary": "Large Vision-Language Models (LVLMs) have achieved substantial progress in cross-modal tasks. However, due to language bias, LVLMs are susceptible to object hallucination, which can be primarily divided into category, attribute, and relation hallucination, significantly impeding the trustworthy AI applications. Editing the internal activations of LVLMs has shown promising effectiveness in mitigating hallucinations with minimal cost. However, previous editing approaches neglect the effective guidance offered by factual textual semantics, thereby struggling to explicitly mitigate language bias. To address these issues, we propose Adaptive Factual-guided Visual-Textual Editing for hallucination mitigation (AFTER), which comprises Factual-Augmented Activation Steering (FAS) and Query-Adaptive Offset Optimization (QAO), to adaptively guides the original biased activations towards factual semantics. Specifically, FAS is proposed to provide factual and general guidance for activation editing, thereby explicitly modeling the precise visual-textual associations. Subsequently, QAO introduces a query-aware offset estimator to establish query-specific editing from the general steering vector, enhancing the diversity and granularity of editing. Extensive experiments on standard hallucination benchmarks across three widely adopted LVLMs validate the efficacy of the proposed AFTER, notably achieving up to a 16.3% reduction of hallucination over baseline on the AMBER benchmark. Our code and data will be released for reproducibility.", "AI": {"tldr": "AFTER is a novel method that uses factual textual semantics to adaptively guide activation editing in Large Vision-Language Models, reducing object hallucination by up to 16.3% on benchmarks.", "motivation": "LVLMs suffer from object hallucination (category, attribute, relation) due to language bias, which hinders trustworthy AI applications. Existing editing approaches fail to leverage factual textual semantics effectively and struggle to explicitly mitigate language bias.", "method": "AFTER consists of two components: 1) Factual-Augmented Activation Steering (FAS) - provides factual and general guidance for activation editing by modeling precise visual-textual associations; 2) Query-Adaptive Offset Optimization (QAO) - introduces a query-aware offset estimator to establish query-specific editing from the general steering vector, enhancing editing diversity and granularity.", "result": "Extensive experiments on standard hallucination benchmarks across three widely adopted LVLMs validate AFTER's efficacy, achieving up to 16.3% reduction of hallucination over baseline on the AMBER benchmark.", "conclusion": "AFTER effectively mitigates object hallucination in LVLMs by adaptively guiding biased activations toward factual semantics through factual-augmented activation steering and query-adaptive optimization, advancing trustworthy AI applications."}}
{"id": "2601.01963", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.01963", "abs": "https://arxiv.org/abs/2601.01963", "authors": ["Arjun Ramesh Kaushik", "Naresh Kumar Devulapally", "Vishnu Suresh Lokhande", "Nalini Ratha", "Venu Govindaraju"], "title": "Forget Less by Learning Together through Concept Consolidation", "comment": "Accepted at WACV-26", "summary": "Custom Diffusion Models (CDMs) have gained significant attention due to their remarkable ability to personalize generative processes. However, existing CDMs suffer from catastrophic forgetting when continuously learning new concepts. Most prior works attempt to mitigate this issue under the sequential learning setting with a fixed order of concept inflow and neglect inter-concept interactions. In this paper, we propose a novel framework - Forget Less by Learning Together (FL2T) - that enables concurrent and order-agnostic concept learning while addressing catastrophic forgetting. Specifically, we introduce a set-invariant inter-concept learning module where proxies guide feature selection across concepts, facilitating improved knowledge retention and transfer. By leveraging inter-concept guidance, our approach preserves old concepts while efficiently incorporating new ones. Extensive experiments, across three datasets, demonstrates that our method significantly improves concept retention and mitigates catastrophic forgetting, highlighting the effectiveness of inter-concept catalytic behavior in incremental concept learning of ten tasks with at least 2% gain on average CLIP Image Alignment scores.", "AI": {"tldr": "FL2T is a novel framework that addresses catastrophic forgetting in Custom Diffusion Models by enabling concurrent, order-agnostic concept learning through inter-concept guidance.", "motivation": "Existing Custom Diffusion Models suffer from catastrophic forgetting when learning new concepts sequentially, and most prior works neglect inter-concept interactions while assuming fixed concept order.", "method": "Proposes FL2T framework with set-invariant inter-concept learning module where proxies guide feature selection across concepts, facilitating knowledge retention and transfer through inter-concept guidance.", "result": "Extensive experiments across three datasets show significant improvement in concept retention and mitigation of catastrophic forgetting, with at least 2% average gain on CLIP Image Alignment scores across ten tasks.", "conclusion": "The FL2T framework effectively addresses catastrophic forgetting in Custom Diffusion Models by leveraging inter-concept catalytic behavior, enabling more robust incremental concept learning."}}
{"id": "2601.01984", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.01984", "abs": "https://arxiv.org/abs/2601.01984", "authors": ["Weijian Ma", "Shizhao Sun", "Tianyu Yu", "Ruiyu Wang", "Tat-Seng Chua", "Jiang Bian"], "title": "Thinking with Blueprints: Assisting Vision-Language Models in Spatial Reasoning via Structured Object Representation", "comment": "Preprint. Under review", "summary": "Spatial reasoning -- the ability to perceive and reason about relationships in space -- advances vision-language models (VLMs) from visual perception toward spatial semantic understanding. Existing approaches either revisit local image patches, improving fine-grained perception but weakening global spatial awareness, or mark isolated coordinates, which capture object locations but overlook their overall organization. In this work, we integrate the cognitive concept of an object-centric blueprint into VLMs to enhance spatial reasoning. Given an image and a question, the model first constructs a JSON-style blueprint that records the positions, sizes, and attributes of relevant objects, and then reasons over this structured representation to produce the final answer. To achieve this, we introduce three key techniques: (1) blueprint-embedded reasoning traces for supervised fine-tuning to elicit basic reasoning skills; (2) blueprint-aware rewards in reinforcement learning to encourage the blueprint to include an appropriate number of objects and to align final answers with this causal reasoning; and (3) anti-shortcut data augmentation that applies targeted perturbations to images and questions, discouraging reliance on superficial visual or linguistic cues. Experiments show that our method consistently outperforms existing VLMs and specialized spatial reasoning models.", "AI": {"tldr": "This paper introduces Blueprint-VLM, a vision-language model that uses object-centric blueprints (JSON-style structured representations) to enhance spatial reasoning by first creating a blueprint of object positions, sizes, and attributes, then reasoning over this structured representation.", "motivation": "Existing approaches to spatial reasoning in VLMs either focus too much on local patches (weakening global spatial awareness) or mark isolated coordinates (overlooking overall object organization). There's a need for better integration of spatial semantic understanding that captures both object-level details and their spatial relationships.", "method": "Three key techniques: (1) blueprint-embedded reasoning traces for supervised fine-tuning to teach basic reasoning skills; (2) blueprint-aware rewards in reinforcement learning to encourage appropriate object inclusion and answer alignment; (3) anti-shortcut data augmentation with targeted perturbations to prevent reliance on superficial cues.", "result": "The method consistently outperforms existing VLMs and specialized spatial reasoning models in experiments.", "conclusion": "Integrating object-centric blueprints into VLMs effectively enhances spatial reasoning by providing structured representations that capture both object-level details and their spatial organization, advancing VLMs from visual perception toward spatial semantic understanding."}}
{"id": "2601.01989", "categories": ["cs.CV", "cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2601.01989", "abs": "https://arxiv.org/abs/2601.01989", "authors": ["Aly R. Elkammar", "Karim M. Gamaleldin", "Catherine M. Elias"], "title": "VIT-Ped: Visionary Intention Transformer for Pedestrian Behavior Analysis", "comment": null, "summary": "Pedestrian Intention prediction is one of the key technologies in the transition from level 3 to level 4 autonomous driving. To understand pedestrian crossing behaviour, several elements and features should be taken into consideration to make the roads of tomorrow safer for everybody. We introduce a transformer / video vision transformer based algorithm of different sizes which uses different data modalities .We evaluated our algorithms on popular pedestrian behaviour dataset, JAAD, and have reached SOTA performance and passed the SOTA in metrics like Accuracy, AUC and F1-score. The advantages brought by different model design choices are investigated via extensive ablation studies.", "AI": {"tldr": "Transformer-based pedestrian intention prediction model achieves SOTA performance on JAAD dataset using multi-modal data.", "motivation": "Pedestrian intention prediction is crucial for advancing from level 3 to level 4 autonomous driving to improve road safety for everyone.", "method": "Transformer/video vision transformer architecture of different sizes using multiple data modalities, with extensive ablation studies to investigate design choices.", "result": "Achieved state-of-the-art performance on JAAD dataset, surpassing previous methods in Accuracy, AUC, and F1-score metrics.", "conclusion": "The transformer-based approach effectively predicts pedestrian crossing intentions and contributes to safer autonomous driving systems."}}
{"id": "2601.01992", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.01992", "abs": "https://arxiv.org/abs/2601.01992", "authors": ["Chen Zhu", "Huiwen Zhang", "Yujie Li", "Mu He", "Xiaotian Qiao"], "title": "API: Empowering Generalizable Real-World Image Dehazing via Adaptive Patch Importance Learning", "comment": null, "summary": "Real-world image dehazing is a fundamental yet challenging task in low-level vision. Existing learning-based methods often suffer from significant performance degradation when applied to complex real-world hazy scenes, primarily due to limited training data and the intrinsic complexity of haze density distributions.To address these challenges, we introduce a novel Adaptive Patch Importance-aware (API) framework for generalizable real-world image dehazing. Specifically, our framework consists of an Automatic Haze Generation (AHG) module and a Density-aware Haze Removal (DHR) module. AHG provides a hybrid data augmentation strategy by generating realistic and diverse hazy images as additional high-quality training data. DHR considers hazy regions with varying haze density distributions for generalizable real-world image dehazing in an adaptive patch importance-aware manner. To alleviate the ambiguity of the dehazed image details, we further introduce a new Multi-Negative Contrastive Dehazing (MNCD) loss, which fully utilizes information from multiple negative samples across both spatial and frequency domains. Extensive experiments demonstrate that our framework achieves state-of-the-art performance across multiple real-world benchmarks, delivering strong results in both quantitative metrics and qualitative visual quality, and exhibiting robust generalization across diverse haze distributions.", "AI": {"tldr": "API framework with adaptive patch importance for generalizable real-world image dehazing, using hybrid data augmentation and multi-negative contrastive loss.", "motivation": "Existing learning-based dehazing methods suffer performance degradation in real-world complex hazy scenes due to limited training data and complex haze density distributions.", "method": "Proposes Adaptive Patch Importance-aware (API) framework with two modules: 1) Automatic Haze Generation (AHG) for hybrid data augmentation, and 2) Density-aware Haze Removal (DHR) for adaptive patch importance-aware dehazing. Introduces Multi-Negative Contrastive Dehazing (MNCD) loss to reduce ambiguity in dehazed details.", "result": "Achieves state-of-the-art performance across multiple real-world benchmarks, with strong quantitative metrics and qualitative visual quality, demonstrating robust generalization across diverse haze distributions.", "conclusion": "The API framework effectively addresses real-world dehazing challenges through adaptive patch importance-aware processing and comprehensive data augmentation, showing superior generalization capability."}}
{"id": "2601.01998", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.01998", "abs": "https://arxiv.org/abs/2601.01998", "authors": ["Chen Zhu", "Huiwen Zhang", "Mu He", "Yujie Li", "Xiaotian Qiao"], "title": "Nighttime Hazy Image Enhancement via Progressively and Mutually Reinforcing Night-Haze Priors", "comment": null, "summary": "Enhancing the visibility of nighttime hazy images is challenging due to the complex degradation distributions. Existing methods mainly address a single type of degradation (e.g., haze or low-light) at a time, ignoring the interplay of different degradation types and resulting in limited visibility improvement. We observe that the domain knowledge shared between low-light and haze priors can be reinforced mutually for better visibility. Based on this key insight, in this paper, we propose a novel framework that enhances visibility in nighttime hazy images by reinforcing the intrinsic consistency between haze and low-light priors mutually and progressively. In particular, our model utilizes image-, patch-, and pixel-level experts that operate across visual and frequency domains to recover global scene structure, regional patterns, and fine-grained details progressively. A frequency-aware router is further introduced to adaptively guide the contribution of each expert, ensuring robust image restoration. Extensive experiments demonstrate the superior performance of our model on nighttime dehazing benchmarks both quantitatively and qualitatively. Moreover, we showcase the generalizability of our model in daytime dehazing and low-light enhancement tasks.", "AI": {"tldr": "A novel framework that enhances nighttime hazy images by mutually reinforcing haze and low-light priors through multi-level experts operating across visual and frequency domains.", "motivation": "Nighttime hazy images suffer from complex degradation distributions where existing methods only address single degradation types (haze or low-light) separately, ignoring their interplay and resulting in limited visibility improvement. The authors observed that domain knowledge between low-light and haze priors can be mutually reinforced for better visibility.", "method": "Proposes a framework that reinforces intrinsic consistency between haze and low-light priors progressively. Uses image-, patch-, and pixel-level experts operating across visual and frequency domains to recover global scene structure, regional patterns, and fine-grained details. Introduces a frequency-aware router to adaptively guide each expert's contribution for robust image restoration.", "result": "Extensive experiments demonstrate superior performance on nighttime dehazing benchmarks both quantitatively and qualitatively. The model also shows generalizability in daytime dehazing and low-light enhancement tasks.", "conclusion": "The proposed framework effectively addresses the complex degradation in nighttime hazy images by mutually reinforcing haze and low-light priors through a progressive multi-level expert approach with adaptive routing, achieving state-of-the-art performance and demonstrating broad applicability."}}
{"id": "2601.02016", "categories": ["cs.CV", "cs.AI", "cs.ET", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.02016", "abs": "https://arxiv.org/abs/2601.02016", "authors": ["Matthias Bartolo", "Dylan Seychell", "Gabriel Hili", "Matthew Montebello", "Carl James Debono", "Saviour Formosa", "Konstantinos Makantasis"], "title": "Enhancing Object Detection with Privileged Information: A Model-Agnostic Teacher-Student Approach", "comment": "Code available on GitHub: https://github.com/mbar0075/lupi-for-object-detection", "summary": "This paper investigates the integration of the Learning Using Privileged Information (LUPI) paradigm in object detection to exploit fine-grained, descriptive information available during training but not at inference. We introduce a general, model-agnostic methodology for injecting privileged information-such as bounding box masks, saliency maps, and depth cues-into deep learning-based object detectors through a teacher-student architecture. Experiments are conducted across five state-of-the-art object detection models and multiple public benchmarks, including UAV-based litter detection datasets and Pascal VOC 2012, to assess the impact on accuracy, generalization, and computational efficiency. Our results demonstrate that LUPI-trained students consistently outperform their baseline counterparts, achieving significant boosts in detection accuracy with no increase in inference complexity or model size. Performance improvements are especially marked for medium and large objects, while ablation studies reveal that intermediate weighting of teacher guidance optimally balances learning from privileged and standard inputs. The findings affirm that the LUPI framework provides an effective and practical strategy for advancing object detection systems in both resource-constrained and real-world settings.", "AI": {"tldr": "LUPI paradigm enhances object detection by using privileged information (like masks, saliency maps, depth) during training but not inference, achieving better accuracy without increasing model complexity.", "motivation": "To leverage fine-grained descriptive information available during training but not at inference time to improve object detection performance without increasing computational overhead during deployment.", "method": "Model-agnostic teacher-student architecture that injects privileged information (bounding box masks, saliency maps, depth cues) into deep learning object detectors during training only.", "result": "LUPI-trained detectors consistently outperform baselines with significant accuracy improvements, especially for medium/large objects, with no inference complexity or model size increase.", "conclusion": "LUPI framework provides effective, practical strategy for advancing object detection in resource-constrained and real-world settings by leveraging privileged training information."}}
{"id": "2601.02018", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.02018", "abs": "https://arxiv.org/abs/2601.02018", "authors": ["Guangqian Guo", "Aixi Ren", "Yong Guo", "Xuehui Yu", "Jiacheng Tian", "Wenli Li", "Yaoxing Wang", "Shan Gao"], "title": "Towards Any-Quality Image Segmentation via Generative and Adaptive Latent Space Enhancement", "comment": "Diffusion-based latent space enhancement helps improve the robustness of SAM", "summary": "Segment Anything Models (SAMs), known for their exceptional zero-shot segmentation performance, have garnered significant attention in the research community. Nevertheless, their performance drops significantly on severely degraded, low-quality images, limiting their effectiveness in real-world scenarios. To address this, we propose GleSAM++, which utilizes Generative Latent space Enhancement to boost robustness on low-quality images, thus enabling generalization across various image qualities. Additionally, to improve compatibility between the pre-trained diffusion model and the segmentation framework, we introduce two techniques, i.e., Feature Distribution Alignment (FDA) and Channel Replication and Expansion (CRE). However, the above components lack explicit guidance regarding the degree of degradation. The model is forced to implicitly fit a complex noise distribution that spans conditions from mild noise to severe artifacts, which substantially increases the learning burden and leads to suboptimal reconstructions. To address this issue, we further introduce a Degradation-aware Adaptive Enhancement (DAE) mechanism. The key principle of DAE is to decouple the reconstruction process for arbitrary-quality features into two stages: degradation-level prediction and degradation-aware reconstruction. Our method can be applied to pre-trained SAM and SAM2 with only minimal additional learnable parameters, allowing for efficient optimization. Extensive experiments demonstrate that GleSAM++ significantly improves segmentation robustness on complex degradations while maintaining generalization to clear images. Furthermore, GleSAM++ also performs well on unseen degradations, underscoring the versatility of our approach and dataset.", "AI": {"tldr": "GleSAM++ enhances SAM's robustness on low-quality images using generative latent enhancement with degradation-aware adaptive reconstruction.", "motivation": "SAMs perform poorly on severely degraded, low-quality images, limiting real-world applicability. Current methods lack explicit degradation guidance, forcing models to implicitly fit complex noise distributions.", "method": "Uses Generative Latent space Enhancement with Feature Distribution Alignment (FDA) and Channel Replication and Expansion (CRE). Introduces Degradation-aware Adaptive Enhancement (DAE) with two-stage process: degradation-level prediction and degradation-aware reconstruction. Minimal additional parameters for efficient optimization.", "result": "Significantly improves segmentation robustness on complex degradations while maintaining generalization to clear images. Performs well on unseen degradations, demonstrating versatility.", "conclusion": "GleSAM++ effectively addresses SAM's limitations on low-quality images through degradation-aware enhancement, enabling robust segmentation across various image qualities with minimal computational overhead."}}
{"id": "2601.02020", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.02020", "abs": "https://arxiv.org/abs/2601.02020", "authors": ["Shihan Peng", "Yuyang Xiong", "Hanyu Zhou", "Zhiwei Shi", "Haoyue Liu", "Gang Chen", "Luxin Yan", "Yi Chang"], "title": "Adapting Depth Anything to Adverse Imaging Conditions with Events", "comment": "This work has been submitted to the IEEE for possible publication", "summary": "Robust depth estimation under dynamic and adverse lighting conditions is essential for robotic systems. Currently, depth foundation models, such as Depth Anything, achieve great success in ideal scenes but remain challenging under adverse imaging conditions such as extreme illumination and motion blur. These degradations corrupt the visual signals of frame cameras, weakening the discriminative features of frame-based depths across the spatial and temporal dimensions. Typically, existing approaches incorporate event cameras to leverage their high dynamic range and temporal resolution, aiming to compensate for corrupted frame features. However, such specialized fusion models are predominantly trained from scratch on domain-specific datasets, thereby failing to inherit the open-world knowledge and robust generalization inherent to foundation models. In this work, we propose ADAE, an event-guided spatiotemporal fusion framework for Depth Anything in degraded scenes. Our design is guided by two key insights: 1) Entropy-Aware Spatial Fusion. We adaptively merge frame-based and event-based features using an information entropy strategy to indicate illumination-induced degradation. 2) Motion-Guided Temporal Correction. We resort to the event-based motion cue to recalibrate ambiguous features in blurred regions. Under our unified framework, the two components are complementary to each other and jointly enhance Depth Anything under adverse imaging conditions. Extensive experiments have been performed to verify the superiority of the proposed method. Our code will be released upon acceptance.", "AI": {"tldr": "ADAE enhances Depth Anything foundation model for robust depth estimation in adverse conditions by fusing frame and event camera data using entropy-aware spatial fusion and motion-guided temporal correction.", "motivation": "Depth foundation models like Depth Anything work well in ideal conditions but struggle under adverse imaging conditions (extreme illumination, motion blur) that corrupt visual signals. While event cameras can help with their high dynamic range and temporal resolution, existing fusion models are trained from scratch and fail to inherit the open-world knowledge and generalization of foundation models.", "method": "ADAE is an event-guided spatiotemporal fusion framework with two key components: 1) Entropy-Aware Spatial Fusion - adaptively merges frame-based and event-based features using information entropy to indicate illumination-induced degradation; 2) Motion-Guided Temporal Correction - uses event-based motion cues to recalibrate ambiguous features in blurred regions. These components complement each other to enhance Depth Anything.", "result": "Extensive experiments verify the superiority of the proposed method. The code will be released upon acceptance.", "conclusion": "ADAE successfully enhances the Depth Anything foundation model for robust depth estimation under dynamic and adverse lighting conditions by effectively fusing frame and event camera data through complementary spatial and temporal fusion strategies."}}
{"id": "2601.02029", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.02029", "abs": "https://arxiv.org/abs/2601.02029", "authors": ["Toshihiko Nishimura", "Hirofumi Abe", "Kazuhiko Murasaki", "Taiga Yoshida", "Ryuichi Tanida"], "title": "Leveraging 2D-VLM for Label-Free 3D Segmentation in Large-Scale Outdoor Scene Understanding", "comment": "19", "summary": "This paper presents a novel 3D semantic segmentation method for large-scale point cloud data that does not require annotated 3D training data or paired RGB images. The proposed approach projects 3D point clouds onto 2D images using virtual cameras and performs semantic segmentation via a foundation 2D model guided by natural language prompts. 3D segmentation is achieved by aggregating predictions from multiple viewpoints through weighted voting. Our method outperforms existing training-free approaches and achieves segmentation accuracy comparable to supervised methods. Moreover, it supports open-vocabulary recognition, enabling users to detect objects using arbitrary text queries, thus overcoming the limitations of traditional supervised approaches.", "AI": {"tldr": "A training-free 3D semantic segmentation method that projects point clouds to 2D images, uses foundation 2D models with natural language prompts, and aggregates multi-view predictions via weighted voting.", "motivation": "To enable 3D semantic segmentation without requiring annotated 3D training data or paired RGB images, overcoming the limitations of traditional supervised approaches that need extensive labeled data.", "method": "Projects 3D point clouds onto 2D images using virtual cameras, performs semantic segmentation via foundation 2D models guided by natural language prompts, and aggregates predictions from multiple viewpoints through weighted voting.", "result": "Outperforms existing training-free approaches and achieves segmentation accuracy comparable to supervised methods, while supporting open-vocabulary recognition with arbitrary text queries.", "conclusion": "The method provides an effective training-free solution for 3D semantic segmentation that leverages 2D foundation models and natural language guidance, enabling flexible open-vocabulary object detection in 3D point clouds."}}
{"id": "2601.02038", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.02038", "abs": "https://arxiv.org/abs/2601.02038", "authors": ["Yihan Zhu", "Mengying Ge"], "title": "AlignVTOFF: Texture-Spatial Feature Alignment for High-Fidelity Virtual Try-Off", "comment": null, "summary": "Virtual Try-Off (VTOFF) is a challenging multimodal image generation task that aims to synthesize high-fidelity flat-lay garments under complex geometric deformation and rich high-frequency textures. Existing methods often rely on lightweight modules for fast feature extraction, which struggles to preserve structured patterns and fine-grained details, leading to texture attenuation during generation.To address these issues, we propose AlignVTOFF, a novel parallel U-Net framework built upon a Reference U-Net and Texture-Spatial Feature Alignment (TSFA). The Reference U-Net performs multi-scale feature extraction and enhances geometric fidelity, enabling robust modeling of deformation while retaining complex structured patterns. TSFA then injects the reference garment features into a frozen denoising U-Net via a hybrid attention design, consisting of a trainable cross-attention module and a frozen self-attention module. This design explicitly aligns texture and spatial cues and alleviates the loss of high-frequency information during the denoising process.Extensive experiments across multiple settings demonstrate that AlignVTOFF consistently outperforms state-of-the-art methods, producing flat-lay garment results with improved structural realism and high-frequency detail fidelity.", "AI": {"tldr": "AlignVTOFF: A parallel U-Net framework with Reference U-Net and Texture-Spatial Feature Alignment for high-fidelity virtual try-off garment generation.", "motivation": "Existing VTOFF methods struggle with texture attenuation and loss of fine-grained details due to lightweight feature extraction modules that fail to preserve structured patterns and high-frequency textures during complex geometric deformation.", "method": "Proposes AlignVTOFF with two key components: 1) Reference U-Net for multi-scale feature extraction and geometric fidelity enhancement, and 2) Texture-Spatial Feature Alignment (TSFA) with hybrid attention (trainable cross-attention + frozen self-attention) to inject reference features into a frozen denoising U-Net.", "result": "Extensive experiments show AlignVTOFF consistently outperforms state-of-the-art methods, producing flat-lay garments with improved structural realism and high-frequency detail fidelity across multiple settings.", "conclusion": "AlignVTOFF effectively addresses texture attenuation in VTOFF tasks through its parallel U-Net architecture and hybrid attention mechanism, enabling robust modeling of deformation while preserving complex patterns and fine details."}}
{"id": "2601.02046", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.02046", "abs": "https://arxiv.org/abs/2601.02046", "authors": ["Shaocheng Shen", "Jianfeng Liang. Chunlei Cai", "Cong Geng", "Huiyu Duan", "Xiaoyun Zhang", "Qiang Hu", "Guangtao Zhai"], "title": "Agentic Retoucher for Text-To-Image Generation", "comment": null, "summary": "Text-to-image (T2I) diffusion models such as SDXL and FLUX have achieved impressive photorealism, yet small-scale distortions remain pervasive in limbs, face, text and so on. Existing refinement approaches either perform costly iterative re-generation or rely on vision-language models (VLMs) with weak spatial grounding, leading to semantic drift and unreliable local edits. To close this gap, we propose Agentic Retoucher, a hierarchical decision-driven framework that reformulates post-generation correction as a human-like perception-reasoning-action loop. Specifically, we design (1) a perception agent that learns contextual saliency for fine-grained distortion localization under text-image consistency cues, (2) a reasoning agent that performs human-aligned inferential diagnosis via progressive preference alignment, and (3) an action agent that adaptively plans localized inpainting guided by user preference. This design integrates perceptual evidence, linguistic reasoning, and controllable correction into a unified, self-corrective decision process. To enable fine-grained supervision and quantitative evaluation, we further construct GenBlemish-27K, a dataset of 6K T2I images with 27K annotated artifact regions across 12 categories. Extensive experiments demonstrate that Agentic Retoucher consistently outperforms state-of-the-art methods in perceptual quality, distortion localization and human preference alignment, establishing a new paradigm for self-corrective and perceptually reliable T2I generation.", "AI": {"tldr": "Agentic Retoucher is a hierarchical decision-driven framework that fixes small-scale distortions in text-to-image generation through a perception-reasoning-action loop, outperforming existing methods in perceptual quality and human preference alignment.", "motivation": "Current text-to-image diffusion models like SDXL and FLUX produce photorealistic images but suffer from pervasive small-scale distortions in limbs, faces, text, etc. Existing refinement approaches are either too costly (iterative re-generation) or rely on vision-language models with weak spatial grounding, leading to semantic drift and unreliable local edits.", "method": "A hierarchical decision-driven framework with three agents: (1) Perception agent learns contextual saliency for fine-grained distortion localization using text-image consistency cues; (2) Reasoning agent performs human-aligned inferential diagnosis via progressive preference alignment; (3) Action agent adaptively plans localized inpainting guided by user preference. The framework also introduces GenBlemish-27K dataset with 6K T2I images and 27K annotated artifact regions across 12 categories for fine-grained supervision.", "result": "Extensive experiments show Agentic Retoucher consistently outperforms state-of-the-art methods in perceptual quality, distortion localization, and human preference alignment. It establishes a new paradigm for self-corrective and perceptually reliable T2I generation.", "conclusion": "The proposed Agentic Retoucher framework successfully addresses the problem of small-scale distortions in T2I generation through a human-like perception-reasoning-action loop, integrating perceptual evidence, linguistic reasoning, and controllable correction into a unified self-corrective decision process."}}
{"id": "2601.02088", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.02088", "abs": "https://arxiv.org/abs/2601.02088", "authors": ["Jiahao Bao", "Huazhen Liu", "Yu Zhuang", "Leran Tao", "Xinyu Xu", "Yongtao Shi", "Mengjia Cheng", "Yiming Wang", "Congshuang Ku", "Ting Zeng", "Yilang Du", "Siyi Chen", "Shunyao Shen", "Suncheng Xiang", "Hongbo Yu"], "title": "PhysSFI-Net: Physics-informed Geometric Learning of Skeletal and Facial Interactions for Orthognathic Surgical Outcome Prediction", "comment": "31 pages, 8 figures", "summary": "Orthognathic surgery repositions jaw bones to restore occlusion and enhance facial aesthetics. Accurate simulation of postoperative facial morphology is essential for preoperative planning. However, traditional biomechanical models are computationally expensive, while geometric deep learning approaches often lack interpretability. In this study, we develop and validate a physics-informed geometric deep learning framework named PhysSFI-Net for precise prediction of soft tissue deformation following orthognathic surgery. PhysSFI-Net consists of three components: a hierarchical graph module with craniofacial and surgical plan encoders combined with attention mechanisms to extract skeletal-facial interaction features; a Long Short-Term Memory (LSTM)-based sequential predictor for incremental soft tissue deformation; and a biomechanics-inspired module for high-resolution facial surface reconstruction. Model performance was assessed using point cloud shape error (Hausdorff distance), surface deviation error, and landmark localization error (Euclidean distances of craniomaxillofacial landmarks) between predicted facial shapes and corresponding ground truths. A total of 135 patients who underwent combined orthodontic and orthognathic treatment were included for model training and validation. Quantitative analysis demonstrated that PhysSFI-Net achieved a point cloud shape error of 1.070 +/- 0.088 mm, a surface deviation error of 1.296 +/- 0.349 mm, and a landmark localization error of 2.445 +/- 1.326 mm. Comparative experiments indicated that PhysSFI-Net outperformed the state-of-the-art method ACMT-Net in prediction accuracy. In conclusion, PhysSFI-Net enables interpretable, high-resolution prediction of postoperative facial morphology with superior accuracy, showing strong potential for clinical application in orthognathic surgical planning and simulation.", "AI": {"tldr": "PhysSFI-Net: A physics-informed geometric deep learning framework for predicting soft tissue deformation after orthognathic surgery with superior accuracy compared to state-of-the-art methods.", "motivation": "Orthognathic surgery requires accurate postoperative facial morphology prediction for preoperative planning. Traditional biomechanical models are computationally expensive, while existing geometric deep learning approaches lack interpretability.", "method": "PhysSFI-Net combines three components: 1) hierarchical graph module with craniofacial and surgical plan encoders using attention mechanisms to extract skeletal-facial interaction features, 2) LSTM-based sequential predictor for incremental soft tissue deformation, and 3) biomechanics-inspired module for high-resolution facial surface reconstruction.", "result": "On 135 patients, PhysSFI-Net achieved point cloud shape error of 1.070\u00b10.088 mm, surface deviation error of 1.296\u00b10.349 mm, and landmark localization error of 2.445\u00b11.326 mm, outperforming state-of-the-art ACMT-Net in prediction accuracy.", "conclusion": "PhysSFI-Net enables interpretable, high-resolution prediction of postoperative facial morphology with superior accuracy, showing strong potential for clinical application in orthognathic surgical planning and simulation."}}
{"id": "2601.02091", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.02091", "abs": "https://arxiv.org/abs/2601.02091", "authors": ["Zhehuan Cao", "Fiseha Berhanu Tesema", "Ping Fu", "Jianfeng Ren", "Ahmed Nasr"], "title": "MCD-Net: A Lightweight Deep Learning Baseline for Optical-Only Moraine Segmentation", "comment": "13 pages, 10 figures. This manuscript is under review at IEEE Transactions on Geoscience and Remote Sensing", "summary": "Glacial segmentation is essential for reconstructing past glacier dynamics and evaluating climate-driven landscape change. However, weak optical contrast and the limited availability of high-resolution DEMs hinder automated mapping. This study introduces the first large-scale optical-only moraine segmentation dataset, comprising 3,340 manually annotated high-resolution images from Google Earth covering glaciated regions of Sichuan and Yunnan, China. We develop MCD-Net, a lightweight baseline that integrates a MobileNetV2 encoder, a Convolutional Block Attention Module (CBAM), and a DeepLabV3+ decoder. Benchmarking against deeper backbones (ResNet152, Xception) shows that MCD-Net achieves 62.3\\% mean Intersection over Union (mIoU) and 72.8\\% Dice coefficient while reducing computational cost by more than 60\\%. Although ridge delineation remains constrained by sub-pixel width and spectral ambiguity, the results demonstrate that optical imagery alone can provide reliable moraine-body segmentation. The dataset and code are publicly available at https://github.com/Lyra-alpha/MCD-Net, establishing a reproducible benchmark for moraine-specific segmentation and offering a deployable baseline for high-altitude glacial monitoring.", "AI": {"tldr": "MCD-Net: A lightweight neural network for moraine segmentation from optical imagery, achieving 62.3% mIoU with 60% computational cost reduction compared to deeper models.", "motivation": "Automated glacial segmentation is challenging due to weak optical contrast and limited high-resolution DEM availability. Current methods struggle with moraine mapping from optical imagery alone.", "method": "Created first large-scale optical-only moraine segmentation dataset (3,340 manually annotated Google Earth images). Developed MCD-Net with MobileNetV2 encoder, CBAM attention module, and DeepLabV3+ decoder for lightweight segmentation.", "result": "MCD-Net achieves 62.3% mIoU and 72.8% Dice coefficient while reducing computational cost by >60% compared to ResNet152 and Xception backbones. Optical imagery alone can provide reliable moraine-body segmentation.", "conclusion": "The study demonstrates optical-only moraine segmentation feasibility, provides public dataset/code for reproducible benchmarking, and offers deployable baseline for high-altitude glacial monitoring despite ridge delineation challenges."}}
{"id": "2601.02098", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.02098", "abs": "https://arxiv.org/abs/2601.02098", "authors": ["Jinlong Fan", "Shanshan Zhao", "Liang Zheng", "Jing Zhang", "Yuxiang Yang", "Mingming Gong"], "title": "InpaintHuman: Reconstructing Occluded Humans with Multi-Scale UV Mapping and Identity-Preserving Diffusion Inpainting", "comment": null, "summary": "Reconstructing complete and animatable 3D human avatars from monocular videos remains challenging, particularly under severe occlusions. While 3D Gaussian Splatting has enabled photorealistic human rendering, existing methods struggle with incomplete observations, often producing corrupted geometry and temporal inconsistencies. We present InpaintHuman, a novel method for generating high-fidelity, complete, and animatable avatars from occluded monocular videos. Our approach introduces two key innovations: (i) a multi-scale UV-parameterized representation with hierarchical coarse-to-fine feature interpolation, enabling robust reconstruction of occluded regions while preserving geometric details; and (ii) an identity-preserving diffusion inpainting module that integrates textual inversion with semantic-conditioned guidance for subject-specific, temporally coherent completion. Unlike SDS-based methods, our approach employs direct pixel-level supervision to ensure identity fidelity. Experiments on synthetic benchmarks (PeopleSnapshot, ZJU-MoCap) and real-world scenarios (OcMotion) demonstrate competitive performance with consistent improvements in reconstruction quality across diverse poses and viewpoints.", "AI": {"tldr": "InpaintHuman: A method for reconstructing complete, animatable 3D human avatars from occluded monocular videos using multi-scale UV representations and identity-preserving diffusion inpainting.", "motivation": "Existing 3D Gaussian Splatting methods struggle with severe occlusions in monocular videos, producing corrupted geometry and temporal inconsistencies. There's a need for robust reconstruction of complete, animatable human avatars from incomplete observations.", "method": "Two key innovations: 1) Multi-scale UV-parameterized representation with hierarchical coarse-to-fine feature interpolation for robust reconstruction of occluded regions while preserving details. 2) Identity-preserving diffusion inpainting module integrating textual inversion with semantic-conditioned guidance for subject-specific, temporally coherent completion. Uses direct pixel-level supervision instead of SDS-based methods.", "result": "Competitive performance on synthetic benchmarks (PeopleSnapshot, ZJU-MoCap) and real-world scenarios (OcMotion). Shows consistent improvements in reconstruction quality across diverse poses and viewpoints.", "conclusion": "InpaintHuman successfully addresses the challenge of reconstructing complete, animatable 3D human avatars from occluded monocular videos through novel representation and inpainting techniques, achieving high-fidelity results with temporal coherence."}}
{"id": "2601.02102", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.02102", "abs": "https://arxiv.org/abs/2601.02102", "authors": ["Jiaqi Yao", "Zhongmiao Yan", "Jingyi Xu", "Songpengcheng Xia", "Yan Xiang", "Ling Pei"], "title": "360-GeoGS: Geometrically Consistent Feed-Forward 3D Gaussian Splatting Reconstruction for 360 Images", "comment": null, "summary": "3D scene reconstruction is fundamental for spatial intelligence applications such as AR, robotics, and digital twins. Traditional multi-view stereo struggles with sparse viewpoints or low-texture regions, while neural rendering approaches, though capable of producing high-quality results, require per-scene optimization and lack real-time efficiency. Explicit 3D Gaussian Splatting (3DGS) enables efficient rendering, but most feed-forward variants focus on visual quality rather than geometric consistency, limiting accurate surface reconstruction and overall reliability in spatial perception tasks. This paper presents a novel feed-forward 3DGS framework for 360 images, capable of generating geometrically consistent Gaussian primitives while maintaining high rendering quality. A Depth-Normal geometric regularization is introduced to couple rendered depth gradients with normal information, supervising Gaussian rotation, scale, and position to improve point cloud and surface accuracy. Experimental results show that the proposed method maintains high rendering quality while significantly improving geometric consistency, providing an effective solution for 3D reconstruction in spatial perception tasks.", "AI": {"tldr": "A feed-forward 3D Gaussian Splatting framework for 360\u00b0 images that improves geometric consistency while maintaining high rendering quality through Depth-Normal regularization.", "motivation": "Traditional multi-view stereo struggles with sparse viewpoints and low-texture regions, while neural rendering requires per-scene optimization and lacks real-time efficiency. Existing 3D Gaussian Splatting methods focus on visual quality but lack geometric consistency, limiting accurate surface reconstruction for spatial perception tasks.", "method": "Proposes a feed-forward 3DGS framework for 360\u00b0 images with Depth-Normal geometric regularization that couples rendered depth gradients with normal information to supervise Gaussian rotation, scale, and position, improving point cloud and surface accuracy.", "result": "Experimental results show the method maintains high rendering quality while significantly improving geometric consistency, providing an effective solution for 3D reconstruction in spatial perception tasks.", "conclusion": "The proposed framework successfully addresses the geometric consistency limitations of existing 3DGS methods while preserving rendering quality, making it suitable for real-time spatial perception applications like AR, robotics, and digital twins."}}
{"id": "2601.02103", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.02103", "abs": "https://arxiv.org/abs/2601.02103", "authors": ["Yating Wang", "Yuan Sun", "Xuan Wang", "Ran Yi", "Boyao Zhou", "Yipengjing Sun", "Hongyu Liu", "Yinuo Wang", "Lizhuang Ma"], "title": "HeadLighter: Disentangling Illumination in Generative 3D Gaussian Heads via Lightstage Captures", "comment": null, "summary": "Recent 3D-aware head generative models based on 3D Gaussian Splatting achieve real-time, photorealistic and view-consistent head synthesis. However, a fundamental limitation persists: the deep entanglement of illumination and intrinsic appearance prevents controllable relighting. Existing disentanglement methods rely on strong assumptions to enable weakly supervised learning, which restricts their capacity for complex illumination. To address this challenge, we introduce HeadLighter, a novel supervised framework that learns a physically plausible decomposition of appearance and illumination in head generative models. Specifically, we design a dual-branch architecture that separately models lighting-invariant head attributes and physically grounded rendering components. A progressive disentanglement training is employed to gradually inject head appearance priors into the generative architecture, supervised by multi-view images captured under controlled light conditions with a light stage setup. We further introduce a distillation strategy to generate high-quality normals for realistic rendering. Experiments demonstrate that our method preserves high-quality generation and real-time rendering, while simultaneously supporting explicit lighting and viewpoint editing. We will publicly release our code and dataset.", "AI": {"tldr": "HeadLighter is a supervised framework for disentangling illumination and appearance in 3D head generative models, enabling controllable relighting while maintaining real-time rendering.", "motivation": "Current 3D-aware head generative models based on 3D Gaussian Splatting achieve real-time, photorealistic synthesis but suffer from deep entanglement of illumination and intrinsic appearance, preventing controllable relighting. Existing disentanglement methods rely on strong assumptions that limit their capacity for complex illumination.", "method": "Introduces HeadLighter with a dual-branch architecture that separately models lighting-invariant head attributes and physically grounded rendering components. Uses progressive disentanglement training to inject head appearance priors, supervised by multi-view images captured under controlled light conditions with a light stage setup. Also includes a distillation strategy to generate high-quality normals for realistic rendering.", "result": "The method preserves high-quality generation and real-time rendering while simultaneously supporting explicit lighting and viewpoint editing. Experiments demonstrate successful disentanglement and controllable relighting capabilities.", "conclusion": "HeadLighter addresses the fundamental limitation of illumination-appearance entanglement in 3D head generative models, enabling physically plausible decomposition and controllable relighting while maintaining the benefits of real-time, photorealistic synthesis. The authors will publicly release code and dataset."}}
{"id": "2601.02107", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.02107", "abs": "https://arxiv.org/abs/2601.02107", "authors": ["Jiancheng Huang", "Mingfu Yan", "Songyan Chen", "Yi Huang", "Shifeng Chen"], "title": "MagicFight: Personalized Martial Arts Combat Video Generation", "comment": "Accepted by ACM MM 2024", "summary": "Amid the surge in generic text-to-video generation, the field of personalized human video generation has witnessed notable advancements, primarily concentrated on single-person scenarios. However, to our knowledge, the domain of two-person interactions, particularly in the context of martial arts combat, remains uncharted. We identify a significant gap: existing models for single-person dancing generation prove insufficient for capturing the subtleties and complexities of two engaged fighters, resulting in challenges such as identity confusion, anomalous limbs, and action mismatches. To address this, we introduce a pioneering new task, Personalized Martial Arts Combat Video Generation. Our approach, MagicFight, is specifically crafted to overcome these hurdles. Given this pioneering task, we face a lack of appropriate datasets. Thus, we generate a bespoke dataset using the game physics engine Unity, meticulously crafting a multitude of 3D characters, martial arts moves, and scenes designed to represent the diversity of combat. MagicFight refines and adapts existing models and strategies to generate high-fidelity two-person combat videos that maintain individual identities and ensure seamless, coherent action sequences, thereby laying the groundwork for future innovations in the realm of interactive video content creation.\n  Website: https://MingfuYAN.github.io/MagicFight/\n  Dataset: https://huggingface.co/datasets/MingfuYAN/KungFu-Fiesta", "AI": {"tldr": "MagicFight introduces personalized martial arts combat video generation for two-person interactions, addressing limitations of existing single-person models by creating a custom Unity-generated dataset and adapting models for coherent two-fighter videos.", "motivation": "Existing text-to-video generation focuses on single-person scenarios (like dancing), but fails to capture the complexities of two-person interactions, especially martial arts combat. Current models suffer from identity confusion, anomalous limbs, and action mismatches when applied to two engaged fighters.", "method": "1) Introduces new task: Personalized Martial Arts Combat Video Generation; 2) Creates custom dataset using Unity physics engine with diverse 3D characters, martial arts moves, and scenes; 3) Develops MagicFight approach that refines and adapts existing models to generate high-fidelity two-person combat videos while maintaining individual identities and coherent action sequences.", "result": "MagicFight successfully generates two-person combat videos that overcome previous limitations like identity confusion and action mismatches. The approach lays groundwork for interactive video content creation and provides a publicly available dataset (KungFu-Fiesta on Hugging Face).", "conclusion": "This work pioneers the domain of two-person martial arts combat video generation, addressing a significant gap in personalized video generation. By creating a specialized dataset and adapting models, MagicFight enables coherent two-fighter interactions and opens new possibilities for interactive video content."}}
{"id": "2601.02112", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.02112", "abs": "https://arxiv.org/abs/2601.02112", "authors": ["Utkarsh Singh", "Absaar Ali", "Adarsh Roy"], "title": "Car Drag Coefficient Prediction from 3D Point Clouds Using a Slice-Based Surrogate Model", "comment": "14 pages, 5 figures. Published in: Bramer M., Stahl F. (eds) Artificial Intelligence XLII. SGAI 2025. Lecture Notes in Computer Science, vol 16302. Springer, Cham", "summary": "The automotive industry's pursuit of enhanced fuel economy and performance necessitates efficient aerodynamic design. However, traditional evaluation methods such as computational fluid dynamics (CFD) and wind tunnel testing are resource intensive, hindering rapid iteration in the early design stages. Machine learning-based surrogate models offer a promising alternative, yet many existing approaches suffer from high computational complexity, limited interpretability, or insufficient accuracy for detailed geometric inputs. This paper introduces a novel lightweight surrogate model for the prediction of the aerodynamic drag coefficient (Cd) based on a sequential slice-wise processing of the geometry of the 3D vehicle. Inspired by medical imaging, 3D point clouds of vehicles are decomposed into an ordered sequence of 2D cross-sectional slices along the stream-wise axis. Each slice is encoded by a lightweight PointNet2D module, and the sequence of slice embeddings is processed by a bidirectional LSTM to capture longitudinal geometric evolution. The model, trained and evaluated on the DrivAerNet++ dataset, achieves a high coefficient of determination (R^2 > 0.9528) and a low mean absolute error (MAE approx 6.046 x 10^{-3}) in Cd prediction. With an inference time of approximately 0.025 seconds per sample on a consumer-grade GPU, our approach provides fast, accurate, and interpretable aerodynamic feedback, facilitating more agile and informed automotive design exploration.", "AI": {"tldr": "A lightweight surrogate model predicts vehicle aerodynamic drag by processing 3D point clouds as sequential 2D slices using PointNet2D and bidirectional LSTM, achieving high accuracy with fast inference for design exploration.", "motivation": "Traditional aerodynamic evaluation methods (CFD, wind tunnel testing) are resource-intensive and slow for early design iteration. Existing machine learning surrogate models often have high computational complexity, limited interpretability, or insufficient accuracy for detailed geometry.", "method": "3D vehicle point clouds are decomposed into ordered sequence of 2D cross-sectional slices along stream-wise axis. Each slice is encoded by lightweight PointNet2D module, and sequence of slice embeddings is processed by bidirectional LSTM to capture longitudinal geometric evolution.", "result": "Achieves high coefficient of determination (R\u00b2 > 0.9528) and low mean absolute error (MAE \u2248 6.046\u00d710\u207b\u00b3) in Cd prediction on DrivAerNet++ dataset. Inference time of ~0.025 seconds per sample on consumer-grade GPU.", "conclusion": "The approach provides fast, accurate, and interpretable aerodynamic feedback, enabling more agile and informed automotive design exploration compared to traditional methods."}}
{"id": "2601.02126", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.02126", "abs": "https://arxiv.org/abs/2601.02126", "authors": ["Xavier Bou", "Elliot Vincent", "Gabriele Facciolo", "Rafael Grompone von Gioi", "Jean-Michel Morel", "Thibaud Ehret"], "title": "Remote Sensing Change Detection via Weak Temporal Supervision", "comment": null, "summary": "Semantic change detection in remote sensing aims to identify land cover changes between bi-temporal image pairs. Progress in this area has been limited by the scarcity of annotated datasets, as pixel-level annotation is costly and time-consuming. To address this, recent methods leverage synthetic data or generate artificial change pairs, but out-of-domain generalization remains limited. In this work, we introduce a weak temporal supervision strategy that leverages additional temporal observations of existing single-temporal datasets, without requiring any new annotations. Specifically, we extend single-date remote sensing datasets with new observations acquired at different times and train a change detection model by assuming that real bi-temporal pairs mostly contain no change, while pairing images from different locations to generate change examples. To handle the inherent noise in these weak labels, we employ an object-aware change map generation and an iterative refinement process. We validate our approach on extended versions of the FLAIR and IAILD aerial datasets, achieving strong zero-shot and low-data regime performance across different benchmarks. Lastly, we showcase results over large areas in France, highlighting the scalability potential of our method.", "AI": {"tldr": "Weak temporal supervision for semantic change detection using existing single-temporal datasets without new annotations, achieving strong zero-shot performance.", "motivation": "Semantic change detection suffers from limited annotated datasets due to costly pixel-level annotation. Existing methods using synthetic data or artificial change pairs have limited out-of-domain generalization.", "method": "Extend single-date remote sensing datasets with temporal observations, train by assuming real bi-temporal pairs contain no change, while pairing images from different locations to generate change examples. Use object-aware change map generation and iterative refinement to handle weak label noise.", "result": "Validated on extended FLAIR and IAILD aerial datasets, achieving strong zero-shot and low-data regime performance across benchmarks. Showcased scalability over large areas in France.", "conclusion": "Proposed weak temporal supervision strategy effectively leverages existing datasets without new annotations, enabling scalable semantic change detection with strong generalization performance."}}
{"id": "2601.02139", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.02139", "abs": "https://arxiv.org/abs/2601.02139", "authors": ["Chenyang Lai", "Shuaiyu Chen", "Tianjin Huang", "Siyang Song", "Guangliang Cheng", "Chunbo Luo", "Zeyu Fu"], "title": "Beyond Segmentation: An Oil Spill Change Detection Framework Using Synthetic SAR Imagery", "comment": null, "summary": "Marine oil spills are urgent environmental hazards that demand rapid and reliable detection to minimise ecological and economic damage. While Synthetic Aperture Radar (SAR) imagery has become a key tool for large-scale oil spill monitoring, most existing detection methods rely on deep learning-based segmentation applied to single SAR images. These static approaches struggle to distinguish true oil spills from visually similar oceanic features (e.g., biogenic slicks or low-wind zones), leading to high false positive rates and limited generalizability, especially under data-scarce conditions. To overcome these limitations, we introduce Oil Spill Change Detection (OSCD), a new bi-temporal task that focuses on identifying changes between pre- and post-spill SAR images. As real co-registered pre-spill imagery is not always available, we propose the Temporal-Aware Hybrid Inpainting (TAHI) framework, which generates synthetic pre-spill images from post-spill SAR data. TAHI integrates two key components: High-Fidelity Hybrid Inpainting for oil-free reconstruction, and Temporal Realism Enhancement for radiometric and sea-state consistency. Using TAHI, we construct the first OSCD dataset and benchmark several state-of-the-art change detection models. Results show that OSCD significantly reduces false positives and improves detection accuracy compared to conventional segmentation, demonstrating the value of temporally-aware methods for reliable, scalable oil spill monitoring in real-world scenarios.", "AI": {"tldr": "The paper introduces Oil Spill Change Detection (OSCD), a new bi-temporal approach using pre- and post-spill SAR images to improve oil spill detection accuracy and reduce false positives compared to traditional single-image segmentation methods.", "motivation": "Current oil spill detection methods using single SAR images struggle to distinguish true oil spills from visually similar oceanic features (biogenic slicks, low-wind zones), leading to high false positive rates and limited generalizability, especially under data-scarce conditions.", "method": "Proposes Temporal-Aware Hybrid Inpainting (TAHI) framework with two components: 1) High-Fidelity Hybrid Inpainting for oil-free reconstruction of pre-spill images, and 2) Temporal Realism Enhancement for radiometric and sea-state consistency. Uses TAHI to create synthetic pre-spill images when real co-registered pre-spill imagery is unavailable, enabling bi-temporal change detection.", "result": "Created the first OSCD dataset and benchmarked state-of-the-art change detection models. Results show OSCD significantly reduces false positives and improves detection accuracy compared to conventional segmentation methods.", "conclusion": "Temporally-aware methods like OSCD demonstrate superior performance for reliable, scalable oil spill monitoring in real-world scenarios by leveraging change detection between pre- and post-spill images rather than relying on single-image analysis."}}
{"id": "2601.02141", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.02141", "abs": "https://arxiv.org/abs/2601.02141", "authors": ["Romain Vo", "Juli\u00e1n Tachella"], "title": "Efficient Unrolled Networks for Large-Scale 3D Inverse Problems", "comment": null, "summary": "Deep learning-based methods have revolutionized the field of imaging inverse problems, yielding state-of-the-art performance across various imaging domains. The best performing networks incorporate the imaging operator within the network architecture, typically in the form of deep unrolling. However, in large-scale problems, such as 3D imaging, most existing methods fail to incorporate the operator in the architecture due to the prohibitive amount of memory required by global forward operators, which hinder typical patching strategies. In this work, we present a domain partitioning strategy and normal operator approximations that enable the training of end-to-end reconstruction models incorporating forward operators of arbitrarily large problems into their architecture. The proposed method achieves state-of-the-art performance on 3D X-ray cone-beam tomography and 3D multi-coil accelerated MRI, while requiring only a single GPU for both training and inference.", "AI": {"tldr": "Domain partitioning strategy enables end-to-end deep learning reconstruction for large-scale 3D imaging problems by incorporating forward operators into network architecture with single GPU requirements.", "motivation": "Current deep learning methods for imaging inverse problems struggle with large-scale 3D imaging due to memory constraints from global forward operators, preventing operator incorporation into network architectures and limiting patching strategies.", "method": "Proposes domain partitioning strategy and normal operator approximations that allow training of end-to-end reconstruction models with forward operators of arbitrarily large problems in their architecture.", "result": "Achieves state-of-the-art performance on 3D X-ray cone-beam tomography and 3D multi-coil accelerated MRI, requiring only a single GPU for both training and inference.", "conclusion": "The proposed method enables practical deep learning reconstruction for large-scale 3D imaging problems by overcoming memory limitations while maintaining state-of-the-art performance."}}
{"id": "2601.02147", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.02147", "abs": "https://arxiv.org/abs/2601.02147", "authors": ["Sunny Gupta", "Shounak Das", "Amit Sethi"], "title": "BiPrompt: Bilateral Prompt Optimization for Visual and Textual Debiasing in Vision-Language Models", "comment": "Accepted at the AAAI 2026 Workshop AIR-FM, Assessing and Improving Reliability of Foundation Models in the Real World", "summary": "Vision language foundation models such as CLIP exhibit impressive zero-shot generalization yet remain vulnerable to spurious correlations across visual and textual modalities. Existing debiasing approaches often address a single modality either visual or textual leading to partial robustness and unstable adaptation under distribution shifts. We propose a bilateral prompt optimization framework (BiPrompt) that simultaneously mitigates non-causal feature reliance in both modalities during test-time adaptation. On the visual side, it employs structured attention-guided erasure to suppress background activations and enforce orthogonal prediction consistency between causal and spurious regions. On the textual side, it introduces balanced prompt normalization, a learnable re-centering mechanism that aligns class embeddings toward an isotropic semantic space. Together, these modules jointly minimize conditional mutual information between spurious cues and predictions, steering the model toward causal, domain invariant reasoning without retraining or domain supervision. Extensive evaluations on real-world and synthetic bias benchmarks demonstrate consistent improvements in both average and worst-group accuracies over prior test-time debiasing methods, establishing a lightweight yet effective path toward trustworthy and causally grounded vision-language adaptation.", "AI": {"tldr": "BiPrompt: A bilateral prompt optimization framework that simultaneously debiases both visual and textual modalities in vision-language models during test-time adaptation without retraining.", "motivation": "Vision-language foundation models like CLIP show impressive zero-shot generalization but remain vulnerable to spurious correlations across modalities. Existing debiasing approaches often address only one modality (visual OR textual), leading to partial robustness and unstable adaptation under distribution shifts.", "method": "BiPrompt simultaneously mitigates non-causal feature reliance in both modalities. On visual side: structured attention-guided erasure to suppress background activations + orthogonal prediction consistency between causal and spurious regions. On textual side: balanced prompt normalization - learnable re-centering mechanism that aligns class embeddings toward isotropic semantic space.", "result": "Extensive evaluations on real-world and synthetic bias benchmarks show consistent improvements in both average and worst-group accuracies over prior test-time debiasing methods.", "conclusion": "BiPrompt establishes a lightweight yet effective path toward trustworthy and causally grounded vision-language adaptation by jointly minimizing conditional mutual information between spurious cues and predictions without retraining or domain supervision."}}
{"id": "2601.02177", "categories": ["cs.CV", "cs.CR"], "pdf": "https://arxiv.org/pdf/2601.02177", "abs": "https://arxiv.org/abs/2601.02177", "authors": ["Oliver Custance", "Saad Khan", "Simon Parkinson"], "title": "Why Commodity WiFi Sensors Fail at Multi-Person Gait Identification: A Systematic Analysis Using ESP32", "comment": null, "summary": "WiFi Channel State Information (CSI) has shown promise for single-person gait identification, with numerous studies reporting high accuracy. However, multi-person identification remains largely unexplored, with the limited existing work relying on complex, expensive setups requiring modified firmware. A critical question remains unanswered: is poor multi-person performance an algorithmic limitation or a fundamental hardware constraint? We systematically evaluate six diverse signal separation methods (FastICA, SOBI, PCA, NMF, Wavelet, Tensor Decomposition) across seven scenarios with 1-10 people using commodity ESP32 WiFi sensors--a simple, low-cost, off-the-shelf solution. Through novel diagnostic metrics (intra-subject variability, inter-subject distinguishability, performance degradation rate), we reveal that all methods achieve similarly low accuracy (45-56\\%, $\u03c3$=3.74\\%) with statistically insignificant differences (p $>$ 0.05). Even the best-performing method, NMF, achieves only 56\\% accuracy. Our analysis reveals high intra-subject variability, low inter-subject distinguishability, and severe performance degradation as person count increases, indicating that commodity ESP32 sensors cannot provide sufficient signal quality for reliable multi-person separation.", "AI": {"tldr": "Commodity ESP32 WiFi sensors cannot reliably separate multiple people using CSI data due to fundamental hardware limitations, not algorithmic issues.", "motivation": "While WiFi CSI works well for single-person gait identification, multi-person identification remains unexplored with commodity hardware. The key question is whether poor multi-person performance is due to algorithmic limitations or fundamental hardware constraints.", "method": "Systematically evaluated six signal separation methods (FastICA, SOBI, PCA, NMF, Wavelet, Tensor Decomposition) across seven scenarios with 1-10 people using commodity ESP32 WiFi sensors. Used novel diagnostic metrics: intra-subject variability, inter-subject distinguishability, and performance degradation rate.", "result": "All methods achieved similarly low accuracy (45-56%, \u03c3=3.74%) with statistically insignificant differences (p > 0.05). Best-performing method (NMF) achieved only 56% accuracy. Analysis revealed high intra-subject variability, low inter-subject distinguishability, and severe performance degradation as person count increased.", "conclusion": "Commodity ESP32 sensors cannot provide sufficient signal quality for reliable multi-person separation. The poor performance is a fundamental hardware limitation, not an algorithmic problem."}}
{"id": "2601.02189", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.02189", "abs": "https://arxiv.org/abs/2601.02189", "authors": ["Cheng Ying Wu", "Yen Jui Chang"], "title": "QuIC: A Quantum-Inspired Interaction Classifier for Revitalizing Shallow CNNs in Fine-Grained Recognition", "comment": null, "summary": "Deploying deep learning models for Fine-Grained Visual Classification (FGVC) on resource-constrained edge devices remains a significant challenge. While deep architectures achieve high accuracy on benchmarks like CUB-200-2011, their computational cost is often prohibitive. Conversely, shallow networks (e.g., AlexNet, VGG) offer efficiency but fail to distinguish visually similar sub-categories. This is because standard Global Average Pooling (GAP) heads capture only first-order statistics, missing the subtle high-order feature interactions required for FGVC. While Bilinear CNNs address this, they suffer from high feature dimensionality and instability during training. To bridge this gap, we propose the Quantum-inspired Interaction Classifier (QuIC). Drawing inspiration from quantum mechanics, QuIC models feature channels as interacting quantum states and captures second-order feature covariance via a learnable observable operator. Designed as a lightweight, plug-and-play module, QuIC supports stable, single-stage end-to-end training without exploding feature dimensions. Experimental results demonstrate that QuIC significantly revitalizes shallow backbones: it boosts the Top-1 accuracy of VGG16 by nearly 20% and outperforms state-of-the-art attention mechanisms (SE-Block) on ResNet18. Qualitative analysis, including t-SNE visualization, further confirms that QuIC resolves ambiguous cases by explicitly attending to fine-grained discriminative features and enforcing compact intra-class clustering.", "AI": {"tldr": "QuIC is a quantum-inspired lightweight module that boosts shallow networks for fine-grained visual classification by capturing second-order feature interactions without exploding dimensionality.", "motivation": "Deep learning models for fine-grained visual classification are too computationally expensive for edge devices, while shallow networks lack the ability to capture subtle feature interactions needed to distinguish visually similar sub-categories.", "method": "Proposes Quantum-inspired Interaction Classifier (QuIC) that models feature channels as interacting quantum states and captures second-order feature covariance via a learnable observable operator. It's a lightweight plug-and-play module supporting stable single-stage end-to-end training.", "result": "QuIC boosts VGG16's Top-1 accuracy by nearly 20% and outperforms state-of-the-art attention mechanisms (SE-Block) on ResNet18. Qualitative analysis shows it resolves ambiguous cases by attending to fine-grained discriminative features and enforcing compact intra-class clustering.", "conclusion": "QuIC successfully bridges the gap between accuracy and efficiency for fine-grained visual classification on resource-constrained devices by capturing high-order feature interactions without the computational overhead of deep architectures or bilinear CNNs."}}
{"id": "2601.02198", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.02198", "abs": "https://arxiv.org/abs/2601.02198", "authors": ["Alexander M\u00f6llers", "Julius Hense", "Florian Schulz", "Timo Milbich", "Maximilian Alber", "Lukas Ruff"], "title": "Mind the Gap: Continuous Magnification Sampling for Pathology Foundation Models", "comment": null, "summary": "In histopathology, pathologists examine both tissue architecture at low magnification and fine-grained morphology at high magnification. Yet, the performance of pathology foundation models across magnifications and the effect of magnification sampling during training remain poorly understood. We model magnification sampling as a multi-source domain adaptation problem and develop a simple theoretical framework that reveals systematic trade-offs between sampling strategies. We show that the widely used discrete uniform sampling of magnifications (0.25, 0.5, 1.0, 2.0 mpp) leads to degradation at intermediate magnifications. We introduce continuous magnification sampling, which removes gaps in magnification coverage while preserving performance at standard scales. Further, we derive sampling distributions that optimize representation quality across magnification scales. To evaluate these strategies, we introduce two new benchmarks (TCGA-MS, BRACS-MS) with appropriate metrics. Our experiments show that continuous sampling substantially improves over discrete sampling at intermediate magnifications, with gains of up to 4 percentage points in balanced classification accuracy, and that optimized distributions can further improve performance. Finally, we evaluate current histopathology foundation models, finding that magnification is a primary driver of performance variation across models. Our work paves the way towards future pathology foundation models that perform reliably across magnifications.", "AI": {"tldr": "This paper analyzes magnification sampling strategies for pathology foundation models, showing that continuous sampling outperforms discrete uniform sampling, especially at intermediate magnifications, with up to 4% accuracy gains.", "motivation": "Pathologists examine tissue at multiple magnifications, but current pathology foundation models have poorly understood performance across magnifications and the effects of magnification sampling during training.", "method": "Model magnification sampling as multi-source domain adaptation problem; develop theoretical framework; introduce continuous magnification sampling; derive optimized sampling distributions; create TCGA-MS and BRACS-MS benchmarks for evaluation.", "result": "Continuous sampling substantially improves over discrete sampling at intermediate magnifications (up to 4% balanced accuracy gains); optimized distributions further improve performance; magnification is primary driver of performance variation across models.", "conclusion": "Continuous magnification sampling and optimized distributions improve pathology foundation model performance across magnifications, paving the way for more reliable models that better mimic pathologists' multi-scale examination."}}
{"id": "2601.02203", "categories": ["cs.CV", "cs.CR"], "pdf": "https://arxiv.org/pdf/2601.02203", "abs": "https://arxiv.org/abs/2601.02203", "authors": ["Oliver Custance", "Saad Khan", "Simon Parkinson", "Quan Z. Sheng"], "title": "Parameter-Efficient Domain Adaption for CSI Crowd-Counting via Self-Supervised Learning with Adapter Modules", "comment": null, "summary": "Device-free crowd-counting using WiFi Channel State Information (CSI) is a key enabling technology for a new generation of privacy-preserving Internet of Things (IoT) applications. However, practical deployment is severely hampered by the domain shift problem, where models trained in one environment fail to generalise to another. To overcome this, we propose a novel two-stage framework centred on a CSI-ResNet-A architecture. This model is pre-trained via self-supervised contrastive learning to learn domain-invariant representations and leverages lightweight Adapter modules for highly efficient fine-tuning. The resulting event sequence is then processed by a stateful counting machine to produce a final, stable occupancy estimate. We validate our framework extensively. On our WiFlow dataset, our unsupervised approach excels in a 10-shot learning scenario, achieving a final Mean Absolute Error (MAE) of just 0.44--a task where supervised baselines fail. To formally quantify robustness, we introduce the Generalisation Index (GI), on which our model scores near-perfectly, confirming its ability to generalise. Furthermore, our framework sets a new state-of-the-art public WiAR benchmark with 98.8\\% accuracy. Our ablation studies reveal the core strength of our design: adapter-based fine-tuning achieves performance within 1\\% of a full fine-tune (98.84\\% vs. 99.67\\%) while training 97.2\\% fewer parameters. Our work provides a practical and scalable solution for developing robust sensing systems ready for real-world IoT deployments.", "AI": {"tldr": "A novel two-stage framework for WiFi-based crowd counting that addresses domain shift through self-supervised contrastive learning and adapter-based fine-tuning, achieving state-of-the-art performance with minimal parameter updates.", "motivation": "Device-free crowd counting using WiFi CSI is promising for privacy-preserving IoT applications, but practical deployment is hindered by the domain shift problem where models trained in one environment fail to generalize to others.", "method": "A two-stage framework with CSI-ResNet-A architecture: 1) Pre-training via self-supervised contrastive learning to learn domain-invariant representations, 2) Lightweight Adapter modules for efficient fine-tuning, followed by stateful counting machine processing for stable occupancy estimates.", "result": "Achieves MAE of 0.44 in 10-shot learning on WiFlow dataset (where supervised baselines fail), near-perfect Generalisation Index, 98.8% accuracy on WiAR benchmark, and adapter fine-tuning achieves 98.84% vs 99.67% full fine-tune while training 97.2% fewer parameters.", "conclusion": "The framework provides a practical and scalable solution for robust WiFi sensing systems ready for real-world IoT deployments, effectively addressing domain shift through domain-invariant representations and efficient adapter-based adaptation."}}
{"id": "2601.02204", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.02204", "abs": "https://arxiv.org/abs/2601.02204", "authors": ["Huichao Zhang", "Liao Qu", "Yiheng Liu", "Hang Chen", "Yangyang Song", "Yongsheng Dong", "Shikun Sun", "Xian Li", "Xu Wang", "Yi Jiang", "Hu Ye", "Bo Chen", "Yiming Gao", "Peng Liu", "Akide Liu", "Zhipeng Yang", "Qili Deng", "Linjie Xing", "Jiyang Liu", "Zhao Wang", "Yang Zhou", "Mingcong Liu", "Yi Zhang", "Qian He", "Xiwei Hu", "Zhongqi Qi", "Jie Shao", "Zhiye Fu", "Shuai Wang", "Fangmin Chen", "Xuezhi Chai", "Zhihua Wu", "Yitong Wang", "Zehuan Yuan", "Daniel K. Du", "Xinglong Wu"], "title": "NextFlow: Unified Sequential Modeling Activates Multimodal Understanding and Generation", "comment": "Project page: https://github.com/ByteVisionLab/NextFlow", "summary": "We present NextFlow, a unified decoder-only autoregressive transformer trained on 6 trillion interleaved text-image discrete tokens. By leveraging a unified vision representation within a unified autoregressive architecture, NextFlow natively activates multimodal understanding and generation capabilities, unlocking abilities of image editing, interleaved content and video generation. Motivated by the distinct nature of modalities - where text is strictly sequential and images are inherently hierarchical - we retain next-token prediction for text but adopt next-scale prediction for visual generation. This departs from traditional raster-scan methods, enabling the generation of 1024x1024 images in just 5 seconds - orders of magnitude faster than comparable AR models. We address the instabilities of multi-scale generation through a robust training recipe. Furthermore, we introduce a prefix-tuning strategy for reinforcement learning. Experiments demonstrate that NextFlow achieves state-of-the-art performance among unified models and rivals specialized diffusion baselines in visual quality.", "AI": {"tldr": "NextFlow is a unified decoder-only transformer trained on 6T text-image tokens that achieves fast multimodal understanding/generation using next-scale prediction for images instead of traditional raster-scan methods.", "motivation": "Motivated by the distinct nature of modalities - text is strictly sequential while images are inherently hierarchical. Traditional raster-scan methods for image generation in autoregressive models are inefficient, so a new approach is needed to handle multimodal content efficiently.", "method": "Uses a unified decoder-only autoregressive transformer with 6T text-image tokens. Retains next-token prediction for text but adopts next-scale prediction for visual generation (departing from raster-scan). Includes robust training recipe for multi-scale generation stability and introduces prefix-tuning strategy for reinforcement learning.", "result": "Generates 1024x1024 images in just 5 seconds - orders of magnitude faster than comparable AR models. Achieves state-of-the-art performance among unified models and rivals specialized diffusion baselines in visual quality.", "conclusion": "NextFlow demonstrates that a unified autoregressive architecture with modality-appropriate prediction strategies (next-token for text, next-scale for images) can efficiently unlock multimodal understanding and generation capabilities while maintaining high quality."}}
{"id": "2601.02206", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.02206", "abs": "https://arxiv.org/abs/2601.02206", "authors": ["Dachun Kai", "Zeyu Xiao", "Huyue Zhu", "Jiaxiao Wang", "Yueyi Zhang", "Xiaoyan Sun"], "title": "Seeing the Unseen: Zooming in the Dark with Event Cameras", "comment": "Accepted to AAAI 2026", "summary": "This paper addresses low-light video super-resolution (LVSR), aiming to restore high-resolution videos from low-light, low-resolution (LR) inputs. Existing LVSR methods often struggle to recover fine details due to limited contrast and insufficient high-frequency information. To overcome these challenges, we present RetinexEVSR, the first event-driven LVSR framework that leverages high-contrast event signals and Retinex-inspired priors to enhance video quality under low-light scenarios. Unlike previous approaches that directly fuse degraded signals, RetinexEVSR introduces a novel bidirectional cross-modal fusion strategy to extract and integrate meaningful cues from noisy event data and degraded RGB frames. Specifically, an illumination-guided event enhancement module is designed to progressively refine event features using illumination maps derived from the Retinex model, thereby suppressing low-light artifacts while preserving high-contrast details. Furthermore, we propose an event-guided reflectance enhancement module that utilizes the enhanced event features to dynamically recover reflectance details via a multi-scale fusion mechanism. Experimental results show that our RetinexEVSR achieves state-of-the-art performance on three datasets. Notably, on the SDSD benchmark, our method can get up to 2.95 dB gain while reducing runtime by 65% compared to prior event-based methods. Code: https://github.com/DachunKai/RetinexEVSR.", "AI": {"tldr": "RetinexEVSR: First event-driven low-light video super-resolution framework using Retinex priors and bidirectional cross-modal fusion between events and RGB frames.", "motivation": "Existing LVSR methods struggle to recover fine details due to limited contrast and insufficient high-frequency information in low-light, low-resolution videos.", "method": "Introduces bidirectional cross-modal fusion strategy: illumination-guided event enhancement module refines event features using Retinex-derived illumination maps, and event-guided reflectance enhancement module recovers reflectance details via multi-scale fusion.", "result": "Achieves state-of-the-art performance on three datasets, with up to 2.95 dB gain on SDSD benchmark while reducing runtime by 65% compared to prior event-based methods.", "conclusion": "RetinexEVSR effectively leverages high-contrast event signals and Retinex priors to enhance video quality in low-light scenarios through novel bidirectional fusion strategy."}}
{"id": "2601.02211", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.02211", "abs": "https://arxiv.org/abs/2601.02211", "authors": ["Binglei Li", "Mengping Yang", "Zhiyu Tan", "Junping Zhang", "Hao Li"], "title": "Unraveling MMDiT Blocks: Training-free Analysis and Enhancement of Text-conditioned Diffusion", "comment": "11 pages", "summary": "Recent breakthroughs of transformer-based diffusion models, particularly with Multimodal Diffusion Transformers (MMDiT) driven models like FLUX and Qwen Image, have facilitated thrilling experiences in text-to-image generation and editing. To understand the internal mechanism of MMDiT-based models, existing methods tried to analyze the effect of specific components like positional encoding and attention layers. Yet, a comprehensive understanding of how different blocks and their interactions with textual conditions contribute to the synthesis process remains elusive. In this paper, we first develop a systematic pipeline to comprehensively investigate each block's functionality by removing, disabling and enhancing textual hidden-states at corresponding blocks. Our analysis reveals that 1) semantic information appears in earlier blocks and finer details are rendered in later blocks, 2) removing specific blocks is usually less disruptive than disabling text conditions, and 3) enhancing textual conditions in selective blocks improves semantic attributes. Building on these observations, we further propose novel training-free strategies for improved text alignment, precise editing, and acceleration. Extensive experiments demonstrated that our method outperforms various baselines and remains flexible across text-to-image generation, image editing, and inference acceleration. Our method improves T2I-Combench++ from 56.92% to 63.00% and GenEval from 66.42% to 71.63% on SD3.5, without sacrificing synthesis quality. These results advance understanding of MMDiT models and provide valuable insights to unlock new possibilities for further improvements.", "AI": {"tldr": "The paper develops a systematic analysis pipeline to understand how different blocks in MMDiT-based diffusion models contribute to image synthesis, revealing insights about semantic information flow and text-condition interactions, then proposes training-free strategies for improved text alignment, editing, and acceleration.", "motivation": "Existing methods only analyze specific components like positional encoding and attention layers in MMDiT-based models, but lack comprehensive understanding of how different blocks and their interactions with textual conditions contribute to the synthesis process.", "method": "Developed a systematic pipeline to investigate each block's functionality by removing, disabling, and enhancing textual hidden-states at corresponding blocks. Built on these observations to propose novel training-free strategies for improved text alignment, precise editing, and acceleration.", "result": "Analysis revealed: 1) semantic information appears in earlier blocks and finer details in later blocks, 2) removing specific blocks is less disruptive than disabling text conditions, 3) enhancing textual conditions in selective blocks improves semantic attributes. The method improved T2I-Combench++ from 56.92% to 63.00% and GenEval from 66.42% to 71.63% on SD3.5 without sacrificing quality.", "conclusion": "The systematic analysis advances understanding of MMDiT models and provides valuable insights for further improvements, demonstrating effective training-free strategies that outperform various baselines across text-to-image generation, editing, and acceleration tasks."}}
{"id": "2601.02212", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.02212", "abs": "https://arxiv.org/abs/2601.02212", "authors": ["Jingjing Wang", "Zhuo Xiao", "Xinning Yao", "Bo Liu", "Lijuan Niu", "Xiangzhi Bai", "Fugen Zhou"], "title": "Prior-Guided DETR for Ultrasound Nodule Detection", "comment": null, "summary": "Accurate detection of ultrasound nodules is essential for the early diagnosis and treatment of thyroid and breast cancers. However, this task remains challenging due to irregular nodule shapes, indistinct boundaries, substantial scale variations, and the presence of speckle noise that degrades structural visibility. To address these challenges, we propose a prior-guided DETR framework specifically designed for ultrasound nodule detection. Instead of relying on purely data-driven feature learning, the proposed framework progressively incorporates different prior knowledge at multiple stages of the network. First, a Spatially-adaptive Deformable FFN with Prior Regularization (SDFPR) is embedded into the CNN backbone to inject geometric priors into deformable sampling, stabilizing feature extraction for irregular and blurred nodules. Second, a Multi-scale Spatial-Frequency Feature Mixer (MSFFM) is designed to extract multi-scale structural priors, where spatial-domain processing emphasizes contour continuity and boundary cues, while frequency-domain modeling captures global morphology and suppresses speckle noise. Furthermore, a Dense Feature Interaction (DFI) mechanism propagates and exploits these prior-modulated features across all encoder layers, enabling the decoder to enhance query refinement under consistent geometric and structural guidance. Experiments conducted on two clinically collected thyroid ultrasound datasets (Thyroid I and Thyroid II) and two public benchmarks (TN3K and BUSI) for thyroid and breast nodules demonstrate that the proposed method achieves superior accuracy compared with 18 detection methods, particularly in detecting morphologically complex nodules.The source code is publicly available at https://github.com/wjj1wjj/Ultrasound-DETR.", "AI": {"tldr": "A prior-guided DETR framework for ultrasound nodule detection that incorporates geometric and structural priors to address challenges like irregular shapes, indistinct boundaries, and speckle noise.", "motivation": "Ultrasound nodule detection is challenging due to irregular shapes, indistinct boundaries, scale variations, and speckle noise that degrades structural visibility, making accurate detection essential for early cancer diagnosis.", "method": "Proposes a prior-guided DETR framework with three key components: 1) Spatially-adaptive Deformable FFN with Prior Regularization (SDFPR) for geometric priors, 2) Multi-scale Spatial-Frequency Feature Mixer (MSFFM) for structural priors, and 3) Dense Feature Interaction (DFI) mechanism for feature propagation.", "result": "Superior accuracy compared with 18 detection methods on two thyroid ultrasound datasets (Thyroid I and II) and two public benchmarks (TN3K and BUSI), particularly effective for morphologically complex nodules.", "conclusion": "The proposed prior-guided DETR framework effectively addresses ultrasound nodule detection challenges by incorporating geometric and structural priors, demonstrating state-of-the-art performance and potential for clinical applications."}}
{"id": "2601.02228", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.02228", "abs": "https://arxiv.org/abs/2601.02228", "authors": ["Duoxun Tang", "Xueyi Zhang", "Chak Hin Wang", "Xi Xiao", "Dasen Dai", "Xinhang Jiang", "Wentao Shi", "Rui Li", "Qing Li"], "title": "FMVP: Masked Flow Matching for Adversarial Video Purification", "comment": null, "summary": "Video recognition models remain vulnerable to adversarial attacks, while existing diffusion-based purification methods suffer from inefficient sampling and curved trajectories. Directly regressing clean videos from adversarial inputs often fails to recover faithful content due to the subtle nature of perturbations; this necessitates physically shattering the adversarial structure. Therefore, we propose Flow Matching for Adversarial Video Purification FMVP. FMVP physically shatters global adversarial structures via a masking strategy and reconstructs clean video dynamics using Conditional Flow Matching (CFM) with an inpainting objective. To further decouple semantic content from adversarial noise, we design a Frequency-Gated Loss (FGL) that explicitly suppresses high-frequency adversarial residuals while preserving low-frequency fidelity. We design Attack-Aware and Generalist training paradigms to handle known and unknown threats, respectively. Extensive experiments on UCF-101 and HMDB-51 demonstrate that FMVP outperforms state-of-the-art methods (DiffPure, Defense Patterns (DP), Temporal Shuffling (TS) and FlowPure), achieving robust accuracy exceeding 87% against PGD and 89% against CW attacks. Furthermore, FMVP demonstrates superior robustness against adaptive attacks (DiffHammer) and functions as a zero-shot adversarial detector, attaining detection accuracies of 98% for PGD and 79% for highly imperceptible CW attacks.", "AI": {"tldr": "FMVP uses flow matching with masking and frequency-gated loss to purify adversarial videos, achieving state-of-the-art robustness and zero-shot detection capabilities.", "motivation": "Video recognition models are vulnerable to adversarial attacks, and existing diffusion-based purification methods suffer from inefficient sampling and curved trajectories. Direct regression fails to recover faithful content due to subtle perturbations, requiring physical destruction of adversarial structures.", "method": "FMVP physically shatters global adversarial structures via masking strategy and reconstructs clean video dynamics using Conditional Flow Matching (CFM) with inpainting objective. Frequency-Gated Loss (FGL) suppresses high-frequency adversarial residuals while preserving low-frequency fidelity. Two training paradigms: Attack-Aware (for known threats) and Generalist (for unknown threats).", "result": "Outperforms state-of-the-art methods (DiffPure, DP, TS, FlowPure) on UCF-101 and HMDB-51, achieving robust accuracy >87% against PGD and >89% against CW attacks. Shows superior robustness against adaptive attacks (DiffHammer) and functions as zero-shot adversarial detector with 98% detection accuracy for PGD and 79% for CW attacks.", "conclusion": "FMVP effectively purifies adversarial videos by physically shattering adversarial structures and decoupling semantic content from noise, achieving strong robustness and detection capabilities across various attack types."}}
{"id": "2601.02242", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.02242", "abs": "https://arxiv.org/abs/2601.02242", "authors": ["Grigorii Alekseenko", "Aleksandr Gordeev", "Irina Tolstykh", "Bulat Suleimanov", "Vladimir Dokholyan", "Georgii Fedorov", "Sergey Yakubson", "Aleksandra Tsybina", "Mikhail Chernyshov", "Maksim Kuprashevich"], "title": "VIBE: Visual Instruction Based Editor", "comment": null, "summary": "Instruction-based image editing is among the fastest developing areas in generative AI. Over the past year, the field has reached a new level, with dozens of open-source models released alongside highly capable commercial systems. However, only a limited number of open-source approaches currently achieve real-world quality. In addition, diffusion backbones, the dominant choice for these pipelines, are often large and computationally expensive for many deployments and research settings, with widely used variants typically containing 6B to 20B parameters. This paper presents a compact, high-throughput instruction-based image editing pipeline that uses a modern 2B-parameter Qwen3-VL model to guide the editing process and the 1.6B-parameter diffusion model Sana1.5 for image generation. Our design decisions across architecture, data processing, training configuration, and evaluation target low-cost inference and strict source consistency while maintaining high quality across the major edit categories feasible at this scale. Evaluated on the ImgEdit and GEdit benchmarks, the proposed method matches or exceeds the performance of substantially heavier baselines, including models with several times as many parameters and higher inference cost, and is particularly strong on edits that require preserving the input image, such as an attribute adjustment, object removal, background edits, and targeted replacement. The model fits within 24 GB of GPU memory and generates edited images at up to 2K resolution in approximately 4 seconds on an NVIDIA H100 in BF16, without additional inference optimizations or distillation.", "AI": {"tldr": "A compact, high-throughput instruction-based image editing pipeline using 2B-parameter Qwen3-VL for guidance and 1.6B-parameter Sana1.5 for generation, achieving competitive performance with much larger models while being computationally efficient.", "motivation": "Current instruction-based image editing models are either too large (6B-20B parameters) for practical deployment or lack real-world quality. There's a need for compact, efficient models that maintain high quality while being accessible for research and deployment.", "method": "Uses Qwen3-VL (2B parameters) as guidance model and Sana1.5 (1.6B parameters) as diffusion backbone. Focuses on architectural decisions, data processing, and training configuration optimized for low-cost inference and strict source consistency across major edit categories.", "result": "Matches or exceeds performance of substantially heavier baselines on ImgEdit and GEdit benchmarks. Particularly strong on edits requiring input preservation (attribute adjustment, object removal, background edits, targeted replacement). Fits within 24GB GPU memory, generates 2K images in ~4 seconds on H100 without optimization.", "conclusion": "Demonstrates that compact models (under 4B total parameters) can achieve competitive instruction-based image editing quality while being computationally efficient and practical for real-world deployment and research settings."}}
{"id": "2601.02246", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.02246", "abs": "https://arxiv.org/abs/2601.02246", "authors": ["Annoor Sharara Akhand"], "title": "A Comparative Study of Custom CNNs, Pre-trained Models, and Transfer Learning Across Multiple Visual Datasets", "comment": null, "summary": "Convolutional Neural Networks (CNNs) are a standard approach for visual recognition due to their capacity to learn hierarchical representations from raw pixels. In practice, practitioners often choose among (i) training a compact custom CNN from scratch, (ii) using a large pre-trained CNN as a fixed feature extractor, and (iii) performing transfer learning via partial or full fine-tuning of a pre-trained backbone. This report presents a controlled comparison of these three paradigms across five real-world image classification datasets spanning road-surface defect recognition, agricultural variety identification, fruit/leaf disease recognition, pedestrian walkway encroachment recognition, and unauthorized vehicle recognition. Models are evaluated using accuracy and macro F1-score, complemented by efficiency metrics including training time per epoch and parameter counts. The results show that transfer learning consistently yields the strongest predictive performance, while the custom CNN provides an attractive efficiency--accuracy trade-off, especially when compute and memory budgets are constrained.", "AI": {"tldr": "Comparison of three CNN approaches (custom training, pre-trained feature extraction, transfer learning) across five real-world image classification tasks shows transfer learning performs best, while custom CNNs offer good efficiency-accuracy trade-off for constrained resources.", "motivation": "To provide a controlled comparison of three common CNN paradigms used in practice for visual recognition tasks, helping practitioners make informed decisions based on performance and efficiency trade-offs.", "method": "Systematic evaluation of three approaches: (1) training compact custom CNN from scratch, (2) using large pre-trained CNN as fixed feature extractor, and (3) transfer learning via partial/full fine-tuning of pre-trained backbone. Tested across five diverse real-world image classification datasets covering road defects, agriculture, disease recognition, pedestrian encroachment, and vehicle recognition.", "result": "Transfer learning consistently yields the strongest predictive performance (accuracy and macro F1-score). Custom CNN provides attractive efficiency-accuracy trade-off, especially when compute and memory budgets are constrained. Efficiency metrics (training time per epoch, parameter counts) complement performance evaluation.", "conclusion": "Transfer learning is recommended for best performance, while custom CNNs offer practical advantages for resource-constrained scenarios. The study provides evidence-based guidance for practitioners choosing among common CNN implementation strategies."}}
{"id": "2601.02249", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.02249", "abs": "https://arxiv.org/abs/2601.02249", "authors": ["Xiantai Xiang", "Guangyao Zhou", "Zixiao Wen", "Wenshuai Li", "Ben Niu", "Feng Wang", "Lijia Huang", "Qiantong Wang", "Yuhan Liu", "Zongxu Pan", "Yuxin Hu"], "title": "SLGNet: Synergizing Structural Priors and Language-Guided Modulation for Multimodal Object Detection", "comment": null, "summary": "Multimodal object detection leveraging RGB and Infrared (IR) images is pivotal for robust perception in all-weather scenarios. While recent adapter-based approaches efficiently transfer RGB-pretrained foundation models to this task, they often prioritize model efficiency at the expense of cross-modal structural consistency. Consequently, critical structural cues are frequently lost when significant domain gaps arise, such as in high-contrast or nighttime environments. Moreover, conventional static multimodal fusion mechanisms typically lack environmental awareness, resulting in suboptimal adaptation and constrained detection performance under complex, dynamic scene variations. To address these limitations, we propose SLGNet, a parameter-efficient framework that synergizes hierarchical structural priors and language-guided modulation within a frozen Vision Transformer (ViT)-based foundation model. Specifically, we design a Structure-Aware Adapter to extract hierarchical structural representations from both modalities and dynamically inject them into the ViT to compensate for structural degradation inherent in ViT-based backbones. Furthermore, we propose a Language-Guided Modulation module that exploits VLM-driven structured captions to dynamically recalibrate visual features, thereby endowing the model with robust environmental awareness. Extensive experiments on the LLVIP, FLIR, KAIST, and DroneVehicle datasets demonstrate that SLGNet establishes new state-of-the-art performance. Notably, on the LLVIP benchmark, our method achieves an mAP of 66.1, while reducing trainable parameters by approximately 87% compared to traditional full fine-tuning. This confirms SLGNet as a robust and efficient solution for multimodal perception.", "AI": {"tldr": "SLGNet is a parameter-efficient multimodal object detection framework that combines hierarchical structural priors and language-guided modulation within frozen ViT backbones to improve cross-modal consistency and environmental awareness.", "motivation": "Existing adapter-based approaches for RGB-IR object detection prioritize efficiency over cross-modal structural consistency, losing critical structural cues in challenging conditions (high-contrast/nighttime). Static fusion mechanisms lack environmental awareness, limiting adaptation to dynamic scene variations.", "method": "Proposes SLGNet with two key components: 1) Structure-Aware Adapter extracts hierarchical structural representations from both RGB and IR modalities and dynamically injects them into frozen ViT to compensate for structural degradation; 2) Language-Guided Modulation module uses VLM-driven structured captions to dynamically recalibrate visual features for environmental awareness.", "result": "Achieves state-of-the-art performance on LLVIP, FLIR, KAIST, and DroneVehicle datasets. On LLVIP benchmark: mAP of 66.1 while reducing trainable parameters by ~87% compared to traditional full fine-tuning.", "conclusion": "SLGNet provides a robust and efficient solution for multimodal perception by synergizing structural priors and language guidance within frozen foundation models, addressing structural degradation and environmental awareness limitations in existing approaches."}}
{"id": "2601.02256", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.02256", "abs": "https://arxiv.org/abs/2601.02256", "authors": ["Shikun Sun", "Liao Qu", "Huichao Zhang", "Yiheng Liu", "Yangyang Song", "Xian Li", "Xu Wang", "Yi Jiang", "Daniel K. Du", "Xinglong Wu", "Jia Jia"], "title": "VAR RL Done Right: Tackling Asynchronous Policy Conflicts in Visual Autoregressive Generation", "comment": "Project page: https://github.com/ByteVisionLab/NextFlow", "summary": "Visual generation is dominated by three paradigms: AutoRegressive (AR), diffusion, and Visual AutoRegressive (VAR) models. Unlike AR and diffusion, VARs operate on heterogeneous input structures across their generation steps, which creates severe asynchronous policy conflicts. This issue becomes particularly acute in reinforcement learning (RL) scenarios, leading to unstable training and suboptimal alignment. To resolve this, we propose a novel framework to enhance Group Relative Policy Optimization (GRPO) by explicitly managing these conflicts. Our method integrates three synergistic components: 1) a stabilizing intermediate reward to guide early-stage generation; 2) a dynamic time-step reweighting scheme for precise credit assignment; and 3) a novel mask propagation algorithm, derived from principles of Reward Feedback Learning (ReFL), designed to isolate optimization effects both spatially and temporally. Our approach demonstrates significant improvements in sample quality and objective alignment over the vanilla GRPO baseline, enabling robust and effective optimization for VAR models.", "AI": {"tldr": "Enhanced GRPO framework for VAR models addresses asynchronous policy conflicts through stabilizing rewards, dynamic time-step reweighting, and mask propagation for better visual generation alignment.", "motivation": "VAR models suffer from asynchronous policy conflicts due to heterogeneous input structures across generation steps, causing unstable training and suboptimal alignment in RL scenarios, which existing methods don't adequately address.", "method": "Three-component framework: 1) stabilizing intermediate reward for early-stage guidance, 2) dynamic time-step reweighting for precise credit assignment, and 3) mask propagation algorithm derived from ReFL principles to isolate optimization effects spatially and temporally.", "result": "Significant improvements in sample quality and objective alignment over vanilla GRPO baseline, enabling robust and effective optimization for VAR models.", "conclusion": "The proposed framework successfully resolves asynchronous policy conflicts in VAR models, providing stable training and better alignment through synergistic reward engineering and optimization isolation techniques."}}
{"id": "2601.02267", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.02267", "abs": "https://arxiv.org/abs/2601.02267", "authors": ["Renke Wang", "Zhenyu Zhang", "Ying Tai", "Jian Yang"], "title": "DiffProxy: Multi-View Human Mesh Recovery via Diffusion-Generated Dense Proxies", "comment": "Page: https://wrk226.github.io/DiffProxy.html, Code: https://github.com/wrk226/DiffProxy", "summary": "Human mesh recovery from multi-view images faces a fundamental challenge: real-world datasets contain imperfect ground-truth annotations that bias the models' training, while synthetic data with precise supervision suffers from domain gap. In this paper, we propose DiffProxy, a novel framework that generates multi-view consistent human proxies for mesh recovery. Central to DiffProxy is leveraging the diffusion-based generative priors to bridge the synthetic training and real-world generalization. Its key innovations include: (1) a multi-conditional mechanism for generating multi-view consistent, pixel-aligned human proxies; (2) a hand refinement module that incorporates flexible visual prompts to enhance local details; and (3) an uncertainty-aware test-time scaling method that increases robustness to challenging cases during optimization. These designs ensure that the mesh recovery process effectively benefits from the precise synthetic ground truth and generative advantages of the diffusion-based pipeline. Trained entirely on synthetic data, DiffProxy achieves state-of-the-art performance across five real-world benchmarks, demonstrating strong zero-shot generalization particularly on challenging scenarios with occlusions and partial views. Project page: https://wrk226.github.io/DiffProxy.html", "AI": {"tldr": "DiffProxy uses diffusion-based generative priors to create multi-view consistent human proxies for mesh recovery, bridging synthetic training and real-world generalization without real-world data.", "motivation": "Real-world datasets have imperfect ground-truth annotations that bias models, while synthetic data with precise supervision suffers from domain gap. Need to bridge synthetic training and real-world generalization.", "method": "1) Multi-conditional mechanism for generating multi-view consistent, pixel-aligned human proxies; 2) Hand refinement module with flexible visual prompts to enhance local details; 3) Uncertainty-aware test-time scaling for robustness during optimization.", "result": "Achieves state-of-the-art performance across five real-world benchmarks with strong zero-shot generalization, particularly on challenging scenarios with occlusions and partial views. Trained entirely on synthetic data.", "conclusion": "DiffProxy effectively bridges synthetic training and real-world generalization by leveraging diffusion-based generative priors, enabling precise mesh recovery from multi-view images without real-world training data."}}
{"id": "2601.02273", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.02273", "abs": "https://arxiv.org/abs/2601.02273", "authors": ["Salim Khazem"], "title": "TopoLoRA-SAM: Topology-Aware Parameter-Efficient Adaptation of Foundation Segmenters for Thin-Structure and Cross-Domain Binary Semantic Segmentation", "comment": null, "summary": "Foundation segmentation models such as the Segment Anything Model (SAM) exhibit strong zero-shot generalization through large-scale pretraining, but adapting them to domain-specific semantic segmentation remains challenging, particularly for thin structures (e.g., retinal vessels) and noisy modalities (e.g., SAR imagery). Full fine-tuning is computationally expensive and risks catastrophic forgetting. We propose \\textbf{TopoLoRA-SAM}, a topology-aware and parameter-efficient adaptation framework for binary semantic segmentation. TopoLoRA-SAM injects Low-Rank Adaptation (LoRA) into the frozen ViT encoder, augmented with a lightweight spatial convolutional adapter and optional topology-aware supervision via differentiable clDice. We evaluate our approach on five benchmarks spanning retinal vessel segmentation (DRIVE, STARE, CHASE\\_DB1), polyp segmentation (Kvasir-SEG), and SAR sea/land segmentation (SL-SSDD), comparing against U-Net, DeepLabV3+, SegFormer, and Mask2Former. TopoLoRA-SAM achieves the best retina-average Dice and the best overall average Dice across datasets, while training only \\textbf{5.2\\%} of model parameters ($\\sim$4.9M). On the challenging CHASE\\_DB1 dataset, our method substantially improves segmentation accuracy and robustness, demonstrating that topology-aware parameter-efficient adaptation can match or exceed fully fine-tuned specialist models. Code is available at : https://github.com/salimkhazem/Seglab.git", "AI": {"tldr": "TopoLoRA-SAM adapts SAM for domain-specific binary segmentation using parameter-efficient LoRA adapters with topology-aware supervision, achieving state-of-the-art results while training only 5.2% of parameters.", "motivation": "Foundation models like SAM have strong zero-shot capabilities but struggle with domain-specific segmentation tasks, especially for thin structures and noisy modalities. Full fine-tuning is computationally expensive and risks catastrophic forgetting.", "method": "Proposes TopoLoRA-SAM: injects Low-Rank Adaptation (LoRA) into frozen ViT encoder, adds lightweight spatial convolutional adapter, and optionally uses topology-aware supervision via differentiable clDice loss for binary semantic segmentation.", "result": "Achieves best retina-average Dice and best overall average Dice across five benchmarks (DRIVE, STARE, CHASE_DB1, Kvasir-SEG, SL-SSDD) while training only 5.2% of parameters (~4.9M). Substantially improves segmentation accuracy on challenging CHASE_DB1 dataset.", "conclusion": "Topology-aware parameter-efficient adaptation can match or exceed fully fine-tuned specialist models, demonstrating effective domain adaptation for SAM with minimal parameter updates while maintaining computational efficiency."}}
{"id": "2601.02281", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.02281", "abs": "https://arxiv.org/abs/2601.02281", "authors": ["Shuai Yuan", "Yantai Yang", "Xiaotian Yang", "Xupeng Zhang", "Zhonghao Zhao", "Lingming Zhang", "Zhipeng Zhang"], "title": "InfiniteVGGT: Visual Geometry Grounded Transformer for Endless Streams", "comment": null, "summary": "The grand vision of enabling persistent, large-scale 3D visual geometry understanding is shackled by the irreconcilable demands of scalability and long-term stability. While offline models like VGGT achieve inspiring geometry capability, their batch-based nature renders them irrelevant for live systems. Streaming architectures, though the intended solution for live operation, have proven inadequate. Existing methods either fail to support truly infinite-horizon inputs or suffer from catastrophic drift over long sequences. We shatter this long-standing dilemma with InfiniteVGGT, a causal visual geometry transformer that operationalizes the concept of a rolling memory through a bounded yet adaptive and perpetually expressive KV cache. Capitalizing on this, we devise a training-free, attention-agnostic pruning strategy that intelligently discards obsolete information, effectively ``rolling'' the memory forward with each new frame. Fully compatible with FlashAttention, InfiniteVGGT finally alleviates the compromise, enabling infinite-horizon streaming while outperforming existing streaming methods in long-term stability. The ultimate test for such a system is its performance over a truly infinite horizon, a capability that has been impossible to rigorously validate due to the lack of extremely long-term, continuous benchmarks. To address this critical gap, we introduce the Long3D benchmark, which, for the first time, enables a rigorous evaluation of continuous 3D geometry estimation on sequences about 10,000 frames. This provides the definitive evaluation platform for future research in long-term 3D geometry understanding. Code is available at: https://github.com/AutoLab-SAI-SJTU/InfiniteVGGT", "AI": {"tldr": "InfiniteVGGT is a causal visual geometry transformer with rolling memory KV cache that enables infinite-horizon streaming 3D geometry understanding without catastrophic drift, outperforming existing streaming methods.", "motivation": "The paper addresses the fundamental dilemma between scalability and long-term stability in 3D visual geometry understanding. Offline models like VGGT have good geometry capability but can't handle live streaming, while existing streaming methods either can't support infinite-horizon inputs or suffer from catastrophic drift over long sequences.", "method": "InfiniteVGGT introduces a causal visual geometry transformer with a bounded yet adaptive and perpetually expressive KV cache. It uses a training-free, attention-agnostic pruning strategy that intelligently discards obsolete information, effectively \"rolling\" the memory forward with each new frame. The method is fully compatible with FlashAttention.", "result": "InfiniteVGGT enables infinite-horizon streaming while outperforming existing streaming methods in long-term stability. The paper also introduces Long3D benchmark with sequences of about 10,000 frames for rigorous evaluation of continuous 3D geometry estimation.", "conclusion": "The paper presents a breakthrough solution to the scalability-stability dilemma in 3D geometry understanding, enabling truly infinite-horizon streaming without performance degradation, and provides a comprehensive benchmark for future research in long-term 3D geometry understanding."}}
{"id": "2601.02289", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.02289", "abs": "https://arxiv.org/abs/2601.02289", "authors": ["Tom Burgert", "Leonard Hackel", "Paolo Rota", "Beg\u00fcm Demir"], "title": "Rank-based Geographical Regularization: Revisiting Contrastive Self-Supervised Learning for Multispectral Remote Sensing Imagery", "comment": "accepted for publication at IEEE/CVF Winter Conference on Applications of Computer Vision", "summary": "Self-supervised learning (SSL) has become a powerful paradigm for learning from large, unlabeled datasets, particularly in computer vision (CV). However, applying SSL to multispectral remote sensing (RS) images presents unique challenges and opportunities due to the geographical and temporal variability of the data. In this paper, we introduce GeoRank, a novel regularization method for contrastive SSL that improves upon prior techniques by directly optimizing spherical distances to embed geographical relationships into the learned feature space. GeoRank outperforms or matches prior methods that integrate geographical metadata and consistently improves diverse contrastive SSL algorithms (e.g., BYOL, DINO). Beyond this, we present a systematic investigation of key adaptations of contrastive SSL for multispectral RS images, including the effectiveness of data augmentations, the impact of dataset cardinality and image size on performance, and the task dependency of temporal views. Code is available at https://github.com/tomburgert/georank.", "AI": {"tldr": "GeoRank is a novel regularization method for contrastive self-supervised learning that optimizes spherical distances to embed geographical relationships into feature representations for multispectral remote sensing images.", "motivation": "Self-supervised learning has been successful in computer vision but applying it to multispectral remote sensing images presents unique challenges due to geographical and temporal variability of the data. There's a need for methods that can effectively leverage geographical relationships in remote sensing data.", "method": "GeoRank introduces a regularization method for contrastive SSL that directly optimizes spherical distances to embed geographical relationships. The paper also systematically investigates key adaptations of contrastive SSL for multispectral RS images, including data augmentations, dataset cardinality, image size, and temporal view dependencies.", "result": "GeoRank outperforms or matches prior methods that integrate geographical metadata and consistently improves diverse contrastive SSL algorithms (BYOL, DINO). The systematic investigation provides insights into effective adaptations for multispectral RS images.", "conclusion": "GeoRank effectively addresses the unique challenges of applying SSL to multispectral remote sensing by incorporating geographical relationships through spherical distance optimization, offering improved performance and systematic insights for the field."}}
{"id": "2601.02299", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.02299", "abs": "https://arxiv.org/abs/2601.02299", "authors": ["Sara In\u00e1cio", "Hugo Proen\u00e7a", "Jo\u00e3o C. Neves"], "title": "SortWaste: A Densely Annotated Dataset for Object Detection in Industrial Waste Sorting", "comment": "9 pages", "summary": "The increasing production of waste, driven by population growth, has created challenges in managing and recycling materials effectively. Manual waste sorting is a common practice; however, it remains inefficient for handling large-scale waste streams and presents health risks for workers. On the other hand, existing automated sorting approaches still struggle with the high variability, clutter, and visual complexity of real-world waste streams. The lack of real-world datasets for waste sorting is a major reason automated systems for this problem are underdeveloped. Accordingly, we introduce SortWaste, a densely annotated object detection dataset collected from a Material Recovery Facility. Additionally, we contribute to standardizing waste detection in sorting lines by proposing ClutterScore, an objective metric that gauges the scene's hardness level using a set of proxies that affect visual complexity (e.g., object count, class and size entropy, and spatial overlap). In addition to these contributions, we provide an extensive benchmark of state-of-the-art object detection models, detailing their results with respect to the hardness level assessed by the proposed metric. Despite achieving promising results (mAP of 59.7% in the plastic-only detection task), performance significantly decreases in highly cluttered scenes. This highlights the need for novel and more challenging datasets on the topic.", "AI": {"tldr": "Introduces SortWaste dataset for waste detection and ClutterScore metric to measure scene complexity, showing current models struggle with cluttered waste scenes.", "motivation": "Manual waste sorting is inefficient and hazardous, while automated systems struggle with real-world waste variability and clutter due to lack of suitable datasets.", "method": "Created SortWaste dataset from Material Recovery Facility and proposed ClutterScore metric using proxies like object count, class/size entropy, and spatial overlap to measure scene complexity.", "result": "Benchmarked state-of-the-art object detection models achieving 59.7% mAP for plastic detection, but performance significantly drops in highly cluttered scenes.", "conclusion": "Current models are insufficient for complex waste sorting, highlighting need for more challenging datasets and improved algorithms for cluttered environments."}}
{"id": "2601.02309", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.02309", "abs": "https://arxiv.org/abs/2601.02309", "authors": ["Xiaopeng Guo", "Yinzhe Xu", "Huajian Huang", "Sai-Kit Yeung"], "title": "360DVO: Deep Visual Odometry for Monocular 360-Degree Camera", "comment": "12 pages. Received by RA-L", "summary": "Monocular omnidirectional visual odometry (OVO) systems leverage 360-degree cameras to overcome field-of-view limitations of perspective VO systems. However, existing methods, reliant on handcrafted features or photometric objectives, often lack robustness in challenging scenarios, such as aggressive motion and varying illumination. To address this, we present 360DVO, the first deep learning-based OVO framework. Our approach introduces a distortion-aware spherical feature extractor (DAS-Feat) that adaptively learns distortion-resistant features from 360-degree images. These sparse feature patches are then used to establish constraints for effective pose estimation within a novel omnidirectional differentiable bundle adjustment (ODBA) module. To facilitate evaluation in realistic settings, we also contribute a new real-world OVO benchmark. Extensive experiments on this benchmark and public synthetic datasets (TartanAir V2 and 360VO) demonstrate that 360DVO surpasses state-of-the-art baselines (including 360VO and OpenVSLAM), improving robustness by 50% and accuracy by 37.5%. Homepage: https://chris1004336379.github.io/360DVO-homepage", "AI": {"tldr": "360DVO is the first deep learning-based monocular omnidirectional visual odometry framework that uses a distortion-aware spherical feature extractor and omnidirectional differentiable bundle adjustment to achieve robust pose estimation from 360-degree images.", "motivation": "Existing omnidirectional visual odometry methods rely on handcrafted features or photometric objectives, which lack robustness in challenging scenarios like aggressive motion and varying illumination. There's a need for more robust deep learning-based approaches for 360-degree camera systems.", "method": "Proposes 360DVO with two key components: 1) DAS-Feat (distortion-aware spherical feature extractor) that adaptively learns distortion-resistant features from 360-degree images, and 2) ODBA (omnidirectional differentiable bundle adjustment) module that uses sparse feature patches to establish constraints for effective pose estimation.", "result": "Extensive experiments on a new real-world OVO benchmark and public synthetic datasets (TartanAir V2 and 360VO) show that 360DVO surpasses state-of-the-art baselines (including 360VO and OpenVSLAM), improving robustness by 50% and accuracy by 37.5%.", "conclusion": "360DVO represents the first successful deep learning-based omnidirectional visual odometry framework that addresses the limitations of traditional methods, demonstrating significant improvements in both robustness and accuracy for 360-degree camera systems."}}
{"id": "2601.02315", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.02315", "abs": "https://arxiv.org/abs/2601.02315", "authors": ["Saurabh Kaushik", "Lalit Maurya", "Beth Tellman"], "title": "Prithvi-Complimentary Adaptive Fusion Encoder (CAFE): unlocking full-potential for flood inundation mapping", "comment": "Accepted at CV4EO Workshop @ WACV 2026", "summary": "Geo-Foundation Models (GFMs), have proven effective in diverse downstream applications, including semantic segmentation, classification, and regression tasks. However, in case of flood mapping using Sen1Flood11 dataset as a downstream task, GFMs struggles to outperform the baseline U-Net, highlighting model's limitation in capturing critical local nuances. To address this, we present the Prithvi-Complementary Adaptive Fusion Encoder (CAFE), which integrate Prithvi GFM pretrained encoder with a parallel CNN residual branch enhanced by Convolutional Attention Modules (CAM). Prithvi-CAFE enables fast and efficient fine-tuning through adapters in Prithvi and performs multi-scale, multi-level fusion with CNN features, capturing critical local details while preserving long-range dependencies. We achieve state-of-the-art results on two comprehensive flood mapping datasets: Sen1Flood11 and FloodPlanet. On Sen1Flood11 test data, Prithvi-CAFE (IoU 83.41) outperforms the original Prithvi (IoU 82.50) and other major GFMs (TerraMind 82.90, DOFA 81.54, spectralGPT: 81.02). The improvement is even more pronounced on the hold-out test site, where Prithvi-CAFE achieves an IoU of 81.37 compared to the baseline U-Net (70.57) and original Prithvi (72.42). On FloodPlanet, Prithvi-CAFE also surpasses the baseline U-Net and other GFMs, achieving an IoU of 64.70 compared to U-Net (60.14), Terramind (62.33), DOFA (59.15) and Prithvi 2.0 (61.91). Our proposed simple yet effective Prithvi-CAFE demonstrates strong potential for improving segmentation tasks where multi-channel and multi-modal data provide complementary information and local details are critical. The code is released on \\href{https://github.com/Sk-2103/Prithvi-CAFE}{Prithvi-CAFE Github}", "AI": {"tldr": "Prithvi-CAFE combines a pretrained Geo-Foundation Model encoder with a CNN residual branch using attention modules to improve flood mapping by capturing local details while preserving long-range dependencies, achieving state-of-the-art results on Sen1Flood11 and FloodPlanet datasets.", "motivation": "Geo-Foundation Models struggle with flood mapping tasks where local details are critical, as shown by their inability to outperform baseline U-Net on the Sen1Flood11 dataset. The models fail to capture important local nuances needed for accurate flood segmentation.", "method": "Prithvi-CAFE integrates a Prithvi GFM pretrained encoder with a parallel CNN residual branch enhanced by Convolutional Attention Modules (CAM). It uses adapters for efficient fine-tuning and performs multi-scale, multi-level fusion between GFM features and CNN features to capture both local details and long-range dependencies.", "result": "Achieves state-of-the-art results: On Sen1Flood11 test data, IoU 83.41 vs original Prithvi (82.50) and other GFMs; on hold-out test site, IoU 81.37 vs baseline U-Net (70.57) and original Prithvi (72.42). On FloodPlanet, IoU 64.70 vs U-Net (60.14) and other GFMs.", "conclusion": "Prithvi-CAFE demonstrates strong potential for improving segmentation tasks where multi-channel/multi-modal data provide complementary information and local details are critical. The simple yet effective architecture successfully addresses GFMs' limitations in capturing local nuances for flood mapping."}}
{"id": "2601.02318", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.02318", "abs": "https://arxiv.org/abs/2601.02318", "authors": ["Roja Sahoo", "Anoop Namboodiri"], "title": "Fusion2Print: Deep Flash-Non-Flash Fusion for Contactless Fingerprint Matching", "comment": "15 pages, 8 figures, 5 tables. Submitted to ICPR 2026", "summary": "Contactless fingerprint recognition offers a hygienic and convenient alternative to contact-based systems, enabling rapid acquisition without latent prints, pressure artifacts, or hygiene risks. However, contactless images often show degraded ridge clarity due to illumination variation, subcutaneous skin discoloration, and specular reflections. Flash captures preserve ridge detail but introduce noise, whereas non-flash captures reduce noise but lower ridge contrast. We propose Fusion2Print (F2P), the first framework to systematically capture and fuse paired flash-non-flash contactless fingerprints. We construct a custom paired dataset, FNF Database, and perform manual flash-non-flash subtraction to isolate ridge-preserving signals. A lightweight attention-based fusion network also integrates both modalities, emphasizing informative channels and suppressing noise, and then a U-Net enhancement module produces an optimally weighted grayscale image. Finally, a deep embedding model with cross-domain compatibility, generates discriminative and robust representations in a unified embedding space compatible with both contactless and contact-based fingerprints for verification. F2P enhances ridge clarity and achieves superior recognition performance (AUC=0.999, EER=1.12%) over single-capture baselines (Verifinger, DeepPrint).", "AI": {"tldr": "Fusion2Print (F2P) is a novel framework that fuses paired flash and non-flash contactless fingerprint images to enhance ridge clarity and achieve superior recognition performance compatible with both contactless and contact-based systems.", "motivation": "Contactless fingerprint recognition offers hygiene and convenience advantages over contact-based systems, but suffers from degraded ridge clarity due to illumination variations, skin discoloration, and specular reflections. Flash captures preserve detail but introduce noise, while non-flash reduces noise but lowers contrast.", "method": "1) Construct custom FNF Database with paired flash-non-flash contactless fingerprints; 2) Perform manual flash-non-flash subtraction to isolate ridge-preserving signals; 3) Use lightweight attention-based fusion network to integrate modalities; 4) Apply U-Net enhancement module for optimally weighted grayscale image; 5) Employ deep embedding model with cross-domain compatibility for unified embedding space.", "result": "F2P achieves superior recognition performance with AUC=0.999 and EER=1.12%, outperforming single-capture baselines like Verifinger and DeepPrint. The framework enhances ridge clarity and enables compatibility between contactless and contact-based fingerprint verification.", "conclusion": "Fusion2Print successfully addresses the limitations of single-capture contactless fingerprint systems by fusing flash and non-flash modalities, resulting in enhanced ridge clarity and robust cross-domain recognition performance that bridges the gap between contactless and contact-based fingerprint verification."}}
{"id": "2601.02329", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.02329", "abs": "https://arxiv.org/abs/2601.02329", "authors": ["Laurent Caraffa"], "title": "BEDS: Bayesian Emergent Dissipative Structures", "comment": "19 pages", "summary": "We present BEDS (Bayesian Emergent Dissipative Structures), a theoretical framework that unifies concepts from non-equilibrium thermodynamics, Bayesian inference, information geometry, and machine learning. The central thesis proposes that learning, across physical, biological, and computational systems, fundamentally constitutes the conversion of flux into structure through entropy export. Building on Prigogine's theory of dissipative structures, we establish a formal isomorphism between thermodynamic processes and Bayesian updating, demonstrating that sustainable learning systems must follow dissipative patterns where crystallized posteriors become priors for subsequent levels of emergence.\n  We derive fundamental mathematical constants (e, \u03c0, \u03c6) as fixed points of Bayesian inference under minimal axioms, suggesting these constants emerge necessarily from any system capable of representing and updating uncertainty. Furthermore, we propose a conjecture linking G\u00f6del's incompleteness theorems to thermodynamic constraints, hypothesizing that pathologies of formal systems (incompleteness, undecidability) are structurally analogous to dissipation deficits in physical systems.\n  As practical validation, we present a peer-to-peer network architecture implementing BEDS principles, achieving six orders of magnitude improvement in energy efficiency compared to existing distributed consensus systems while enabling continuous learning. This work bridges fundamental physics, mathematical logic, and practical system design, offering both theoretical insights into the nature of learning and computation, and a concrete pathway toward sustainable artificial intelligence.", "AI": {"tldr": "BEDS framework unifies thermodynamics, Bayesian inference, and learning as flux-to-structure conversion via entropy export, deriving mathematical constants as inference fixed points and linking G\u00f6del's theorems to thermodynamic constraints, with practical P2P implementation achieving massive energy efficiency gains.", "motivation": "To create a unified theoretical framework connecting non-equilibrium thermodynamics, Bayesian inference, and learning across physical, biological, and computational systems, bridging fundamental physics with practical AI system design.", "method": "Establishes formal isomorphism between thermodynamic processes and Bayesian updating, derives mathematical constants as fixed points of Bayesian inference under minimal axioms, proposes conjecture linking G\u00f6del's incompleteness to thermodynamic constraints, and implements peer-to-peer network architecture based on BEDS principles.", "result": "Derives fundamental constants (e, \u03c0, \u03c6) as necessary emergent properties of uncertainty-representing systems, proposes structural analogy between formal system pathologies and dissipation deficits, and demonstrates practical P2P implementation achieving six orders of magnitude energy efficiency improvement over existing distributed consensus systems.", "conclusion": "Learning fundamentally involves flux-to-structure conversion through entropy export, mathematical constants emerge necessarily from uncertainty-representing systems, formal system limitations have thermodynamic analogs, and BEDS principles enable sustainable AI with dramatically improved energy efficiency."}}
{"id": "2601.02339", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.02339", "abs": "https://arxiv.org/abs/2601.02339", "authors": ["Jingming He", "Chongyi Li", "Shiqi Wang", "Sam Kwong"], "title": "Joint Semantic and Rendering Enhancements in 3D Gaussian Modeling with Anisotropic Local Encoding", "comment": "Accepted by ICCV 2025", "summary": "Recent works propose extending 3DGS with semantic feature vectors for simultaneous semantic segmentation and image rendering. However, these methods often treat the semantic and rendering branches separately, relying solely on 2D supervision while ignoring the 3D Gaussian geometry. Moreover, current adaptive strategies adapt the Gaussian set depending solely on rendering gradients, which can be insufficient in subtle or textureless regions. In this work, we propose a joint enhancement framework for 3D semantic Gaussian modeling that synergizes both semantic and rendering branches. Firstly, unlike conventional point cloud shape encoding, we introduce an anisotropic 3D Gaussian Chebyshev descriptor using the Laplace-Beltrami operator to capture fine-grained 3D shape details, thereby distinguishing objects with similar appearances and reducing reliance on potentially noisy 2D guidance. In addition, without relying solely on rendering gradient, we adaptively adjust Gaussian allocation and spherical harmonics with local semantic and shape signals, enhancing rendering efficiency through selective resource allocation. Finally, we employ a cross-scene knowledge transfer module to continuously update learned shape patterns, enabling faster convergence and robust representations without relearning shape information from scratch for each new scene. Experiments on multiple datasets demonstrate improvements in segmentation accuracy and rendering quality while maintaining high rendering frame rates.", "AI": {"tldr": "A joint enhancement framework for 3D semantic Gaussian modeling that synergizes semantic and rendering branches using anisotropic 3D Gaussian Chebyshev descriptors and adaptive Gaussian allocation with semantic/shape signals.", "motivation": "Current 3DGS semantic segmentation methods treat semantic and rendering branches separately, rely solely on 2D supervision while ignoring 3D Gaussian geometry, and use rendering gradients that are insufficient for subtle/textureless regions.", "method": "1) Introduces anisotropic 3D Gaussian Chebyshev descriptor using Laplace-Beltrami operator to capture fine-grained 3D shape details. 2) Adaptively adjusts Gaussian allocation and spherical harmonics with local semantic and shape signals. 3) Uses cross-scene knowledge transfer module to continuously update learned shape patterns.", "result": "Experiments on multiple datasets demonstrate improvements in segmentation accuracy and rendering quality while maintaining high rendering frame rates.", "conclusion": "The proposed joint enhancement framework effectively synergizes semantic and rendering branches, reducing reliance on noisy 2D guidance and improving both segmentation and rendering performance through better 3D shape understanding and adaptive resource allocation."}}
{"id": "2601.02353", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.02353", "abs": "https://arxiv.org/abs/2601.02353", "authors": ["Shahnawaz Alam", "Mohammed Mudassir Uddin", "Mohammed Kaif Pasha"], "title": "Meta-Learning Guided Pruning for Few-Shot Plant Pathology on Edge Devices", "comment": null, "summary": "Farmers in remote areas need quick and reliable methods for identifying plant diseases, yet they often lack access to laboratories or high-performance computing resources. Deep learning models can detect diseases from leaf images with high accuracy, but these models are typically too large and computationally expensive to run on low-cost edge devices such as Raspberry Pi. Furthermore, collecting thousands of labeled disease images for training is both expensive and time-consuming. This paper addresses both challenges by combining neural network pruning -- removing unnecessary parts of the model -- with few-shot learning, which enables the model to learn from limited examples. This paper proposes Disease-Aware Channel Importance Scoring (DACIS), a method that identifies which parts of the neural network are most important for distinguishing between different plant diseases, integrated into a three-stage Prune-then-Meta-Learn-then-Prune (PMP) pipeline. Experiments on PlantVillage and PlantDoc datasets demonstrate that the proposed approach reduces model size by 78\\% while maintaining 92.3\\% of the original accuracy, with the compressed model running at 7 frames per second on a Raspberry Pi 4, making real-time field diagnosis practical for smallholder farmers.", "AI": {"tldr": "Combines neural network pruning with few-shot learning to create lightweight plant disease detection models that run efficiently on low-cost edge devices like Raspberry Pi, enabling real-time field diagnosis for farmers.", "motivation": "Farmers in remote areas need accessible plant disease diagnosis tools, but existing deep learning models are too large for edge devices and require extensive labeled data that's expensive to collect.", "method": "Proposes Disease-Aware Channel Importance Scoring (DACIS) integrated into a three-stage Prune-then-Meta-Learn-then-Prune (PMP) pipeline that identifies important neural network parts for disease detection and combines pruning with few-shot learning.", "result": "Reduces model size by 78% while maintaining 92.3% of original accuracy, with compressed model running at 7 FPS on Raspberry Pi 4, making real-time field diagnosis practical.", "conclusion": "The approach successfully addresses both computational efficiency and data scarcity challenges, enabling practical deployment of plant disease detection systems for smallholder farmers using affordable edge devices."}}
{"id": "2601.02356", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.02356", "abs": "https://arxiv.org/abs/2601.02356", "authors": ["Jing Tan", "Zhaoyang Zhang", "Yantao Shen", "Jiarui Cai", "Shuo Yang", "Jiajun Wu", "Wei Xia", "Zhuowen Tu", "Stefano Soatto"], "title": "Talk2Move: Reinforcement Learning for Text-Instructed Object-Level Geometric Transformation in Scenes", "comment": "Project page: https://sparkstj.github.io/talk2move", "summary": "We introduce Talk2Move, a reinforcement learning (RL) based diffusion framework for text-instructed spatial transformation of objects within scenes. Spatially manipulating objects in a scene through natural language poses a challenge for multimodal generation systems. While existing text-based manipulation methods can adjust appearance or style, they struggle to perform object-level geometric transformations-such as translating, rotating, or resizing objects-due to scarce paired supervision and pixel-level optimization limits. Talk2Move employs Group Relative Policy Optimization (GRPO) to explore geometric actions through diverse rollouts generated from input images and lightweight textual variations, removing the need for costly paired data. A spatial reward guided model aligns geometric transformations with linguistic description, while off-policy step evaluation and active step sampling improve learning efficiency by focusing on informative transformation stages. Furthermore, we design object-centric spatial rewards that evaluate displacement, rotation, and scaling behaviors directly, enabling interpretable and coherent transformations. Experiments on curated benchmarks demonstrate that Talk2Move achieves precise, consistent, and semantically faithful object transformations, outperforming existing text-guided editing approaches in both spatial accuracy and scene coherence.", "AI": {"tldr": "Talk2Move is an RL-based diffusion framework for text-instructed spatial transformation of objects in scenes, addressing challenges in object-level geometric manipulation through novel optimization techniques.", "motivation": "Existing text-based manipulation methods struggle with object-level geometric transformations (translating, rotating, resizing) due to scarce paired supervision data and limitations of pixel-level optimization approaches.", "method": "Uses Group Relative Policy Optimization (GRPO) to explore geometric actions through diverse rollouts from input images and lightweight textual variations. Incorporates spatial reward guided model, off-policy step evaluation, active step sampling, and object-centric spatial rewards for displacement, rotation, and scaling.", "result": "Outperforms existing text-guided editing approaches in spatial accuracy and scene coherence on curated benchmarks, achieving precise, consistent, and semantically faithful object transformations.", "conclusion": "Talk2Move provides an effective RL-based solution for text-instructed spatial manipulation of objects without requiring costly paired data, enabling interpretable and coherent geometric transformations."}}
{"id": "2601.02358", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.02358", "abs": "https://arxiv.org/abs/2601.02358", "authors": ["Junyi Chen", "Tong He", "Zhoujie Fu", "Pengfei Wan", "Kun Gai", "Weicai Ye"], "title": "VINO: A Unified Visual Generator with Interleaved OmniModal Context", "comment": "Project page: https://sotamak1r.github.io/VINO-web/", "summary": "We present VINO, a unified visual generator that performs image and video generation and editing within a single framework. Instead of relying on task-specific models or independent modules for each modality, VINO uses a shared diffusion backbone that conditions on text, images and videos, enabling a broad range of visual creation and editing tasks under one model. Specifically, VINO couples a vision-language model (VLM) with a Multimodal Diffusion Transformer (MMDiT), where multimodal inputs are encoded as interleaved conditioning tokens, and then used to guide the diffusion process. This design supports multi-reference grounding, long-form instruction following, and coherent identity preservation across static and dynamic content, while avoiding modality-specific architectural components. To train such a unified system, we introduce a multi-stage training pipeline that progressively expands a video generation base model into a unified, multi-task generator capable of both image and video input and output. Across diverse generation and editing benchmarks, VINO demonstrates strong visual quality, faithful instruction following, improved reference and attribute preservation, and more controllable multi-identity edits. Our results highlight a practical path toward scalable unified visual generation, and the promise of interleaved, in-context computation as a foundation for general-purpose visual creation.", "AI": {"tldr": "VINO is a unified visual generator that handles both image and video generation/editing using a single diffusion model with multimodal conditioning, avoiding task-specific modules.", "motivation": "Current approaches rely on separate models for different visual tasks (images vs videos), leading to fragmented systems. The authors aim to create a unified framework that can handle diverse visual creation and editing tasks within a single model.", "method": "VINO combines a vision-language model with a Multimodal Diffusion Transformer (MMDiT). It encodes multimodal inputs (text, images, videos) as interleaved conditioning tokens to guide diffusion. Uses multi-stage training pipeline to expand a video generation base model into a unified multi-task generator.", "result": "VINO demonstrates strong visual quality, faithful instruction following, improved reference/attribute preservation, and better controllable multi-identity edits across diverse generation and editing benchmarks.", "conclusion": "The work presents a practical path toward scalable unified visual generation and highlights interleaved, in-context computation as a promising foundation for general-purpose visual creation systems."}}
{"id": "2601.02359", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.02359", "abs": "https://arxiv.org/abs/2601.02359", "authors": ["Kaede Shiohara", "Toshihiko Yamasaki", "Vladislav Golyanik"], "title": "ExposeAnyone: Personalized Audio-to-Expression Diffusion Models Are Robust Zero-Shot Face Forgery Detectors", "comment": "17 pages, 8 figures, 11 tables; project page: https://mapooon.github.io/ExposeAnyonePage/", "summary": "Detecting unknown deepfake manipulations remains one of the most challenging problems in face forgery detection. Current state-of-the-art approaches fail to generalize to unseen manipulations, as they primarily rely on supervised training with existing deepfakes or pseudo-fakes, which leads to overfitting to specific forgery patterns. In contrast, self-supervised methods offer greater potential for generalization, but existing work struggles to learn discriminative representations only from self-supervision. In this paper, we propose ExposeAnyone, a fully self-supervised approach based on a diffusion model that generates expression sequences from audio. The key idea is, once the model is personalized to specific subjects using reference sets, it can compute the identity distances between suspected videos and personalized subjects via diffusion reconstruction errors, enabling person-of-interest face forgery detection. Extensive experiments demonstrate that 1) our method outperforms the previous state-of-the-art method by 4.22 percentage points in the average AUC on DF-TIMIT, DFDCP, KoDF, and IDForge datasets, 2) our model is also capable of detecting Sora2-generated videos, where the previous approaches perform poorly, and 3) our method is highly robust to corruptions such as blur and compression, highlighting the applicability in real-world face forgery detection.", "AI": {"tldr": "ExposeAnyone: A self-supervised diffusion-based method for detecting unknown deepfake manipulations by personalizing to specific subjects and using reconstruction errors for identity verification.", "motivation": "Current deepfake detection methods fail to generalize to unseen manipulations due to overfitting on supervised training data, while existing self-supervised approaches struggle to learn discriminative representations.", "method": "A fully self-supervised diffusion model that generates expression sequences from audio, personalizes to specific subjects using reference sets, and computes identity distances via diffusion reconstruction errors for person-of-interest face forgery detection.", "result": "Outperforms previous SOTA by 4.22% average AUC on multiple datasets, successfully detects Sora2-generated videos where previous methods fail, and shows high robustness to corruptions like blur and compression.", "conclusion": "ExposeAnyone demonstrates superior generalization to unseen manipulations through self-supervised learning, making it highly applicable for real-world face forgery detection scenarios."}}
